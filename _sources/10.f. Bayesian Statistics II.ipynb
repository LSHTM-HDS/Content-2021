{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Multiparameter models \n",
    "\n",
    "Suppose now that our likelihood has two unknown parameters, $(\\mu,\\sigma^2).$ In this case, we would need a prior distribution for both parameters, and our posterior distribution will now be bivariate. If desired we can summarise this by the mean and covariance matrix or by HPD contour maps. However, often in applications, interest focusses only on one parameter, say $\\mu;$ the other parameter is usually referred to as a nuisance parameter. In Bayesian inference, we typically use simulation to draw from the posterior distribution of $(\\mu,\\sigma^2).$ For marginal inference for $\\mu,$ we summarise the draws from $\\mu$ in the usual way, across all simulated values of $\\sigma^2$. Analytically, this is equivalent to integrating the posterior over $\\sigma^2$: \n",
    "\n",
    "$$\n",
    "p(\\mu | y) = \\int p(\\mu,\\sigma^2 |y) \\, d\\sigma^2,\n",
    "$$\n",
    "\n",
    "where we have used Bayes' theorem to obtain the posterior, i.e.$p(\\mu,\\sigma^2 |y).$ This integral may be intractable (hence the preference for simulation approaches). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
