{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Confidence intervals of a parameter\n",
    "\n",
    "Providing confidence intervals (CI) of a parameter is a common thing to do in data science. The true meaning perhaps depends on who the audience is;\n",
    "* For a **general audience** a definition would be \"a range of plausible values for a parameter\".\n",
    "* For a **scientific audience**, and within the frequentist frameowrk the definition should be more precise; \"an interval that contained the true value of the parameter with some specified probability\". It is standard practice to specify the interval that contains the true value with a 0.95 (95%) probability, which corresponds to a *confidence level*.\n",
    "\n",
    "How exactly CIs are generated depends on the distribution and how accurate the estimates need to be. There are several commonly used approaches;\n",
    "* CIs using Central Limit Theorem \n",
    "* Approximate CIs from using a quadratic equation \n",
    "* Approximate CIs from bootstrap sampling (covered in section 01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. CIs using Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 CIs using quadratic equations\n",
    "\n",
    "For approximations of parameter CIs for other distributions, we also rely on statisitical theory. In situations where the sample size is large the LLR at different values of a parameter can be approximated using a quadratic function; the maximum value is zero (at the MLE) and the LLR is symmetrical about this point. The LLR is then;\n",
    "\n",
    "\\begin{equation}\n",
    "LLR(\\theta) = -\\frac{1}{2}\\left(\\frac{\\hat\\theta-\\theta}{S}\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "For a parameter $\\theta$ the 95% CIs are obtained using;\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\theta \\pm 1.96 S\n",
    "\\end{equation}\n",
    "\n",
    "We already know that the first derivative give the MLE. The second derivative of log-likelihood gives the *observed information* which is useful for statisitical inference. The proofs for this rule is outside of the scope of this module, but if you are interested in reading further the chapters X in Y are very useful.\n",
    "\n",
    "As this aproach for estimating CIs is only an *approximation* we need to know under what conditions it gives us estimates that are useful. Much of this depends on the sample size of the data and the distribution that the parameter comes from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
