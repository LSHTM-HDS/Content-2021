{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Continuous probability distributions\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Intended learning objectives:</b> \n",
    "    \n",
    "By the end of this session you will be able to:\n",
    "    \n",
    "* explain the concept of a continuous random variable\n",
    "* define several continuous probability distributions, and relationships between parameters, expectations and variance\n",
    "* understand the relationship between normally distributed data and standard scores\n",
    "* evaluate the appropriateness of assuming normality in data and other options\n",
    "* understand properties of joint distributions, such as the multivariate normal distribution\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Continuous random variables\n",
    "\n",
    "We have previously seen several discrete probability distributions (the binomial and the Poisson). We now extend random variables to those that are continuous. A continuous random variable is one that can take a value in continuous space; this may vary from $-\\infty$ to $+\\infty$ (like the normal distribution) or have limits set on the lower (eg. the log-normal) or upper bound (eg. the uniform). Previously we could assign a probability to a specific value, but, paradoxically, the probability of a specific value for a continuous random variable is zero. Using the concept of a *probability density function*, we can evaluate the area under the curve, which coresponds to the probability that a random variable would take a value between these two limits. The area under the entire curve of the distribution is equal to 1. It is then a matter of defining the probability density function.\n",
    "\n",
    "Generally, a random variable $X$ has density $f_X$ where\n",
    "\n",
    "* $f(x) \\geq 0$ for all of $x$\n",
    "\n",
    "* $\\int_{-\\infty}^{\\infty} f(x) \\hspace{0.2cm} dx = 1.00$\n",
    "\n",
    "which states that the sum of all probabilities of $f(x)$ from the minimum to the maximum is equal to 1.00.\n",
    "\n",
    "We can obtain various useful probabilities from this density function. We can calculate the probability that the variable takes a value within a given interval, the probability that it is below or above a given value. For example:\n",
    "\n",
    "$$Pr(X>b) = \\int_{b}^{max} f(X) \\hspace{0.2cm} dx $$\n",
    "\n",
    "Further information about continuous probability distributions are given in the **[Maths refresher](https://statsfizz.github.io/Maths_Refresher/pr4_continuous_variables.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Useful continuous distributions\n",
    "\n",
    "Below are several useful probability distributions for data science in health. Some of the information below is a repeat of the **[Maths refresher](https://statsfizz.github.io/Maths_Refresher/pr6_distributions.html)**, but we include some practical applications of each distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 The normal distribution\n",
    "\n",
    "The normal distribution is defined with the following probability density function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp \\Big[-\\Big(\\frac{1}{2}\\Big)\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\Big]\n",
    "$$\n",
    "\n",
    "with parameters mean $\\mu$ and standard deviation $\\sigma$. If we have a random variable $X$ that is normally distributed we can specify this using $X {\\sim} N(\\mu, \\sigma^2)$. There will be more on the specific of random variables and parameters in the session on sampling. The limits of a normal distribution are $-\\infty$ to $+\\infty$. \n",
    "\n",
    "A *standard normal* has a mean of 0 and a variance of 1. A standard normal random variable is usually represented by $Z {\\sim} N(0,1)$ and is sometimes called the *Z-score*.\n",
    "\n",
    "The expected value is given by $E[X]=\\mu$ and the variance is given by $Var[X] = \\sigma^2$. \n",
    "\n",
    "So much of statistics relies on the normal distribution, so it is an important distribution to be familiar with. \n",
    "\n",
    "### 3.2.2 The log-normal distribution\n",
    "\n",
    "The log-normal distribution is essentialy a transformed version of the normal distribution, and has its own probability density function;\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}exp\\Big(-\\frac{(ln(x)-\\mu)^2}{2\\sigma^2}\\Big)\n",
    "$$\n",
    "\n",
    "If a random variable $X$ is log-normally distributed, $Y=ln(X)$ has a normal distribution, and if $Y$ is a normal distribution then $X=exp(Y)$ has a log-normal distribution. These simple transformations mean that calculations using transformed data is the standard approach (note that $x$ is present in 2 places of the pdf). The parameters $\\mu$ and $\\sigma$ refer to the mean and standard deviation on the *normal scale*. Consequently, the median of a log-normally distributed sample is $exp(\\mu)$. A log-normally distributed random variable is bounded by $[0,+\\infty)$.\n",
    "\n",
    "Many biological datasets are log-normally distributed, for example most measurements (height, weight, speed) will be above 0, and will often be right-skewed. A good approach to take with these sorts of data is to log the data, and work on the *log scale*. Any inference should be converted back to the *natural scale*. Sometimes measurements are sufficiently greater than 0 that they become more centered. In this case, it may not be necessary to assume that are log-normal, and assuming normality may be acceptible.\n",
    "\n",
    "### 3.2.3 The $\\chi^2$ distribution\n",
    "\n",
    "The $\\chi^2$ distribution is here because we will use the properties of this distribution later in hypothesis testing. Its origins come from a random sample of the *standard normal*, where the $\\chi^2$ distribution is the distribution of the sum of squared standard normals. The degrees of freedom come from the number of standard normal random variables being summed. It is not necessary to know the parameters or estimates of the $\\chi^2$ parameters. A variable which follows the chi-squared distribution can only take positive values (i.e. greater than zero).\n",
    "\n",
    "### 3.2.4 The t-distribution\n",
    "\n",
    "Studentâ€™s t-distribution arises as the ratio of the sample mean to its standard error. The t-distribution has a complex density function which we shall not state here.\n",
    "\n",
    "For now we note that the t-distribution has an additional parameter of sorts, known as the degrees of freedom (d.f.). The density function is similar to that of the standard normal, but the t-distribution has heavier tails. If $X$ follows a t-distribution with $\\nu$ degrees of freedom, we write\n",
    "\n",
    "$$X \\sim t_\\nu$$\n",
    "\n",
    "The expectation and variance of a variable $X$ which follows a t-distribution with $\\nu$ degrees of freedom are given by:\n",
    "\n",
    "* $E[X] = 0$\n",
    "* $Var[X] = \\frac{\\nu}{\\nu-2}$ if $\\nu>2$; $\\infty$ for $1<\\nu<2$; undefined otherwise\n",
    "\n",
    "As the number of degrees of freedom increases the t-distribution gets closer and closer to the standard normal distribution. \n",
    "\n",
    "### 3.2.5 The F distribution\n",
    "\n",
    "The F distribution doesn't have a simple mathematical formula, but is used extensively to compare equality of variances of two normal populations (*think anova*), and is used in linear regression. \n",
    "\n",
    "For two normal populations with variances $\\sigma_1^2$ and $\\sigma_2^2$, the two random samples of size $n_1$ and $n_2$ with corresponding sample variance(s) $s_1^2$ and $s_2^2$ has the variable\n",
    "\n",
    "$$F = \\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}$$\n",
    "\n",
    "with $n_1-1$ and $n_2-1$ degrees of freedom.\n",
    "\n",
    "### 3.2.6 The exponential distribution\n",
    "\n",
    "The exponential distribution is defined with the probability density function:\n",
    "\n",
    "$$f(x)=\\lambda e^{-\\lambda x}$$\n",
    "\n",
    "with parameter $\\lambda$, which is usually described as the rate. The limits of the distribution are $[0,\\infty)$, which means values of $x$ are always greater than 0 (and not including it). \n",
    "\n",
    "The expected value is given by $E[X]=\\frac{1}{\\lambda}$ and variance $Var[X]=\\frac{1}{\\lambda^2}$.\n",
    "\n",
    "The exponential distribution is really useful in statistics because its distribution nicely describes *the time to which something occurs*, if the event happens at a roughly constant rate in time. Health related examples include injuries, births and deaths (although in reality not all occur at a constant rate). The exponential distribution is important in methods such as *survival analysis*.   \n",
    "\n",
    "### 3.2.7 The uniform distribution\n",
    "\n",
    "The uniform distribution is in some ways the simplest to conceptualise. A random variable that is uniformly distributed can have any value between the parameters $a$ (min) and $b$ (max) with equal probability;\n",
    "\n",
    "$$f(x)= \\frac{1}{b-a}$$\n",
    "\n",
    "Outside of these limits, the probability density is 0. The expected value is $E[X] = \\frac{(a+b)}{2}$ and variance $Var[X] = \\frac{(b-a)^2}{12}$.\n",
    "\n",
    "The uniform distribution is very commonly used when randomly allocating outcomes. An example in statistical modelling includes stochastic infectious disease modelling; here several different events (transmission, death) may have a corresponding probability and one event needs to be selected from the two options. A uniform distribution (where the maximum is the total probability of all events) is used to select "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The standard Normal distribution and its use \n",
    "\n",
    "*What is the probability of having a 'healthy' weight?*\n",
    "\n",
    "A healthy weight is often is often measured using the Body Mass Index (BMI - although see [here](https://www.health.harvard.edu/blog/how-useful-is-the-body-mass-index-bmi-201603309339) and [here](https://www.bbc.co.uk/news/health-43895508) for a discussion on why this may be too simplistic a measure). Using an individuals height and weight a BMI (with the formula $BMI = \\frac{mass(kg)}{height(m)^2}$) is returned  and used to classify people as:\n",
    "\n",
    "| Classification | BMI | \n",
    "|:-|:-|\n",
    "| Underweight |<18.5 |\n",
    "| Normal | 18.5-24.9 |\n",
    "| Overweight | 25-29.9 |\n",
    "| Obese | 30-39.9 |\n",
    "| Extremely obese | >40 |\n",
    "\n",
    "In a study that looked at whether cleaners being told they had an active lifestyle influences their BMI (n=76), the data are provided to investigate the above question. Analysing the data means that we have estimates of $\\mu$ as 26.5 and $\\sigma$ as 18.1. So what is the probability a randomly selected person in this sample has a normal BMI? \n",
    "\n",
    "To do this, an option is to make use of pre-calculated probabilities of the standard normal, to do this we tranform the data to take this distribution. From theory we can write;\n",
    "\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "which means that a random variable $X$ with mean $\\mu$ and standard deviation $\\sigma$ has an associated standard Z score. Given values for $\\mu$ and $\\sigma$ we can go from the *X scale* to the *Z scale* and *vice versa*. The important point about describing a distribution on the Z scale is that this opens the ability to calculate specific probabilities. You may have previously used standard normal tables, and a data scientist might use these, or use software to calculate these (which ultimately are also using tables). So back to answering the question...\n",
    "\n",
    "From the table above we can see that a normal weight is classified as a BMI between 18.5 and 24.9, and we want to know what the probability is that a randomly selected person falls between these limits. We write this as;\n",
    "\n",
    "$P(24.9>X>18.5|\\mu,\\sigma) = P(X<24.9|\\mu,\\sigma) - P(X<18.5|\\mu,\\sigma)$\n",
    "\n",
    "If we were to use tables we would find that $z_{min}$ = -1.87 (2 s.f.) and $z_{max}$ = -0.37 (2 s.f.). From *z-tables* (found [online](http://www.z-table.com/) or at the back of most stats books) we look up what the corresponding probability is for each z-score.\n",
    "\n",
    "$P(z_{max}<-0.37) - P(z_{min}<-1.87) = 0.3557 - 0.0307 = 0.3250$\n",
    "\n",
    "Using R to do the same calculation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "\"cannot open file 'Practicals/Datasets/BMI/MindsetMatters.csv': No such file or directory\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n",
      "1. read.csv(\"Practicals/Datasets/BMI/MindsetMatters.csv\")",
      "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)",
      "3. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "# BMI dataset\n",
    "\n",
    "dat <- read.csv(\"Practicals/Datasets/BMI/MindsetMatters.csv\")\n",
    "head(dat)\n",
    "#remove observatiosn with no BMI data\n",
    "dat <- dat[!is.na(dat$BMI),]\n",
    "#estimate mu and sigma\n",
    "mu <- mean(dat$BMI)\n",
    "print(paste0(\"value of mu is \",round(mu,2)))\n",
    "sig <- sd(dat$BMI)\n",
    "print(paste0(\"value of sigma is \",round(sig,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) if we were to use Z tables within R (to illustrate the point)\n",
    "\n",
    "z_min <- (18.5-mu)/sig\n",
    "z_max <- (24.9-mu)/sig\n",
    "\n",
    "# note when using pnorm we don't need to specify mu and sigma as the \n",
    "# function assumes mu=0 and sigma=1 unless specified.\n",
    "print(paste0(\"z_max is \",round(z_max,2),\" and z_min is \",round(z_min,2)))\n",
    "print(paste0(\"Probability of having a healthy BMI is (z-score) \",round(pnorm(z_max)-pnorm(z_min),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) if we were to directly estimate\n",
    " \n",
    "print(paste0(\"Probability of having a healthy BMI is (direct) \",round(pnorm(24.9,mu,sig)-pnorm(18.5,mu,sig),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) provide a sanity check against the data\n",
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "library(ggplot2)\n",
    "\n",
    "ggplot(dat,aes(x=BMI)) + geom_histogram(bins = 30,fill=\"steelblue\",col=\"grey80\") +\n",
    "    geom_vline(xintercept = c(18.5,24.9))\n",
    "#hist(dat$BMI,col=\"steelblue\")\n",
    "#abline(v=c(18.5,24.9),lty=2)\n",
    "print(paste0(\"Within the data a healthy BMI is seen \",\n",
    "             round(100*((sum(dat$BMI<24.9)-sum(dat$BMI<18.5))/length(dat$BMI)),1),\"%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the sample estimate (35.1%) is roughly similar to the population estimate of about 33%. Calculating directly gives the same result as using a z-score in R, and this returns the same information as using z-tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Data and their relationship with statistical distributions\n",
    "\n",
    "We often have data on a particular characteristic and want to make general statements about it, such as: the probability of it being greater than or less than something, provide a range in which \"most\" observations will lie, what is the central value (e.g. mean/median), etc. However, \n",
    "* we rarely **know** the true distribution that a variable follows\n",
    "* often, a distribution will not quite fit the data but will form a sufficiently good approximation to address the questions above with sufficient accuracy. \n",
    "\n",
    "So we often want to find a distribution which fits our data well enough. How do we make a decision? Some of this comes with experience, but there are some useful steps to go through when confronted with data (this is covered in more detail in the lecture),\n",
    "* plot your data. What does the data look like? Consider the lower and upper bounds, the most common number, and evidence of symmetry\n",
    "* summarise your data. Report the minimum, maximum, mean and mode. This should aid with thinking about the criteria of specific distributions\n",
    "* depending upon the application and what the data looks like, you may want to consider using the empirical distribution function rather than assumption a specific form. However, this gives you fewer options for inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Are the data normally distributed?\n",
    "\n",
    "Many analyses and tests of data rely on the data being normally distributed. A simple example would be using a t-test to check whether the mean of 2 groups are different, more complex examples would include linear regression analysis. If the outcome being analysed is a qualitive outcome, or successes and failures, it should be obvious that the data aren't normally distributed. But what if the data are continuous or count values, and they look like they are centered, but have some skewness? Is it safe to proceed as if they are normal?\n",
    "\n",
    "First steps, as always, is to plot the data to see what they look like. A histogram, as above, or density plot is a good step forward. Additionally, a *quantile-quantile* plot calculates the correlation between a sample and the equivalent normal distribution with the same mean $\\mu$ and standard deviation $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "ggplot(dat,aes(sample=BMI)) + stat_qq() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure you can see that the theoretical quantiles follow the diagonals reasonably well, and especially at the extremes do not move away much from the diagnoal. A plot like this would be enough to show that the data approximately follow normally distribution. Looking at plots such as this to assess normality is a *judgement* which you will build up during this module.\n",
    "\n",
    "To formally test for normality we can use the Shapiro-Wilk test, described briefly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro.test(dat$BMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we haven't yet covered hypothesis testing, this tests returns a p-value of 0.067. The approach begins with the (null) assumption that the data are normally distributed, as the p-value > 0.05 we can accept the null hypothesis. In other words, there is no strong evidence against normality of the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approaches to non-normally distributed data\n",
    "\n",
    "A really useful approach to dealing with non-normally distributed data is transformations. The most often used approach is to apply a log-transformation, either on the *natural* ($Y = log_e(X)$) or *log10* ($Y = log_{10}(X)$) scale, and the transformed data may behave more like normally distributed data. An example is given below for weights of babies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "library(ggplot2)\n",
    "# mother-baby dataset\n",
    "\n",
    "dat <- read.csv(\"Practicals/Datasets/MotherBaby/baby.csv\")\n",
    "head(dat)\n",
    "\n",
    "# plot the data on maternal age\n",
    "ggplot(dat,aes(x=Maternal.Age)) + geom_histogram(bins=30,fill=\"steelblue\",col=\"grey80\")\n",
    "# plot a quantile plot of this log-normally distributed data\n",
    "ggplot(dat,aes(sample=Maternal.Age)) + stat_qq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see clearly that maternal age is centered but right skewed. This is a classic log-normal distribution. The quantile plot is not straight along the diagonal but forms an *s-shape*. This confirms that the data does not conform to normally distribued data.\n",
    "\n",
    "So the next steps would be to log the data on the natural scale. Ideally, we want to retain a meaningful x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "# plot the data on maternal age\n",
    "ggplot(dat,aes(x=Maternal.Age)) + \n",
    "    geom_histogram(bins=30,fill=\"steelblue\",col=\"grey80\") +\n",
    "    scale_x_continuous(trans = \"log\")\n",
    "\n",
    "# but note that any analysis should be carried out on the transformed variable\n",
    "y <- data.frame(age_log=log(dat$Maternal.Age))\n",
    "# and here we should check whether this is normally distributed using a qqplot\n",
    "ggplot(y,aes(sample=age_log)) + stat_qq()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-transformed data now looks more symmetrical in the histogram. And the quantile plot is much less *s-shaped*. While it's not perfectly straight, it's probably *good enough* for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Joint distributions and correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the BMI dataset, a high BMI is indicative of being overweight and this is likely to mean that an individual may have a high percentage of body fat. Typically, those individuals with high BMI may also be at risk of health conditions such as heart disease, which may be indicated by high blood pressure. These *dependencies* will be covered in the regression modelling sessions.\n",
    "\n",
    "As data scientists we want to explore the evidence for these statements in detail. Here we will use this to explore concepts such as **joint distributions** and **correlation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have two random variables $X$ and $Y$, the cumulative distribution function (CDF) is, \n",
    "\n",
    "$$F(x,y) = P(X \\leq x,Y \\leq y)$$\n",
    "\n",
    "regardless of whether $X$ and $Y$ are continuous or discrete. For continuous random variables the joint density function will be $f(x,y)$ and will be non-negative and \n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y)\\: dy\\: dx = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might sometimes want to think about the marginal density of, say, $X$. This means we want to know the probability of $X$ irrespective of $Y$, and consequently we will need to integrate over all possible values of $Y$. This is different to assuming that $X$ is independent of $Y$. The marginal cdf of $X$, or $F_X$ is \n",
    "\n",
    "$$F_X (x) = P(X \\leq x)$$\n",
    "$$ = \\lim_{y \\rightarrow \\infty} F(x,y)$$\n",
    "$$ = \\int_{-\\infty}^{x} \\int_{-\\infty}^{\\infty} f(u,y)\\: dy\\: du$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, it follows that the density function of $X$ alone, known as the **marginal density** of $X$, is\n",
    "\n",
    "$$f_x (x) = F_{X}'(x) = \\int_{-\\infty}^{\\infty} f(x,y)\\: dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does this mean in practical terms? Returning to the BMI data we can report that the average BMI ($\\mu_X$) is 26.46 and the average body fat percentage ($\\mu_Y$) is 35.31. If BMI and body fat were independent variables knowing BMI would tell us nothing about body fat and *vice versa*. But plotting the data (and some common sense) tells us that this is not the case; if we know one we can say quite a lot about the other. We could explore the correlation between the data (more about this later), but we can also describe these variables together using a joint distribution. By defining them using a joint distribution we are saying nothing about *cause and effect*, just that they are dependent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "# BMI dataset\n",
    "\n",
    "dat <- read.csv(\"Practicals/Datasets/BMI/MindsetMatters.csv\")\n",
    "head(dat)\n",
    "#remove observations with no BMI data\n",
    "dat <- dat[!is.na(dat$BMI),]\n",
    "# scatter plot of BMI and body fat\n",
    "ggplot(dat,aes(x=BMI,y=Fat)) + geom_point()\n",
    "\n",
    "# report the mean of each variable\n",
    "# note that some values of Y are missing...we need to add na.rm otherwise the estimate will be NA\n",
    "mux <- mean(dat$BMI)\n",
    "print(paste0(\"value of mu_x is \",round(mux,2)))\n",
    "muy <- mean(dat$Fat,na.rm=T)\n",
    "print(paste0(\"value of mu_y is \",round(muy,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this joint distribution has a joint cdf, $F(x,y)$ and a continuous piecewise density function $f(x,y)$. The joint mean is defined as $\\mu_x,\\mu_y$ What about the variance? Here we need to consider the variance and covaraince between $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between variables\n",
    "dat2 <- dat[!is.na(dat$Fat),]\n",
    "round(cov(x=cbind(dat2$BMI,dat2$Fat)),3)\n",
    "paste0(\"variance of BMI = \",round(var(dat2$BMI),3))\n",
    "paste0(\"variance of fat = \",round(var(dat2$Fat),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *covariance matrix* is returned. The diagnoals return the variance of each parameter, and the off-diagnoals the covariance, indicating a positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between covariance and correlation is given by;\n",
    "\n",
    "$$\\rho(X,Y) = Corr(X,Y) = \\frac{Cov(X,Y)}{SD(X)SD(Y)}$$ \n",
    "\n",
    "which corresponds to the Pearson's correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this helps us define BMI from body fat and *vice versa*. Examples of when this might be useful include;\n",
    "* Inputing missing data\n",
    "* Summarising many variables with one metric (more about this in the Machine learning module)\n",
    "* Efficient sampling of distributions, which is used in Monte Carlo Markov Chain (MCMC) estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
