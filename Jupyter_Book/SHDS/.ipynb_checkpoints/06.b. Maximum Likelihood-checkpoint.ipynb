{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Likelihood and log-likelihood with $n$ independent observations\n",
    "\n",
    "Suppose that you collect a sample of $n$ observations. If the $n$ observations are independent, then the joint likelihood function from these $n$ observations has a very convenient form; it is the product of the likelihood from each observation. If $X_1,..., X_n$ are i.i.d., and we observe a sample $\\mathbf{x} = \\left\\{ x_1, x_2, ..., x_n \\right\\}$: \n",
    "\n",
    "\\begin{align*}\n",
    "L \\left( \\theta \\mid \\mathbf{x} \\right) &=  L\\left( \\theta \\mid x_1 \\right) L\\left( \\theta \\mid x_2 \\right) ...  L \\left( \\theta \\mid x_n \\right) \\\\\n",
    " &= \\prod_{i=1}^n  L\\left( \\theta \\mid x_i \\right)\n",
    "\\end{align*}.\n",
    "\n",
    "Recall that we often prefer to work with the log-likelihood function, as it simplifies the algebra when it comes to finding the MLE. The log-likelihood function for $n$ independent observations is:\n",
    "\\begin{align*}\n",
    "l \\left( \\theta \\mid \\mathbf{x} \\right) \n",
    " &= log \\prod_{i=1}^n L\\left( \\theta \\mid x_i \\right) \\\\\n",
    "  &= \\sum_{i=1}^n log L\\left( \\theta \\mid x_i \\right) \\\\\n",
    "   &= \\sum_{i=1}^n l\\left( \\theta \\mid x_i \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Finding the MLE involves the same three steps as we saw in Session 5, but the log-likelihood function is now a joint function for the $n$ observations:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<b> Method for finding MLEs:</b>   \n",
    "1.  Obtain the derivative of the log-likelihood: $\\frac{d l(\\theta \\mid \\mathbf{x})}{d \\theta}$  \n",
    "2.  Set $\\frac{d l(\\theta \\mid \\mathbf{x})}{d \\theta}=0$ and solve for $\\theta$   \n",
    "3.  Verify that it is a maximum by showing that the second derivative $\\frac{d ^2 l(\\theta \\mid  \\mathbf{x})}{d \\theta ^2 }$ is negative when the MLE is substituted for $\\theta$.    \n",
    "    \n",
    "</div>\n",
    "\n",
    "## 6.1.1 Example: Exponential distribution \n",
    "Recall the example from Session 5 on the time that patients wait until their GP appointment in a particular practice. The receptionist records the time that elapses between when a patient walks through the door, and when they are called through for their appointment for a random sample of eight people. These times (in minutes) are: 8.75, 10.20, 15.29, 7.89, 7.04, 12.04, 19.04, 17.50.      \n",
    "\n",
    "As a reminder, we can model the waiting time until a specific event using the exponential distribution with parameter $\\lambda$, which has a probability density function given by:  \n",
    "\n",
    "\\begin{equation}  \n",
    "f _X\\left(x \\mid \\lambda \\right)=\\lambda e^{-x\\lambda} , x > 0, \\lambda > 0  \n",
    "\\end{equation}\n",
    "\n",
    "*Remember that the mean of this distribution is equal to one over the rate parameter $\\lambda$, i.e. $E(X) = \\frac{1}{\\lambda}$.*\n",
    "\n",
    "We have that the log-likelihood is: \n",
    "\n",
    "\\begin{align}\n",
    "\\log L\\left( \\lambda \\mid \\mathbf{x} \\right) &= \\sum_{i=1}^n \\log  L(\\lambda \\mid x_i) \\\\ \n",
    "&= \\sum_{i=1}^n \\log \\left( \\lambda e^{-x_i \\lambda } \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\log \\lambda -x_i\\lambda \\\\  \n",
    "&= n \\log \\lambda -\\lambda \\sum_{i=1}^n x_i \n",
    "\\end{align}\n",
    "\n",
    "We can make a plot of this log-likelihood, using the data from our example with eight observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "97.75"
      ],
      "text/latex": [
       "97.75"
      ],
      "text/markdown": [
       "97.75"
      ],
      "text/plain": [
       "[1] 97.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nrkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "#six independent observations for waiting times \n",
    "obs <- c(8.75, 10.20, 15.29, 7.89, 7.04, 12.04, 19.04, 17.50)\n",
    "n <- length(obs)\n",
    "\n",
    "#possible values for the parameter lambda\n",
    "lambda <- seq(0, 2, 0.01)\n",
    "\n",
    "#plot the log-likelihood\n",
    "plot(lambda, n*log(lambda) - lambda*sum(obs), type=\"l\", xlab= expression(lambda), ylim=c(-100,0),ylab=\"Log-likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, we observe that the maximum is between 0 and 0.25. We will use the three steps, as before, to derive the MLE algebraically:\n",
    "\n",
    "**Step1**: Taking the derivative of the log-likelihood with respect to $\\lambda$:\n",
    "\\begin{equation}\n",
    "\\frac{d log L\\left( \\lambda \\mid x_1 ,..., x_n \\right) }{d \\lambda} = \\frac{n}{\\lambda}- \\sum_{i=1}^n x_i \n",
    "\\end{equation}\n",
    "\n",
    "**Step2:** Set the derivative equal to zero and solve for $\\lambda$:\n",
    "\\begin{equation}\n",
    "0 = \\frac{n}{\\lambda}- \\sum_{i=1}^n x_i \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\lambda}= \\frac{n }{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}}\n",
    "\\end{equation}\n",
    "\n",
    "The MLE is $\\hat{\\lambda}= \\frac{1}{\\bar{x}}$. And to check that this provides a maximum, we go on to the next step:  \n",
    "\n",
    "**Step3:** Find the second derivative: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d l^2 \\left( \\lambda \\mid \\boldsymbol{x} \\right)}{d \\lambda ^2} \n",
    "= - \\frac{n}{\\lambda^2}\n",
    "\\end{equation}\n",
    "When ${\\lambda}=\\frac{1}{\\bar{x}}$, we have: \n",
    "\\begin{align}\n",
    " \\frac{d l^2 \\left( \\lambda \\mid \\boldsymbol{x} \\right)}{d \\lambda ^2}  \n",
    " &=-n \\bar{x}^2\n",
    " \\end{align}\n",
    "which is negative. This verifies that we found the maximum likelihood estimate. \n",
    "\n",
    "Going back to our example of eight patients waiting for their GP appointment, the maximum likelihood estimate $\\lambda$ is given by one over the average of the eight waiting times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "12.21875"
      ],
      "text/latex": [
       "12.21875"
      ],
      "text/markdown": [
       "12.21875"
      ],
      "text/plain": [
       "[1] 12.21875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0818414322250639"
      ],
      "text/latex": [
       "0.0818414322250639"
      ],
      "text/markdown": [
       "0.0818414322250639"
      ],
      "text/plain": [
       "[1] 0.08184143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1/mean(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that $\\hat{\\lambda}=0.0818$ minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Example: Normal distribution \n",
    "\n",
    "We will now consider the normal distribution. Remember that the normal distribution has two parameters, $\\mu$ and $\\sigma^2$. We will first obtain the MLE for $\\mu$ (treating $\\sigma^2$ as a constant), and in the practical, we will obtain the MLE for $\\sigma^2$ (treating $\\mu$ as a constant). Recall that normal distribution has probability density function given by: \n",
    "\n",
    "\\begin{equation}  \n",
    "f_X \\left( x \\mid \\mu, \\sigma^2 \\right)= (2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "We have that the log-likelihood given an i.i.d. sample of size $n$ is: \n",
    "\n",
    "\\begin{align}\n",
    "l \\left(\\mu, \\sigma^2 \\mid  \\boldsymbol{x}  \\right) &=  \\sum_{i=1}^n \\log \\left\\{ (2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right\\} \\right\\} \\\\\n",
    "&= \\sum_{i=1}^n \\left\\{ \\log (2 \\pi \\sigma^2)^{-\\frac{1}{2}}+ \\log \\exp  \\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right\\}  \\right\\} \\\\\n",
    "&= \\sum_{i=1}^n \\left\\{ -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{(x_i-\\mu)^2}{2\\sigma^2}  \\right\\} \\\\\n",
    "&=  {-\\frac{n}{2}}\\log (2 \\pi \\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2 \n",
    "\\end{align}\n",
    "\n",
    "We will first find the MLE for the parameter $\\mu$. \n",
    "\n",
    "**Step1**: Take the derivative of the log-likelihood with respect to  $\\mu$. Note that this requires use of the chain rule:\n",
    "\n",
    "\\begin{align}  \n",
    "\\frac{d l \\left(\\mu, \\sigma^2 \\mid  \\mathbf{x}  \\right) }{d \\mu}\n",
    "&=  -\\frac{2}{2\\sigma^2}(-1) \\sum_{i=1}^n (x_i-\\mu) \\\\\n",
    "&=  \\frac{ \\sum_{i=1}^n (x_i-\\mu)}{\\sigma^2} \\\\\n",
    "&=  \\frac{ \\sum_{i=1}^n x_i-n\\mu}{\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "**Step2:** Setting the derivative equal to zero and solving for $\\mu$:\n",
    "\n",
    "\\begin{align}  \n",
    "0 &=  \\frac{ \\sum_{i=1}^n x_i-n\\mu}{\\sigma^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since $\\sigma^2 > 0$, we have that: \n",
    "\n",
    "\\begin{equation}  \n",
    "0 = \\sum_{i=1}^n x_i-n\\mu \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}  \n",
    "\\hat{\\mu} =\\frac{ \\sum_{i=1}^n x_i}{n} = \\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "We have that the MLE for $\\mu$ is the sample mean, $\\bar{x}$. \n",
    "\n",
    "**Step3:** Find the second derivative: \n",
    "\n",
    "\n",
    "\\begin{align}  \n",
    "\\frac{d^2 l \\left(\\mu, \\sigma^2 \\mid  \\mathbf{x}  \\right) }{d \\mu^2 }\n",
    "&=  -\\frac{n}{\\sigma^2},\n",
    "\\end{align}\n",
    "\n",
    "since both $n>0$ and $\\sigma^2 >0$, we have that the second derivative is negative, verifying that we have found the maximum.    \n",
    "\n",
    "In the practical, we will find the MLE for $\\sigma^2$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
