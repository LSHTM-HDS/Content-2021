{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Estimation of the population parameters \n",
    "\n",
    "In the specification of the simple linear regression model there are three population parameters ($\\beta_0$, $\\beta_1$, and $\\sigma$). Since we do not know these parameters, we need to estimate them based on a sample from our population. We will use the symbols $\\hat{\\beta}_0$ $\\hat{\\beta}_1$, and $\\hat{\\sigma}$ to represent the sample estimates of the true population parameters. \n",
    "\n",
    "There are many different methods available for obtaining estimates of the parameters $\\beta_0$ and $\\beta_1$. In this section, we focus on an approach that works by minimising the amount of error in the model. These estimates are called the **ordinary least squares estimates** (the reason for this name will become clear in the next section). \n",
    "\n",
    "### 12.4.1 Ordinary least squares estimates\n",
    "\n",
    "The ordinary least square (OLS) estimates are those which minimise the sum of squared deviations from the fitted regression line. Since the residuals, $\\epsilon$, measure deviations from the fitted regression line, the sum is denoted by $SS_{RES}$ (\"$SS$\" stands for Sum of Squares and \"$RES$\" is shorthand for RESiduals). Formally, the OLS estimators are the values of $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that minimise: \n",
    "\n",
    "$$\n",
    "SS_{RES} = \\sum_{i=1}^n \\hat{\\epsilon_i}^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta_0} -\\hat{\\beta_1}x_i)^2.\n",
    "$$\n",
    "\n",
    "The ordinary least squares estimates of $\\beta_0$ and $\\beta_1$ are given by the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x} \\\\\n",
    "\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\bar{y}=\\frac{\\sum_{i=1}^n y_i}{n}$ and $\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}$. A proof of this result is given at the end of this session.\n",
    "\n",
    "\n",
    "### 12.4.2 Estimation of the residual variance\n",
    "\n",
    "The residual variance, $\\sigma^2$, is equal to the mean squared size of the residuals. So, an intuitively appealing estimator of $\\sigma^2$ is given by dividing the residual sum of squares by the number of observations: \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma^2} = \\sum_{i=1}^n \\frac{\\epsilon_i^2}{n} = \\sum_{i=1}^n (y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)^2/n\n",
    "$$\n",
    "\n",
    "However, this is a biased estimator. The bias arises because the observed values tend, on average, to lie closer to the fitted line (defined by $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$) than they do to the true regression line (defined by $\\beta_0$ and $\\beta_1$). This is an exact parallel to the way the variablility of a sample around its mean underestimates the variability around the population mean. \n",
    "\n",
    "It can be shown that an unbiased estimator of the residual variance in the simple linear regression model is given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma^2}  = \\sum_{i=1}^n \\frac{\\hat{\\epsilon_i}^2}{n-1}=\\sum_{i=1}^n (y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)^2/(n-2)\n",
    "$$\n",
    "\n",
    "This quantity is referred to as the residual mean square. It is often denoted by $MS_{RES}$, where \"$MS$\" stands for Mean Square and \"$RES$\" is shorthand for residual. The denominator is $(n-2)$ because fitting the model first requires the estimation of two parameters ($\\beta_0$ and $\\beta_1$) and the estimation of these parameters is said to reduce the information about the variance by two degrees of freedom.\n",
    "\n",
    "\n",
    "### 12.4.3 Maximum likelihood estimation \n",
    "\n",
    "An alternative approach to estimating the model parameters is maximum likelihood estimation. This approach selects the estimates which maximise the likelihood (or equivalently, the log-likelihood) of the parameter values. It can be shown that the ordinary least square estimates for $\\beta_0$ and $\\beta_1$ are also the maximum likelihood estimates (a proof of this result is at the end of the lesson).  \n",
    "\n",
    "The maximum likelihood estimate of $\\sigma^2$ is equal to the biased estimate given above, obtained by dividing the residual sum of squares by the number of observations. \n",
    "\n",
    "### 12.4.4 Properties of $\\hat{\\beta}_1$\n",
    "\n",
    " 1. The parameter estimator for $\\beta_1$ can also be written as the ratio of the covariance between the independent variable and the outcome to the variance of the independent variable:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_1} = \\frac{cov(X,Y)}{SD(X)^2}\n",
    "$$\n",
    "\n",
    "2. If we use the notation $\\hat{\\beta}_{y|x}$ to denote the estimate of the slope from a simple linear regression model with predictor variable $X$ and outcome variable $Y$, it follows from the above result that: \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{y|x}=\\frac{cov(X,Y)}{SD(X)^2}\n",
    "$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{x|y}=\\frac{cov(X,Y)}{SD(Y)^2}.\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{y|x}\\hat{\\beta}_{x|y} = r^2_{x,y}\n",
    "$$\n",
    "\n",
    "Where $r_{x,y}$ is the correlation coefficient between $X$ and $Y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
