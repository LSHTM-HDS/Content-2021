{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Bayesian Statistics II: Normal data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous session, we looked at Bayesian inference for proportions. We now consider continuous data and explore Bayesian inference for data when they are assumed to follow a Normal distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Intended learning outcomes</b> \n",
    "    \n",
    "By the end of this session you will be able to:\n",
    "\n",
    "* Find the posterior for a Normally distributed mean when the variance of the data is known\n",
    "* Find credible and HPD intervals for a Normally distributed mean \n",
    "* Find the Bayesian predictive distributions for Normal data and data summaries.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Example: CD4 cell counts\n",
    "\n",
    "In this session, we will use a dataset on CD4 cell counts which is available in R through the *boot* package. CD4 cells are in our blood as part of our immune system. Since these cells die in people who have HIV, CD4 cell counts are used in HIV patients to determine the health of their immune system and susceptibility to opportunistic infections. In this dataset, there are 20 patients with HIV. Their CD4 cell counts are recorded before and after they were put on treatment. We wish to investigate whether this treatment increased their CD4 cell counts.    \n",
    "We install the *boot* package where the data is stored and we look at the data. Note that the unit of CD4 cell count is 100 $cells/mm^3$. We are interested in the difference in CD4 cell counts before and after treatment. We look at the summary statistics of the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>baseline</th><th scope=col>oneyear</th><th scope=col>y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>2.12 </td><td>2.47 </td><td> 0.35</td></tr>\n",
       "\t<tr><td>4.35 </td><td>4.61 </td><td> 0.26</td></tr>\n",
       "\t<tr><td>3.39 </td><td>5.26 </td><td> 1.87</td></tr>\n",
       "\t<tr><td>2.51 </td><td>3.02 </td><td> 0.51</td></tr>\n",
       "\t<tr><td>4.04 </td><td>6.36 </td><td> 2.32</td></tr>\n",
       "\t<tr><td>5.10 </td><td>5.93 </td><td> 0.83</td></tr>\n",
       "\t<tr><td>3.77 </td><td>3.93 </td><td> 0.16</td></tr>\n",
       "\t<tr><td>3.35 </td><td>4.09 </td><td> 0.74</td></tr>\n",
       "\t<tr><td>4.10 </td><td>4.88 </td><td> 0.78</td></tr>\n",
       "\t<tr><td>3.35 </td><td>3.81 </td><td> 0.46</td></tr>\n",
       "\t<tr><td>4.15 </td><td>4.74 </td><td> 0.59</td></tr>\n",
       "\t<tr><td>3.56 </td><td>3.29 </td><td>-0.27</td></tr>\n",
       "\t<tr><td>3.39 </td><td>5.55 </td><td> 2.16</td></tr>\n",
       "\t<tr><td>1.88 </td><td>2.82 </td><td> 0.94</td></tr>\n",
       "\t<tr><td>2.56 </td><td>4.23 </td><td> 1.67</td></tr>\n",
       "\t<tr><td>2.96 </td><td>3.23 </td><td> 0.27</td></tr>\n",
       "\t<tr><td>2.49 </td><td>2.56 </td><td> 0.07</td></tr>\n",
       "\t<tr><td>3.03 </td><td>4.31 </td><td> 1.28</td></tr>\n",
       "\t<tr><td>2.66 </td><td>4.37 </td><td> 1.71</td></tr>\n",
       "\t<tr><td>3.00 </td><td>2.40 </td><td>-0.60</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " baseline & oneyear & y\\\\\n",
       "\\hline\n",
       "\t 2.12  & 2.47  &  0.35\\\\\n",
       "\t 4.35  & 4.61  &  0.26\\\\\n",
       "\t 3.39  & 5.26  &  1.87\\\\\n",
       "\t 2.51  & 3.02  &  0.51\\\\\n",
       "\t 4.04  & 6.36  &  2.32\\\\\n",
       "\t 5.10  & 5.93  &  0.83\\\\\n",
       "\t 3.77  & 3.93  &  0.16\\\\\n",
       "\t 3.35  & 4.09  &  0.74\\\\\n",
       "\t 4.10  & 4.88  &  0.78\\\\\n",
       "\t 3.35  & 3.81  &  0.46\\\\\n",
       "\t 4.15  & 4.74  &  0.59\\\\\n",
       "\t 3.56  & 3.29  & -0.27\\\\\n",
       "\t 3.39  & 5.55  &  2.16\\\\\n",
       "\t 1.88  & 2.82  &  0.94\\\\\n",
       "\t 2.56  & 4.23  &  1.67\\\\\n",
       "\t 2.96  & 3.23  &  0.27\\\\\n",
       "\t 2.49  & 2.56  &  0.07\\\\\n",
       "\t 3.03  & 4.31  &  1.28\\\\\n",
       "\t 2.66  & 4.37  &  1.71\\\\\n",
       "\t 3.00  & 2.40  & -0.60\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| baseline | oneyear | y |\n",
       "|---|---|---|\n",
       "| 2.12  | 2.47  |  0.35 |\n",
       "| 4.35  | 4.61  |  0.26 |\n",
       "| 3.39  | 5.26  |  1.87 |\n",
       "| 2.51  | 3.02  |  0.51 |\n",
       "| 4.04  | 6.36  |  2.32 |\n",
       "| 5.10  | 5.93  |  0.83 |\n",
       "| 3.77  | 3.93  |  0.16 |\n",
       "| 3.35  | 4.09  |  0.74 |\n",
       "| 4.10  | 4.88  |  0.78 |\n",
       "| 3.35  | 3.81  |  0.46 |\n",
       "| 4.15  | 4.74  |  0.59 |\n",
       "| 3.56  | 3.29  | -0.27 |\n",
       "| 3.39  | 5.55  |  2.16 |\n",
       "| 1.88  | 2.82  |  0.94 |\n",
       "| 2.56  | 4.23  |  1.67 |\n",
       "| 2.96  | 3.23  |  0.27 |\n",
       "| 2.49  | 2.56  |  0.07 |\n",
       "| 3.03  | 4.31  |  1.28 |\n",
       "| 2.66  | 4.37  |  1.71 |\n",
       "| 3.00  | 2.40  | -0.60 |\n",
       "\n"
      ],
      "text/plain": [
       "   baseline oneyear y    \n",
       "1  2.12     2.47     0.35\n",
       "2  4.35     4.61     0.26\n",
       "3  3.39     5.26     1.87\n",
       "4  2.51     3.02     0.51\n",
       "5  4.04     6.36     2.32\n",
       "6  5.10     5.93     0.83\n",
       "7  3.77     3.93     0.16\n",
       "8  3.35     4.09     0.74\n",
       "9  4.10     4.88     0.78\n",
       "10 3.35     3.81     0.46\n",
       "11 4.15     4.74     0.59\n",
       "12 3.56     3.29    -0.27\n",
       "13 3.39     5.55     2.16\n",
       "14 1.88     2.82     0.94\n",
       "15 2.56     4.23     1.67\n",
       "16 2.96     3.23     0.27\n",
       "17 2.49     2.56     0.07\n",
       "18 3.03     4.31     1.28\n",
       "19 2.66     4.37     1.71\n",
       "20 3.00     2.40    -0.60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "-0.6000  0.2675  0.6650  0.8050  1.3775  2.3200 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(boot)\n",
    "ydata <- cd4$oneyear - cd4$baseline\n",
    "data <- cbind(cd4, y=ydata)\n",
    "data\n",
    "summary(ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classical framework, we could use a paired t-test to see if the mean change in CD4 cell counts is significantly different from the null hypothesis value of zero ($H_0: \\mu = \\mathbb{E}[Y]=0)$.   \n",
    "\n",
    "For our Bayesian analysis, we will assume these measurements come from a Normal distribution with an unknown mean $\\mu$, \n",
    "which represents the mean change in CD4 counts. We will assume that the variance is known to be $\\sigma^2 = 0.7$. This is slightly artificial as, in a real example, we may not know what the true variance is; however, we might be able to infer the variability of CD4 counts from earlier studies. Having both $\\mu$ and $\\sigma^2$ unknown requires a more complicated analysis which we will not cover in this course. The Bayesian analysis involves constructing a likelihood for the data, specifying an appropriate prior distribution and combining them to obtain a posterior distribution. We will then describe how credible intervals for $\\mu$, and prior and posterior predictive distributions can be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Calculating the posterior for the mean of a Normal distribution\n",
    "\n",
    "In this section, we obtain the posterior for the mean of a Normal distribution with known variance, $\\sigma^2$. Suppose we have $n$ observed independent data points, each assumed to come from the Normal distribution: $y_1,\\dots,y_n \\sim N(\\mu,\\sigma^2)$. Recall that the Normal distribution has probability density function given by\n",
    "\\begin{align}\n",
    "p(y \\mid \\mu, \\sigma^2) = \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{1/2} \n",
    "\\exp\\left\\{-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "Note that some authors will parameterize the Normal distribution with the *precision* instead of the variance: $\\eta=\\frac{1}{\\sigma^2}$.\n",
    "\n",
    "### 10.2.1 Likelihood\n",
    "\n",
    "Since we assume all observations are independent, dropping the conditioning on $\\sigma^2$, the likelihood is the product of the $n$ individual p.d.f.s:\n",
    "\n",
    "\\begin{align}\\notag\n",
    "p(y_1,\\dots,y_n \\mid \\mu) \n",
    "&= p(y_1 \\mid \\mu) p(y_2 \\mid \\mu) \\dots p(y_n \\mid \\mu) \\\\\n",
    "&= \\prod_{i=1}^n p(y_i \\mid \\mu) \\\\\n",
    "&=\n",
    "\\left( \\frac{1}{2\\pi \\sigma^2}\\right)^{n/2} \\exp\\left\\{\n",
    "-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2 \\right\\}\n",
    "\\end{align}\n",
    "Notice that\n",
    "\\begin{align*}\\sum_{i=1}^n (y_i - \\mu)^2 & = \\sum_{i=1}^n (y_i - \\bar\n",
    "y + \\bar y  -\n",
    "\\mu)^2 \\\\\n",
    "& = \\sum_{i=1}^n (y_i-\\bar y)^2 + n(\\bar y - \\mu)^2, \\mbox{ since }\n",
    "\\sum_{i=1}^n (y_i-\\bar y)=0,\\\\\n",
    "& = (n-1)s^2 + n(\\bar y - \\mu)^2,\n",
    "\\end{align*}\n",
    "\n",
    "where (as usual) $s^2 = \\sum_{i=1}^n (y_i - \\bar y)^2 /(n-1).$\n",
    "Thus the Likelihood can be written:\n",
    "\\begin{equation}\n",
    "p(y_1,\\dots,y_n \\mid \\mu) = \n",
    "\\label{eq:normal} \\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{n/2} \\exp\\left\\{\n",
    "-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2 + n(\\bar y - \\mu)^2 \\right]   \\right\\}.\n",
    "\\end{equation}\n",
    "Since we are interested in the posterior for $\\mu,$ we can drop all\n",
    "terms not involving $\\mu,$ so the likelihood is proportional to\n",
    "\\begin{align}\\notag\n",
    "p(y_1,\\dots,y_n \\mid \\mu) \\propto \n",
    "\\exp\\left\\{ -\\frac{n}{2\\sigma^2} (\\bar y - \\mu)^2 \\right\\}.\n",
    "\\end{align}\n",
    "Notice that this also has the same form of a Normal distribution for the mean $\\bar{y}$, specifically, $\\bar{y} \\sim N(\\mu, \\frac{\\sigma^2}{n})$.\n",
    "\n",
    "\n",
    "### 10.2.2 Prior \n",
    "\n",
    "We noted in the previous session that the Normal distribution is a conjugate prior when the likelihood is a Normal distribution. Thus, for convenience, we will use a Normal distribution as a prior for $\\mu$: \n",
    "$$\n",
    "\\mu  \\sim N(\\phi, \\tau^{2}),\n",
    "$$\n",
    "as the posterior distribution will conveniently be a Normal distribution as well. The prior parameters $\\phi$ and $\\tau^2$ should be specified based on prior knowledge of $\\mu$ and the uncertainty around this prior knowledge. It may come from previous research or formally elicited from investigators. If no prior evidence is available, we assign an appriopriately large value to $\\tau$. \n",
    "\n",
    "### 10.2.3 Posterior \n",
    "\n",
    "To derive the posterior for the mean $\\mu$, we need to find the  distribution of that parameter conditional on the data (both the empirical data and prior distribution). In the following calculation, we are only interested in the parts of the p.d.f. that depend on $\\mu$. Any terms not involving $\\mu$ are part of the *normalisation constant*. This is part of the p.d.f., but does not affect the shape of the density. \n",
    "\n",
    "The posterior is given by\n",
    "\\begin{align}\\notag\n",
    "p(\\mu \\mid y_1,\\dots,y_n) \n",
    "&\\propto p(y_1,\\dots,y_n \\mid \\mu) p(\\mu) \\\\\n",
    "&\\propto \\exp\\left\\{ -\\frac{n}{2\\sigma^2} (\\bar y - \\mu)^2 \\right\\}\n",
    "\\exp\\left\\{-\\frac{1}{2\\tau^2}(\\mu-\\phi)^2\\right\\} \\\\ \\notag\n",
    "& = \\exp\\left\\{ -\\frac{n}{2\\sigma^2}(\\bar{y}-\\mu)^2\n",
    "-\\frac{1}{2\\tau^2}(\\mu-\\phi)^2\\right\\} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Expanding the brackets and retaining only terms containing $\\mu$: \n",
    "\n",
    "\\begin{align}\\notag\n",
    "p(\\mu \\mid y_1,\\dots,y_n) \n",
    "& \\propto \\exp\\left\\{ -\\frac{n}{2\\sigma^2 \\tau^2}  (-2n\\bar{y}\\mu\\tau^2 - n\\mu^2\\tau^2+\\mu^2\\sigma^2-2\\mu\\phi\\sigma^2)\n",
    "\\right\\} \n",
    "\\end{align}\n",
    "\n",
    "Completing the squared term for $\\mu$:\n",
    "\n",
    "\\begin{align}\\notag\n",
    "p(\\mu \\mid y_1,\\dots,y_n) \n",
    "&\\propto \\exp\\left\\{ -\\frac{\\tau^2 n + \\sigma^2}{2 \\sigma^2 \\tau^2}\\left(\\mu - \\frac{ \\tau^2 n\\bar{y} -\\sigma^2\\phi}{\\tau^2n+\\sigma^2}\\right)^2\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We can recognise this has the form the p.d.f. of the Normal distribution, therefore we see that\n",
    "\\begin{align}\n",
    "\\label{equ_posterior_mean}\n",
    "\\mu \\vert y_1,\\dots,y_n \\sim N\\left\\{ \\frac{ \\tau^2 n\\bar{y} + \\sigma^2\\phi }{\\tau^2 n + \\sigma^2}, \\frac{\\sigma^2\\tau^2}{\\tau^2n+\\sigma^2} \\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We see that: \n",
    "1. the Normal prior is *conjugate* for a Normal Likelihood, as the posterior is also Normal. \n",
    "2. The posterior mean, $\\frac{ \\tau^2 n\\bar{y} + \\sigma^2\\phi }{\\tau^2 n + \\sigma^2}$ is a weighted average of the data $\\bar y $ and the prior mean $\\phi$: we can write it as $w \\bar{y} + (1-w) \\phi$, where $w= \\frac{\\tau^2 n}{\\tau^2 n + \\sigma^2}$ . Hence the posterior combines the information from the likelihood (data) and prior (a priori belief).    \n",
    "3. The variance of the posterior is  $\\frac{\\sigma^2\\tau^2}{\\tau^2n+\\sigma^2}$. In a larger study, since $n$ becomes very large, we have $\\tau^2 >> \\frac{\\sigma^2}{n}$, so the posterior variance tends to zero.\n",
    "4. In smaller studies, $\\tau^2 << \\frac{\\sigma^2}{n}$, the posterior mean is closer to $\\phi$ and the posterior variance depends both on the prior and sampling variance $\\frac{\\sigma^2\\tau^2}{\\tau^2n+\\sigma^2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Credible Intervals\n",
    "\n",
    "We saw in the previous session that a Bayesian $100(1 − \\alpha)\\%$ credible interval is an interval which contains $100(1 − \\alpha)\\% $ of the posterior distribution of the parameter, and the $100(1 -\\alpha) \\%$ Highest Posterior Density (HPD) interval ($\\alpha \\in (0,1)$) is the credible interval with the smallest range of values for $\\theta$. \n",
    "\n",
    "Given that the posterior distribution has mean $\\psi$ and variance $\\gamma^{2}$, the $95\\%$ HPD interval is given by \n",
    "$\\psi \\pm 1.96 \\times \\gamma$. Thus, for a standard Normal posterior, the 95% HPD interval is $(-1.96,1.96).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 CD4 cell counts example: \n",
    "\n",
    "\n",
    "In the CD4 cell count example, suppose that we have very strong prior information that suggests the treatment is not effective, and we expect that the difference in cell counts is approximately zero. Let us denote by $y$ the difference in CD4 cell counts. We set $\\mu \\sim N(0, 0.1)$ to reflect that there is only about $2.5\\%$ chance that the treatment increases mean CD4 counts by more than 0.62 (1.96 $\\times \\sqrt{0.1}$) and a $50\\%$ chance that it will actually decrease the mean CD4 count).   \n",
    "\n",
    "Summarizing the information we have: \n",
    "\n",
    "> sample size $n = 20$  \n",
    "> mean of data $\\bar{y} = 0.805$  \n",
    "> variance of data (assumed known) $\\sigma^2 = 0.7$    \n",
    "> prior mean $ \\phi = 0$    \n",
    "> prior variance $\\tau^2= 0.1$     \n",
    "\n",
    "We find the posterior distribution:\n",
    " \n",
    "\\begin{align}\n",
    "\\mu \\vert y_1,\\dots,y_n &\\sim N\\left\\{ \\frac{ \\tau^2 n\\bar{y} + \\sigma^2\\phi }{\\tau^2 n + \\sigma^2}, \\frac{\\sigma^2\\tau^2}{\\tau^2n+\\sigma^2} \\right\\} \\\\\n",
    " &\\sim N\\left\\{ \\frac{ 0.1 \\times 20 \\times 0.805 + 0 }{0.1 \\times 20 + 0.7}, \\frac{0.7 \\times 0.1}{0.1 \\times 20 +0.7 } \\right\\} \\\\\n",
    " & \\sim N\\left\\{ 0.596, 0.0259 \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "We plot below the prior distribution (in blue), the distribution of $\\bar{y}$ (red) and the posterior distribution (purple). We observe that the mean of the posterior distribution is in between the mean of the prior and that of the likelihood. Note that in R, the Normal distribution is parameterized by the standard deviation rather than the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAAJYCAMAAABvmDbGAAAAOVBMVEUAAAAAAP9NTU1oaGh8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\nQAAhAQIICRBASIAAQgIE/AfHqEfznJfRSAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\10. Bayesian Statistics II_9_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=5)\n",
    "x <- seq(-2, 2, 0.01)\n",
    "#plot the prior \n",
    "y1 <- dnorm(x, mean=0, sd=sqrt(0.1))\n",
    "plot(x, y1, type=\"l\", lwd=1, col=\"blue\", ylim=c(0,3), ylab=\"Density\", xlab=expression(mu))\n",
    "legend(\"topleft\", legend=c(\"Prior distribution\", \"Distribution of mean of y\", \"Posterior distribution\"),\n",
    "       col=c(\"blue\", \"red\", \"purple\"), lty=1)\n",
    "#plot the observed distribution \n",
    "y2 <- dnorm(x, mean=0.805, sd=sqrt(0.7/20))\n",
    "lines(x, y2, type=\"l\", lwd=1, col=\"red\")\n",
    "y3 <- dnorm(x, mean=0.596, sd=sqrt(0.0259))\n",
    "lines(x, y3, type=\"l\", lwd=1, col=\"purple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  $95\\%$ HPD interval can be calculated as $0.596 \\pm 1.96 \\times \\sqrt{0.0259} = (0.281, 0.911)$. This interval lies wholly above zero, so we can state that we have a strong posterior belief that there is an increase in CD4 cell counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Predictions \n",
    "\n",
    "### 10.4.1 Prior predictive distributions\n",
    "\n",
    "Finding the predictive distribution for a new patient $y$ before making any observations involves finding the following distribution:\n",
    "\n",
    "\\begin{align}\n",
    "p(y | \\sigma^2, \\phi, \\tau^2) &= \\int p(y, \\mu | \\sigma^2, \\phi, \\tau^2) d \\mu\\\\\n",
    "&= \\int p(y | \\mu, \\sigma^2, \\phi, \\tau^2) p(\\mu |  \\phi, \\tau^2) d \\mu\n",
    "\\end{align}\n",
    "\n",
    "This calculation involves a lot of algebra. We instead use a different approach: note that we can write the observation as $y = \\mu + \\epsilon$, where $\\mu \\sim N(\\phi, \\tau^2)$ and $\\epsilon \\sim N(0, \\sigma^2)$. Then, since $\\mu$ and $\\epsilon$ are independent, we can use this result: \n",
    "\n",
    "> If X and Y be independent random variables that are Normally distributed, $X\\sim N(\\mu _{X},\\sigma _{X}^{2})$ and $Y\\sim N(\\mu _{Y},\\sigma _{Y}^{2})$, then their sum is also Normally distributed: $X + Y \\sim N(\\mu _{X}+\\mu _{Y},\\sigma _{X}^{2}+\\sigma _{Y}^{2})$.\n",
    "\n",
    "Thus we have that $y \\sim N(\\phi, \\tau^2 + \\sigma^2)$.\n",
    "\n",
    "\n",
    "In our example, before collecting any data, suppose we wish to predict the probability that the difference in cell counts is greater than 0.3 (30 $cells/mm^3$). We have that $y \\sim N(0, 0.1 + 0.7)$. We compute $p(y > 0.3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.368657838608209"
      ],
      "text/latex": [
       "0.368657838608209"
      ],
      "text/markdown": [
       "0.368657838608209"
      ],
      "text/plain": [
       "[1] 0.3686578"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1-pnorm(0.3, 0, sqrt(0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our prior distribution alone, the probability that the change in CD4 count for a new patient will exceed 0.3 (30 $cells/mm^3$) is approximately 0.369."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.2 Posterior predictive distributions\n",
    "\n",
    "\n",
    "Suppose that have observed $y_1, ..., y_n $, and we want to predict future observations $z$, assuming that $z$ and $y_i$ are independent for all $1 \\leq i \\leq n$, conditional on $\\mu$. The posterior predictive distribution for $z$ is given by,\n",
    "\n",
    "\\begin{align}\n",
    "p(z| y_1, ..., y_n,  \\sigma^2, \\phi, \\tau^2) &= \\int p(z, \\mu | y_1, ..., y_n,  \\sigma^2, \\phi, \\tau^2) d \\mu \\\\\n",
    "     &= \\int p(z | y_1, ..., y_n,\\mu,  \\sigma^2) p(\\mu |y_1, ..., y_n,\\sigma^2, \\phi, \\tau^2  ) d \\mu. \\\\ \n",
    "\\end{align}\n",
    "\n",
    "Again, this involves some fiddly algebra but we can use a similar method to that we used for the prior predictive distribution. We wish to know what the predictive distribution of a new patient $z$ is, given the previous observations $y_1, ..., y_n$. We can write $z  = \\mu + \\epsilon$. We have that $\\mu \\vert y_1,\\dots,y_n \\sim N\\left\\{ \\frac{ \\tau^2 n\\bar{y} + \\sigma^2\\phi }{\\tau^2 n + \\sigma^2}, \\frac{\\sigma^2\\tau^2}{\\tau^2n+\\sigma^2} \\right\\}, $ and $\\epsilon \\sim N(0, \\sigma^2)$. \n",
    "\n",
    "Using the result for the sum of two independent Normal distributions, the posterior predictive distribution has the form $ N\\left\\{ \\frac{ \\tau^2 n\\bar{y} + \\sigma^2\\phi }{\\tau^2 n + \\sigma^2}, \\frac{\\sigma^2\\tau^2}{\\tau^2n+\\sigma^2} + \\sigma ^2\\right\\}$  \n",
    "\n",
    "In our example, based on both prior and observed data, the predictive distribution for cell counts in a new patient being greater than 0.3 (30 $cells/mm^3$) is $N(0.596, 0.0259 + 0.7)$. We can compute $f(z | y_1, ..., y_n > 0.3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.635861643314828"
      ],
      "text/latex": [
       "0.635861643314828"
      ],
      "text/markdown": [
       "0.635861643314828"
      ],
      "text/plain": [
       "[1] 0.6358616"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1- pnorm(0.3, 0.596, sqrt(0.7259))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having observed the data, the predictive probability that the next patient will have a difference in CD4 cell counts of greater than 0.3 (30 $cells/mm^3$) has increased substantially to 0.636."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Multiparameter models \n",
    "\n",
    "Suppose now that our likelihood has two unknown parameters, $(\\mu,\\sigma^2).$ In this case, we would need a prior distribution for both parameters, and our posterior distribution will now be bivariate. If desired we can summarise this by the mean and covariance matrix or by HPD contour maps. However, often in applications, interest focusses only on one parameter, say $\\mu;$ the other parameter is usually referred to as a nuisance parameter. In Bayesian inference, we typically use simulation to draw from the posterior distribution of $(\\mu,\\sigma^2).$ For marginal inference for $\\mu,$ we summarise the draws from $\\mu$ in the usual way, across all simulated values of $\\sigma^2$. Analytically, this is equivalent to integrating the posterior over $\\sigma^2$: \n",
    "\n",
    "$$p(\\mu | y) = \\int p(\\mu,\\sigma^2 |y) \\, d\\sigma^2,$$\n",
    "\n",
    "where we have used Bayes' theorem to obtain the posterior, i.e.$p(\\mu,\\sigma^2 |y).$ This integral may be intractable (hence the preference for simulation approaches). This will be covered in more detail in the Bayesian course next term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Further Reading\n",
    "\n",
    "\n",
    "### 10.6.1 Resources for learning \n",
    "\n",
    "These textbooks are recommended for further learning and examples:\n",
    "\n",
    "> - [Bayesian data analysis by Gelman et. al](http://www.stat.columbia.edu/~gelman/book/) can be downloaded in PDF format.  \n",
    "> - [The Bugs Book by Lunn et. al.](https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-the-bugs-book/) is available at the LSHTM library.    \n",
    "> - [an introductory book by Jim Stone with nice examples](http://jim-stone.staff.shef.ac.uk/BookBayes2012/HTML_BayesRulev5EbookHTMLFiles/ops/xhtml/ch01BayesJVSone.html) The first chapter is freely available online.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 10.6.2 Examples of applications \n",
    "\n",
    "\n",
    "> - [Article on Nature providing guidelines Bayesian analyses for genetic association studies](https://www.nature.com/articles/nrg2615).   \n",
    "> - The potential benefits of incorporating prior information in the context of health care evaluation is discussed by [David Spiegelhalter in this article](https://projecteuclid.org/euclid.ss/1089808280).    \n",
    "> - We mentioned earlier that Bayesian approaches can be helpful for overcoming challenges with small sample sizes in clinical trials for rare diseases; you can read more about this [in an article by Lilford et. al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2551510/pdf/bmj00623-0045.pdf)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}