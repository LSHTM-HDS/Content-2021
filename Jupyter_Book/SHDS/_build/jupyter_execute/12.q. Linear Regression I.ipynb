{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.16 Analysis of Variance \n",
    "\n",
    "When fitting statistical models, we may wish to compare how well two models fit the data, to see which is most appropriate. Considering the models we have used in this chapter, we may want to compare (for example):\n",
    "\n",
    "\n",
    "> Comparison 1: Model 1 (birthweight$\\sim$length of pregnancy) vs Model 3 (birthweight$\\sim$length of pregnancy+mothers height) \n",
    "\n",
    "> Comparison 2: Model 2 (birthweight$\\sim$mother's smoking status) vs Model 3 (birthweight$\\sim$length of pregnancy+mothers height)\n",
    "\n",
    "In these examples, Comparison 1 is much simpler than Comparison 2, because the models in Comparison 1 are **nested**. \n",
    "\n",
    "Statistical models are said to be **nested** when one model (the simpler model) contains a subset of the covariates in the other one (the complex model) and no other additional variables. In Comparison 2, the models are not nested because the simpler model (Model 2) contains mother's smoking status as a variable, which is not included in Model 3. Nested models can be compared using **Analysis of Variance (ANOVA)** (the comparison of non-nested models is much more complicated and is beyond the scope of this module). \n",
    "\n",
    "The main idea of ANOVA is that: if the complex model better describes the data than the simpler model, then we would expect a reasonably large amount of the residual variation that is unexplained by the simpler model to be explained by the complex one. ANOVA provides a statistical framework that can formally test this. \n",
    "\n",
    "We will first consider ANOVA in the context of simple linear regression, where the simpler model assumes no association between the outcome and the independent variable (the **null** model). We will then consider ANOVA in the context of multivariable linear regression and we end by learning how ANOVA can be used to test for differences between groups in a categorical variable. \n",
    "\n",
    "### 12.16.1 ANOVA for simple linear regression \n",
    "\n",
    "The total variation in $Y$ (otherwise known as the **sum of squares of the $Y$'s**), is denoted by $SS_{TOT}$ and is equal to:\n",
    "\n",
    "$$ \n",
    "SS_{TOT} = \\sum_{i=1}^n (y_i-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "This can be partitioned into two components: the predictable variation in $Y$ ($SS_{REG}$) and the unpredictable variation in $Y$ ($SS_{RES}$). A key result for ANOVA is:\n",
    "\n",
    "$$\n",
    "SS_{TOT} = SS_{REG}+SS_{RES} \\\\\n",
    "\\rightarrow \\sum_{i=1}^n (Y_i-\\bar{Y})^2 = \\sum_{i=1}^n (\\hat{Y_i}-\\bar{Y})^2 + \\sum_{i=1}^n(Y_i-\\hat{Y_i})^2\n",
    "$$\n",
    "\n",
    "In the above equations, $SS_{TOT}$ (i.e the TOTal sum of squares), represents all of the variation in $Y$ about its mean value. $SS_{REG}$ (i.e. the REGression sum of squares) represents the variation of the predicted values $\\hat{Y}$ about the mean. $SS_{RES}$ (i.e. the RESidual sum of squares) represents the variation of the observed values about their predicted values. \n",
    "\n",
    "Using the sums of squares defined above, we can calculate the proportion of variance explained by statistical model, known as the **coefficient of determination**. \n",
    "\n",
    "### 12.16.2 The coefficient of determination\n",
    "\n",
    "The proportion of variation which is explained by a statistical model is denoted by $R^2$ and is given by: \n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SS_{REG}}{SS_{TOT}}.\n",
    "$$\n",
    "\n",
    "*Example.* The coefficient of determination is given in the ```summary()``` output for a linear regression in R. In Model 1, $R^2=0.1661$ (see output below). This means that Model 1 explains 16.6\\% of the total variation in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Birth.Weight ~ Gestational.Days, data = data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-49.348 -11.065   0.218  10.101  57.704 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      -10.75414    8.53693   -1.26    0.208    \n",
       "Gestational.Days   0.46656    0.03054   15.28   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 16.74 on 1172 degrees of freedom\n",
       "Multiple R-squared:  0.1661,\tAdjusted R-squared:  0.1654 \n",
       "F-statistic: 233.4 on 1 and 1172 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The coefficient of determination\n",
    "data<- read.csv('https://www.inferentialthinking.com/data/baby.csv')\n",
    "model1<-lm(Birth.Weight~Gestational.Days, data=data)\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While $R^2$ is sometimes used an overall measure of goodness-of-fit (or predictive performance), it isn't used to formally compare models. This is because $R^2$ will never decrease when new covariates are added to a model (provided that the number and identity of observations remains the same). Therefore, using $R^2$ for model comparisons, we would always conclude that the more complex model is at least as good a fit as the simpler model, even if this is not true. We can calculate an adjusted $R^2$ (the adjusted $R^2$ for Model 1 is given in the output above) that accounts for this issue. Alternatively, we can use analysis of variance (ANOVA). To describe ANOVA, we need to first define the remaining statistics that are commonly summarised in what is known as the ANOVA table. \n",
    "\n",
    "### 12.16.3 The ANOVA table. \n",
    "\n",
    "Each of the sum of squares defined above ($SS_{TOT}$, $SS_{REG}$ and $SS_{RES}$) have an associated **degrees of freedom (d.f.)**. The d.f. for the total sum of squares is $(n-1)$, since the variance of $Y$ is $\\sum_{i=1}^n (Y_i-\\bar{Y})^2/(n-1)$. The d.f. for the regression sum of squares in the number of covariates in the regression model (when a simple linear regression model is used this is equal to 1). The residual d.f. is found by subtracting the regression d.f. from the total d.f. The sums of squares also have associated mean squares, which are obtained by dividing the sum of squares by its associated degrees of freedom (note that the residual mean square is then equal to $\\hat{\\sigma}^2$). These statistics are summarised in an ANOVA table for simple linear regression (Table 2). \n",
    "\n",
    "\n",
    "Source      | d.f.  | SS         | Mean Square                        | \n",
    "------------|-------|------------|------------------------------------|\n",
    "Regression  | 1     | $SS_{REG}$ | $MS_{REG}=\\frac{SS_{REG}}{1}$      |\n",
    "Residual    | n-2   | $SS_{RES}$ | $MS_{RES}=\\frac{SS_{RES}}{n-2}$  | \n",
    "Total       | n-1   | $SS_{TOT}$ | $MS_{TOT}=\\frac{SS_{TOT}}{n-1}$    | \n",
    "\n",
    "Table 2: The ANOVA Table for simple linear regression \n",
    "\n",
    "The values in this table can be used to conduct formal hypothesis tests. \n",
    "\n",
    "### 12.16.4 Hypothesis testing using ANOVA\n",
    "\n",
    "ANOVA is used to test the null hypothesis that the simpler of the two nested models better fits the data. In simple linear regression, the simpler model is the null model, in which case:\n",
    "\n",
    "+ $H_0:$ The null model is a better fit\n",
    "+ $H_1:$ The simple linear regression model is a better fit\n",
    "\n",
    "To test the null hypothesis defined above, we use an $F$ statistic, defined as:\n",
    "\n",
    "$$\n",
    "F = \\frac{MS_{REG}}{MS_{RES}}\n",
    "$$\n",
    "\n",
    "This ratio measures how much more variation in $Y$ is explained by the model than would be expected by chance. If the model does not fit the data well, then we would expect this ratio to be equal to 1. The larger the value of $F$, the stronger the evidence that the complex model is a better fit. To obtain a $p$-value for a formal hypothesis test, $F$ can be compared to the $F_{1,(n-2)}$ distribution (where 1 and (n-2) are the relevant degrees of freedom for the mean squares).\n",
    "\n",
    "### 12.16.5 Similarity between F tests and t-tests in simple linear regression \n",
    "\n",
    "Since the null model is defined as: $Y=\\beta_0+\\epsilon$ and the simple linear regression model is defined as: $Y=\\beta_0+\\beta_1X+\\epsilon$. The hypotheses above can be rewritten as:\n",
    "\n",
    "+ $H_0: \\beta_1 = 0$ \n",
    "+ $H_1: \\beta_1 \\neq 0$ \n",
    "\n",
    "In other words, the $F$-test for a simple linear regression model is the same as the $t$-test of the null hypothesis that the slope parameter is equal to 0. Indeed, the two tests are equivalent with $F=t^2$. Consequently, it is not particularly common to use $F$-tests in the simple linear regression model, they are more useful for assessing more complex models with multiple covariates. \n",
    "\n",
    "### 12.16.6 Example using Model 1\n",
    "\n",
    "The $F$ test statistic and associated $p$-value are given in the summary output for linear models (shown below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Birth.Weight ~ Gestational.Days, data = data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-49.348 -11.065   0.218  10.101  57.704 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      -10.75414    8.53693   -1.26    0.208    \n",
       "Gestational.Days   0.46656    0.03054   15.28   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 16.74 on 1172 degrees of freedom\n",
       "Multiple R-squared:  0.1661,\tAdjusted R-squared:  0.1654 \n",
       "F-statistic: 233.4 on 1 and 1172 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F-test\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use ```anova()``` which outputs the ANOVA table as well as the relevant $F$ statistic and $p$-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Gestational.Days</th><td>   1        </td><td> 65449.51   </td><td>65449.5131  </td><td>233.4293    </td><td>3.395226e-48</td></tr>\n",
       "\t<tr><th scope=row>Residuals</th><td>1172        </td><td>328608.34   </td><td>  280.3825  </td><td>      NA    </td><td>          NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\\\\n",
       "\\hline\n",
       "\tGestational.Days &    1         &  65449.51    & 65449.5131   & 233.4293     & 3.395226e-48\\\\\n",
       "\tResiduals & 1172         & 328608.34    &   280.3825   &       NA     &           NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Df | Sum Sq | Mean Sq | F value | Pr(>F) |\n",
       "|---|---|---|---|---|---|\n",
       "| Gestational.Days |    1         |  65449.51    | 65449.5131   | 233.4293     | 3.395226e-48 |\n",
       "| Residuals | 1172         | 328608.34    |   280.3825   |       NA     |           NA |\n",
       "\n"
      ],
      "text/plain": [
       "                 Df   Sum Sq    Mean Sq    F value  Pr(>F)      \n",
       "Gestational.Days    1  65449.51 65449.5131 233.4293 3.395226e-48\n",
       "Residuals        1172 328608.34   280.3825       NA           NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F-test using anova()\n",
    "anova(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from both outputs above, the $F$-statistic is equal to 233.4 with $p$-value $<2.2\\times10^{-16}$. With such a small $p$-value there is strong evidence against the null and therefore we conclude that the model which includes gesational days is a better fit (i.e. we conclude that $\\beta_1 \\neq 0$). \n",
    "\n",
    "Notice that in the ```summmary()``` output, the $p$-values for the F-test and the t-test for gestational days are identical, and that $F=233.4=15.24^2=t^2$. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}