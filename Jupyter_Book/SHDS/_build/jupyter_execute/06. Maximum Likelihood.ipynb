{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Maximum Likelihood \n",
    "\n",
    "[ADD SOMETHING ABOUT HOW THIS ALL FITS IN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Intended learning outcomes</b> \n",
    "    \n",
    "By the end of this session you will be able to:\n",
    " * Derive the likelihood and log-likelihood functions given an i.i.d. sample\n",
    " * Derive maximum likelihood estimator from single and multi-parameter distributions given an i.i.d. sample\n",
    " * Describe the main properties of MLEs    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Maximum Likelihood estimation \n",
    "\n",
    "In inferential statistics, the problem we are often faced is this: we have collected some data, and we have a statistical model for how this data was generated. However, we do not know what the values of the parameters of this model are. We need to find a way to estimate these parameters. In Session 5, we were introduced to the likelihood function, ${L}(\\theta | x)$. It measures how consistent different values of the parameter $\\theta$ are with the data that we observe. We then looked at how you can use calculus to obtain the maximum likelihood estimator for the parameter.    \n",
    "\n",
    "So far, we only looked at examples where our data consists of one observation - surely, this is not a sufficient sample size! We will now consider the more realistic scenario, where we have a random sample of observations from a particular distribution - in this case, we say that the sample is independently and identically distributed (i.i.d). We will see what the likelihood function looks like for $n$ i.i.d observations, and how we can obtain the maximum likelihood estimator. We will also demonstrate some important properties of maximum likelihood estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Likelihood and log-likelihood with $n$ independent observations\n",
    "\n",
    "Suppose that you collect a sample of $n$ observations. If the $n$ observations are independent, then the joint likelihood function from these $n$ observations has a very convenient form; it is the product of the likelihood from each observation. If $X_1,..., X_n$ are i.i.d., and we observe a sample $\\mathbf{x} = \\left\\{ x_1, x_2, ..., x_n \\right\\}$: \n",
    "\n",
    "\\begin{align*}\n",
    "L \\left( \\theta \\mid \\mathbf{x} \\right) &=  L\\left( \\theta \\mid x_1 \\right) L\\left( \\theta \\mid x_2 \\right) ...  L \\left( \\theta \\mid x_n \\right) \\\\\n",
    " &= \\prod_{i=1}^n  L\\left( \\theta \\mid x_i \\right)\n",
    "\\end{align*}.\n",
    "\n",
    "Recall that we often prefer to work with the log-likelihood function, as it simplifies the algebra when it comes to finding the MLE. The log-likelihood function for $n$ independent observations is:\n",
    "\\begin{align*}\n",
    "l \\left( \\theta \\mid \\mathbf{x} \\right) \n",
    " &= log \\prod_{i=1}^n L\\left( \\theta \\mid x_i \\right) \\\\\n",
    "  &= \\sum_{i=1}^n log L\\left( \\theta \\mid x_i \\right) \\\\\n",
    "   &= \\sum_{i=1}^n l\\left( \\theta \\mid x_i \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Finding the MLE involves the same three steps as we saw in Session 5, but the log-likelihood function is now a joint function for the $n$ observations:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<b> Method for finding MLEs:</b>   \n",
    "1.  Obtain the derivative of the log-likelihood: $\\frac{d l(\\theta \\mid \\mathbf{x})}{d \\theta}$  \n",
    "2.  Set $\\frac{d l(\\theta \\mid \\mathbf{x})}{d \\theta}=0$ and solve for $\\theta$   \n",
    "3.  Verify that it is a maximum by showing that the second derivative $\\frac{d ^2 l(\\theta \\mid  \\mathbf{x})}{d \\theta ^2 }$ is negative when the MLE is substituted for $\\theta$.    \n",
    "    \n",
    "</div>\n",
    "\n",
    "## 6.3 Example: Exponential distribution \n",
    "Recall the example from Session 5 on the time that patients wait until their GP appointment in a particular practice. The receptionist records the time that elapses between when a patient walks through the door, and when they are called through for their appointment for a random sample of eight people. These times (in minutes) are: 8.75, 10.20, 15.29, 7.89, 7.04, 12.04, 19.04, 17.50.      \n",
    "\n",
    "As a reminder, we can model the waiting time until a specific event using the exponential distribution with parameter $\\lambda$, which has a probability density function given by:  \n",
    "\n",
    "\\begin{equation}  \n",
    "f _X\\left(x \\mid \\lambda \\right)=\\lambda e^{-x\\lambda} , x > 0, \\lambda > 0  \n",
    "\\end{equation}\n",
    "\n",
    "*Remember that the mean of this distribution is equal to one over the rate parameter $\\lambda$, i.e. $E(X) = \\frac{1}{\\lambda}$.*\n",
    "\n",
    "We have that the log-likelihood is: \n",
    "\n",
    "\\begin{align}\n",
    "\\log L\\left( \\lambda \\mid \\mathbf{x} \\right) &= \\sum_{i=1}^n \\log  L(\\lambda \\mid x_i) \\\\ \n",
    "&= \\sum_{i=1}^n \\log \\left( \\lambda e^{-x_i \\lambda } \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\log \\lambda -x_i\\lambda \\\\  \n",
    "&= n \\log \\lambda -\\lambda \\sum_{i=1}^n x_i \n",
    "\\end{align}\n",
    "\n",
    "We can make a plot of this log-likelihood, using the data from our example with eight observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nrkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\06. Maximum Likelihood_4_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "#six independent observations for waiting times \n",
    "obs <- c(8.75, 10.20, 15.29, 7.89, 7.04, 12.04, 19.04, 17.50)\n",
    "n <- length(obs)\n",
    "\n",
    "#possible values for the parameter lambda\n",
    "lambda <- seq(0, 2, 0.01)\n",
    "\n",
    "#plot the log-likelihood\n",
    "plot(lambda, n*log(lambda) - lambda*sum(obs), type=\"l\", xlab= expression(lambda), ylim=c(-100,0),ylab=\"Log-likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, we observe that the maximum is between 0 and 0.25. We will use the three steps, as before, to derive the MLE algebraically:\n",
    "\n",
    "**Step1**: Taking the derivative of the log-likelihood with respect to $\\lambda$:\n",
    "\\begin{equation}\n",
    "\\frac{d log L\\left( \\lambda \\mid x_1 ,..., x_n \\right) }{d \\lambda} = \\frac{n}{\\lambda}- \\sum_{i=1}^n x_i \n",
    "\\end{equation}\n",
    "\n",
    "**Step2:** Set the derivative equal to zero and solve for $\\lambda$:\n",
    "\\begin{equation}\n",
    "0 = \\frac{n}{\\lambda}- \\sum_{i=1}^n x_i \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\lambda}= \\frac{n }{\\sum_{i=1}^n x_i} = \\frac{1}{\\bar{x}}\n",
    "\\end{equation}\n",
    "\n",
    "The MLE is $\\hat{\\lambda}= \\frac{1}{\\bar{x}}$. And to check that this provides a maximum, we go on to the next step:  \n",
    "\n",
    "**Step3:** Find the second derivative: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d l^2 \\left( \\lambda \\mid \\boldsymbol{x} \\right)}{d \\lambda ^2} \n",
    "= - \\frac{n}{\\lambda^2}\n",
    "\\end{equation}\n",
    "When ${\\lambda}=\\frac{1}{\\bar{x}}$, we have: \n",
    "\\begin{align}\n",
    " \\frac{d l^2 \\left( \\lambda \\mid \\boldsymbol{x} \\right)}{d \\lambda ^2}  \n",
    " &=-n \\bar{x}^2\n",
    " \\end{align}\n",
    "which is negative. This verifies that we found the maximum likelihood estimate. \n",
    "\n",
    "Going back to our example of eight patients waiting for their GP appointment, the maximum likelihood estimate $\\lambda$ is given by one over the average of the eight waiting times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0818414322250639"
      ],
      "text/latex": [
       "0.0818414322250639"
      ],
      "text/markdown": [
       "0.0818414322250639"
      ],
      "text/plain": [
       "[1] 0.08184143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1/mean(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that $\\hat{\\lambda}=0.0818$ minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Example: Normal distribution \n",
    "\n",
    "We will now consider the normal distribution. Remember that the normal distribution has two parameters, $\\mu$ and $\\sigma^2$. We will first obtain the MLE for $\\mu$ (treating $\\sigma^2$ as a constant), and in the practical, we will obtain the MLE for $\\sigma^2$ (treating $\\mu$ as a constant). Recall that normal distribution has probability density function given by: \n",
    "\n",
    "\\begin{equation}  \n",
    "f_X \\left( x \\mid \\mu, \\sigma^2 \\right)= (2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "We have that the log-likelihood given an i.i.d. sample of size $n$ is: \n",
    "\n",
    "\\begin{align}\n",
    "l \\left(\\mu, \\sigma^2 \\mid  \\boldsymbol{x}  \\right) &=  \\sum_{i=1}^n \\log \\left\\{ (2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right\\} \\right\\} \\\\\n",
    "&= \\sum_{i=1}^n \\left\\{ \\log (2 \\pi \\sigma^2)^{-\\frac{1}{2}}+ \\log \\exp  \\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right\\}  \\right\\} \\\\\n",
    "&= \\sum_{i=1}^n \\left\\{ -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{(x_i-\\mu)^2}{2\\sigma^2}  \\right\\} \\\\\n",
    "&=  {-\\frac{n}{2}}\\log (2 \\pi \\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2 \n",
    "\\end{align}\n",
    "\n",
    "We will first find the MLE for the parameter $\\mu$. \n",
    "\n",
    "**Step1**: Take the derivative of the log-likelihood with respect to  $\\mu$. Note that this requires use of the chain rule:\n",
    "\n",
    "\\begin{align}  \n",
    "\\frac{d l \\left(\\mu, \\sigma^2 \\mid  \\mathbf{x}  \\right) }{d \\mu}\n",
    "&=  -\\frac{2}{2\\sigma^2}(-1) \\sum_{i=1}^n (x_i-\\mu) \\\\\n",
    "&=  \\frac{ \\sum_{i=1}^n (x_i-\\mu)}{\\sigma^2} \\\\\n",
    "&=  \\frac{ \\sum_{i=1}^n x_i-n\\mu}{\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "**Step2:** Setting the derivative equal to zero and solving for $\\mu$:\n",
    "\n",
    "\\begin{align}  \n",
    "0 &=  \\frac{ \\sum_{i=1}^n x_i-n\\mu}{\\sigma^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since $\\sigma^2 > 0$, we have that: \n",
    "\n",
    "\\begin{equation}  \n",
    "0 = \\sum_{i=1}^n x_i-n\\mu \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}  \n",
    "\\hat{\\mu} =\\frac{ \\sum_{i=1}^n x_i}{n} = \\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "We have that the MLE for $\\mu$ is the sample mean, $\\bar{x}$. \n",
    "\n",
    "**Step3:** Find the second derivative: \n",
    "\n",
    "\n",
    "\\begin{align}  \n",
    "\\frac{d^2 l \\left(\\mu, \\sigma^2 \\mid  \\mathbf{x}  \\right) }{d \\mu^2 }\n",
    "&=  -\\frac{n}{\\sigma^2},\n",
    "\\end{align}\n",
    "\n",
    "since both $n>0$ and $\\sigma^2 >0$, we have that the second derivative is negative, verifying that we have found the maximum.    \n",
    "\n",
    "\n",
    "In the practical, we will find the MLE for $\\sigma^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Properties of maximum likelihood estimators\n",
    "Maximum likelihood estimators can be shown to have some very useful properties. In particular, there are some very important asymptotic properties (properties that we observe as the sample size of our data gets very very large). To explore these properties, have a look at the simulation below. We generate a sample of size 8 from the exponential distribution where $\\lambda=12.22$. The MLE is calculated from this sample, by taking its average. We repeat this 100 times, and we plot a histogram of the 100 MLEs that we obtain. Change the sample size, $n$, to larger numbers and see what you notice about the histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAND0lEQVR4nO2d26KqIBRFl5fMysv//+0WtAJFMwS1ued4ONtUWMg4KmCStAQa\nOboAJC4UDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWD\nQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4F\ng0PB4FAwOOcXLCL20nuFyWWXwlwTkXckUdR6sdbLjuKJyS5ltAER/Eh2OZCrsjQSfNOLNwr2\nZZXgnSovFansqCKZXswo2JeJ4OWddirL8+NzjSwI3qNks/yg4OFPc1VnTX5rX2eJ3ul+UVfR\n+5Ck7j5lpZGyTqXolm55t5wW9TO/MpX00bZlItnDDm/lN5XX5aO23fUSBfswI7hOBq2ZJTgb\nlnOd4jHs8k6Z6gTPveTRr+0/18Vr3Qszv8l1tvtQ9vfki16iYB9mBF9086bpBJRGzeev2502\nnLw+PlOKSta5yJq2LSxvIon5f6PHys8pOJdEB8op2BMxGVb0/6oLbNOdlK916kopZdNdvUVf\nObumbaL+JO+USqxqK9VWTt1a5Set2tISMsrPIa8q1SnfXSnKalawVfq9+VnBStrrVvusVXWl\n1J8LfeUcbpC6DzPsdR9l3f/7sP68dxjl5xLcdYCvqvtUU7AnM4Kv/YrB8XtToz/XekXyrNPx\n5m6HW5HJS3A7+fNKZ+bnEtwFSbtLQtJSsCfvirFrsBhqLaknm55LMhXcf76lRp0vC7aWXIK7\ns/yuzu95wduOfyO/K7htbn0TN7M2vc64xHkG64/qkp1ennfNlWdwMt44CL7ppvmNgn2ZFazQ\nvdT3uvzjPVhvTYf1HwXnn+/Bwyh0TcG+zAhOh5PrfWo1s61oGdX98PfzGfy5FV3rkqiWfE3B\nfswI7uo+q3VbS41MqTa1+vsawegfCUz7wTqjTO98Tz4KHufnlFcMsd8ZmEFl9Hlvflbwq5Gl\nB/svz4Wnkf6Rz33YZSRvGOBSIxuPZcGj/JyC7/bpTcFfMie4v/9m/T1S3S0Hp5fE6CBXaiz6\nPpGnVicX1YfNPwi283MKbqS/W1DwYTT9jfpfAi1Y+sfxVWYPMP8roAW/m0j2EOV/Alrw65Gi\nbuX+T6AFdx1Y9bwvufzb8xddMKFgdCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFg\ncCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGh\nYHAoGBwKBoeCwaFgcCgYnEiCo0zBefC0nr/JDjUVTnCEPOGhYHAoGBwKBoeCwaFgcCgYHAoG\nh4LBoWBwKBgcCgaHgsGhYHAoGBwKBsesqfRaxw4RKiMKXotZUyISwzEFH4lZU83tEsMxBR/J\nuKYe1zS0Ywo+EkdNVeo3O8uoITZnRMFrmdbUvf/V3WxV8rI73/MPv+1JwUcyqqnm2p2+6b3p\nLC//Znb/xdVsza/zUvCRWDX1UI2souo3LNeh3lxI0bRtXSxf0Cn4SKx+cHfyls1zQ7KcTiVM\nRO/dSLo2xCYo2AOrH/zpbmruK+37LF8+2yn4SKx+8DfpVMLLU/Di2U7BR2LVVFMoU0mxwnTX\neL6Wd7npVMutLAo+ErOm6mS48CafxzmMl8C6/Rf/R1DwkZg1lclFmerOyOUukqaqyjLPdVPr\nwxlPwUdiP2wYLwQPESojCl6LWVNDr6fr9myuvzjvBzvzD5U7KGb9FJI9uj+P7MPQ1IYQoTIS\n10riwKqfYeRx5Ti0V4hAGVHwWuz6ueVKb8AnSdMQYTKi4LX41o/I6tssBR+Jb/2UFPwbeNdP\nlay9U1PwkVj1o76us7rvUa1ta1PwkZj1c/2uc1lK9XWITVCwB/ZAR+D28zREqIwoeC3Oocp4\nIUJlRMFrMesnl2+eCHuFCJURBa/FflyohypjhgiVEQWvxb5ERxnBp+AjoWBwdqgfCj4SCgbH\nrp97rq7OedjXCyn4SKbPg9W3YIMapuAjMeunlEx/W6eUS6wQoTKi4LWMv5NlvbEQPkSojFyC\n+T0tF+Ohyl8WHD4iAGZdpMMZXC2/TLYlRKiMKHgtjnvwPfBTJQo+Eqsu8p/+ViUFu5j2gyW/\nxQwRJiMKXsvvj2Q52s4U/Ob3BS8uEQoGB+hxIQW7oGBwHHXxyFa8/70txOaMKHgtrrpotj9s\nCPh+sDMjCl6Lsy5OdYl2eqPgtbjqovwwCVqAEF7JKdgDdyPrGivEpuQU7IFLcBr2DRYKPpLz\nD3RQ8CYoGJyZgY6Qgx0HCOa3d15gCg4W/PexKuCaqPmEH6snZ/AIsSU5BXtgVsB1eGW/WjNX\npV+ITckp2AP7Ej1eCB5iU3IK9sCsgOR1Bp/pW5UUvAmzAgrR9+CTfauSgjdhVcBzrsqgc5FS\n8KHYFaDnqvzipzk8QmxITsEeoI9kUfDpQ1DwJuwKOOML4BS8iWkjqz3ZC+AUvAmzAs75AjgF\nb8Ie6DjjC+AUvInxUCUFg2FWwDlfAKfgTTjuwRyqRMKqgFO+AE7Bm5j2g8/2AvhGwa7Xh//T\nF3r+z0jWhx1RMQ8xD/sUyRViU3IK9mDcTfqSMv389ImCj2TcTVqdTifM1jw/puAjMQ+xyddP\n6a8FF6J+G7oulrtVFHwk9iV6fetS7zL84HCzPDBCwUeySfBzv+n+IV8Ad5WVgtfie4ha2+Up\nePF9Ygo+En/B+bW8ixoTaYrlVhYFH8nzEL+9kBqXX5FksfVNwUdiC/5Cc1WVZZ7rplax3Lui\n4CPxFvx9iM3JKdgDCgaHgsGhYHAoGJy34CjTN5ghNienYA8oGJwdDpGCj4SCwaFgcCgYHAoG\nh4LBoWBw/pFg1zsOHsX5sfci/pHg5R2/L85vnP8U7F0cCg4TgoI3QcHexaHgMCEoeBNnECzL\nuDLaR/BycT5tPgenEPz10l6Ct+R9EijYu2Dogh/XfkqPvPjwSiIFH4lvgZrUuO8sT9pCwUfi\nW6BCklv/AwD1Pdn4bhIFR8S3QM/fd1BUG98upOCI+BbI6g9sfD/4Qzfpt/Csz2jscAaTI9lw\nD773s0p/vAeTI/G+pGTGZSldPzsP2ZkN/eBC94OT/Lp6ah6yPzs3Cg5q+fw0G2s8jLiI4U6b\n5MQlC5d8h3CnTXLikoVLvkO40yY5ccnCJd8h3GmTnLhk4ZLvEO60SU5csnDJdwh32iQnLlm4\n5DuEO22SE5csXPIdwp02yYlLFi75DuFOm+TEJQuXfIdwp01y4pKFS75DuNMmOXHJwiXfIdxp\nk5y4ZOGSk7NDweBQMDgUDA4Fg0PB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDE5cwUUi\n9m+TGism28JFadtS5rcFC1OmOxxMcxG5VDMlWEFUwf07xKlzxWRbuChqzgGZ3RYsTKFX6N9O\njngwiV5RObetIabghyRVWyXycKyYbAsXpVWfZG5bsDCVXBp1qbhEPZhC5V9I7tq2ipiCC7l3\n/97k6lgx2RYuSlfp2SA4XJRpVnkfQkWKeDCJNEMQzygxBeeiZvGo9H+/8YrJtnBRWime8/6E\nizKblYoU82D6IMlCCZaJKVjE/GOvmGwLF6WtxitDTG40k1WjpvmLeTCKQsr5EnzKcXOZFvI+\nSPBkZUTBpbpuxj2Ym/SzGFHw7Mp4guskDxrGlVOZJ/q+S8GzK6MJbpIsbJiZnC7qGn0+wcm4\nRMaKybZwUYxP4aK4s8rSwGFmcmpUK8svSvxWdD1uRdfvVnQdrOE5yslqRYeI4sqqTrM6cJi5\nnN5t9W+jxBR81R23+3uiQ2PFZFu4KIpBcLgojqzur3myIx5M3w+u1fiVXxTIkayX4JgjWfV7\nHvTYI1lNru7B5xvJavtZ4Y12iLHCWAwdxVgIF2US5mLMRBfxYJKNVRZVcKMff/RxZLTCWAwd\nxVgIF2USxpxqMObBdCvScrxtPVEFk+OhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaH\ngsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoG\nh4LB+ZeCk7zsp8epy1zNAmlOTSTmm/sAgBzGd3T2LnrhMnikYCxEUn3itknqEHxMmWIBdjjr\nECn0HOpV95eCARG56wl6S7lRMCIijZ4QMJeaghHpJKZ6qquknW9kHVW4wKAcx1d09gp5tI+u\nLU3BiHT21G9bXOXmEnxYsaIAdjjr6CSqqUQzqSkYkn5Gcj3LNgUjoiReRM/US8GIKIldD7i7\nBVMwJEpi1wNWU+Q/Bb9azhyLBkDLS/TPiVEw+W0oGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgc\nCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgY\nHAoG5w9WqnRLsWnqUAAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of MLE\""
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\06. Maximum Likelihood_10_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n <- 8  #  make this sample size bigger, and see what happens to the histogram! \n",
    "\n",
    "#MLEs will be stored in this vector\n",
    "mle <- rep(0, 100)\n",
    "\n",
    "for (i in 1:100){\n",
    "  #generate a sample of size n from an exponential distribution with lambda=0.0818\n",
    "  sample <- rexp(n, rate=0.0818)\n",
    "  #calculate the MLE (the reciprocal mean of the sample) and store it \n",
    "  mle[i] <- 1/mean(sample)\n",
    "}\n",
    "\n",
    "#plot a histogram of the 100 MLEs \n",
    "hist(mle, breaks=20, \n",
    "     xlim=c(0, 0.3), \n",
    "     main=\"Histogram of MLE\", \n",
    "     xlab=\"MLE\")\n",
    "#add red line to indicate true lambda \n",
    "abline(v=12.22, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that, as $n$ becomes large, the distribution of the MLE becomes more and more concentrated around the true value, and the histrogram appears to look more bell-shaped.    \n",
    "\n",
    "Denoting the parameter of interest as $\\theta$ and its MLE as $\\hat{\\theta}$, MLE are: \n",
    "1. Asymptotically unbiased: $\\mathbb{E}(\\hat{\\theta}) \\rightarrow \\theta$ as $n \\rightarrow \\infty$.\n",
    "2. Consistent: $\\hat{\\theta} \\rightarrow \\theta$ in probability as $n \\rightarrow \\infty$.\n",
    "3. Asymptotically efficient: $Var(\\hat{\\theta})$ is smallest variance amongst all unbiased estimators as $n \\rightarrow \\infty$.\n",
    "4. Asymptotically Normal: $\\hat{\\theta} \\sim N(\\theta ,Var(\\hat{\\theta} ))$ as $n \\rightarrow \\infty$. \n",
    "5. Transformation invariant: if $\\hat{\\theta}$ is the MLE for $\\theta$, $g(\\hat{\\theta})$ is the MLE of $g(\\theta)$ for any function $g$. \n",
    "\n",
    "The approximate normal distribution of the MLE means that confidence intervals and hypothesis tests for the parameters can be constructed easily. The confidence interval will be the narrowest amongst confidence intervals of estimators that are linear and unbiased. But you might question to what extent these asymptotic properties are useful in practical examples where the sample size is relatively small. Further, in the cases that we have covered so far, it is fairly straightforward to compute the likelihood function and to find the value that maximizes it, but in many situations, this will be a complex task that requires numerical approaches. In Sessions 9 and 10 on Bayesian Statistics, we will see a different paradigm for making inference which can address some of these issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Summary\n",
    "\n",
    "We now know how to obtain the likelihood and log-likelihood functions when you have an i.i.d. sample of observations. We can then obtain the maximum likelihood estimators of the parameters of the distribution. The MLE is an important tool as it has a number of important asymptotic properties, as we demonstrated using a simulation in R. Finally, we introduced the idea of a log-likelihood ratio, which is a way of comparing estimates of a parameter with the maximum likelihood estimate.   \n",
    "\n",
    "You may be wondering how you might measure the precision of your estimator. We will cover this in Session 7, where we will meet the concept of confidence intervals. Note that the maximum likelihood estimator, and confidence intervals, are tools from the \"frequentist\" or \"classical\" approach to statistics. In Sessions 9 and 10, you will meet the Bayesian approach to statistics, where the Likelihood will also play an important role. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Extra Reading \n",
    "\n",
    "\n",
    "## A1: Log-likelihood ratios\n",
    "\n",
    "So far we have used the MLE to find an estimate of a parameter. Typically, the estimate is computed from a sample, so if we were to _sample again_ we would expect the estimate to vary a little. But what about others values; what steps are involved to compare other estimates to the MLE? How much would the sample estimates vary? The **log-likelihood ratio (LLR)** is a useful approach. The LLR gives a measure of consistency of a value of $\\theta$ relative to the most likely value.\n",
    "\n",
    "The LLR is defined as,\n",
    "\n",
    "$$ log\\frac{L(\\theta)}{L(\\hat\\theta)} $$\n",
    "\n",
    "where $L(\\theta)$ is the likelihood evaluated at any value, and $L(\\hat\\theta)$ is the likelihood evaluated at the MLE.\n",
    "\n",
    "Alternatively, and especially when evaluating in software, the following is used;\n",
    "\n",
    "$$ LLR(\\theta) = l(\\theta)-l(\\hat\\theta) $$\n",
    "\n",
    "Let's explore the LLR and its properties with an small example.  \n",
    "\n",
    "For a simple coin-flipping example, from a trial of 10 coin-flips, 4 were heads ($X=4$) and the remainder were tails. From this experiment, we know that the MLE (ie. $\\hat\\theta$) is 0.4, but we also want to use the LLR to compare other estimates of $\\theta$. Looking at the LLR graphically we note that the LLR is a negative value, the further away from zero the less consistent the parameter value with to the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAM1BMVEUAAAAAAP9NTU1oaGh8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\neSMKzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\06. Maximum Likelihood_14_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    " x <- 4; n<-10\n",
    " theta_vals <- seq(0,1,0.01)\n",
    " LLR <- dbinom(x,n,theta_vals,log=T)-dbinom(x,n,x/n,log=T)\n",
    " plot(theta_vals,LLR,type='l',col='blue')\n",
    "# add additional things\n",
    "lines(x=theta_vals,y=rep(0,length(theta_vals)),lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above experiment, for a _fair_ coin it would not be unusual to observe 4 heads from 10 trials. The MLE is 0.4 but we _know_ for a fair coin that the true parameter $\\pi$ will be 0.5. The MLE is a *sample* of the distribution for $\\pi$. So let's zoom in on the figure previously generated;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAAP9NTU1oaGh8\nfHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD/AAD////xw1/KAAAACXBIWXMA\nABJ0AAASdAHeZh94AAAMCElEQVR4nO2dibaiOhBF0wjihFz//2dbwIlJkyKphMPZ6z2vDRQZ\ntgwiVMyNQGNiV4CEhYLBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgc\nCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgY\nHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAo\nGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBw\nKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFg\ncCgYHAoGR0GwId4Q9L5/oRGK2AoUDA4Fg0PB4KQjeO7M4Plv5b//2r///v2bXe4vSr2s/n52\nn8CEe0iCRUzz78V40nDZP9WaSaHgB9MSvy1BwXK0Bf9QO70oBctRFexgtxdAwXL0BDvbfYX9\no2A5SoKFdh/8LYrWYsOCl/r5W/oJUWGzgper+XusJ23HGxXsw8rrGJy0400K9iPk8yQrXcUb\nFOxLRv8sOlXF2xPsTcTwa1Kahrcm2OOGNvoenORGvDHBPhVMXOhIUPGmBPvt/8krWckp3pJg\nz30/c6kyMcUbEuy742evRSdleDuCvXf7/I8NKW3EmxHsv8+//ZqUjuGNCA6xTX39uTCZjVhR\nsOkToog5gvT2j9+DEzGsKPgYTXCYvv71g38aG7HmLrrK8tBFTBKoo3/f0ZGCYdVjcGXK0EVM\nEKqbLW7ZScCw7knW0VShixgSbkdpc09WfMPpnEVbH6CdCNjDVjfdRTecjuAgRYTsX7u7KmOf\namELDtq5trfNxjUcQ/DvPbAnwWG71vq+6KiGKViM/Y3vMQ0jCw7crw5PNkQ0DCw4dK+6PLoS\nzzCu4OB96vRsUjTDsILD96jbw2exDKN+TVLoT8enCyN9IQYVrNGZzo+PRlFMwWIEzwdHMIwp\nWKUjKVjOwiJ0+lHyhL++YQoWI0rhoG4YUbBSJ8pydGgbpmAxwiQsyoYBBWv1oDTLjq5hPMFq\n/SdOo6RqGE6wXu/J82RpGqZgMQsSoSka9iW4KpbW5GcRVih23ZJMd3rVXCL4khuTt/fBVoXH\nGyFvCwRr7vzgBV+6O1yr2/Wu1/aW9mC16liLYL2KLhCcN1JLk5/veos6dq1aVM9PlyUj1arq\nAsHdXtmYzBR2zyu4F+HKigRr1dWD4N3FT02WP9mgewlhaTphndp6EOyxNsMiHFmXYJ3qIglW\nvsq7PCG4RoUpWMwGBId5IFBWK/0f4jyk9FeoMgWL8TFmQ/g641yLVr9XwsugHMFrTcFiKFiO\noAj929n8DKsTut6+BEc/BlPwNCiCI9xx7GlgrMA1p2AxvkY+C1t1EMExnvqhYDmbEhy28hQs\nxt/glCFrj3ElK8qjtx5HHw1YfwoWAy9YyjEzu6PXIuI8O+9z/OBwLdAUXBUmO94O7fb+PbEw\nBXtDUXDVmi3Nvm7uw/y6DbsVESm/idcRwIO1QfEset/dhZk172uz81crBMHBGqEo+HEHSGGx\nPAV7Q13wqds3dxuyl1rFykDlV3CoZqjuovfP2+Pr/fcnISjYG4qC6+y1kPm+ATvVKlqSQM+C\nAzVE9VJl+dSaTWy/0qsmMILDtGT9V7Io+CurFxwvUa93wUHaEuOeLK/ZZin4OxQsxr/gEI1Z\nu+CIyfIDCA7QHAoWQ8GzK6DgWby3Z+WCYw5YQ8FytivYe4soWAwFy9mwYN9NWrfgqKMCUrCc\nLQv23CgKFkPBciyLiDsybyjBfptFwWIoWM62BXtt15oFx/VLwQvYuGCfLaNgMRQsZ+uCPTZt\nxYIj+6XgBWxesL/GUbAYCpZjU0Rsv2EFe2teOoJdb7IGF+yrfekIdi2Cgq2gYDGBBXtq4GoF\nR/dLwQug4AYvTaRgMRQsh4JbfLRxrYLj+6XgBVBwh4dWUrAYCpZDwR0UHBMFwR6auVLBCfil\n4AVQ8AMKjoiG4OUNpWAxFCznVxEp+NURvLipFCyGgqc57owpzsuKoGBrFAV3N+Lk3T05X7MJ\nU/AHC9uqLbg0ZX27XctlYzZQsDXagjPTJgVfNmZDEn4peBRnni8ff0VFbErwwtZqC94/BS8Z\ns4GC7VEVXByOZ3O6v63LRWM2bEvwsuaqCn7d1G5MVn9d9PuaKNgeze/BVXU8FkV7qlWO/do/\n2ZCGXwpeAAX3WNJgChZDwbMr+LkGCu6xMcGJ+FUUvKTJiQpeA3+xK2CHwI9Eal+fWnHL6hqv\n6Ij1puDkgyk4/aLXLVixOArWD1ctjoL1w1WLo2D9cNXiKFg/XLU4CtYPVy2OgvXDVYujYP1w\n1eIoWD9ctTgK1g8nqUPB4FAwOBQMDgWDQ8HgUDA4FAwOBYNDweBQMDgUDA4Fg0PB4FAwOBQM\njobgMjP9LACjCS7Bx5198GRJF9s2j4KrvTH7qyy4dmn0nWO/lo7RbxQEd+nwdl8muASX7YTv\nWUG+llRnlm0eBZ8dih4GX7Mu2PLjcav6jwS5dFmf8IIvJqtuVWYusxNcgiuzr5uP915UdENh\n+TTVODi7T6iLH4kbZ4L3bVhpV+9bE/pZS5cuGxBecGmaZKUnc5id4BJcdBW2kzRV0sn2KdtR\n8Kl1VH9PCjYXbFzqff8A570lXbpsQHjBhWl2S5UpZie4BD+w66iJ6Oug6xyC96ayCpwMfhwX\nrD4d98XKfgtdumy4KvcQ1xKGn12XD/PMsrXJRUU3B7OrpeBR8M7cDll7hBAEHx67aLttsBo0\n2W3779fEPcS1hACCj+ZHjuq56IM52XbTRL2L9jxJVvKxOcvKvqblnVrD3Ors1+Me4lqCf8HX\nzG5fNYpud3Jywc1J1t5qI5z6aDXYH0S3LLjOrHbQU3vZ5juOXHBzDL5afVkZBR+bXfT902G9\nCa9HcDas3GiCS3BDbvt9cBi9b/fslt00Ktqll0fBuza19o/M2p/0inHpsuF63EMc6c4Ar8Oz\n6KvDWXRv2esut71aMIx2Skc0Ue/HSiTBztvgxFm0XZcN1+Me4sih3WzO7+sDowkuwff3lvvn\niWgnwTP1vlqVPwrutkG7L9GPun5bnT1ru5Jl179fS5JeyboffevmMHqSBHfDW/zIrD1by6Sv\nZN0PPw2tl67WHxOcg/dOOd9GRfffOQYfFtT7cTXZ/tPZHy7BpcsG6xHEONL9jtKVZgYTnIPd\nkvqNiu6/cw0+5+J6P34Psgv+rKV7lw3WIwki64GCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGh\nYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LB\noWBwKBgcCgYHWnCbDm/6gX6rTHlvJAmM0mC1Fbdg1zZu0s3Osd0UnCJmXrCrMApOEQq+QQt+\nJOO5/1+arMsC2oz3cHzPup0LM0xe88w22OQefM/uls6NyR0P3tHZguA2C3DjtXhmm3rM6hJf\nDZKT5Y+0gfnn7GbpY/dP+4zASQAs+LWLzuu7nF2bBbG+1XmTFvA569Tm+O9FdYnzm+SBH7Ob\nl6zJNnsSjYwRkS0IvjzeF4+Mr4Oc0cPja6vwfZr9EmzsspAnxhYEP18/suQ9pV7Ph9EYDvv7\nPvra7bdfs5uX8r6zr+yHbUiEbQvOp9IiXu776LLd7N+z25dD5jLyUSJsS/Bg1t7sjufxIB3Z\nrvmvN/uZrLLc8RicDkPBxfsg+jFrLLg0x/ZE62O2eyrTVFhZdZ0w7Reet8tTk3X5fkJdvGdd\nbtV4HKW70/Z07GN287LrTqq5BSfDrh0D52NjzV/jB3azyscxeZhne9dlZv6Y3X0+JhdOHGTB\nl91AcHMlqxs+tJvVpBfPL+fRUAinx778Pft9JWtlfqEFkxsFw0PBLe/xWNA6BK09QiiYrBQK\nBoeCwaFgcCgYHAoGh4LBoWBwKBgcCgaHgsGhYHAoGBwKBoeCwaFgcCgYHAoGh4LBoWBwKBgc\nCgaHgsGhYHAoGBwKBoeCwfkPkuQDJnJyfToAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\06. Maximum Likelihood_16_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    " x <- 4; n<-10\n",
    " theta_vals <- seq(0,1,0.01)\n",
    " LLR <- dbinom(x,n,theta_vals,log=T)-dbinom(x,n,x/n,log=T)\n",
    " plot(theta_vals,LLR,type='l',col='blue',ylim=c(-5,0.01))\n",
    "# add additional things\n",
    "lines(x=theta_vals,y=rep(0,length(theta_vals)),lty=2)\n",
    "lines(x=rep(0.5,2),y=c(-10,0.01),col='red',lty=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the figure (red line) that when $\\theta=0.5$ the LLR is very close to 0. Values of $\\theta$ further away from the MLE than 0.5 will have a even lower LLR. So we can make qualitative statements using the LLR in relation to the MLE. \n",
    "\n",
    "Let's increase the sample size and observe what happens to the LLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAAP9NTU1oaGh8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\nDA4Fg0PB4FAwOBQMzj+0sIHwCJVAkAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\06. Maximum Likelihood_18_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    " ns<-c(10,20,40,80,160)\n",
    "# assume we have an 'unfair coin' with heads being more likely than tails\n",
    "# rather than taking a random sample, assume a sample consistent with the null hypothesis - this is so the MLE remains 0.5\n",
    " x1<-round(ns[1]*0.4,0)\n",
    " x2<-round(ns[2]*0.4,0)\n",
    " x3<-round(ns[3]*0.4,0)\n",
    " x4<-round(ns[4]*0.4,0) \n",
    " x5<-round(ns[5]*0.4,0) \n",
    " theta_vals <- seq(0,1,0.01)\n",
    " LLR01 <- dbinom(x1,ns[1],theta_vals,log=T)-dbinom(x1,ns[1],x1/ns[1],log=T)\n",
    " LLR02 <- dbinom(x2,ns[2],theta_vals,log=T)-dbinom(x2,ns[2],x2/ns[2],log=T)\n",
    " LLR03 <- dbinom(x3,ns[3],theta_vals,log=T)-dbinom(x3,ns[3],x3/ns[3],log=T)\n",
    " LLR04 <- dbinom(x4,ns[4],theta_vals,log=T)-dbinom(x4,ns[4],x4/ns[4],log=T)\n",
    " LLR05 <- dbinom(x5,ns[5],theta_vals,log=T)-dbinom(x5,ns[5],x5/ns[5],log=T)\n",
    " plot(theta_vals,LLR01,type='l',col='blue',lwd=0.5,ylim=c(-5,0.01))\n",
    "# compare to large sample sizes\n",
    " lines(theta_vals,LLR02,col='blue',lwd=1)\n",
    " lines(theta_vals,LLR03,col='blue',lwd=1.5)\n",
    "lines(theta_vals,LLR04,col='blue',lwd=2)\n",
    "lines(theta_vals,LLR05,col='blue',lwd=2.5)\n",
    "lines(x=theta_vals,y=rep(0,length(theta_vals)),lty=2)\n",
    "lines(x=rep(0.5,2),y=c(-10,0.01),col='red',lty=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At smaller sample sizes the LLR is slightly left skewed when the sample mean is 0.4. As the sample size increases you can see that the LLR becomes more symmetrical about the sample mean, and that the slope of the LLR at values away from the sample mean is steeper. We can start to see the relationship between sample size and the precision of the sample mean. Qualitatively, if we wanted to test whether the coin was fair, it is clear that a larger sample would enable us to have more confidence in our assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the fact that the data are a sample from a population distribution, we can explore what happens when multiple samples of the same size are drawn. MLE is a sample of the true parameter we can perform the above experiment multiple times and identify the parameters space where the LLR will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nHAhmDgQzB4KZ8x8x8sWN/K2koQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of mles\""
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\emsuewil\\Documents\\Work\\Teaching\\MSc_HDS\\Statistics\\Git_SHDS\\Jupyter_Book\\SHDS\\_build\\jupyter_execute\\06. Maximum Likelihood_21_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    " x <- 4; n<-10\n",
    " sampl <- rbinom(n=1000,size=n,prob=0.5)\n",
    "mles <- sampl/n \n",
    " hist(mles,col=\"red\",breaks=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram above you can see that repeating the experiment (different samples) will return different values of the MLE and corresponding LLR. The LLR ratio can be used to assess how consistent different values of the parameter are with the MLE. \n",
    "\n",
    "The principles behind the LLR also relate to construction of confidence intervals, which we will cover in Section 08. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}