{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 15.3 The logistic regression model\n",
    "\n",
    "### 15.3.1 The logit function\n",
    "\n",
    "Before introducing the logistic regression model, we introduce the $\\mathrm{logit}$ function and the concept of odds. As we said, we cannot model directly $\\pi_i=E(Y_i|X_i)=P(Y_i=1|X_i)$ by the linear predictor $\\eta_i = \\sum_{k=0}^p \\beta_k X_{i,k}$. The *logit* function is defined as follows for all $\\pi\\in(0,1)$:\n",
    "\n",
    "$$\\mathrm{logit}(\\pi) = \\log \\left(\\frac{\\pi}{1-\\pi}\\right).$$\n",
    "\n",
    "The main advantage of the *logit* function is that it maps $(0,1)$ into the set of real numbers $\\mathbb{R}$ allowing to link $\\pi_i$ and the linear predictor $\\eta_i$. \n",
    "\n",
    "The quantity $\\frac{\\pi}{1-\\pi}=\\frac{P(Y=1)}{P(Y=0)}$ where $Y$ is a binary variable which denotes the presence of an event ($Y=1$) or its absence ($Y=0$) is called the *odds* of this event. When the odds is equal to $1$, it means that $Y$ is equally likely to be to $0$ or $1$. When the odds is greater than $1$, it means that $Y$ is more likely to be equal to $1$ than it is to be equal to $0$. When the odds is less than $1$, it means that $Y$ is more likely to be equal to $0$ than it is to be equal to $1$. For example, if a patient has an odds of developing dementia of $9$ it is very likely that this patient will have a dementia status of $1$.\n",
    "\n",
    "### 15.3.2 The logistic model\n",
    "\n",
    "Thanks to the $\\mathrm{logit}$ *link function*, we can map the probability $\\pi_i=P(Y_i=1|X_i)$ to the linear predictor $\\eta_i=\\sum_{k=0}^p \\beta_k X_{i,k}$ and that actually defines the logistic regression model\n",
    "\n",
    "$$\\mathrm{logit}(P(Y_i=1|X_i))=\\sum_{k=0}^p \\beta_k X_{i,k}$$\n",
    "\n",
    "which can be equivalently written as\n",
    "\n",
    "$$P(Y_i=1|X_i)=\\frac{\\exp(\\sum_{k=0}^p \\beta_k X_{i,k})}{1+\\exp(\\sum_{k=0}^p \\beta_k X_{i,k})}$$\n",
    "\n",
    "by inverting the $\\mathrm{logit}$ function. In the dementia example, we could model the probability of dementia according to the sex of the patient with the model\n",
    "\n",
    "$$\\mathrm{logit}(\\pi_i) = \\beta_0 + \\beta_1 sex_i$$\n",
    "\n",
    "where $\\pi_i$ is the parameter of the binomial variable $Y_i|sex_i$ where $Y_i$ is the dementia status of the patient $i$. The odds of the dementia of patient $i$ is $\\frac{\\pi_i}{1-\\pi_i}$.\n",
    "\n",
    "> *Exercise:* prove the equivalence between both logistic regression model expressions: \n",
    "> $$ \\mathrm{logit}(P(Y_i=1|X_i))=\\sum_{k=0}^p \\beta_k X_{i,k} \\quad\\text{and}\\quad P(Y_i=1|X_i)=\\frac{\\exp(\\sum_{k=0}^p \\beta_k X_{i,k})}{1+\\exp(\\sum_{k=0}^p \\beta_k X_{i,k})}$$\n",
    "\n",
    "### 15.3.3 Inference of the logistic regression model\n",
    "\n",
    "Let us assume that we have $n$ observations $(y_i)$ from the random variable $Y_i$ with $i=1,\\dots,n$. We want to estimate the $p+1$ parameters $\\beta_k$ of the logistic regression model that includes the covariate $(X_i)$ whose observed values are denoted $(x_i)$. The estimation is done by maximizing the log-likelihood. We first need to derive the likelihood of the model.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L(\\beta) &= \\prod_{i=1}^n Pr(Y_i = y_i|X_i=x_i) \\\\\n",
    "&= \\prod_{i=1}^n Pr(Y_i = 1 | X_i = x_i)^{y_i}Pr(Y_i = 0 | X_i = x_i)^{1-y_i}\\\\\n",
    "&= \\prod_{i=1}^n \\left( \\frac{\\exp(\\sum_{k=0}^p \\beta_k x_{i,k})}{1+\\exp(\\sum_{k=0}^p \\beta_k x_{i,k})} \\right)^{y_i} \\left( \\frac{1}{1+\\exp(\\sum_{k=0}^p \\beta_k x_{i,k})} \\right)^{1-y_i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Taking the log of the above likelihood, we derive the following log-likelihood\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\ell(\\beta) &= \\sum_{i=1}^n y_i \\sum_{k=0}^p \\beta_k x_{i,k} - \\log(1+\\exp(\\beta^{\\top}x_i))\n",
    "\\end{aligned}$$\n",
    "\n",
    "By maximizing this log-likelihood over $\\beta$, we can estimate the parameters. Note that, contrarily to the simple linear model, the maximisation over $\\beta$ is done numerically as there exist no closed-form solution to this optimisation problem. Once the optimisation algorithm has converged, we obtain a vector of estimates $\\hat{\\beta}=(\\hat{\\beta}_k)_{k=0,\\dots,p}$ and a vector of estimated standard errors $\\hat{se}(\\hat{\\beta})$. We know from maximum likelihood theory that, from these, we can compute a $95\\%$ confidence interval for the parameters: $$\\left[\\hat{\\beta}_k - 1.96\\hat{se}(\\hat{\\beta}_k) ;~ \\hat{\\beta}_k + 1.96\\hat{se}(\\hat{\\beta}_k)\\right].$$\n",
    "\n",
    "Because the model is estimated using maximum likelihood theory, we can use the three likelihood based tests on the estimated parameters: the Wald test, the score test and the likelihood ratio test. From the likelihood theory, we know that these tests are asymptotically equivalent. The likelihood ratio test is often used to compare nested models and for null hypothesis that entails testing the nullity of several parameters simultaneously. The Wald test is often used to test separately the nullity of each parameter and both its statistics and p-value are often given as an output of the estimation of a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}