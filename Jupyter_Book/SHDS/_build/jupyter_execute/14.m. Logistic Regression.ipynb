{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 14.12 Common pitfalls\n",
    "\n",
    "### 14.12.1 Perfect separation\n",
    "\n",
    "Perfect separation happens when the outcome can be directly predicted from one of the predictor variables. For example, let say that we model an outcome $Y$ using one explanatory standard gaussian variable $X_1$ and that $Y$ is such that $Y=0$ whenever $X_1\\leq0$ and $Y=1$ whenever $X1>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x1 <- rnorm(1000, 0, 1)\n",
    "y <- (x1 <= 0)*1\n",
    "\n",
    "data_sep <- data.frame(y,x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let us try to estimate this logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "model_sep <- glm(y ~ x1, data = data_sep, family = binomial(link=\"logit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "`R` detects the perfect separation and prompts an error that states that `fitted probabilities numerically 0 or 1 occured`. The reason of this error is that, due to the perfect separation, the maximum likelihood of the parameter $\\beta_1$ for the variable $X_1$ can not be estimated as its value is actually infinite thus leading to the prompted error. There exists some options to consider when facing this issue among which:\n",
    "\n",
    "* removing the problematic variable from the model\n",
    "* setting $\\beta_1$ at an arbitrary high value and estimate the model\n",
    "* changing the model or manipulating the data\n",
    "\n",
    "Note that, in practice, perfect separation is not very likely to happen. However, *quasi_perfect* separation is totally possible and needs to be tackled. For more details about how to handle separation, one can read the following articles:\n",
    "\n",
    "> *Heinze, G., & Schemper, M. (2002). A solution to the problem of separation in logistic regression. Statistics in Medicine* \n",
    ">   \n",
    "> *Firth, D. (1993). Bias Reduction of Maximum Likelihood Estimates. Biometrika*\n",
    "\n",
    "### 14.12.2 Low events per variable\n",
    "\n",
    "A common issue when estimating logistic regression model is the problem of the ratio between the number of events and the number of predictive variables. This ratio is known as *Events Per Variable* When this ratio is low, it can lead to biased estimation and model with poor predictive abilities.\n",
    "\n",
    "If the biomedical literature, the so-called *ten events per variable rule* is commonly used. However, we emphasize here the absence of theoretical justification and even the lack of actual evidence that this rule gives good results. If you want more information about the issues raised by this commonly used rule, you can read the following article:\n",
    "\n",
    "> *Smeden, M., de Groot, J.A., Moons, K.G. et al. (2016) No rationale for 1 variable per 10 events criterion for binary logistic regression analysis. BMC Med Res Methodol*.\n",
    "\n",
    "\n",
    "### 14.12.3 Influential values\n",
    "\n",
    "Another aspect to take into account when estimating a logistic regression model is the presence of influential values among the observations which, as their names indicates, might have a huge impact on the estimation of the model. The Cook's distance is a useful measure to assess how influential an observation is. It measures how much the outcome would be modifier by removing this observation from the data.\n",
    "\n",
    "In, `R`, the Cook's distance can be easily plotted and directly plotted by specifying `which = 4` as an argument to the `plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in plot(dementia2, which = 4): object 'dementia2' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in plot(dementia2, which = 4): object 'dementia2' not found\nTraceback:\n",
      "1. plot(dementia2, which = 4)"
     ]
    }
   ],
   "source": [
    "plot(dementia2, which = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above example, some observations seems to have higher influence than the other. However, if we look at the y-axis scale, this difference is not huge. It is important to be careful when using this plot for several reasons. As we have seen, the scale of the y-axis can have a very reduced range. Also, if some outliers seems to be identified, additional analysis should be performed for these apparent outliers before ruling them as actual influential values. If they happen to be, one of the solution is to remove them from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 14.12.4 Multicolinearity\n",
    "\n",
    "Multicolinearity arises when one or several predictor variables can be described as a linear combination of a set of other predictor variables. In the case of perfect multicolinearity, the model becomes unidentifiable meaning that there no unique set of parameters can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "x1 <- rnorm(1000, 0, 1)\n",
    "x2 <- rnorm(1000, 0, 1)\n",
    "x3 <- rnorm(1000, 0, 1)\n",
    "x4 <- x1 + x2\n",
    "prob = exp(x1 + x2 + x3 + x4)/(1+exp(x1 + x2 + x3 + x4))\n",
    "y <- rbinom(1, 1000, prob)\n",
    "\n",
    "data_multicol <- data.frame(y,x1,x2,x3,x4)\n",
    "model_multicol <- glm(y ~ x1 + x2 + x3 + x4, data = data_sep, family = binomial(link=\"logit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We obtain the same error message that we got in the *perfect separation* case which makes sense as both issues leads to unidentifiable model and the impossibility of converging towards a unique solution. In less severe cases of multicolinearity, the model estimation sometimes converge but some parameters estimate might be biased and their standard error estimates can have high values meaning that there is a lot of uncertainty on their actual values.\n",
    "\n",
    "There exist several measures of the interrelationship between the predictor variables. Two of the most commonly used are the *tolerance* and the *variance inflation factor* (VIF). The tolerance is defined as $1-R^2_j$ where $R^2_j$ is the $R^2$ of the regression of the covariate $X_j$ on all other covariates and informs and the VIF is defined as the reciprocal of the tolerance. Tolerance ranges between $0$ and $1$ and a low tolerance is an indicator of a multicolinearity issue. As a consequence, a high VIF also is an indicator of a multicolinearity issue. As every such indicator, several rules of thumb are being used by scientists for these indicators as the following article describes.\n",
    "\n",
    "> Oâ€™brien, R. M. (2007). A Caution Regarding Rules of Thumb for Variance Inflation Factors. Quality & Quantity\n",
    "\n",
    "The VIF can be computed in `R` using the function `vif` from the `car` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "also installing the dependency 'lme4'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  There are binary versions available but the source versions are later:\n",
      "     binary   source needs_compilation\n",
      "lme4 1.1-26 1.1-27.1              TRUE\n",
      "car  3.0-10   3.0-11             FALSE\n",
      "\n",
      "  Binaries will be installed\n",
      "package 'lme4' successfully unpacked and MD5 sums checked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\emsuewil\\AppData\\Local\\Temp\\Rtmpc74Xy0\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing the source package 'car'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: carData\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'carData' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "install.packages(\"car\", repos='http://cran.us.r-project.org')\n",
    "library(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>x1</dt>\n",
       "\t\t<dd>1.34133018633163</dd>\n",
       "\t<dt>x2</dt>\n",
       "\t\t<dd>4.3374665035143</dd>\n",
       "\t<dt>x3</dt>\n",
       "\t\t<dd>2.17260928250927</dd>\n",
       "\t<dt>x4</dt>\n",
       "\t\t<dd>2.94543927452314</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[x1] 1.34133018633163\n",
       "\\item[x2] 4.3374665035143\n",
       "\\item[x3] 2.17260928250927\n",
       "\\item[x4] 2.94543927452314\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "x1\n",
       ":   1.34133018633163x2\n",
       ":   4.3374665035143x3\n",
       ":   2.17260928250927x4\n",
       ":   2.94543927452314\n",
       "\n"
      ],
      "text/plain": [
       "      x1       x2       x3       x4 \n",
       "1.341330 4.337467 2.172609 2.945439 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vif(model_multicol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "When multicolinearity arises, different options might be considered. A first is to remove from the covariates highly correlated ones, i.e. those with high VIF. A second option is to proceed to a dimensionality reduction approach as a preliminary step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}