{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.20 Proofs\n",
    "\n",
    "### 12.20.1 Proof for the ordinary least squares estimates in simple linear regression\n",
    "\n",
    "Recall the ordinary least square estimates of the intercept ($\\beta_0$) and slope ($\\beta_1$) in simple linear regression are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_0}\\bar{x} \\\\\n",
    "\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^2 (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x_i})^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "To solve for the value of $\\beta_0$ that minimises $SS_{RES}$, we differentiate $SS_{RES}$ with respect to $\\hat{\\beta}_0$ and set the differential to zero:\n",
    "\n",
    "$$ \n",
    "\\frac{d(SS_{RES})}{d(\\hat{\\beta}_0)} = \\sum_{i=1}^n -2(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)=0\n",
    "$$\n",
    "\n",
    "Since $\\sum_{i=1}^n (y_i)=n\\bar{y}$ and $\\sum_{i=1}^n(x_i)=n\\bar{x}$, we can simplify to:\n",
    "\n",
    "$$\n",
    "-n\\bar{y}+n\\hat{\\beta}_0+n\\hat{\\beta}_1\\bar{x}=0\n",
    "$$\n",
    "\n",
    "Rearranging the above and divide by $n$ to give:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}.\n",
    "$$\n",
    "\n",
    "To solve for the value of $\\beta_1$ that minimises $SS_{RES}$, we have to differentiate with respect to $\\hat{\\beta}_1$. First, we substitute in our solution for $\\hat{\\beta}_0$ as follows: \n",
    "\n",
    "$$\n",
    "SS_{RES}=\\sum_{i=1}^n(y_i-(\\bar{y}-\\hat{\\beta}_1\\bar{x})-\\hat{\\beta}_1x_i)^2=\\sum_{i=1}^n ((y_i-\\bar{y})-\\hat{\\beta}_1(x_i-\\bar{x}))^2\n",
    "$$\n",
    " \n",
    "Now differentiating the above with respect to $\\hat{\\beta}_1$ and setting the differential to zero gives: \n",
    "\n",
    "$$\n",
    "\\frac{d(SS_{RES})}{d(\\hat{\\beta}_1)} = \\sum_{i=1}^n -2(x_i-\\bar{x})(y_i-\\bar{y})+2\\hat{\\beta}_1(x_i-\\bar{x})^2=0\n",
    "$$\n",
    "\n",
    "Rearranging gives:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1\\sum_{i=1}^n (x_i-\\bar{x})^2 = \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n",
    "$$\n",
    "\n",
    "### 12.20.2 Proof that the OLS estimates are also the maximum likelihood estimates\n",
    "\n",
    "If $Y_i \\sim NID(\\mu, \\sigma^2)$, the log likelihood function is:\n",
    "\n",
    "$$\n",
    "l(\\mu | y_1,...,y_n) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "So, for the simple linear regression model:\n",
    "\n",
    "$$\n",
    "l(\\beta_0, \\beta_1 | y_1,...,y_n) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\beta_1x_i-\\beta_0)^2.\n",
    "$$ \n",
    "\n",
    "Therefore, for any fixed positive value for $\\sigma^2$, maximising the log likelihood function is equivalent to minimising $SS_{RES}$ and so the OLS estimates are also maximum likelihood estimates of $\\beta_0$ and $\\beta_1$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}