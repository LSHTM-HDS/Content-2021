%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=0,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Preamble}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Statistics for Health Data Science}
\date{Sep 19, 2021}
\release{}
\author{MSc Health Data Science, LSHTM}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{00. Welcome::doc}}


\sphinxAtStartPar
These notes provide the core material for the MSc module, Statistics for Health Data Science.

\sphinxAtStartPar
This is a compulsory module for the programme MSc Health Data Science. The module provides an introduction to the key statistical concepts and methods for health data science. Topics covered include probability, initial data description and
exploration, frequentist and Bayesian approaches to statistical inference and  regression modelling. These topics provide the framework needed for subsequent modules. The module places a focus on learning through practical examples and incorporates directed learning, lectures, group discussion, and computer practical exercises.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large 1.1 Overall aim of the module}
\end{DUlineblock}

\sphinxAtStartPar
The overall module aims are to introduce:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the motivation and critical thinking towards solving a question in health science through interrogation of data and drawing conclusions from evidence;

\item {} 
\sphinxAtStartPar
the principles of probability, regression modelling and statistical inference within frequentist and Bayesian frameworks.

\end{itemize}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large 1.2 Module Intended Learning Outcomes}
\end{DUlineblock}



\sphinxAtStartPar
Upon successful completion of the module you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
evaluate the application of different probability distributions to model health data (including Poisson, Binomial and Normal);

\item {} 
\sphinxAtStartPar
critically analyse frameworks for frequentist and Bayesian inference and evaluate their strengths, limitations and differences;

\item {} 
\sphinxAtStartPar
examine the concepts of sampling variability, estimators, bias, confidence intervals and credible intervals;

\item {} 
\sphinxAtStartPar
examine the theoretical basis of linear regression and generalized linear models;

\item {} 
\sphinxAtStartPar
assess the application of regression modelling to address specific health data science questions;

\item {} 
\sphinxAtStartPar
critically evaluate strengths and limitations of different statistical methods, including regression models, within a health data science project;

\item {} 
\sphinxAtStartPar
draw conclusions from the results of a data analysis and justify those conclusions, appropriately acknowledging uncertainty in the results.

\end{itemize}



\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large 1.3 Module Content}
\end{DUlineblock}

\sphinxAtStartPar
The module is split into 16 taught sessions, each building statistical knowledge for health data science. The sessions are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Introduction

\item {} 
\sphinxAtStartPar
Probability and Discrete Probability Distributions

\item {} 
\sphinxAtStartPar
Continuous Probability Distribution

\item {} 
\sphinxAtStartPar
Populations and Sampling

\item {} 
\sphinxAtStartPar
Likelihood

\item {} 
\sphinxAtStartPar
Maximum Likelihood Estimation

\item {} 
\sphinxAtStartPar
Frequentist Inference I

\item {} 
\sphinxAtStartPar
Frequentist Inference II

\item {} 
\sphinxAtStartPar
Bayesian Statistics I

\item {} 
\sphinxAtStartPar
Bayesian Statistics II

\item {} 
\sphinxAtStartPar
Types of Investigation

\item {} 
\sphinxAtStartPar
Linear Regresion I

\item {} 
\sphinxAtStartPar
Linear Regresion II

\item {} 
\sphinxAtStartPar
Linear Regresion III

\item {} 
\sphinxAtStartPar
Logistic Regression

\item {} 
\sphinxAtStartPar
GLMs and Poisson Regression

\end{enumerate}

\sphinxAtStartPar
A final short section (17) connects the regression models to the session regarding types of investigation. This is optional reading and does not have an accompanying taught session.


\part{Preamble}


\chapter{Acknowledgements}
\label{\detokenize{00. Acknowledgements:acknowledgements}}\label{\detokenize{00. Acknowledgements::doc}}
\sphinxAtStartPar
Many people have contributed to this document over time, including a large number of previous and current members of the Department of Medical Statistics at the London School of Hygiene and Tropical Medicine. In particular, we would like to acknowledge contributions from Corentin Segalas, Elizabeth Williamson, Emily Granger, Emily Nightingale, Kathleen O’Reilly, Linda Sharples, Melanie Smuk, Mia Tackney, Nicholas Jewell and Ruth Keogh.

\sphinxAtStartPar
We thank Jennifer Nicholas, whose notes  were particularly useful in the development of the linear regression sessions.  We thank Katy Morgan for notes which helped inform the development of the section about inference for maximum likelihood models.

\sphinxAtStartPar
The notes for the Bayesian Inference sessions are heavily based on the Foundations course material created by Alex Lewin and Alexina Mason, which was previously developed by James Carpenter, Marcel Zwahlen and Beat Neuenschwander. Some sections are inspired also by notes from Michail Papathomas. We are grateful for their work and permission to re\sphinxhyphen{}use.


\section{Version}
\label{\detokenize{00. Acknowledgements:version}}
\sphinxAtStartPar
This document was last updated: September 2021

\sphinxAtStartPar
Inevitably there will be some typos in these notes. Please do let us know any you spot (at: \sphinxurl{mailto:mscHDS@lshtm.ac.uk}) and we will correct them.


\chapter{How to use this book}
\label{\detokenize{00. How_to_use:how-to-use-this-book}}\label{\detokenize{00. How_to_use::doc}}
\sphinxAtStartPar
This book contains the core content for the module. It is designed to be read in conjunction with the practical sessions and accompanying videos.

\sphinxAtStartPar
Each numbered session has an accompanying lecture and practical session. Each section of the book additionally has a short summary of the contents of that section to help you build an overview of the material. The final sections of the book do not have accompanying taught sessions but have some brief comments to help you pull all the material together.


\section{R code}
\label{\detokenize{00. How_to_use:r-code}}
\sphinxAtStartPar
Analyses are illustrated using R code. Each page which contains R code should be self\sphinxhyphen{}contained. You should be able to copy and paste each chunk of R code (using the little icon to the top\sphinxhyphen{}right of the code cell) into R and run all the code in that page.

\sphinxAtStartPar
A number of packages will be required. These are loaded using the \sphinxcode{\sphinxupquote{library}} function, at the appropriate points. If you are using your local version of R you may need to install these packages. Below, you will find code to install all packages used in these notes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Graphics package}
\PYG{n+nf}{install.packages}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{ggplot2\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Will be used for fitting some generalised models:}
\PYG{n+nf}{install.packages}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{VGAM\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Contains some useful goodness\PYGZhy{}of\PYGZhy{}fit diagnostics:}
\PYG{n+nf}{install.packages}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{pscl\PYGZdq{}}\PYG{p}{)}

\PYG{n+nf}{install.packages}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{sandwich\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\part{Overview}


\chapter{1 Introduction}
\label{\detokenize{01. Introduction:introduction}}\label{\detokenize{01. Introduction::doc}}
\sphinxAtStartPar
Statistics for Health Data Science is the scientific approach behind investigating health. The organisers of this module have been specific in the wording. Of course, we will learn techniques to interogate data using statistics! But also, the focus on the approach is to think about a problem scientifically, for example to consider a research question, or a hypothesis. The scientific approach is important, and is described in more detail in this Introduction and the associated lecture. The scientific inquiry is applied to health data, which can take a number of forms, including ‘found data’. We think this is what makes data science for health unique; found data presents great advantages as there may be a lot of found data avaialble, but also challenges as the origin of the data and the potential for biases in the data can make analysis more challenging.


\begin{itemize}
\item {} 
\sphinxAtStartPar
consider the concept of Statistics for Health Data Science and the bigger picture of scientific inquiry

\item {} 
\sphinxAtStartPar
understand the data by identifying broad issues of structure, type, provenance and design

\item {} 
\sphinxAtStartPar
describe variable types

\item {} 
\sphinxAtStartPar
understand the concept of selection bias

\item {} 
\sphinxAtStartPar
think about data summaries using exploratory data analysis and visualizations (simple examples)

\item {} 
\sphinxAtStartPar
consider what to measure and why in a scientific study

\item {} 
\sphinxAtStartPar
have a basic understanding of the difference between frequentist (Fisher) and Bayesian inference

\end{itemize}




\section{1.1 What you will learn}
\label{\detokenize{01. Introduction:what-you-will-learn}}
\sphinxAtStartPar
By engaging with module you will acquire skills that a data scientist will need to interogate data to answer a health related question. Much of the focus is on the statistical tools that are most often used, as shown above in the \sphinxstyleemphasis{Intended Learning Objectives}. The module is designed towards using statistics within a problem solving cycle (more in Section 1.3).

\sphinxAtStartPar
Consider this book to be a \sphinxstyleemphasis{practical guide} in using statistics. Every session provides some statistical theory and examples so you can see the theory in action. Especially in the earlier sessions, some of the examples can be done without using a computer. As the module progresses many of the calculations are carried out using R, and we will increasingly apply the concepts to real data. We provide the code for each example.

\sphinxAtStartPar
As you work through the sessions, your ability to use statistics in health data science should improve. We will begin with relatively simple questions that we want to answer. As the module progresses, the questions will be more relevant to a health, and the steps involved will require more statistical inference and scientific inquiry.


\section{Module layout}
\label{\detokenize{01. Introduction:module-layout}}
\sphinxAtStartPar
The module is roughly divided into three sections: \sphinxstylestrong{Basic Probability}, \sphinxstylestrong{Statistical inference} and \sphinxstylestrong{Statistical modelling}. Each sections build upon the previous. Probability is perhaps an obvious underpinning of statistics; it gives us the \sphinxstyleemphasis{building blocks} for both statistical inference and modelling. In probability (sessions 2 and 3) we cover discrete and continuous distributions, and make use of the \sphinxstyleemphasis{maths refresher} in several sessions. Fundamentally, when dealing with data we often need to make assumptions about what \sphinxstyleemphasis{distribution} the data is drawn from, and knowing the properties of these distributions then enables us to carry out statistical inference. When we move to statistical inference (sessions 4 to 10) this gives us the understanding of how statistical theory can be used to make statements during our investigations. An important consideration is thinking about the statistical theory that enables us to invesitgate our data, but then use this knowledge to then make statements about the wider population (or target population). With this knowledge we then move to Bayesian statistics, where we apply our knowledge to specific health questions and make use of prior knowledge. We then move onto applications that are more likely to be encountered in health applications; and consider the process of investigation and statistical modelling (sessions 11 to 17). These sessions are linked together because as well as being able to run a model and generate results, it is very important to articulate why it was done, ie. identify the purpose of investigation. In the sessions on regression modelling several classes of model will be described and illustrated in detail.

\sphinxAtStartPar
At the end of the module there will be a revision session, and an assessment (more detail is provided on Moodle).


\section{1.2 What you won’t learn}
\label{\detokenize{01. Introduction:what-you-won-t-learn}}
\sphinxAtStartPar
We provide important aspects of statistical theory in order for you to understand the reasoning behind the approaches, but there are aspects of statistical theory that are outside of the module scope. In this case, we may provide further reading. Other modules within the LSHTM may cover this in more detail, such as \sphinxstyleemphasis{Foundations of Medical Statistics}.

\sphinxAtStartPar
This module provides the basics that further modules in the MSc may require, such as \sphinxstyleemphasis{Data Challege} and \sphinxstyleemphasis{Machine Learning}. As the name might imply, the statistical techniques used in machine learning are covered in the other module.

\sphinxAtStartPar
The programming associated with this module is carried out in R. The statistical analysis can be carried out in other software, such as Python. In some of the later sessions, we will provide the equivalent Python code, but we do not expect you to use this (and you will not be assessed on this).


\section{1.3 The Data Science Project}
\label{\detokenize{01. Introduction:the-data-science-project}}
\sphinxAtStartPar
Once a health related question is posed, this could be the start of a Data Science Project. Typically, we try to frame the question around a scientific hypothesis, identify some data that can be used to address this question/hypothesis, carry out some anlaysis and draw a conclusion. In David Spiegelhalter’s Book \sphinxstyleemphasis{The Art of Statistics} this process is referred to as the PPDAC cycle (fig. 1.1). If doing data science is new to you, this might be considered a linear process. But, in many circumstances the problem solving process is a cycle, where a problem may be solved using a iterative process. The iterative process doesn’t mean that the first attempt was \sphinxstyleemphasis{wrong}, but instead this iterative way of thinking enables the data scientist to think critically about each stage of the cycle and identify strengths, weaknesses and opportunities for improvement.

\sphinxAtStartPar
Note that there are many ways to describe the cycle of a Data Science Project, and more examples are given in the lecture by Prof. Nick Jewell. Some might chime with you more (or less) than the PPDAC presented here.

\sphinxAtStartPar
\sphinxincludegraphics{{01_intro_PPDAC_adapt}.png}

\sphinxAtStartPar
 Fig. 1.1 The PPDAC cycle (From \sphinxstyleemphasis{The Art of Statistics} by David Spiegelhalter) 


\subsection{1.3.1 Identify the problem and generate a hypothesis}
\label{\detokenize{01. Introduction:identify-the-problem-and-generate-a-hypothesis}}
\sphinxAtStartPar
Typically Health Data Science projects start with a question. The question may be framed around one of three investigation types: \sphinxstylestrong{description}, \sphinxstylestrong{prediction} and \sphinxstylestrong{causality and explanation}. The specific of this is explored in more detail in Session 11 (Types of Investigation).

\sphinxAtStartPar
When developing a Data Science Project there is a need to create a question that is answerable within the timeframe available, and sufficiently precise. It is also preferable to frame a question around a hypothesis that can be a testable prediction, and this is where statistics can be used, because a lot of statistics are framed using a hypothesis (this is especialy true of frequentist statistics). However, it is not always necessary to have a hypothesis, for example if the question is exploratory.


\subsection{1.3.2 Develop a plan, consider the data design}
\label{\detokenize{01. Introduction:develop-a-plan-consider-the-data-design}}
\sphinxAtStartPar
The plan to answer the question/hypothesis will involve some data. For data science it is likely that the data has not been collected specifically for the purposes of answering the question. Examples may include surveillance data for infectious diseases (eg. self\sphinxhyphen{}reported cases of influenza\sphinxhyphen{}like illness to a public website) or internet searches for “sore throat”. In this case, it is important to recognise specific attributes of the data:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Where did the data come from? What is its provenance?

\item {} 
\sphinxAtStartPar
How and why was the data collected?

\item {} 
\sphinxAtStartPar
What kind of individuals provided data, and why were they selected?

\end{itemize}

\sphinxAtStartPar
These are important questions because they relate to the principles of \sphinxstyleemphasis{statistical inference}, which is covered in sessions 4\sphinxhyphen{}10. Central to using data to draw a conclusion is that your \sphinxstyleemphasis{sample} data is representive of the \sphinxstyleemphasis{population}. Consequently, we can carry out an analysis on the data and make statements about the wider population. This is covered in more detail in session 4 (\sphinxstyleemphasis{Populations and Samples}).

\sphinxAtStartPar
At this stage it is important to identify the “outcome” variable and the “explanatory variables” present in the dataset, and whether we know already that some explanatory variables are associated with the outcome variable. It is also a good idea to idenitfy what type of data each variable corresponds to: continuous, ordinal, categorical.

\sphinxAtStartPar
The design of the dataset is also important, as this helps us understand the structure of the data, and a framework for analyses on the data. Commonly encountered designs are (note that these are also covered in the \sphinxstyleemphasis{Epidemiology for Health Data Science} module):
\begin{itemize}
\item {} 
\sphinxAtStartPar
A cross\sphinxhyphen{}sectional design

\item {} 
\sphinxAtStartPar
A cohort design

\item {} 
\sphinxAtStartPar
An outcome\sphinxhyphen{}based design

\item {} 
\sphinxAtStartPar
A longitudinal design

\end{itemize}

\sphinxAtStartPar
At this stage, you may start to consider the appropriate analysis to make considering the data. As the module (and others, for example the \sphinxstyleemphasis{Data Challenge} module) develops you will identify the analysis steps that can be undertaken according to the question.


\subsection{1.3.3 The data}
\label{\detokenize{01. Introduction:the-data}}
\sphinxAtStartPar
There are several aspects of the data that need to be considered, and some of which are covered in the module \sphinxstyleemphasis{Health Data Management}, such as entering the data, managing the data, and cleaning the data.

\sphinxAtStartPar
Here we will focus on aspects which might affect the analysis and conclusions that you make later in the PPDAC cycle.

\sphinxAtStartPar
The first is the presence of potential data filtering, ie. is there any reason to suspect that data are missing or censored, in reference to to wider population? If so, this could result in potential bias. The most commonly encountered bias is \sphinxstyleemphasis{selection bias}, where extrapolation to the wider population may be challenging. Additionally, \sphinxstyleemphasis{collider bias} may result in inappropriate conclusions being made on the effect of explanaotry variables on the outcome.

\sphinxAtStartPar
The second consideration is confounding, where there may be a common cause for both a explanatory variable and the outcome. The result is that an assoication between the explanatory variable and the outcome may be identified, but the relationship is not causal.


\subsection{1.3.4 Data analysis}
\label{\detokenize{01. Introduction:data-analysis}}
\sphinxAtStartPar
Exploratory data anlaysis, and especially \sphinxstylestrong{plotting your data} is a really important part of the Data Science Project. As you progress through the module, this will become more and more familiar. Plotting your data is important to \sphinxstyleemphasis{sense check} the data and identify any errors, outliers or omissions (this is especially important with found data). Further to this, many statistical anlaysis benefit from plotting the results, for example by plotting the residuals of a linear model against the outcome to check that the model is correctly specified. Often, suitable plots may carry with them \sphinxstyleemphasis{parameter estimates} from the data, for example the mean number of influenza\sphinxhyphen{}like illnesses reported per week when the data are available daily.

\sphinxAtStartPar
It is at this stage that you do the analysis. This is where the concepts covered in this \sphinxstyleemphasis{statistics module} become useful. What we want to emphasize here is that this is done while considering all the others factors within the PPDAC cycle.


\subsection{1.3.5 Conclusions}
\label{\detokenize{01. Introduction:conclusions}}
\sphinxAtStartPar
So you’ve gotten this far! An ideal conclusion has brought all the other aspects together, and at most stages some form of statistical inference is considered. The conclusion then needs to consider the statistical result \sphinxstyleemphasis{in the context of the other considerations}, such as wanting to make inference about the population from the sample of data.

\sphinxAtStartPar
For example, let’s say the influenza\sphinxhyphen{}like illness data from the internet reported 40 cases per 100,000 of the population from November to January. Reporting symptoms might be skewed towards people who regularly use the internet, which might exclude elderly individuals. Consequently, this mean estimate may be an under\sphinxhyphen{}estimate of the population incidence due to selection bias.


\section{1.6 Why we teach both frequentist and Bayesian statistics}
\label{\detokenize{01. Introduction:why-we-teach-both-frequentist-and-bayesian-statistics}}
\sphinxAtStartPar
A majority of the module covers statistical inference from the \sphinxstyleemphasis{frequentist} perspective. Much of frequentist statistical inference was developed by Ronald Fisher, who has been described as the founder of modern statistics, and much of his focus was on experimental design in agriculture. A simple explanation of the philosophy behind frequentist statistics is that a \sphinxstyleemphasis{fact} is either true or not true, and data can be used to assess which of these outcomes can be accepted. In contrast, Bayesian statistics suggests that a probability can be assigned to whether the fact is true. The field of Bayesian statistics is named after Reverend Thomas Bayes, who developed the theory almost 200 years before Fisher was alive (and owing to improvements in computation the theory is now much more accessible and has overtaken frequentist approaches in some scientific fields). In addition, frequentist statistics makes use of the data available, and there is little (or any) ability to incorporate additional knowledge. Within a Bayesian framework, inclusion of prior knowledge is inherent, and this prior knowledge can be combined with data.

\sphinxAtStartPar
Some argue that the philosophies are diametrically opposed to each other, and statisticians should choose a side. This is a strong view (and perhaps not the majority view?), but first it is important to understand the principles behind each approach. We have opted to teach both because of this reason, and leave it up to you to consider the advantages and disadvantages of each approach. Ultimately, both have data central to the approach in making statistical inference, so it is likely that both should be considered as perspectives in a Data Science Project.


\part{Basic probability}


\chapter{Probability and statistics}
\label{\detokenize{02. Probability.Intro:probability-and-statistics}}\label{\detokenize{02. Probability.Intro::doc}}
\sphinxAtStartPar
We assume that students are already familiar with the basic probability concepts covered in the pre\sphinxhyphen{}course \sphinxhref{https://lshtm-hds.github.io/Math-Refresher}{Refresher}.


\section{How does probability relate to statistics?}
\label{\detokenize{02. Probability.Intro:how-does-probability-relate-to-statistics}}
\sphinxAtStartPar
Probability theory describes the chance of an event occurring while statistics concerns the collection, organization, analysis and interpretation of data. However, probability theory and statistics are intrinsically linked.

\sphinxAtStartPar
A typical \sphinxstylestrong{probability} problem is as follows. We are planning to run a small clinical study, which involves giving 8 patients a particular drug. We are told that the probability that a single patient experiences a side effect from a particular drug is 0.23. From this information, we can calculate the probability of various complex events occurring. For example, we might want to know the probability that more than 6 of the 8 patients will experience a side effect. Or we might wish to know the probability that none of the 8 patients experience a side effect. Here, we are assuming that a characteristic (parameter) of the population is known. Specifically, we are assuming that we know the true probability of a single patient experiencing a side effect.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{Probability_vs_inference}.png}
\end{figure}

\sphinxAtStartPar
This is not how real life works! Typically, in health data science studies, we have observed some data which we believe can be modelled using a particular distribution (such as the binomial distribution that we will soon meet), but the parameters of that distribution are unknown. For the small clinical study, for example, in real life we would run the study and observe how many of the 8 patients did in fact experience a side effect. But the probability of a patient experiencing a side effect would be unknown. The study aim would be to use the observed data to make statements \sphinxhyphen{} \sphinxstylestrong{inferences} \sphinxhyphen{} about this unknown probability. So in some senses, the problem is the opposite way round.

\sphinxAtStartPar
It turns out that the process of statistical inference relies very heavily on probability calculations. Suppose we conduct our small clinical study and, for example, observe that in fact 2 of the 8 patients experience the side effect. Loosely speaking, the process of inference involves the following steps. We use probability theory to calculate the probability that 2 people within our sample of 8 experience the side effect, for every possible value of the unknown probability of experiencing a side effect. We then use these probabilities to make statements about plausible values of the unknown probability. As we will see, we can take various approaches to this inference, in particular using the frequentist or Bayesian frameworks.

\sphinxAtStartPar
Therefore, the next two lectures, concerning probability theory, comprise building blocks that you will need when you subsequently meet ideas about likelihood, inference and regression modelling.


\chapter{2. Discrete Distributions}
\label{\detokenize{02.a. Probability.Discrete:discrete-distributions}}\label{\detokenize{02.a. Probability.Discrete::doc}}
\sphinxAtStartPar
This session is the first of two sessions covering useful elements and applications of basic probability. In this first session, we focus on variables which have a \sphinxstylestrong{discrete} distribution. In the next session, we extend these ideas to variables which have a \sphinxstylestrong{continuous} distribution.


\begin{itemize}
\item {} 
\sphinxAtStartPar
apply Bayes’ Theorem to obtain useful properties of screening tests

\item {} 
\sphinxAtStartPar
derive the binomial and Poisson probability distribution functions

\item {} 
\sphinxAtStartPar
apply the binomial and Poisson distributions to health settings

\item {} 
\sphinxAtStartPar
evaluate the appropriateness of the binomial and Poisson distributions to health settings

\end{itemize}



\sphinxAtStartPar
The first part of this session explores Bayes’ Theorem. This is a crucial probability theorem, which underlies the Bayesian approach to inference. Another important use is its application to quantify how well a screening test or prognostic classification tool is performing. In this session, we focus on this latter application but you will return to Bayes’ Theorem in the later sessions about Bayesian statistics.

\sphinxAtStartPar
The second and third parts of this lecture explore the binomial distribution and the Poisson distribution. For these sessions, we will assume we know characteristics of the population (e.g. the true prevalence of a disease, or incidence rate of a disease) and will explore how to calculate probabilities of various outcomes occurring. In subsequent sessions we will see how these sorts of calculations are used within the important area of statistical inference.


\section{2.1 Application of Bayes’ Theorem}
\label{\detokenize{02.b. Probability.Discrete:application-of-bayes-theorem}}\label{\detokenize{02.b. Probability.Discrete::doc}}
\sphinxAtStartPar
Bayes’ Theorem has important and powerful applications in medical statistics. One important link, which you will return to later, is its connection to Bayesian statistics. In this session, we focus on another common application of Bayes’ theorem, in the area of assessing the accuracy of screening tests and prognostic scores.


\subsection{2.1.1 Screening tests and prognostic scores}
\label{\detokenize{02.b. Probability.Discrete:screening-tests-and-prognostic-scores}}
\sphinxAtStartPar
Screening tests are tests that attempt to identify people with a particular condition or disease of interest. Babies are often screened for cystic fibrosis at birth, for example. Sometimes, screening tests attempt to identify high risk people rather than those who already have the condition of interest. Cervical screening, for example, which is offered to all women and people with a cervix aged 25 to 64 in the UK by the National Health Service, is a test to help prevent cancer. It doesn’t look for existing cancer, but instead looks for certain viruses which can increase the subsequent risk of cancer. Similarly, prognostic tests or prognostic scores are used to identify a high risk group.

\sphinxAtStartPar
Screening tests or prognostic scores can be based on one genetic marker, as in our example below, or many. Or they might incorporate information from other sources (e.g. biomarkers, family history of the disease). These processes typically result in a binary classification of “positive” or “negative”.


\subsection{2.1.2 Bayes’ Theorem}
\label{\detokenize{02.b. Probability.Discrete:bayes-theorem}}
\sphinxAtStartPar
Suppose we have an event \(A\) and a set of events \(B_1, B_2, ..., B_n\) that partition the sample space. Suppose that we have information about the conditional probability of \(A\) conditional on event \(B_j\), i.e. we know \(P(A | B_j)\), for each \(j\). However, what we actually want to know about is \(P(B_j | A)\).

\sphinxAtStartPar
Bayes’ Theorem provides a way of reversing the conditioning.



\sphinxAtStartPar
 Bayes’ Theorem:
\begin{equation*}
\begin{split}
P(B_{j}|A) = \frac{P(A|B_{j}) P(B_{j})}{P(A)} = \frac{P(A|B_{j}) P(B_{j})}{\sum^{n}_{k=1} P(A|B_{k}) P(B_{k})}.
\end{split}
\end{equation*}



\subsection{2.1.3 Example: Genetic marker in childhood cancer}
\label{\detokenize{02.b. Probability.Discrete:example-genetic-marker-in-childhood-cancer}}
\sphinxAtStartPar
In a population, 10\% of people develop a particular childhood cancer.  Of those who develop the cancer (\(C\)), 1 in 4 carry a genetic marker, \(M\), whereas of those who don’t develop the cancer, 1 in 10 carry \(M\).  A newly born infant is tested for the genetic marker and is found to carry it. What is the probability that this infant will develop cancer?

\sphinxAtStartPar
The first couple of sentences tell us that \(P(C) = 0.1\), \(P(M|C) = 0.25\)  and \(P(M | \bar{C})=0.1.\) Our interest lies in \(P(C|M)\). So we wish to reverse the conditioning. We can obtain this by applying Bayes’ Theorem:
\begin{equation*}
\begin{split}
P(C|M) = \frac{P(M|C)P(C)}{P(M|C)P(C) + P(M|\bar{C})P(\bar{C})}
\end{split}
\end{equation*}
\sphinxAtStartPar
Substituting in the values above gives
\begin{equation*}
\begin{split}
P(C|M) = \frac{0.25\times 0.1}{0.25\times 0.1 + 0.1\times 0.9}=0.22
\end{split}
\end{equation*}
\sphinxAtStartPar
\(P(C|M)\) is called the \sphinxstylestrong{positive predictive value} (PPV) of the test.  It is the probability, given a positive test result, that the individual actually will develop the disease.  i.e. in this case there is a 22\% chance that the infant will develop the disease if they tested positive.


\subsection{2.1.4 The confusion matrix}
\label{\detokenize{02.b. Probability.Discrete:the-confusion-matrix}}
\sphinxAtStartPar
More generally, suppose we have a procedure that results in a binary classification (a binary prediction). This might be a screening test, which could be based on one or more genetic markers or biomarkers. It could be based on the output from a  prognostic risk score or a label derived from an algorithm. Whatever the procedure, suppose we end up with a binary classification: “Positive” or “Negative”. In the health context, this terminology (positive/negative) might represent pairs such as: “Diseased” and “Undiseased”; “Dead” and “Alive” or “Hospitalised” and “Not hospitalised”. We can contrast the binary classification with the (binary) true status. In the general discussion below, we will also use the terms positive and negative to denote the two possible true statuses.

\sphinxAtStartPar
The following table is often called a \sphinxstylestrong{confusion matrix}, or sometimes error matrix. The name confusion matrix stems from the fact that the matrix allows you to see whether the classification is \sphinxstyleemphasis{confusing} two classes. The values \(A\), \(B\), \(C\) and \(D\) are the numbers in each category. The name comes from the fact that the table allows you to see if the classification procedure is “confusing” two categories.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
classification
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Truth:
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Positive
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Negative
\\
\hline
\sphinxAtStartPar
Positive
&
\sphinxAtStartPar

&
\sphinxAtStartPar
A
&
\sphinxAtStartPar
B
\\
\hline
\sphinxAtStartPar
Negative
&
\sphinxAtStartPar

&
\sphinxAtStartPar
C
&
\sphinxAtStartPar
D
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Two groups of people were correctly classified:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{True Positives}. The \(A\) individuals are people who are, in truth, positive (for the disease or outcome of interest) and were classified as positive. So they are often called true positives.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{True Negatives}. The \(D\) individuals are people who are, in truth, negative and and were classified as negative.

\end{itemize}

\sphinxAtStartPar
Two groups of people were incorrectly classified:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{False Positives}. The \(B\) individuals are people who are, in truth, negative but were incorrectly classified as positive. These are sometimes called Type I errors.

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{False Negatives}. The \(C\) individuals are people who are, in truth, positive but were incorrectly classified as negative. These are sometimes called Type II errors.

\end{itemize}

\sphinxAtStartPar
Now let us imagine the same table but with joint probabilities rather than numbers from a sample.  So, for instance, \(p_A\) is the joint probability of being classified as positive \sphinxstyleemphasis{and} being truly positive for the outcome.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Prediction
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Truth:
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Positive
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Negative
\\
\hline
\sphinxAtStartPar
Positive
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(p_A\)
&
\sphinxAtStartPar
\(p_B\)
\\
\hline
\sphinxAtStartPar
Negative
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(p_C\)
&
\sphinxAtStartPar
\(p_D\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
We can obtain estimates of various useful quantities from this matrix. We will use the following notation: \(O\) represents being, in truth, positive for the outcome of interest, and \(\bar{O}\) represents being truly negative. \(P\) represents being classified as positive and \(\bar{P}\) being classified as negative.

\sphinxAtStartPar
The tabs below show various useful quantities.
\subsubsection*{Outcome prevalence}

\sphinxAtStartPar
The prevalence of the outcome is:
\begin{equation*}
\begin{split}
P(O) = \frac{p_A+p_C}{p_A+p_B+p_C+p_D}
\end{split}
\end{equation*}
\sphinxAtStartPar
Prevalence is another word for risk or proportion. It tells us the fraction of the population of interest who have the outcome.
\subsubsection*{Sensitivity}

\sphinxAtStartPar
This is a property of the test. The sensitivity of the test remains the same, irrespective of how common or rare the outcome is.

\sphinxAtStartPar
The sensitivity is:
\begin{equation*}
\begin{split}
P(P|O)=\frac{p_A}{p_A+p_C}
\end{split}
\end{equation*}
\sphinxAtStartPar
The terminology comes from the setting of clinical tests, i.e. how sensitive this test is to the presence of the disease. It is also often called the recall in the fields of machine learning and computer science.

\sphinxAtStartPar
This is also sometimes referred to as the true positive rate.
\subsubsection*{Specificity}

\sphinxAtStartPar
This is a property of the test. The specificity of the test remains the same, irrespective of how common or rare the outcome is.

\sphinxAtStartPar
The specificity is:
\begin{equation*}
\begin{split}
P(\bar{P}|\bar{O})=\frac{p_D}{p_B+p_D}
\end{split}
\end{equation*}
\sphinxAtStartPar
As for the sensitivity, this terminology comes from the setting of clinical tests, i.e. how specific  this test is to the presence of this disease (versus other diseases). So a test which is only positive for this specific disease is very specific. A test which picks up the presence of this disease, and other similar diseases, is not very specific.

\sphinxAtStartPar
This is sometimes called the selectivity or true negative rate.
\subsubsection*{PPV}

\sphinxAtStartPar
PPV stands for Positive Predictive Value, which is shorthand for the predictive value of a positive classification. This is very common terminology in clinical settings.

\sphinxAtStartPar
This quantity is often of most interest to the person having the test. It answers the question: “what is the probability that I have (or will have) the outcome, given that I have just received a positive classification?”

\sphinxAtStartPar
The PPV is:
\begin{equation*}
\begin{split}
P(O|P) = \frac{p_A}{p_A+p_B}
\end{split}
\end{equation*}
\sphinxAtStartPar
In machine learning, it is typically called the precision.
\subsubsection*{Accuracy}

\sphinxAtStartPar
The accuracy is:
\begin{equation*}
\begin{split}
P(\mbox{correct classification}) = \frac{p_A + p_D}{p_A+p_B+p_C+p_D}
\end{split}
\end{equation*}
\sphinxAtStartPar
This quantity is less used in medical settings but is commonly used elsewhere.


\section{2.2 The binomial distribution}
\label{\detokenize{02.c. Probability.Discrete:the-binomial-distribution}}\label{\detokenize{02.c. Probability.Discrete::doc}}
\sphinxAtStartPar
The binomial distribution is used to model the number of successes out of a fixed number of trials.

\sphinxAtStartPar
In the following calculations, we will assume that we know the true probability of success within each trial. In practice, of course, this probability is often the unknown quantity that we are trying to estimate. Later sessions will revisit this example, under the more realistic scenario where this probability is unknown and we are using the sample of data to \sphinxstyleemphasis{make inferences} about the probability. The calculations in the current session will form important building blocks for those later sessions.
\begin{quote}

\sphinxAtStartPar
Note on terminology:    \sphinxhyphen{} Do not confuse the word “trial” here with the idea of a clinical trial or randomised controlled trial. In our discussion of the binomial distribution, we simply mean a Bernoulli trial, which is a statistical experiment which results in a binary outcome. So the trial in question could be whether or not a baby is a male; whether or not someone is alive in 30 days time; whether or not someone experiences a side effect.    \sphinxhyphen{} Similarly, the word “success” can be confusing. We use the word success to denote having the event of interest. It does not imply that this is a good event. In fact, the event we are interested in, in health applications, is often a bad one. It might be diagnosis of cancer or death, in which case a success would refer to someone having cancer or dying. Conversely, if our study was looking at treatments for improving pregnancy rates, our event, and thus the definition of success, might be a couple becoming pregnant. So the word success, in this context, does not necessarily refer to a good event (although sometimes it does!).
\end{quote}


\subsection{2.2.1 Example of a binomial distribution}
\label{\detokenize{02.c. Probability.Discrete:example-of-a-binomial-distribution}}
\sphinxAtStartPar
A small study of 8 participants is being run. All 8 participants will be given an experimental drug. The aim of this study is to obtain data about how many people will experience a side\sphinxhyphen{}effect of the drug.

\sphinxAtStartPar
From previous data, the clinical researcher running the trial believes that the probability of the side\sphinxhyphen{}effect is 0.23.

\sphinxAtStartPar
Let \(X\) be the number of people in the study (i.e. among the 8 participants) who experience a side\sphinxhyphen{}effect. Suppose we are happy to assume that \(X\) follows a binomial distribution. Then, using the formula for the probability distribution function that we derive below, we can calculate the probability that \(P(X=x)\) for all possible values \(x=0,1,...,8\).

\sphinxAtStartPar
The code below (in R) does that calculation and displays a bar chart of the probability distribution function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Obtain the probability distribution function (for values x=0,1,...,8)}
\PYG{n}{x} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{8}\PYG{p}{)}
\PYG{k+kc}{pi} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{0.23}
\PYG{n}{px} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m}{8}\PYG{p}{,} \PYG{k+kc}{pi}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create bar chart of PDF}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}
\PYG{n+nf}{barplot}\PYG{p}{(}\PYG{n}{height}\PYG{o}{=}\PYG{n}{px}\PYG{p}{,} \PYG{n}{names}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{02.c. Probability.Discrete_3_0}.png}


\subsection{Deriving the binomial distribution}
\label{\detokenize{02.c. Probability.Discrete:deriving-the-binomial-distribution}}
\sphinxAtStartPar
Suppose we are conducting research on quadruplets (sets of four siblings born within the same pregnancy). In this session we will consider the number of boys among a set of quadruplets.

\sphinxAtStartPar
Let \(X\) be the number of boys within a particular set of quadruplets. The sample space for \(X\) (the set of possible values \(X\) could take) is: \(\{0, 1, 2, 3, 4 \}\). We will now derive the full probability distribution function for \(X\).  In our calculations, we will assume that the proportion of males at birth is 0.51 and that the gender of each birth is an independent event.

\sphinxAtStartPar
Consider one set of quadruplets. We will start by calculating the probability of no boys i.e. the probability of four girls. By applying the multiplication rule (using the assumption of independence between sex of the children) we obtain:
\begin{equation*}
\begin{split}
P(X=0) = P(\mbox{four girls}) = P(GGGG) = 0.49^4,
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(GGGG\) is shorthand for the event that the first child is a girl, \sphinxstyleemphasis{and} the second is a girl, \sphinxstyleemphasis{and} the third is a girl, \sphinxstyleemphasis{and} the fourth is a girl,

\sphinxAtStartPar
Consider now the probability of one boy and three girls.  This may occur in one of four ways:
BGGG, GBGG, GGBG and GGGB, each of which has probability \(0.49^3\times 0.51\).  Thus
\begin{equation*}
\begin{split}
P(X=1) = P(\mbox{one boy}) = 4 \times 0.49^3 \times 0.51.
\end{split}
\end{equation*}
\sphinxAtStartPar
A family of 2 boys and 2 girls will arise in one of the following 6 ways: BBGG, BGBG, BGGB, GBBG, GBGB, GGBB each with a probability \(0.49^2 \times 0.51^2\) and a total probability of
\begin{equation*}
\begin{split}
P(X=2) =  P(\mbox{two boys}) = 6 \times 0.49^2 \times 0.51^2.
\end{split}
\end{equation*}
\sphinxAtStartPar
With similar reasoning we have that
\begin{equation*}
\begin{split}
P(X=3) =  P(\mbox{three boys}) = 4 \times 0.49 \times 0.51^3 \\
\end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split}
P(X=4) =  P(\mbox{four boys}) = 0.51^4.
\end{split}
\end{equation*}
\sphinxAtStartPar
We now let \(X\) be the random variables which records the number of boys in a randomly selected family of size four. This random variable takes four possible values: 0, 1, 2, 3 or 4. Its probability distribution is given by the following table:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
x
&\sphinxstyletheadfamily 
\sphinxAtStartPar
P(X=\(x\))
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.49\(^4\) = 0.0576
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
4 \(\times\) 0.49\(^3\) \(\times\) 0.51 = 0.2400
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
6 \(\times\) 0.49\(^2\) \(\times\) 0.51\(^2\) = 0.3747
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
4 \(\times\) 0.49 \(\times\) 0.51\(^3\) = 0.2600
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
0.51\(^4\)=0.0677
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
More generally, consider a sequence of \(n\) independent observations/trials (in the example above it was four). Each observation results in a binary outcome, e.g. each trial is a success or a failure. In fact, a Binomial sequence is the sum of \(n\) independent Bernoulli trials (i.e. \(n\) independent Bernoulli variables). Let \(\pi\) denote the probability of an individual success (or the defined binary feature, e.g. boy vs. girl).

\sphinxAtStartPar
How do we obtain the probability distribution for the random variable \(X\) which records the number of successes in a sequence of \(n\) trials? The possible values for the random variable are \(0,1,..,n-1,n\). We saw from the previous example that the probability of \(x\) successes and \(n-x\) failures is
\begin{equation*}
\begin{split}
P(X=x) = \pi^{x} (1-\pi)^{n-x} \times \mbox{number of ways of obtaining } x \mbox{ successes}.
\end{split}
\end{equation*}
\sphinxAtStartPar
The multiplying factor on the right above is the binomial coefficient, i.e. the number of combinations of \(x\) objects chosen from \(n\).  The number of ways \(x\) successes can be obtained from \(n\) observations is equal to \(^n C_x\) as we are not interested in the order of the successes, only the number of combinations in which such a number of successes could have occurred, and a “success” can be considered the same as “choosing” an object: we are “choosing” \(x\) successes and \(n-x\) failures out of a “bag” of \(n\) successes and failures.

\sphinxAtStartPar
So we have that
\begin{equation*}
\begin{split}
P(X=x) = \begin{pmatrix}n \\ x \end{pmatrix} \pi^{x} (1-\pi)^{n-x}
\end{split}
\end{equation*}

\subsection{General form of the binomial distribution}
\label{\detokenize{02.c. Probability.Discrete:general-form-of-the-binomial-distribution}}
\sphinxAtStartPar
Suppose we have a sequence of \(n\) independent Bernoulli trials (i.e. \(n\) independent Bernoulli variables). Let \(\pi\) denote the probability of an individual “success”.  To write that \(X\) follows a binomial distribution with these features, we write \(X\sim binomial(n,\pi)\), (where \(\sim\) means “follows”).
\begin{equation*}
\begin{split}
P(X=x) = \begin{pmatrix}n \\ x \end{pmatrix} \pi^{x} (1-\pi)^{n-x}, \mbox{ for } x=0,1,2,..,n.
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstyleemphasis{Expectation and variance}

\sphinxAtStartPar
The expected value of a binomial variable is \(E(X) = n\pi\)

\sphinxAtStartPar
The variance of a binomial variable is \(Var(X) = n\pi (1-\pi)\)


\subsection{Applications of the Binomial distribution}
\label{\detokenize{02.c. Probability.Discrete:applications-of-the-binomial-distribution}}
\sphinxAtStartPar
\sphinxstyleemphasis{Assumptions}

\sphinxAtStartPar
In order for a variable to follow a binomial distribution, some “structural” things need to be true.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
There must be a fixed number of Bernoulli trials

\item {} 
\sphinxAtStartPar
Each trial must result in a binary outcome (success or failure)

\item {} 
\sphinxAtStartPar
The outcome we are interested in must be defined as the total number of successes.

\end{enumerate}

\sphinxAtStartPar
There are also two key \sphinxstyleemphasis{statistical assumptions}, implied by our derivation above:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The Bernoulli trials must be \sphinxstyleemphasis{independent} of one another

\item {} 
\sphinxAtStartPar
The probability of success must be the same across Bernoulli trials

\end{enumerate}

\sphinxAtStartPar
\sphinxstyleemphasis{Applications}

\sphinxAtStartPar
Suppose we are interested in a particular disease within a large population of \(N\) individuals. If, in the population, \(M\) is the number of individuals with the disease of interest, then the probability of “success” (i.e. an individual having the disease) is \(\pi = M/N\).

\sphinxAtStartPar
Suppose we take a random sample of \(n\) individuals from the large population. We will use \(X\) to be the random variable for the number of “successes” out of the \(n\) individuals. Then we might be happy to assume that \(X\) follows a Binomial distribution.
\begin{quote}

\sphinxAtStartPar
Notes    \sphinxhyphen{} In order for the probability \(\pi\) to remain constant, if we took another sample of \(n\) we would have to “replace” the original \(n\) individuals, so there would be some small possibility of picking the same person twice. In practice, people are not sampled twice. But populations are usually so large that we can ignore this.   \sphinxhyphen{} We also need to assume that individual outcomes (here, having the disease or not) are independent. There are many ways in which this could be violated. People within the same family have shared genetics, shared environments, etc. all of which might lead to outcomes that are more similar between family members than between individuals from different families.
\end{quote}


\section{2.3 The Poisson distribution}
\label{\detokenize{02.d. Probability.Discrete:the-poisson-distribution}}\label{\detokenize{02.d. Probability.Discrete::doc}}
\sphinxAtStartPar
The Poisson distribution is used to model the \sphinxstyleemphasis{number of events} occurring in a fixed time interval.

\sphinxAtStartPar
Similarly to our approach with the Binomial distribution, in the following calculations we will assume that we know the true rate at which events occur. In practice, of course, this rate is often the unknown quantity that we are trying to estimate. Later sessions will revisit this example, under the more realistic scenario where this rate is unknown and we are using the sample of data to \sphinxstyleemphasis{make inferences} about the rate. The calculations in the current session will form important building blocks for those later sessions.


\subsection{2.3.1 Example of the Poisson distribution}
\label{\detokenize{02.d. Probability.Discrete:example-of-the-poisson-distribution}}
\sphinxAtStartPar
A clinical research is interested in modelling the number of asthma attacks that people with asthma experience in one year. Based on a large sample the researcher has estimated that the average number of attacks in a year is 2.5.

\sphinxAtStartPar
If we let \(X\) be the variable for the number of attacks a randomly selected person with asthma will experience in a year and we are happy to assume that \(X\) follows a Poisson distribution, then we can calculate \(P(X=x)\) for any given value of \(x\).

\sphinxAtStartPar
The code below (in R) does this calculation and plots the probability distribution function of the number of asthma attacks in a year.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Obtain the probability distribution function (for values x=0,1,...,10)}
\PYG{n}{x} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{10}\PYG{p}{)}
\PYG{n}{lambda} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{2.5}
\PYG{n}{px} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dpois}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{lambda}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create bar chart of PDF}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}
\PYG{n+nf}{barplot}\PYG{p}{(}\PYG{n}{height}\PYG{o}{=}\PYG{n}{px}\PYG{p}{,} \PYG{n}{names}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{02.d. Probability.Discrete_2_0}.png}


\subsection{2.3.2 Deriving the Poisson distribution}
\label{\detokenize{02.d. Probability.Discrete:deriving-the-poisson-distribution}}
\sphinxAtStartPar
To give a heuristic derivation of the probability distribution function of the Poisson, we divide the total time \(T\) into a very large number of small intervals (see Figure below). As the number of intervals we divide \(T\) into increases, at most one event will occur in each interval, and so \(X\) will equal the number of intervals in which an event occurs. Since the occurrence of events in each interval are assumed independent of each other, \(X \sim Bin(n,\pi)\), where \(n\) is the number of intervals and \(\pi\) is the probability of an event occurring in any given interval.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{poisson}.png}
\caption{Derivation of Poisson distribution by dividing time into small intervals}\label{\detokenize{02.d. Probability.Discrete:poisson}}\end{figure}

\sphinxAtStartPar
With a rate of \(\lambda\) events per unit of time, we expect \(\mu=\lambda T\) events in the whole period, and therefore we expect \(\lambda T / n = \mu/n\) events in each interval. Thus \(\pi=\mu/n\). Therefore, using the probability distribution function for the binomial we have that
\begin{equation*}
\begin{split}
P(X=x) = {n \choose x} \pi^{x} (1-\pi)^{n-x} = {n \choose x} \left(\frac{\mu}{n}\right)^{x} \left(1-\frac{\mu}{n}\right)^{n-x}
\end{split}
\end{equation*}
\sphinxAtStartPar
Then we have that
\begin{equation*}
\begin{split}
P(X=x) = {n \choose x} \left(\frac{\mu}{n}\right)^{x} \left(1-\frac{\mu}{n}\right)^{n-x} 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
= \frac{n!}{x! (n-x)!} \left(\frac{\mu}{n}\right)^{x} \left(1-\frac{\mu}{n}\right)^{n-x}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
= \frac{n!}{n^{x} (n-x)!}  \frac{\mu^{x}}{x!} \left(1-\frac{\mu}{n}\right)^{n-x}
\end{split}
\end{equation*}
\sphinxAtStartPar
Now to simplify the first term, we note that:
\begin{equation*}
\begin{split}
\frac{n!}{n^{x} (n-x)!} = \frac{n(n-1)...(n-x+1)}{n^{x}} \rightarrow 1 \mbox{ as } n \rightarrow \infty,
\end{split}
\end{equation*}
\sphinxAtStartPar
and to simplify the third term, we note that:
\begin{equation*}
\begin{split}
\left(1-\frac{\mu}{n}\right)^{n-x} \rightarrow \left(1-\frac{\mu}{n}\right)^{n} \rightarrow e^{-\mu}
\end{split}
\end{equation*}
\sphinxAtStartPar
Replacing the first and third terms by these limits gives
\begin{equation*}
\begin{split}
P(X=x) \rightarrow \frac{\mu^{x}}{x!} e^{-\mu} \mbox{ as } n \rightarrow \infty.
\end{split}
\end{equation*}

\subsection{2.3.3 General form of the Poisson distribution}
\label{\detokenize{02.d. Probability.Discrete:general-form-of-the-poisson-distribution}}
\sphinxAtStartPar
We can now define a Poisson distribution for the number of events occurring in a fixed interval \(T\) at a constant rate \(\lambda\) with parameter \(\mu=\lambda T\), which we write as
\begin{equation*}
\begin{split}
X \sim \mbox{Poisson}(\mu=\lambda T)
\end{split}
\end{equation*}
\sphinxAtStartPar
as the distribution which has probability distribution function
\begin{equation*}
\begin{split}
P(X=x) = \frac{\mu^{x}}{x!} e^{-\mu}, \ \mbox{ for } x=0,1,2,...
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstyleemphasis{Expectation and variance}

\sphinxAtStartPar
The derivation of the expectation and variance of a Poisson random variable \(X\) with parameter \(\mu\) will be set as a practical question.


\subsection{2.3.4 Applications of the Poisson distribution}
\label{\detokenize{02.d. Probability.Discrete:applications-of-the-poisson-distribution}}
\sphinxAtStartPar
\sphinxstyleemphasis{Assumptions}

\sphinxAtStartPar
The Poisson distribution is used to model the \sphinxstyleemphasis{number of events} occurring in a fixed time interval \(T\) when:
\begin{itemize}
\item {} 
\sphinxAtStartPar
events occur randomly in time,

\item {} 
\sphinxAtStartPar
they occur at a constant rate \(\lambda\) per unit time,

\item {} 
\sphinxAtStartPar
they occur independently of each other.

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Applications}

\sphinxAtStartPar
A random variable \(X\) which follows a Poisson distribution can  take any non\sphinxhyphen{}negative integer value. Examples where the Poisson distribution might be appropriate include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Emissions from a radioactive source,

\item {} 
\sphinxAtStartPar
The number of deaths in a large cohort of people over a year,

\item {} 
\sphinxAtStartPar
The number of accidental deaths occurring in a city over a year.

\end{itemize}


\subsection{2.3.5 Approximating the binomial by a Poisson}
\label{\detokenize{02.d. Probability.Discrete:approximating-the-binomial-by-a-poisson}}
\sphinxAtStartPar
When \(n\) is large relative to \(\pi\), the binomial distribution can be approximated by a Poisson with a
mean \(n\pi\). That this approximation is reasonable follows directly from our earlier heuristic derivation
of how a Poisson distribution arises as an approximation to a binomial distribution when the number
of trials tends to infinity.

\sphinxAtStartPar
There are many such approximations. Nowadays, we may not need to use them because we have enormous computing power at our disposal. In earlier times, in contrast, calculations could take a long time so any simplification that could be reasonably applied could provide meaningful extra calculation speed.


\section{2.4 Summary}
\label{\detokenize{02.e. Probability.Discrete:summary}}\label{\detokenize{02.e. Probability.Discrete::doc}}
\sphinxAtStartPar
In this session, we have met a number of useful applications of discrete probability theory.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Bayes’ Theorem can be used to quantify properties of screening and prognostic tests and forms the basis of Bayesian statistics. We will return to Bayes’ Theorem when we explore Bayesian inference.

\item {} 
\sphinxAtStartPar
The binomial distribution is used to model the number of successes out of a fixed number of trials. We will revisit the binomial distribution in the sessions about likelihood. Logistic regression, a very commonly used regression model, is based on an underlying Bernoulli distribution (remember that the binomial distribution can be obtained by summing multiple identical independent Bernoulli distributions). As such, our work with the binomial distribution is closely connected to the later logistic regression sessions.

\item {} 
\sphinxAtStartPar
The Poisson distribution is used to model the number of events occurring in a fixed time interval. It forms the basis for Poisson regression, which you will meet later in this module.

\end{itemize}


\chapter{3. Continuous distributions}
\label{\detokenize{03.a. Continuous Probability Distributions:continuous-distributions}}\label{\detokenize{03.a. Continuous Probability Distributions::doc}}
\sphinxAtStartPar
This session is the second of two sessions covering basic probability. In this session, we extend the ideas from discrete distributions, from the previous session, to variables which have a \sphinxstylestrong{continuous} distribution.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain the concept of a continuous random variable

\item {} 
\sphinxAtStartPar
define several continuous probability distributions, and relationships between parameters, expectations and variance

\item {} 
\sphinxAtStartPar
understand the relationship between normally distributed data and standard scores

\item {} 
\sphinxAtStartPar
evaluate the appropriateness of assuming normality in data and other options

\item {} 
\sphinxAtStartPar
understand properties of joint distributions, such as the multivariate normal distribution

\end{itemize}



\sphinxAtStartPar
The five sub\sphinxhyphen{}sections describe properties of continuous random variables, explore a number of useful continuous distributions, consider direct applications for the standard Normal (Gaussian) distribution, consider how to assess whether a variable follows a normal distribution and, finally, describe joint distributions and correlations.


\section{3.1 Continuous random variables}
\label{\detokenize{03.b. Continuous Probability Distributions:continuous-random-variables}}\label{\detokenize{03.b. Continuous Probability Distributions::doc}}
\sphinxAtStartPar
We have previously seen several discrete probability distributions (including the binomial and the Poisson). We now extend random variables to those that are continuous. A continuous random variable is one that can take a value in continuous space; this may vary from \(-\infty\) to \(+\infty\) (like the normal distribution) or have limits set on the lower (eg. the log\sphinxhyphen{}normal) or upper bound (eg. the uniform).


\subsection{3.1.1. The probability density function}
\label{\detokenize{03.b. Continuous Probability Distributions:the-probability-density-function}}
\sphinxAtStartPar
Previously we characterised the distribution of a variable by assigning a probability to each specific value. However, because athere are infinitely many values that could be taken by a continuous variable, paradoxically, the probability of a continuous random variable taking any specific value is zero. Therefore, we cannot use a probability distribution function to characterise the distribution of a continuous variable.

\sphinxAtStartPar
Instead, we turn to something called a \sphinxstylestrong{probability density function}. Instead of attaching a probability to each value the variable could take, the probability density function tells us the probability that a continuous variable lies within each possible interval (range of values). Specifically, the area under the curve (of the probability density function) between two limits tells us the probability that the continuous variable takes a value between those two limits.

\sphinxAtStartPar
Generally, a random variable \(X\) has density \(f_X\) where
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(f(x) \geq 0\) for all of \(x\)

\item {} 
\sphinxAtStartPar
\(\int_{-\infty}^{\infty} f(x) \hspace{0.2cm} dx = 1.00\)

\end{itemize}

\sphinxAtStartPar
which states that the “sum” of all probabilities of \(f(x)\) from the minimum to the maximum is equal to 1.

\sphinxAtStartPar
We can obtain various useful probabilities from this density function. We can calculate the probability that the variable takes a value within a given interval, the probability that it is below or above a given value. For example:
\begin{equation*}
\begin{split}Pr(X>b) = \int_{b}^{max} f(X) \hspace{0.2cm} dx \end{split}
\end{equation*}
\sphinxAtStartPar
Further information about continuous probability distributions are given in the \sphinxhref{https://lshtm-hds.github.io/Math-Refresher}{Refresher}.


\section{3.2 Useful continuous distributions}
\label{\detokenize{03.c. Continuous Probability Distributions:useful-continuous-distributions}}\label{\detokenize{03.c. Continuous Probability Distributions::doc}}
\sphinxAtStartPar
Below are several useful probability distributions for data science in health. Some of the information below is a repeat of the \sphinxstylestrong{\sphinxhref{https://statsfizz.github.io/Maths\_Refresher/pr6\_distributions.html}{Maths refresher}}, but we include some practical applications of each distribution.


\subsection{3.2.1 The normal distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-normal-distribution}}
\sphinxAtStartPar
The normal distribution is defined with the following probability density function:
\begin{equation*}
\begin{split}
f(x) = \frac{1}{\sqrt{2\pi}\sigma}exp \Big[-\Big(\frac{1}{2}\Big)\Big(\frac{x-\mu}{\sigma}\Big)^2\Big]
\end{split}
\end{equation*}
\sphinxAtStartPar
for values \(x\) in \((-\infty, +\infty)\). If we have a random variable \(X\) that is normally distributed we can specify this using \(X {\sim} N(\mu, \sigma^2)\). The expected value is given by \(E[X]=\mu\) and the variance is given by \(Var[X] = \sigma^2\).

\sphinxAtStartPar
A \sphinxstylestrong{standard normal} distribution has a mean of 0 and a variance of 1. A standard normal random variable is usually represented by \(Z {\sim} N(0,1)\) and is sometimes called the \sphinxstyleemphasis{Z\sphinxhyphen{}score}.

\sphinxAtStartPar
So much of statistics relies on the normal distribution, so it is an important distribution to be familiar with. We will see that the normal distribution has an important role to play in statistical inference. It is also sometimes a good distribution for directly modelling continuous variables, for example blood pressure.


\subsection{3.2.2 The log\sphinxhyphen{}normal distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-log-normal-distribution}}
\sphinxAtStartPar
The log\sphinxhyphen{}normal distribution is essentialy a transformed version of the normal distribution, and has its own probability density function;
\begin{equation*}
\begin{split}
f(x) = \frac{1}{x \sigma \sqrt{2\pi}}exp\Big(-\frac{(ln(x)-\mu)^2}{2\sigma^2}\Big)
\end{split}
\end{equation*}
\sphinxAtStartPar
for values \(x\) in  \([0,+\infty)\). If a random variable \(X\) is log\sphinxhyphen{}normally distributed, \(Y=ln(X)\) has a normal distribution, and if \(Y\) is a normal distribution then \(X=exp(Y)\) has a log\sphinxhyphen{}normal distribution. These simple transformations mean that calculations using transformed data is the standard approach. The parameters \(\mu\) and \(\sigma\) refer to the mean and standard deviation on the \sphinxstyleemphasis{normal scale}. Consequently, the median of a log\sphinxhyphen{}normally distributed sample is \(exp(\mu)\).

\sphinxAtStartPar
Many biological datasets are log\sphinxhyphen{}normally distributed, for example most measurements (height, weight, speed) will be above 0, and will often be right\sphinxhyphen{}skewed. A good approach to take with these sorts of data is to log the data, and work on the \sphinxstyleemphasis{log scale}. Any inference should be converted back to the \sphinxstyleemphasis{natural scale}. Sometimes measurements are sufficiently greater than 0 that they become more centered. In this case, it may not be necessary to assume that are log\sphinxhyphen{}normal, and assuming normality may be acceptible.


\subsection{3.2.3 The \protect\(\chi^2\protect\) distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-chi-2-distribution}}
\sphinxAtStartPar
The \(\chi^2\) distribution is here because we will use the properties of this distribution later in hypothesis testing. Its origins come from a random sample of the \sphinxstyleemphasis{standard normal}, where the \(\chi^2\) distribution is the distribution of the sum of squared standard normals. The degrees of freedom come from the number of standard normal random variables being summed. It is not necessary to know the parameters or estimates of the \(\chi^2\) parameters. A variable which follows the chi\sphinxhyphen{}squared distribution can only take positive values (i.e. greater than zero).


\subsection{3.2.4 The t\sphinxhyphen{}distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-t-distribution}}
\sphinxAtStartPar
Student’s t\sphinxhyphen{}distribution arises as the ratio of the sample mean to its standard error. The t\sphinxhyphen{}distribution has a complex density function which we shall not state here.

\sphinxAtStartPar
For now we note that the t\sphinxhyphen{}distribution has an additional parameter of sorts, known as the degrees of freedom (d.f.). The density function is similar to that of the standard normal, but the t\sphinxhyphen{}distribution has heavier tails. If \(X\) follows a t\sphinxhyphen{}distribution with \(\nu\) degrees of freedom, we write
\begin{equation*}
\begin{split}X \sim t_\nu\end{split}
\end{equation*}
\sphinxAtStartPar
The expectation and variance of a variable \(X\) which follows a t\sphinxhyphen{}distribution with \(\nu\) degrees of freedom are given by:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(E[X] = 0\)

\item {} 
\sphinxAtStartPar
\(Var[X] = \frac{\nu}{\nu-2}\) if \(\nu>2\); \(\infty\) for \(1<\nu<2\); undefined otherwise

\end{itemize}

\sphinxAtStartPar
As the number of degrees of freedom increases the t\sphinxhyphen{}distribution gets closer and closer to the standard normal distribution.


\subsection{3.2.5 The F distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-f-distribution}}
\sphinxAtStartPar
The F distribution doesn’t have a simple mathematical formula, but is used extensively to compare equality of variances of two normal populations (\sphinxstyleemphasis{think anova}), and is used in linear regression.

\sphinxAtStartPar
For two normal populations with variances \(\sigma_1^2\) and \(\sigma_2^2\), the two random samples of size \(n_1\) and \(n_2\) with corresponding sample variance(s) \(s_1^2\) and \(s_2^2\) has the variable
\begin{equation*}
\begin{split}F = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}\end{split}
\end{equation*}
\sphinxAtStartPar
with \(n_1-1\) and \(n_2-1\) degrees of freedom.


\subsection{3.2.6 The exponential distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-exponential-distribution}}
\sphinxAtStartPar
The exponential distribution is defined with the probability density function:
\begin{equation*}
\begin{split}f(x)=\lambda e^{-\lambda x}\end{split}
\end{equation*}
\sphinxAtStartPar
with parameter \(\lambda\), which is usually described as the rate. The limits of the distribution are \([0,\infty)\), which means values of \(x\) are always greater than 0 (and not including it).

\sphinxAtStartPar
The expected value is given by \(E[X]=\frac{1}{\lambda}\) and variance \(Var[X]=\frac{1}{\lambda^2}\).

\sphinxAtStartPar
The exponential distribution is really useful in statistics because its distribution nicely describes \sphinxstyleemphasis{the time to which something occurs}, if the event happens at a roughly constant rate in time. Health related examples include injuries, births and deaths (although in reality not all occur at a constant rate). The exponential distribution is important in methods such as \sphinxstyleemphasis{survival analysis}.


\subsection{3.2.7 The uniform distribution}
\label{\detokenize{03.c. Continuous Probability Distributions:the-uniform-distribution}}
\sphinxAtStartPar
The uniform distribution is in some ways the simplest to conceptualise. A random variable that is uniformly distributed can have any value between the parameters \(a\) (min) and \(b\) (max) with equal probability;
\begin{equation*}
\begin{split}f(x)= \frac{1}{b-a}\end{split}
\end{equation*}
\sphinxAtStartPar
Outside of these limits, the probability density is 0. The expected value is \(E[X] = \frac{(a+b)}{2}\) and variance \(Var[X] = \frac{(b-a)^2}{12}\).

\sphinxAtStartPar
The uniform distribution is very commonly used when randomly allocating outcomes. An example in statistical modelling includes stochastic infectious disease modelling; here several different events (transmission, death) may have a corresponding probability and one event needs to be selected from the two options. A uniform distribution (where the maximum is the total probability of all events) is used to select


\section{3.3 Uses of the standard Normal distribution}
\label{\detokenize{03.d. Continuous Probability Distributions:uses-of-the-standard-normal-distribution}}\label{\detokenize{03.d. Continuous Probability Distributions::doc}}
\sphinxAtStartPar
Suppose we wanted to answer the following question:
\begin{quote}

\sphinxAtStartPar
What is the probability of having a ‘healthy’ weight?
\end{quote}

\sphinxAtStartPar
A healthy weight is often is often measured using the Body Mass Index (BMI \sphinxhyphen{} although see \sphinxhref{https://www.health.harvard.edu/blog/how-useful-is-the-body-mass-index-bmi-201603309339}{here} and \sphinxhref{https://www.bbc.co.uk/news/health-43895508}{here} for a discussion on why this may be too simplistic a measure). An individual’s BMI can be calculated using their height and weight, using the formula BMI \(= \frac{mass(kg)}{height(m)^2}\). Then people can be classified as:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Classification
&\sphinxstyletheadfamily 
\sphinxAtStartPar
BMI
\\
\hline
\sphinxAtStartPar
Underweight
&
\sphinxAtStartPar
<18.5
\\
\hline
\sphinxAtStartPar
Normal
&
\sphinxAtStartPar
18.5\sphinxhyphen{}24.9
\\
\hline
\sphinxAtStartPar
Overweight
&
\sphinxAtStartPar
25\sphinxhyphen{}29.9
\\
\hline
\sphinxAtStartPar
Obese
&
\sphinxAtStartPar
30\sphinxhyphen{}39.9
\\
\hline
\sphinxAtStartPar
Extremely obese
&
\sphinxAtStartPar
>40
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
To address our question, we will use data taken from a study undertaken among a group of 76 cleaners, that investigated whether telling the cleaners they had an active lifestyle influenced their BMI. We will assume that values of BMI approximately follow a normal distribution. We do not know the true values of \(\mu\) and \(\sigma\) so we will replace these with the sample mean and standard deviation. This gives us values of \(\mu=\)26.5 and \(\sigma^2=\) 18.1, as demonstrated in the snippet of code below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} BMI dataset}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/BMI/MindsetMatters.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{head}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}remove observations with no BMI data}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{dat}\PYG{p}{[}\PYG{o}{!}\PYG{n+nf}{is.na}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{,}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{}estimate mu and sigma}
\PYG{n}{mu} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{value of mu is \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sig} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sd}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{value of sigma is \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{sig}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A data.frame: 6 × 14
\begin{tabular}{r|llllllllllllll}
  & Cond & Age & Wt & Wt2 & BMI & BMI2 & Fat & Fat2 & WHR & WHR2 & Syst & Syst2 & Diast & Diast2\\
  & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <int> & <int>\\
\hline
	1 & 0 & 43 & 137 & 137.4 & 25.1 & 25.1 & 31.9 & 32.8 & 0.79 & 0.79 & 124 & 118 & 70 & 73\\
	2 & 0 & 42 & 150 & 147.0 & 29.3 & 28.7 & 35.5 &   NA & 0.81 & 0.81 & 119 & 112 & 80 & 68\\
	3 & 0 & 41 & 124 & 124.8 & 26.9 & 27.0 & 35.1 &   NA & 0.84 & 0.84 & 108 & 107 & 59 & 65\\
	4 & 0 & 40 & 173 & 171.4 & 32.8 & 32.4 & 41.9 & 42.4 & 1.00 & 1.00 & 116 & 126 & 71 & 79\\
	5 & 0 & 33 & 163 & 160.2 & 37.9 & 37.2 & 41.7 &   NA & 0.86 & 0.84 & 113 & 114 & 73 & 78\\
	6 & 0 & 24 &  90 &  91.8 & 16.5 & 16.8 &   NA &   NA & 0.73 & 0.73 &  NA &  NA & 78 & 76\\
\end{tabular}\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}value of mu is 26.46\PYGZdq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}value of sigma is 4.25\PYGZdq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
So what is the probability a randomly selected person in this sample has a normal BMI?


\subsection{Approach 1: Manual calculation}
\label{\detokenize{03.d. Continuous Probability Distributions:approach-1-manual-calculation}}
\sphinxAtStartPar
One option is to make use of pre\sphinxhyphen{}calculated probabilities of the standard normal distribution. If we write \(X\) to represent the value of a person’s BMI, then we are assuming that
\begin{equation*}
\begin{split}
X \sim N(\mu=26.5, \sigma^2=18.1)
\end{split}
\end{equation*}
\sphinxAtStartPar
To make use of the pre\sphinxhyphen{}calculated probabilities for the standard normal distribution, we must first transform our normally distributed variable to have a standard normal distribution. We know that the transformed variable \(Z\) (the \sphinxstyleemphasis{Z score}) has a standard normal distribution, where
\begin{equation*}
\begin{split}
Z = \frac{X - \mu}{\sigma}
\end{split}
\end{equation*}
\sphinxAtStartPar
Given values for \(\mu\) and \(\sigma\) we can go from the \sphinxstyleemphasis{X scale} to the \sphinxstyleemphasis{Z scale} and \sphinxstyleemphasis{vice versa}. The important point about describing a distribution on the Z scale is that this opens the ability to calculate specific probabilities. So back to answering the question…

\sphinxAtStartPar
From the table above we can see that a normal weight is classified as a BMI between 18.5 and 24.9, and we want to know what the probability is that a randomly selected person falls between these limits. We write this as;
\begin{equation*}
\begin{split}
P(18.5 < X < 24.9) 
\end{split}
\end{equation*}
\sphinxAtStartPar
On the Z\sphinxhyphen{}scale, this is equivalent to saying that:
\begin{equation*}
\begin{split}
P(-1.87 < Z < -0.37) = P(Z < -0.37) - P(Z < -1.87)
\end{split}
\end{equation*}
\sphinxAtStartPar
Tables exist containing a range of pre\sphinxhyphen{}calculated probabilities that a variable following a standard normal distribution takes a value of less than \(k\), for a range of possible values of \(k\). These are often called \sphinxstyleemphasis{z\sphinxhyphen{}tables} (found \sphinxhref{http://www.z-table.com/}{online} or at the back of most stats books). From these tables, we can look up the corresponding probability for each z\sphinxhyphen{}score, giving:
\begin{equation*}
\begin{split}
0.3557 - 0.0307 = 0.325
\end{split}
\end{equation*}

\subsection{Approach 2: Using R to do the same calculation}
\label{\detokenize{03.d. Continuous Probability Distributions:approach-2-using-r-to-do-the-same-calculation}}
\sphinxAtStartPar
Using this approach, R is ultimately using the same pre\sphinxhyphen{}calculated probability tables. However, it is considerably quicker and easier to ask R to look up the values rather than finding them in tables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} a) if we were to use Z tables within R (to illustrate the point)}

\PYG{n}{z\PYGZus{}min} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{p}{(}\PYG{l+m}{18.5}\PYG{o}{\PYGZhy{}}\PYG{n}{mu}\PYG{p}{)}\PYG{o}{/}\PYG{n}{sig}
\PYG{n}{z\PYGZus{}max} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{p}{(}\PYG{l+m}{24.9}\PYG{o}{\PYGZhy{}}\PYG{n}{mu}\PYG{p}{)}\PYG{o}{/}\PYG{n}{sig}

\PYG{c+c1}{\PYGZsh{} note when using pnorm we don\PYGZsq{}t need to specify mu and sigma as the }
\PYG{c+c1}{\PYGZsh{} function assumes mu=0 and sigma=1 unless specified.}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{z\PYGZus{}max is \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{z\PYGZus{}max}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{ and z\PYGZus{}min is \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{z\PYGZus{}min}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Probability of having a healthy BMI is (z\PYGZhy{}score) \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{n}{z\PYGZus{}max}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{n}{z\PYGZus{}min}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}z\PYGZus{}max is \PYGZhy{}0.37 and z\PYGZus{}min is \PYGZhy{}1.87\PYGZdq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Probability of having a healthy BMI is (z\PYGZhy{}score) 0.326\PYGZdq{}
\end{sphinxVerbatim}


\subsection{Approach 3: Using R to do the  calculation on the untransformed scale}
\label{\detokenize{03.d. Continuous Probability Distributions:approach-3-using-r-to-do-the-calculation-on-the-untransformed-scale}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} b) if we were to directly estimate}
 
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Probability of having a healthy BMI is (direct) \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{l+m}{24.9}\PYG{p}{,}\PYG{n}{mu}\PYG{p}{,}\PYG{n}{sig}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{l+m}{18.5}\PYG{p}{,}\PYG{n}{mu}\PYG{p}{,}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Probability of having a healthy BMI is (direct) 0.326\PYGZdq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
Calculating directly gives the same result as using a z\sphinxhyphen{}score in R, and this returns the same information as using z\sphinxhyphen{}tables.

\sphinxAtStartPar
In answer to our question, we estimate that the probability of having a ‘healthy’ weight is 32.6\%. We can compare this to the observed proportion of our sample of data with a ‘healthy’ BMI.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} c) provide a sanity check against the data}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
\PYG{n+nf}{library}\PYG{p}{(}\PYG{n}{ggplot2}\PYG{p}{)}

\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nf}{geom\PYGZus{}histogram}\PYG{p}{(}\PYG{n}{bins} \PYG{o}{=} \PYG{l+m}{30}\PYG{p}{,}\PYG{n}{fill}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{steelblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{grey80\PYGZdq{}}\PYG{p}{)} \PYG{o}{+}
    \PYG{n+nf}{geom\PYGZus{}vline}\PYG{p}{(}\PYG{n}{xintercept} \PYG{o}{=} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{18.5}\PYG{p}{,}\PYG{l+m}{24.9}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}hist(dat\PYGZdl{}BMI,col=\PYGZdq{}steelblue\PYGZdq{})}
\PYG{c+c1}{\PYGZsh{}abline(v=c(18.5,24.9),lty=2)}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Within the data a healthy BMI is seen \PYGZdq{}}\PYG{p}{,}
             \PYG{n+nf}{round}\PYG{p}{(}\PYG{l+m}{100}\PYG{o}{*}\PYG{p}{(}\PYG{p}{(}\PYG{n+nf}{sum}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{o}{\PYGZlt{}}\PYG{l+m}{24.9}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{sum}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{o}{\PYGZlt{}}\PYG{l+m}{18.5}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n+nf}{length}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{\PYGZpc{}\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Within the data a healthy BMI is seen 35.1\PYGZpc{}\PYGZdq{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03.d. Continuous Probability Distributions_8_1}.png}

\sphinxAtStartPar
So we can see that the sample estimate (35.1\%) is roughly similar to the population estimate of 32.6\%.


\section{3.4 Are the data normally distributed?}
\label{\detokenize{03.e. Continuous Probability Distributions:are-the-data-normally-distributed}}\label{\detokenize{03.e. Continuous Probability Distributions::doc}}

\subsection{3.4.1 Data and their relationship with statistical distributions}
\label{\detokenize{03.e. Continuous Probability Distributions:data-and-their-relationship-with-statistical-distributions}}
\sphinxAtStartPar
We often have data on a particular characteristic and want to make general statements about it, such as: the probability of it being greater than or less than something, provide a range in which “most” observations will lie, what is the central value (e.g. mean/median), etc.
\begin{quote}

\sphinxAtStartPar
However,
 \sphinxhyphen{} we rarely \sphinxstyleemphasis{know} the true distribution that a variable follows
 \sphinxhyphen{} a distribution will not quite fit the data but will form a sufficiently good approximation to address the questions above with sufficient accuracy.
\end{quote}

\sphinxAtStartPar
So we often want to find a distribution which fits our data well enough. How do we make a decision? Some of this comes with experience, but there are some useful steps to go through when confronted with data (this is covered in more detail in the lecture). Think back to the \sphinxstylestrong{PPDAC} cycle in the first session;
\begin{itemize}
\item {} 
\sphinxAtStartPar
plot your data. What does the data look like? Consider the lower and upper bounds, the most common number, and evidence of symmetry

\item {} 
\sphinxAtStartPar
summarise your data. Report the minimum, maximum, mean and mode. This should aid with thinking about the criteria of specific distributions

\item {} 
\sphinxAtStartPar
depending upon the application and what the data looks like, you may want to consider using the empirical distribution function rather than assumption a specific form. However, this gives you fewer options for inference

\end{itemize}


\subsection{3.4.2 Are the data normally distributed?}
\label{\detokenize{03.e. Continuous Probability Distributions:id1}}
\sphinxAtStartPar
Many analyses and tests of data start with the assumption that the data are normally distributed. A simple example would be using a t\sphinxhyphen{}test to check whether the mean of 2 groups are different, more complex examples would include linear regression analysis. If the outcome being analysed is a qualitive outcome, or successes and failures, it should be obvious that the data aren’t normally distributed. But what if the data are continuous or count values, and they look like they are centered, but have some skewness? Is it safe to proceed as if they are normal?

\sphinxAtStartPar
The first step, as always, is to plot the data to see what they look like. A histogram, as above, or density plot is a good step forward. Additionally, a \sphinxstyleemphasis{quantile\sphinxhyphen{}quantile} plot calculates the correlation between a sample and the equivalent normal distribution with the same mean \(\mu\) and standard deviation \(\sigma\). If a variable follows a normal distribution, the quantile\sphinxhyphen{}quantile plot will follow the diagonal line.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} BMI dataset}
\PYG{n+nf}{library}\PYG{p}{(}\PYG{n}{ggplot2}\PYG{p}{)}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/BMI/MindsetMatters.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{dat}\PYG{p}{[}\PYG{o}{!}\PYG{n+nf}{is.na}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{,}\PYG{p}{]}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nf}{stat\PYGZus{}qq}\PYG{p}{(}\PYG{p}{)} 
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03.e. Continuous Probability Distributions_2_0}.png}

\sphinxAtStartPar
From the figure you can see that the theoretical quantiles follow the diagonals reasonably well, and especially at the extremes do not move away much from the diagonal. A plot like this would be enough to show that the data approximately follows normally distribution. Looking at plots such as this to assess normality is a \sphinxstyleemphasis{judgement} which you will build up during this module.

\sphinxAtStartPar
To formally test for normality we can use the Shapiro\sphinxhyphen{}Wilk test, described briefly below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{shapiro.test}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
	Shapiro\PYGZhy{}Wilk normality test

data:  dat\PYGZdl{}BMI
W = 0.9692, p\PYGZhy{}value = 0.06756
\end{sphinxVerbatim}

\sphinxAtStartPar
Although we haven’t yet covered hypothesis testing (see \sphinxstyleemphasis{Session 8}), this is testing the null hypothesis that the data follow a normal distribution. In this case, the test returns a p\sphinxhyphen{}value of 0.067. This p\sphinxhyphen{}value suggests some, but not strong, evidence against normality of the data.


\subsection{3.4.3 Approaches to non\sphinxhyphen{}normally distributed data}
\label{\detokenize{03.e. Continuous Probability Distributions:approaches-to-non-normally-distributed-data}}
\sphinxAtStartPar
A really useful approach to dealing with non\sphinxhyphen{}normally distributed data is transformations. The most often used approach is to apply a log\sphinxhyphen{}transformation, either on the \sphinxstyleemphasis{natural} (\(Y = log_e(X)\)) or \sphinxstyleemphasis{log10} (\(Y = log_{10}(X)\)) scale. The transformed data may behave more like normally distributed data.

\sphinxAtStartPar
An example is given below for weights of 1174 babies. First we will look at the distribution of (untransformed) birth weights.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
\PYG{n+nf}{library}\PYG{p}{(}\PYG{n}{ggplot2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} mother\PYGZhy{}baby dataset}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/MotherBaby/baby.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{head}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{)}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{dat}\PYG{p}{[}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{100}\PYG{p}{,}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} we will use just the first 100 observations}

\PYG{c+c1}{\PYGZsh{} plot the data on maternal age}
\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{Maternal.Age}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nf}{geom\PYGZus{}histogram}\PYG{p}{(}\PYG{n}{bins}\PYG{o}{=}\PYG{l+m}{30}\PYG{p}{,}\PYG{n}{fill}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{steelblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{grey80\PYGZdq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} plot a quantile plot of this log\PYGZhy{}normally distributed data}
\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{Maternal.Age}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nf}{stat\PYGZus{}qq}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A data.frame: 6 × 6
\begin{tabular}{r|llllll}
  & Birth.Weight & Gestational.Days & Maternal.Age & Maternal.Height & Maternal.Pregnancy.Weight & Maternal.Smoker\\
  & <int> & <int> & <int> & <int> & <int> & <chr>\\
\hline
	1 & 120 & 284 & 27 & 62 & 100 & False\\
	2 & 113 & 282 & 33 & 64 & 135 & False\\
	3 & 128 & 279 & 28 & 64 & 115 & True \\
	4 & 108 & 282 & 23 & 67 & 125 & True \\
	5 & 136 & 286 & 25 & 62 &  93 & False\\
	6 & 138 & 244 & 33 & 62 & 178 & False\\
\end{tabular}\end{split}
\end{equation*}
\noindent\sphinxincludegraphics{{03.e. Continuous Probability Distributions_7_1}.png}

\noindent\sphinxincludegraphics{{03.e. Continuous Probability Distributions_7_2}.png}

\sphinxAtStartPar
We can see clearly that maternal age is right skewed. This is a classic log\sphinxhyphen{}normal distribution. The quantile plot is not straight along the diagonal but forms an \sphinxstyleemphasis{s\sphinxhyphen{}shape}. This confirms that the data does not conform to a normally distribution.

\sphinxAtStartPar
A sensible next step would be to log\sphinxhyphen{}transform the data using the natural logarithm. The distribution of the transformed birth weights is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot the data on maternal age}
\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{Maternal.Age}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} 
    \PYG{n+nf}{geom\PYGZus{}histogram}\PYG{p}{(}\PYG{n}{bins}\PYG{o}{=}\PYG{l+m}{30}\PYG{p}{,}\PYG{n}{fill}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{steelblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{grey80\PYGZdq{}}\PYG{p}{)} \PYG{o}{+}
    \PYG{n+nf}{scale\PYGZus{}x\PYGZus{}continuous}\PYG{p}{(}\PYG{n}{trans} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{log\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} but note that any analysis should be carried out on the transformed variable}
\PYG{n}{y} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{data.frame}\PYG{p}{(}\PYG{n}{age\PYGZus{}log}\PYG{o}{=}\PYG{n+nf}{log}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Age}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} and here we should check whether this is normally distributed using a qqplot}
\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{age\PYGZus{}log}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nf}{stat\PYGZus{}qq}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{03.e. Continuous Probability Distributions_9_0}.png}

\noindent\sphinxincludegraphics{{03.e. Continuous Probability Distributions_9_1}.png}

\sphinxAtStartPar
The log\sphinxhyphen{}transformed data now looks more symmetrical in the histogram. And the quantile plot is much less \sphinxstyleemphasis{s\sphinxhyphen{}shaped}. While it’s not perfectly straight, it’s probably \sphinxstyleemphasis{good enough} for further analysis which relies on the assumption of normality.


\section{3.5 Joint distributions and correlations}
\label{\detokenize{03.f. Continuous Probability Distributions:joint-distributions-and-correlations}}\label{\detokenize{03.f. Continuous Probability Distributions::doc}}
\sphinxAtStartPar
We are often interested not in the distribution of a single variable but in the relationship between two or more variables. This requires us to understand the concepts of \sphinxstylestrong{joint distributions} and \sphinxstylestrong{correlation}.

\sphinxAtStartPar
Returning to the BMI dataset, a high BMI is indicative of being overweight and this is likely to mean that an individual may have a high percentage of body fat. Typically, those individuals with high BMI may also be at risk of health conditions such as heart disease, which may be indicated by high blood pressure.

\sphinxAtStartPar
If we wish to address questions relating to two or more variables, we need to understand their joint distribution.


\subsection{3.5.1. Joint distributions}
\label{\detokenize{03.f. Continuous Probability Distributions:joint-distributions}}
\sphinxAtStartPar
If we have two random variables \(X\) and \(Y\), the cumulative joint distribution function (CDF) is,
\begin{equation*}
\begin{split}F(x,y) = P(X \leq x,Y \leq y)\end{split}
\end{equation*}
\sphinxAtStartPar
regardless of whether \(X\) and \(Y\) are continuous or discrete. For continuous random variables the joint density function will be \(f(x,y)\) and will be non\sphinxhyphen{}negative and
\begin{equation*}
\begin{split}\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y)\: dy\: dx = 1.\end{split}
\end{equation*}

\subsection{3.5.2 Marginal distributions}
\label{\detokenize{03.f. Continuous Probability Distributions:marginal-distributions}}
\sphinxAtStartPar
We might sometimes want to think about the marginal density of, say, \(X\). This means we want to know the probability of \(X\) irrespective of \(Y\), and consequently we will need to integrate over all possible values of \(Y\). The marginal cdf of \(X\), or \(F_X\) is
\begin{equation*}
\begin{split}F_X (x) = P(X \leq x)\end{split}
\end{equation*}\begin{equation*}
\begin{split} = \lim_{y \rightarrow \infty} F(x,y)\end{split}
\end{equation*}\begin{equation*}
\begin{split} = \int_{-\infty}^{x} \int_{-\infty}^{\infty} f(u,y)\: dy\: du\end{split}
\end{equation*}
\sphinxAtStartPar
From this, it follows that the density function of \(X\) alone, known as the \sphinxstylestrong{marginal density} of \(X\), is
\begin{equation*}
\begin{split}f_x (x) = F_{X}'(x) = \int_{-\infty}^{\infty} f(x,y)\: dy\end{split}
\end{equation*}
\sphinxAtStartPar
Note that this is different to assuming that \(X\) is independent of \(Y\).

\sphinxAtStartPar
So what does this mean in practical terms? Returning to the BMI data we can report that the average BMI (\(\mu_X\)) is 26.46 and the average body fat percentage (\(\mu_Y\)) is 35.31. If BMI and body fat were independent variables knowing BMI would tell us nothing about body fat and \sphinxstyleemphasis{vice versa}. But plotting the data (and some common sense) tells us that this is not the case; if we know one we can say quite a lot about the other. We could explore the correlation between the data (more about this later), but we can also describe these variables together using a joint distribution. By defining them using a joint distribution we are saying nothing about \sphinxstyleemphasis{cause and effect}, just that they are dependent variables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} BMI dataset}

\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/BMI/MindsetMatters.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{head}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}remove observations with no BMI data}
\PYG{n}{dat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{dat}\PYG{p}{[}\PYG{o}{!}\PYG{n+nf}{is.na}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{,}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} scatter plot of BMI and body fat}
\PYG{n+nf}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,}\PYG{n+nf}{aes}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{BMI}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n}{Fat}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nf}{geom\PYGZus{}point}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} report the mean of each variable}
\PYG{c+c1}{\PYGZsh{} note that some values of Y are missing...we need to add na.rm otherwise the estimate will be NA}
\PYG{n}{mux} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{value of mu\PYGZus{}x is \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{mux}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{muy} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{Fat}\PYG{p}{,}\PYG{n}{na.rm}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{value of mu\PYGZus{}y is \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{muy}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A data.frame: 6 × 14
\begin{tabular}{r|llllllllllllll}
  & Cond & Age & Wt & Wt2 & BMI & BMI2 & Fat & Fat2 & WHR & WHR2 & Syst & Syst2 & Diast & Diast2\\
  & <int> & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <int> & <int> & <int> & <int>\\
\hline
	1 & 0 & 43 & 137 & 137.4 & 25.1 & 25.1 & 31.9 & 32.8 & 0.79 & 0.79 & 124 & 118 & 70 & 73\\
	2 & 0 & 42 & 150 & 147.0 & 29.3 & 28.7 & 35.5 &   NA & 0.81 & 0.81 & 119 & 112 & 80 & 68\\
	3 & 0 & 41 & 124 & 124.8 & 26.9 & 27.0 & 35.1 &   NA & 0.84 & 0.84 & 108 & 107 & 59 & 65\\
	4 & 0 & 40 & 173 & 171.4 & 32.8 & 32.4 & 41.9 & 42.4 & 1.00 & 1.00 & 116 & 126 & 71 & 79\\
	5 & 0 & 33 & 163 & 160.2 & 37.9 & 37.2 & 41.7 &   NA & 0.86 & 0.84 & 113 & 114 & 73 & 78\\
	6 & 0 & 24 &  90 &  91.8 & 16.5 & 16.8 &   NA &   NA & 0.73 & 0.73 &  NA &  NA & 78 & 76\\
\end{tabular}\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Error} \PYG{o+ow}{in} \PYG{n}{ggplot}\PYG{p}{(}\PYG{n}{dat}\PYG{p}{,} \PYG{n}{aes}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{BMI}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{Fat}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{n}{could} \PYG{o+ow}{not} \PYG{n}{find} \PYG{n}{function} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ggplot}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n+ne}{Traceback}:
\end{sphinxVerbatim}

\sphinxAtStartPar
So this joint distribution has a joint cdf, \(F(x,y)\) and a continuous piecewise density function \(f(x,y)\). The joint mean is defined as \(\mu_x,\mu_y\) What about the variance? Here we need to consider the variance and covaraince between \(X\) and \(Y\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} correlation between variables}
\PYG{n}{dat2} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{dat}\PYG{p}{[}\PYG{o}{!}\PYG{n+nf}{is.na}\PYG{p}{(}\PYG{n}{dat}\PYG{o}{\PYGZdl{}}\PYG{n}{Fat}\PYG{p}{)}\PYG{p}{,}\PYG{p}{]}
\PYG{n+nf}{round}\PYG{p}{(}\PYG{n+nf}{cov}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n+nf}{cbind}\PYG{p}{(}\PYG{n}{dat2}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{,}\PYG{n}{dat2}\PYG{o}{\PYGZdl{}}\PYG{n}{Fat}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}
\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{variance of BMI = \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n+nf}{var}\PYG{p}{(}\PYG{n}{dat2}\PYG{o}{\PYGZdl{}}\PYG{n}{BMI}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{paste0}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{variance of fat = \PYGZdq{}}\PYG{p}{,}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n+nf}{var}\PYG{p}{(}\PYG{n}{dat2}\PYG{o}{\PYGZdl{}}\PYG{n}{Fat}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 2 × 2 of type dbl
\begin{tabular}{ll}
	 15.850 & 20.696\\
	 20.696 & 36.282\\
\end{tabular}\end{split}
\end{equation*}\begin{equation*}
\begin{split}'variance of BMI = 15.85'\end{split}
\end{equation*}\begin{equation*}
\begin{split}'variance of fat = 36.282'\end{split}
\end{equation*}
\sphinxAtStartPar
The \sphinxstyleemphasis{covariance matrix} is returned. The diagnoals return the variance of each parameter, and the off\sphinxhyphen{}diagnoals the covariance, indicating a positive correlation.


\subsection{3.5.3  Correlation}
\label{\detokenize{03.f. Continuous Probability Distributions:correlation}}
\sphinxAtStartPar
Correlation and covariance are closely related.  Pearson’s correlation coefficient is defined as:
\begin{equation*}
\begin{split} 
\rho(X,Y) = Corr(X,Y) = \frac{Cov(X,Y)}{SD(X)SD(Y)}
\end{split}
\end{equation*}
\sphinxAtStartPar
So this helps us define BMI from body fat and \sphinxstyleemphasis{vice versa}. Examples of when this might be useful include;
\begin{itemize}
\item {} 
\sphinxAtStartPar
Inputing missing data

\item {} 
\sphinxAtStartPar
Summarising many variables with one metric (more about this in the Machine learning module)

\item {} 
\sphinxAtStartPar
Efficient sampling of distributions, which is used in Monte Carlo Markov Chain (MCMC) estimation

\end{itemize}


\subsection{3.5.4 Connections to regression modelling}
\label{\detokenize{03.f. Continuous Probability Distributions:connections-to-regression-modelling}}
\sphinxAtStartPar
Later sessions exploring regression modelling will provide a powerful and flexible approach to exploring and quantifying \sphinxstyleemphasis{dependencies} between variables.


\part{Statistical Inference}


\chapter{Statistical Inference}
\label{\detokenize{04. Inference.Intro:statistical-inference}}\label{\detokenize{04. Inference.Intro::doc}}
\sphinxAtStartPar
This section of the notes concerns a really important part of statistics: \sphinxstyleemphasis{statistical inference}.

\sphinxAtStartPar
Statistical analysis is often separated into two types: descriptive and inferential. Descriptive statistics attempt to describe the data at hand (the sample). Inferential statistics go further \sphinxhyphen{} they attempt to use the data at hand to make statements about a wider population.

\sphinxAtStartPar
There is more than one framework for statistical inference. The traditional and most widely used is the frequentist or “classical” approach. An important alternative, the Bayesian approach, is increasingly influential.


\section{Overview of the statistical inference sessions}
\label{\detokenize{04. Inference.Intro:overview-of-the-statistical-inference-sessions}}
\sphinxAtStartPar
This section of the notes comprises 7 sessions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Population and samples

\item {} 
\sphinxAtStartPar
Likelihood (x2)

\item {} 
\sphinxAtStartPar
Frequentist inference (x2)

\item {} 
\sphinxAtStartPar
Bayesian inference (x2)

\end{itemize}

\sphinxAtStartPar
The first session introduces the concept of \sphinxstylestrong{statistical inference}, defining populations, samples and estimators. The second half of the session introduces the idea of \sphinxstylestrong{sampling distributions}, a fundamental building block for frequentist inference. The sampling distribution gives us information about how different our estimate of the unknown population quantity of interest might have been, had we selected a different sample. In other words, the sampling distribution describes how our estimate behaves under \sphinxstylestrong{repeated sampling}. One particular feature of the sampling distribution, the \sphinxstylestrong{standard error}, gives us information about the amount we might expect our estimate to change if we took a different sample (i.e. it describes the variability of the estimate between different samples).

\sphinxAtStartPar
The idea of sampling distributions is crucial to frequentist inference, but while it gives us important information about how our estimate behaves under repeated sampling, it does not provide a recipe for choosing an estimator. \sphinxstylestrong{Maximum likelihood estimation} (MLE), the subject of the following two sessions, does exactly this. Given a statistical model for the data, MLE provides a method for choosing an estimator with desirable statistical properties.

\sphinxAtStartPar
Having explored MLE to obtain our estimator, we return to the idea of sampling distributions in the following two frequentist inference sessions. We see how the idea of sampling distributions allows us to create \sphinxstylestrong{confidence intervals}, which are ranges of values of the population quantity which we believe are consistent with the observed data. A complementary frequentist inference tool, hypothesis testing, allows us to assess the evidence against a \sphinxstylestrong{null hypothesis}, which proposes a specific value (or range of values) for the unknown population parameter.

\sphinxAtStartPar
Thus far, our attention has been largely on the frequentist paradigm. The last two sessions focus instead on an important alternative approach, \sphinxstylestrong{Bayesian inference}. In this paradigm we do not base our inference on the idea of repeated sampling. Instead, we use the likelihood to update prior information (in the form of a probability distribution) about the unknown parameter, to provide a \sphinxstylestrong{posterior distribution} for the unknown parameter. The posterior can be summarised by obtaining its mean, or a \sphinxstylestrong{credible interval} (interval within which  the unknown parameter falls with a particular probability).


\chapter{4. Populations and Samples}
\label{\detokenize{04.a. Population.and.samples:populations-and-samples}}\label{\detokenize{04.a. Population.and.samples::doc}}
\sphinxAtStartPar
In this session we will begin thinking about statistical inference. Loosely, this describes the process of using a sample of data to make statements about a wider population. There is more than one framework for statistical inference. The traditional and most widely used approach is termed the “classical” or frequentist, and this is the one pursued in this and the next few sessions. An important alternative, the Bayesian approach, is increasingly influential. You will meet Bayesian inference later in this module.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
describe the process of frequentist statistical inference

\item {} 
\sphinxAtStartPar
define the terms population, sample, estimator and estimate

\item {} 
\sphinxAtStartPar
explain what a sampling distribution is and how it relates to the idea of repeated sampling

\item {} 
\sphinxAtStartPar
understand that sample estimates will vary as defined by the standard error

\item {} 
\sphinxAtStartPar
appreciate that the uncertainty in estimates can be described using central limit theorem and resampling (bootstrapping)

\end{itemize}



\sphinxAtStartPar
The five sub\sphinxhyphen{}sections in this session explore the concept of sampling from a population and define parameters and estimators. They formally define the concept of a statistical model. Sampling distributions are described, and the standard error defined, along with key ways of obtaining the approximate sampling distribution.


\section{4.1 Sampling from a population}
\label{\detokenize{04.b. Population.and.samples:sampling-from-a-population}}\label{\detokenize{04.b. Population.and.samples::doc}}
\sphinxAtStartPar
Much of statistical inference is concerned with making statements about properties of populations, based on properties of samples from the populations. A helpful mental picture of the population and the sample to have in mind is as follows. Imagine that the \sphinxstylestrong{population} is a very large number of “objects” contained in a large urn, from which we can randomly sample a relatively small number of the “objects” at a time to provide our \sphinxstylestrong{sample}.

\sphinxAtStartPar
The objects in our population are often called \sphinxstylestrong{sampling units}. For many health research questions these sampling units are  individual patients. If we were collecting information about different hospitals in order to make comparison between different providers, then the sampling unit might be hospitals.

\sphinxAtStartPar
Making statements about a population using information contained in a sample of data relies critically on the process of how the sample was drawn from the population (i.e. the sampling process). A common example of a sample process is random sampling. Under random sampling, each object in the population has the same chance of appearing in the selected sample. Inference procedures tend to be most straightforward in this setting.

\sphinxAtStartPar
In many cases, sampling is not random. For example, many populations have intrinsic structure that might facilitate sampling. If our population is people in rural Gambia, for example, then the easiest way to sample individual people might be to choose 5 villages and then go and survey the people in those villages. While it is not necessarily difficult to modify the process of statistical inference for such situations, statistically invalid conclusions can be reached if such modifications are not undertaken.


\subsection{4.1.1 Parameters and estimators}
\label{\detokenize{04.b. Population.and.samples:parameters-and-estimators}}
\sphinxAtStartPar
In statistical inference, the aim is to make statements about certain features of the population, using the information contained in the sample data. Typically, we quantify the features of interest in terms of unknown population quantities (some examples might be a population mean, standard deviation, proportion, or risk ratio) and attempt to \sphinxstylestrong{estimate} these population quantities. We call these unknown population quantities population \sphinxstylestrong{parameters}. Parameters are typically denoted using Greek letters. Often, certain letters tend to be used for certain types of quantities. For instance, \(\mu\) will often denote a population mean and \(\pi\) will often denote a population proportion. When we are talking about a general “parameter of interest”, we often use the letter \(\theta\).

\sphinxAtStartPar
A \sphinxstylestrong{statistic}, is any quantity that can be calculated from the known measurements on the sample data. It can be any function or combination of random variables that does not depend on unknown parameters for its calculation. As for population parameters, certain letters tend to be used for certain sample statistics. For instance, \(\bar{x}\) (“x bar”) will often denote a sample mean and \(p\) a sample proportion.

\sphinxAtStartPar
We often want to estimate a population parameter from the sample. We do this by using sample statistics to estimate population parameters. For example, the obvious statistic to use to estimate a population mean is the sample mean.  When a sample statistic is used for the purpose of estimating a population parameter it is known as an \sphinxstylestrong{estimator}.  So an estimator is a statistic that is  designed to be a “guess” at a particular parameter of a population.  When we use a sample statistic to esitmate a population parameter we use a “hat” to denote the estimator, e.g. \(\hat{\mu}\) is an estimator for the population quantity \(\mu\) and \(\hat{\theta}\) is an estimator for the population quantity \(\theta\).

\sphinxAtStartPar
Once we have drawn our sample of data and calculated the value of the estimator in that sample, we refer to this as the \sphinxstylestrong{estimate}. In other words, the term estimate is used for the value obtained by substituting sample data values into the formula for the estimator.

\sphinxAtStartPar
The basic structure of frequentist inference can be represented diagrammatically as follows:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{Inference}.png}
\caption{Statistical inference}\label{\detokenize{04.b. Population.and.samples:freq-inference}}\end{figure}


\subsection{4.1.2 Example}
\label{\detokenize{04.b. Population.and.samples:example}}
\sphinxAtStartPar
To explore these issues further, we will consider a study question investigated by a recent MSc student at LSHTM as part of their summer project. The student explored the question of whether people who engage with victims of violence themselves suffer from emotional distress. This question was assessed using a sample of 53 violence researchers in Uganda. Subsequently, these violence researchers took part in a randomised trial, but we will focus on the initial description of the sample. The full article can be found here \sphinxurl{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455179/}

\sphinxAtStartPar
For the purposes of illustration, in this session we will take a smaller sample of 10 violence researchers. Among our 10 sampled violence researchers, the sample mean age and the sample proportion suffering from emotional distress are
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sample mean age \(\bar{x}= 29.75\); sample standard deviation of age \(SD = 4.49\)

\item {} 
\sphinxAtStartPar
Proportion suffering from emotional distress \(p = 26\%\) (14 out of 53)

\end{itemize}

\sphinxAtStartPar
Let \(X_1, ..., X_{10}\) be random variables representing the ages of 53 sampled researchers. In other words, \(X_1, ..., X_{10}\) represent the random process by which the eventual 10 values of age are obtained. We call the realisation of these random variables (i.e. the observed data) \(x_1, ..., x_{10}\).

\sphinxAtStartPar
We will initially focus on the population mean age (\(\mu\)) and its estimator. The obvious estimator for the population mean age is the sample mean age. In terms of the general random variables \(X_1, ..., X_{10}\), we can write this estimator as,
\begin{equation*}
\begin{split}
\bar{X} = \frac{1}{10} \sum_{i=1}^{10} X_i
\end{split}
\end{equation*}
\sphinxAtStartPar
This is a random variable representing the mean of the random variables \(X_1\),…,\(X_{10}\). The sample statistic estimate is,
\begin{equation*}
\begin{split}
\bar{x} = \frac{1}{10} \sum_{i=1}^n x_i = \mbox{sample mean age} = 29.75
\end{split}
\end{equation*}
\sphinxAtStartPar
The estimate is the sample mean age, which is the realisation of \(\bar{X}\) in the observed data. Since we are now viewing this as an estimate of \(\mu\), we can also write \(\hat{\mu} = \bar{x}\).
\begin{quote}

\sphinxAtStartPar
\sphinxstylestrong{Discussion question}
 \sphinxhyphen{} From above, our “best guess” at (our estimate of) the population mean age is 29.75.
 \sphinxhyphen{} Is this estimate a “good” estimate of the population mean? Do we think it is close to being correct?
 \sphinxhyphen{} If we sampled a different 10 researchers would we be likely to see a similar sample mean age? Or could we see a very different sample mean? Is it possible that, just by chance, this is a particularly old (or young) group of researchers?
\end{quote}


\section{4.2 Statistical models}
\label{\detokenize{04.c. Population.and.samples:statistical-models}}\label{\detokenize{04.c. Population.and.samples::doc}}
\sphinxAtStartPar
To extract information about population quantities from sample statistics we need a precise and formal description of the whole sampling process from population to sample. This description is called the \sphinxstylestrong{statistical model}. Relevant features of the population are represented by parameters, such as the mean, variance, or correlation. The structure of the population, together with the sampling process, allows a model to be formulated that describes the statistical behaviour of the sample.

\sphinxAtStartPar
The crucial importance of the statistical model is that, given a certain value of the population parameter (in the simple case where there is only one parameter of interest), it allows us to calculate the probability of drawing a sample with the properties we observe: this will allow us to quantify the compatibility between the observed data and possible values of the population parameter.


\subsection{4.2.1 Example: a statistical model}
\label{\detokenize{04.c. Population.and.samples:example-a-statistical-model}}
\sphinxAtStartPar
We will now write down a formal statistical model for the (sub\sphinxhyphen{}sample from the) emotional distress study. Remember that \(X_1, ...,X_{10}\) are random variables representing the ages of 10 sampled researchers and \(x_1, ..., x_{10}\) are the realised values of these random variables (i.e. the observed ages).

\sphinxAtStartPar
We will assume that each random variable is drawn from the same population distribution, and that the observations are independent of each other. We use the term \sphinxstylestrong{independent and identically distributed} as a succinct way of describing these assumptions, often abbreviated as \sphinxstylestrong{iid}.

\sphinxAtStartPar
Finally, we will assume that ages of violence researchers in the wider population follows a normal distribution with population mean \(\mu\) and population variance \(\sigma^2\).

\sphinxAtStartPar
This model can be compactly written as follows
\begin{equation*}
\begin{split} 
X_i \overset{\small{iid}}{\sim} N(\mu, \sigma^2), \qquad i=1,2,...,10
\end{split}
\end{equation*}

\section{4.3 Sampling distributions}
\label{\detokenize{04.d. Population.and.samples:sampling-distributions}}\label{\detokenize{04.d. Population.and.samples::doc}}
\sphinxAtStartPar
In order to use our estimate (the value of the estimator in our sample of data) to make any sort of statement about the true but unknown value of the population parameter, we need to consider questions such as:
\begin{quote}

\sphinxAtStartPar
How precise do we believe our estimate is?
 Are we fairly certain that the true parameter is close to the estimate, or do we believe the estimate may well be far from the true value?
\end{quote}

\sphinxAtStartPar
The following thought experiment might help to develop these ideas. Suppose our population is a large bucket full of identical marbles. We want to know the population mean weight of a marble (our population parameter of interest). To estimate this population mean, we can simply sample a single marble from the bucket. So our estimator is the weight of the single sampled marble. Now suppose we took two samples: we sample a single marble, weigh it, put it back in the bucket, sample another marble and weight that one. In this case, our estimate (the weight of the sampled marble) would be exactly the same as the estimate from the first sample. No matter how many different samples we took, the sample estimate would be identical. In this case, because all possible samples would give us an identical estimate of the mean, we can confidently say what the population mean is using a single sample of one marble.

\sphinxAtStartPar
Now consider a bucket full of different marbles. In this case, randomly sampling a single marble and using the weight of that marble as an estimate of the population mean weight could give us a weight far too large (if we just happened to sample one of the very large marbles) or far too small (if we happened to pick a very small marble). However, if we were to pick 100 marbles and take the sample mean of those 100 marbles as our estimator, we would expect our estimate to be closer to the population mean. If we were to resample another 100 marbles we would expect the sample mean weight to be fairly close to the mean weight of the previous 100 marbles. Conversely, if we took two samples containing one marble each, we might expect those two weights to be quite different from one\sphinxhyphen{}another.

\sphinxAtStartPar
This thought experiment makes it clear that in order to use our single sample of data to make statements about a wider population, we need to think about what would happen if we repeated our sampling: if we re\sphinxhyphen{}did our study many times, each time calculating the sample estimate, what values would those different sample estimates take? In fact, this is exactly what the \sphinxstylestrong{sampling distribution} is. It is the distribution of the \sphinxstylestrong{estimator} (the statistic we have chosen to use to estimate the population parameter of interest) under repeated sampling.


\subsection{4.3.2 Simulated data: sampling distribution of a mean}
\label{\detokenize{04.d. Population.and.samples:simulated-data-sampling-distribution-of-a-mean}}
\sphinxAtStartPar
We will return again to the emotional distress study. In reality, we do not know the true population mean and standard deviation. However, for the purposes of illustration, for the rest of the session we will imagine that we do know these values. Suppose that, in truth, the population mean age (\(\mu\)) is 30 and the population standard deviation (which will will call \(\sigma\)) is 4.8. Further, suppose that age follows a normal distribution in the population.

\sphinxAtStartPar
Under this scenario, the following code draws many (10,000) different samples from this population, with each sample containing the ages of 10 people. Note the line \sphinxcode{\sphinxupquote{set.seed(1042)}} is coded to keep the same pseudo random number starting point.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Population parameters}
\PYG{n}{mu} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{30}
\PYG{n}{sd} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{4.8}
\PYG{n}{n\PYGZus{}in\PYGZus{}each\PYGZus{}study} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{10}

\PYG{c+c1}{\PYGZsh{} Draw samples and ages for sampled individuals, for 100 different studies}
\PYG{c+c1}{\PYGZsh{} in this example we\PYGZsq{}re going to have a list which generates study\PYGZus{}measurements\PYGZus{}age repeatedly}
\PYG{n}{different\PYGZus{}studies} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{10000}
\PYG{n+nf}{set.seed}\PYG{p}{(}\PYG{l+m}{1042}\PYG{p}{)}
\PYG{n}{study\PYGZus{}measurements\PYGZus{}age} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{list}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nf}{for }\PYG{p}{(}\PYG{n}{i} \PYG{n}{in} \PYG{l+m}{1}\PYG{o}{:}\PYG{n}{different\PYGZus{}studies}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{n}{study\PYGZus{}measurements\PYGZus{}age}\PYG{p}{[[}\PYG{n}{i}\PYG{p}{]]} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{round}\PYG{p}{(}\PYG{n+nf}{rnorm}\PYG{p}{(}\PYG{n}{n\PYGZus{}in\PYGZus{}each\PYGZus{}study}\PYG{p}{,} \PYG{n}{mu}\PYG{p}{,} \PYG{n}{sd}\PYG{p}{)}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Print the sample data for two of the studies}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Ages of the 10 participants selected in study 1:\PYGZdq{}}\PYG{p}{)}
\PYG{n}{study\PYGZus{}measurements\PYGZus{}age}\PYG{p}{[[}\PYG{l+m}{1}\PYG{p}{]]}

\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Ages of the 10 participants selected in study 5:\PYGZdq{}}\PYG{p}{)}
\PYG{n}{study\PYGZus{}measurements\PYGZus{}age}\PYG{p}{[[}\PYG{l+m}{5}\PYG{p}{]]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Ages of the 10 participants selected in study 1:\PYGZdq{}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}\begin{enumerate*}
\item 17.897
\item 30.27
\item 26.896
\item 35.448
\item 28.514
\item 33.891
\item 42.021
\item 25.994
\item 31.061
\item 28.756
\end{enumerate*}\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Ages of the 10 participants selected in study 5:\PYGZdq{}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}\begin{enumerate*}
\item 28.502
\item 33.725
\item 35.155
\item 26.544
\item 31.147
\item 31.732
\item 39.582
\item 31.223
\item 25.802
\item 16.168
\end{enumerate*}\end{split}
\end{equation*}
\sphinxAtStartPar
Now we will calculate the sample mean for each sample of 10 people and plot them on a histogram. In the graphs below, we see a graph of sample means from 10,000 different studies (i.e. 10,000 different samples). This only gives us an approximation to the true sampling distribution, because it is based on a finite number of samples (10,000 samples). However, this is a large number so it will give us a fairly good approxiation to the sampling distribution of the sample mean.

\sphinxAtStartPar
For this estimator and this population, we can see that the sampling distribution follows a normal distribution. Note that the sampling distribution is centred around the true population value of 30. We also see that almost all sample means lie within 4 or so years of the mean either way (i.e. most sample means are between 26 and 34).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{)}
\PYG{n}{sample.means}   \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n}{study\PYGZus{}measurements\PYGZus{}age}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw graphs using base R}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{sample.means}\PYG{p}{[}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{10000}\PYG{p}{]}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,}
     \PYG{n}{breaks}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{25.5}\PYG{p}{,} \PYG{l+m}{26.5}\PYG{p}{,} \PYG{l+m}{27.5}\PYG{p}{,} \PYG{l+m}{28.5}\PYG{p}{,} \PYG{l+m}{29.5}\PYG{p}{,} \PYG{l+m}{30.5}\PYG{p}{,} \PYG{l+m}{31.5}\PYG{p}{,} \PYG{l+m}{32.5}\PYG{p}{,} \PYG{l+m}{33.5}\PYG{p}{,} \PYG{l+m}{34.5}\PYG{p}{,} \PYG{l+m}{100}\PYG{p}{)}\PYG{p}{,} 
     \PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{26}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{green4\PYGZdq{}}\PYG{p}{,}
     \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sample mean age (years)\PYGZdq{}}\PYG{p}{,} 
     \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution of the sample mean age \PYGZbs{}n (from samples of 10)\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{04.d. Population.and.samples_4_0}.png}


\subsection{4.3.3 The standard error of an estimate}
\label{\detokenize{04.d. Population.and.samples:the-standard-error-of-an-estimate}}
\sphinxAtStartPar
When we are talking about the sampling distribution (i.e. the distribution of an \sphinxstyleemphasis{estimator}), we call the standard deviation the \sphinxstylestrong{standard error}. The standard error refers to the variability we might expect in estimates of the parameter, because we are inferring the estimates from a sample. When we have two different estimators for the population parameter of interest, we would typically choose the one with the lower standard error.



\sphinxAtStartPar
If an independently distributed random variable \(X\) has population mean (\(\mu\)) and population variance (\(\sigma^2\)), the sampling distribution of sample means (of samples of size \(n\)) has population mean \(\mu\) and population variance \(\sigma^2/n\). This irrespective of the population distribution; it does not need to be \sphinxstyleemphasis{normal}. In other words the standard error is \(\sigma_{\bar{X}} = \sigma/\sqrt{n}\).




\section{4.4 Obtaining the sampling distribution}
\label{\detokenize{04.e. Population.and.samples:obtaining-the-sampling-distribution}}\label{\detokenize{04.e. Population.and.samples::doc}}
\sphinxAtStartPar
The sampling distribution is hypothetical: in reality, we are not going to repeat our study many times to see how much our estimates differ from sample to sample.

\sphinxAtStartPar
In many cases we can describe the sampling distribution of our estimator well enough to do statistical inference, i.e. well enough to make useful statements about the population parameter. There are three main approaches to obtaining the (approximate) sampling distribution of an estimator:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Algebraic calculation}. Sometimes we can algebraically obtain the distribution of the estimator from our statistical model. An example is given below for the sampling distribution for the sample mean age in the emotional distress example.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{The Central Limit Theorem}. If we have an estimator which can be written as the sum of independent random variables, then for large samples, the estimator will have an approximately normal distribution. This is described in more detail shortly.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resampling}. In many situations, we can use a resampling principle to obtain an approximate sampling distribution.

\end{enumerate}

\sphinxAtStartPar
Returning to the question posed at the start of the previous section, those questions can be answered using the sampling distribution:
\begin{quote}

\sphinxAtStartPar
 How precise do we believe our estimate is?
 Are we fairly certain that the true parameter is close to the estimate, or do we believe the estimate may well be far from the true value?
\end{quote}

\sphinxAtStartPar
The first question asks whether the sampling distribution is centred at the right value (i.e. the population parameter being estimated). If it is, we say the estimator is \sphinxstylestrong{unbiased}. In the epidemiology module and earlier in this module, you have already come across how the \sphinxstyleemphasis{sample} can be biased. Here, we are referring to whether the estimator is biased, which is sometimes referred to as \sphinxstyleemphasis{statistical bias}. Most estimators are unbiased, and this can be shown using statistical theory. For a small number of estimators it can be shown that they are in fact biased, and sometimes a correction can be applied to account for this. An example of exploring whether estimators are biased is given in the Appendix.

\sphinxAtStartPar
The second question will be examined when looking at the standard error and forms the basis for constructing 95\% confidence intervals.


\subsection{4.4.1 Algebraic calculation}
\label{\detokenize{04.e. Population.and.samples:algebraic-calculation}}
\sphinxAtStartPar
We will illustrate the idea of obtaining a sampling distribution via algebraic calculation by revisiting the sub\sphinxhyphen{}sample from the emotional distress study.

\sphinxAtStartPar
For the moment, we will assume that in truth, ages follow a normal distribution with population mean \(\mu=30\) and population standard deviation \(\sigma=4.8\). Of course, in real life we would not have this information.

\sphinxAtStartPar
We now imagine that the population value of \(\mu\) is unknown to the investigators undertaking the study; indeed, making inferences about \(\mu\) is the aim of the study. We further imagine the rather unrealistic (but simplifying) situation that the investigators know the true value of the population standard deviation, \(\sigma=4.8\).

\sphinxAtStartPar
Our model for the emotional distress study states that:
\begin{equation*}
\begin{split} 
X_i \overset{\small{iid}}{\sim} N(\mu, 4.8^2), \qquad i=1,2,...,10
\end{split}
\end{equation*}
\sphinxAtStartPar
Under this statistical model, we want to know the distribution of our estimator for \(\mu\):
\begin{equation*}
\begin{split}
\hat{\mu} = \frac{1}{10} \sum_{i=1}^n X_i
\end{split}
\end{equation*}
\sphinxAtStartPar
In this case, it’s quite easy to derive the sampling distribution algebraically. We use the following fact:
\begin{quote}

\sphinxAtStartPar
The mean of independent normally distributed variables also follows a normal distribution
\end{quote}

\sphinxAtStartPar
It is then  easy to calculate the expectation and variance of \(\hat{\mu}\) using techniques covered in the maths refresher. So we know that the sampling distribution of \(\hat{\mu}\) is:
\begin{equation*}
\begin{split}
\hat{\mu} \sim N\left(\mu, 1.52^2 \right) 
\end{split}
\end{equation*}
\sphinxAtStartPar
where the variance of this normal distribution was obtained as
\begin{equation*}
\begin{split}
Var(\hat{\mu}) = \frac{1}{10^2} \times 10 \times Var(X_i) = \frac{4.8^2}{10}.
\end{split}
\end{equation*}
\sphinxAtStartPar
This gives us a lot of useful information about how the sampling distribution in relation to the unknown parameter \(\mu\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
It follows a normal distribution (has a symmetric bell\sphinxhyphen{}shape)

\item {} 
\sphinxAtStartPar
It is centred around the true (unknown) population value

\item {} 
\sphinxAtStartPar
The standard error of the sample mean (the standard deviation of the estimator) is  \(1.52\).

\end{itemize}

\sphinxAtStartPar
In many situations, this sort of algebraic calculation is possible. If not, we often rely on the central limit theorem to obtain the approximate sampling distribution in large samples.


\subsection{4.4.2 The Central Limit Theorem}
\label{\detokenize{04.e. Population.and.samples:the-central-limit-theorem}}
\sphinxAtStartPar
The Central Limit Theorem (CLT) is a key concept in statistics and in estimation. When we use the mean from a sample to estimate a parameter, we already acknowledge that there will be some error around this estimate, as described above. The CLT takes this further;



\sphinxAtStartPar
If a random variable \(X\) has population mean \(\mu\) and population variance \(\sigma^2\) the sample mean \(\bar{X}\), based on \(n\) observations, is \sphinxstyleemphasis{approximately} normally distributed with mean \(\mu\) and variance \(\sigma^2/n\), for sufficiently large \(n\).



\sphinxAtStartPar
So even for situations where \(X\) follows a distribution that is not even close to being normal (e.g. \(X\) might be Poisson, or Binomial, or some wacky distribution), for sufficiently large samples the \sphinxstyleemphasis{mean} will follow a normal distribution. An example where \(X\) is a binary variable is given in the Appendix to this session.

\sphinxAtStartPar
This theorem is hugely powerful. We will see that this allows us to conduct large\sphinxhyphen{}sample inference fairly easily on any type of data.


\subsection{4.4.3 Resampling}
\label{\detokenize{04.e. Population.and.samples:resampling}}
\sphinxAtStartPar
An alternative approach, which is computationally intensive but very flexible, is to use a resampling approach.

\sphinxAtStartPar
For a population of interest, we want to estimate a parameter \(\theta\) using a sample \(S\) of \(n\) individuals (for our example \(n=10\)) from the population. We have an estimator \(\hat{\theta}\) from our sample \(S\). We want to know about the sampling distribution of the estimator \(\hat{\theta}\).

\sphinxAtStartPar
We discussed above the idea that we could obtain the sampling distribution by repeatedly sampling from the population and calculating our estimate in each sample. Then a histogram of those many estimates would give us (approximately) the sampling distribution. In practice, it is logistically impossible to repeat the study a large number of times. However, we can mimic this process by using resampling.

\sphinxAtStartPar
The basic idea is to pretend that the observed data are the population and repeatedly sample from the data to learn about the relationship between \(\hat{\theta}\) and the estimates obtained from the re\sphinxhyphen{}sampled data, which we will call \(\hat{\theta}^*\).

\sphinxAtStartPar
Suppose we sample with replacement from the sample \(S\) to obtain “sub\sphinxhyphen{}samples” also of size \(n\). These “sub\sphinxhyphen{}samples” are called \sphinxstylestrong{bootstrap samples}.

\sphinxAtStartPar
For example, suppose we have a sample \(S\) of size 10 (\(n=10\)):
\begin{equation*}
\begin{split}
S = \{ x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_{10} \}
\end{split}
\end{equation*}
\sphinxAtStartPar
And suppose our estimate is the sample mean,
\begin{equation*}
\begin{split}
\hat{\theta} = \frac{(x_1 + x_2 +  x_3 +  x_4 +  x_5 + x_6 + x_7 + x_8 + x_9 + x_{10})}{10}
\end{split}
\end{equation*}
\sphinxAtStartPar
Then a bootstrap sample might be:
\begin{equation*}
\begin{split}
S^*_1 = \{x_{10}, x_3, x_2, x_8, x_6, x_2, x_4, x_1, x_8, x_1 \}
\end{split}
\end{equation*}
\sphinxAtStartPar
Another bootstrap sample could be:
\begin{equation*}
\begin{split}
S^*_2 = \{ x_5, x_9, x_4, x_7, x_{10}, x_9, x_3, x_4, x_6, x_2 \}
\end{split}
\end{equation*}
\sphinxAtStartPar
In each bootstrap sample, we obtain a new estimate (the sample mean in the bootstrap sample):
\begin{equation*}
\begin{split}
\hat{\theta}^*_1 = \frac{(x_{10} + x_3 + x_2 + x_8 + x_6 + x_2 + x_4 + x_1 + x_8 + x_1)}{10}
\end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split}
\hat{\theta}^*_2 = \frac{(x_5 + x_9 + x_4 + x_7 + x_{10} + x_9 + x_3 + x_4 + x_6 + x_2)}{10}
\end{split}
\end{equation*}
\sphinxAtStartPar
We do this a very large number of times to obtain lots of estimates from different bootstrap samples. Then we can draw a histogram of these many bootstrap estimates to see the shape and dispersion of the distribution.

\sphinxAtStartPar
The \sphinxstylestrong{bootstrap principle} says that the distribution of \(\hat{\theta}\) given \(\theta\) (i.e. the sampling distribution) is approximated by the distribution of \(\hat{\theta}^*\) given \(\hat{\theta}\).  For example, if we find that our values of \(\hat{\theta}^*\) are approximately normally distributed and centred around \(\hat{\theta}\) then the bootstrap principle tells us that \(\hat{\theta}\) follows a normal distribution centred around \(\theta\).


\subsubsection{4.4.3.1 Example: resampling}
\label{\detokenize{04.e. Population.and.samples:example-resampling}}
\sphinxAtStartPar
We illustrate the idea of resampling using the sub\sphinxhyphen{}sample of the emotional distress study. Suppose our data \sphinxhyphen{} the 10 sampled ages \sphinxhyphen{} are the set: \(\{ 28.1, 27.5, 25, 29.9, 29.7, 29.9, 39.9, 33.6, 21.3, 30.8 \}\). Our estimate of the population age is the sample mean age, which is: \(\hat{\mu} = 29.57\).

\sphinxAtStartPar
To obtain an approximation to the sampling distribution for the sample mean age, using a resampling approach, we first take a large number of bootstrap samples from the data. The code below does this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Our sample of data (ages for 10 sampled researchers)}
\PYG{n}{ages} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{28.1}\PYG{p}{,}\PYG{l+m}{27.5}\PYG{p}{,}\PYG{l+m}{25}\PYG{p}{,}\PYG{l+m}{29.9}\PYG{p}{,}\PYG{l+m}{29.7}\PYG{p}{,}\PYG{l+m}{29.9}\PYG{p}{,}\PYG{l+m}{39.9}\PYG{p}{,}\PYG{l+m}{33.6}\PYG{p}{,}\PYG{l+m}{21.3}\PYG{p}{,}\PYG{l+m}{30.8}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Randomly select 10,000 bootstrap samples (each of size 10)}
\PYG{n+nf}{set.seed}\PYG{p}{(}\PYG{l+m}{532}\PYG{p}{)} 
\PYG{n}{bootstrap\PYGZus{}samples} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{lapply}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{10000}\PYG{p}{,} \PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)} \PYG{n+nf}{sample}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{,} \PYG{n}{replace} \PYG{o}{=} \PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} List some of the bootstrap samples}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{First bootstrap sample:\PYGZdq{}}\PYG{p}{)}
\PYG{n}{bootstrap\PYGZus{}samples}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Third bootstrap sample:\PYGZdq{}}\PYG{p}{)}
\PYG{n}{bootstrap\PYGZus{}samples}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{500th bootstrap sample:\PYGZdq{}}\PYG{p}{)}
\PYG{n}{bootstrap\PYGZus{}samples}\PYG{p}{[}\PYG{l+m}{500}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}First bootstrap sample:\PYGZdq{}
\end{sphinxVerbatim}
\begin{align*}\!\begin{aligned}
\begin{enumerate}
\item \begin{enumerate*}
\item 27.5
\item 29.9
\item 28.1
\item 27.5
\item 29.9
\item 21.3
\item 27.5
\item 28.1
\item 29.9
\item 30.8
\end{enumerate*}\\
\end{enumerate}\\
\end{aligned}\end{align*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Third bootstrap sample:\PYGZdq{}
\end{sphinxVerbatim}
\begin{align*}\!\begin{aligned}
\begin{enumerate}
\item \begin{enumerate*}
\item 30.8
\item 28.1
\item 21.3
\item 29.9
\item 39.9
\item 21.3
\item 27.5
\item 29.9
\item 29.9
\item 29.7
\end{enumerate*}\\
\end{enumerate}\\
\end{aligned}\end{align*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}500th bootstrap sample:\PYGZdq{}
\end{sphinxVerbatim}
\begin{align*}\!\begin{aligned}
\begin{enumerate}
\item \begin{enumerate*}
\item 21.3
\item 29.9
\item 39.9
\item 29.9
\item 29.9
\item 27.5
\item 29.9
\item 29.9
\item 29.9
\item 21.3
\end{enumerate*}\\
\end{enumerate}\\
\end{aligned}\end{align*}
\sphinxAtStartPar
The next step is to calculate the estimate (the sample mean, in our case) in each bootstrap sample. These estimates are called \(\hat{\mu}^*_1, \hat{\mu}^*_2, .., \hat{\mu}^*_{10,000}\). Then we can plot the histogram of all the estimates across the bootstrap samples.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Calculate the sample mean in each of the bootstrap samples}
\PYG{n}{r.mean} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}samples}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw a histogram with a red vertical line indicating the original sample mean age}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{r.mean}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Distribution of sample means \PYGZbs{}n across the bootstrap samples\PYGZdq{}}\PYG{p}{,} 
     \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Bootstrap sample means\PYGZdq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{cornflowerblue\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{)}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{04.e. Population.and.samples_7_0}.png}

\sphinxAtStartPar
We see a number of features from the graph above;
\begin{quote}

\sphinxAtStartPar
\sphinxhyphen{} The histogram follows an approximately normal distribution (has a symmetric bell\sphinxhyphen{}shape)
\sphinxhyphen{} It is centred around the sample mean age (from the original sample, \(\hat{\mu} = 29.57\))
\sphinxhyphen{} The code below tells us that the standard deviation of this distribution is 1.51.
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{n+nf}{var}\PYG{p}{(}\PYG{n}{r.mean}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}1.50868637530863\end{split}
\end{equation*}
\sphinxAtStartPar
So we have seen that the bootstrap approximation of the distribution of \(\hat{\mu}^*\) given \(\hat{\mu}\) is a normal distribution centred around \(\hat{\mu}\) with standard deviation of 1.51. The bootstrap principle tells us that the distribution of \(\hat{\mu}\) given \(\mu\) is approximately the same. In other words, approximately:
\begin{equation*}
\begin{split}
\hat{\mu} \sim N(\mu, 1.51^2)
\end{split}
\end{equation*}
\sphinxAtStartPar
Remember, that we obtained the true distribution algebraically above and found that
\begin{equation*}
\begin{split}
\hat{\mu} \sim N(\mu, 1.52^2)
\end{split}
\end{equation*}
\sphinxAtStartPar
So the resampling (bootstrap) approach has given us a very good approximation to the true sampling distribution. The code below redraws the histogram above, with the approximate (bootstrap) sampling distribution and the algebraically\sphinxhyphen{}calculated one.

\sphinxAtStartPar
We see that the bootstrap sampling distribution (shown in red) is simply a shift of the normal distribution which best follows the histogram (shown in orange), and that the bootstrap and true (algebraic, shown in blue) sampling distributions are very similar.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Histogram of estimates (sample means) in bootstrap samples}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{r.mean}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Distribution of sample means \PYGZbs{}n across the bootstrap samples\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Bootstrap sample means\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add the normal distribution which most closely follows the histogram}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{25}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{,} \PYG{n}{by}\PYG{o}{=}\PYG{n}{.}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{25}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{,} \PYG{n}{by}\PYG{o}{=}\PYG{n}{.}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{)}\PYG{p}{,} \PYG{l+m}{1.52}\PYG{p}{)}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{orange\PYGZdq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add the bootstrap approximation to the sampling distribution: normal distribution with mean mu=30 SD=1.51}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{25}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{,} \PYG{n}{by}\PYG{o}{=}\PYG{n}{.}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{25}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{,} \PYG{n}{by}\PYG{o}{=}\PYG{n}{.}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,} \PYG{l+m}{30}\PYG{p}{,} \PYG{l+m}{1.51}\PYG{p}{)}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add the algebraic sampling distribution: normal distribution with mean mu=30 SD=1.52}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{25}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{,} \PYG{n}{by}\PYG{o}{=}\PYG{n}{.}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{25}\PYG{p}{,} \PYG{l+m}{35}\PYG{p}{,} \PYG{n}{by}\PYG{o}{=}\PYG{n}{.}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,} \PYG{l+m}{30}\PYG{p}{,} \PYG{l+m}{1.52}\PYG{p}{)}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add a vertical line at original sample mean}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{)}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{green\PYGZdq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{04.e. Population.and.samples_11_0}.png}


\subsection{4.4.5 What do we use a sampling distribution for?}
\label{\detokenize{04.e. Population.and.samples:what-do-we-use-a-sampling-distribution-for}}
\sphinxAtStartPar
In practice, we have a single sample of data and a single estimate from that sample of the population parameter of interest.  When we present our single estimate of a population quantity, we need to be able to say something about how precise it is. Is it likely to be close to the true value? Can we provide a range of values within which we believe the true value lies?

\sphinxAtStartPar
We can only answer these questions, within the framework of frequentist statistical inference, by thinking about what estimates we might have got had we chosen a different sample. This leads us to the sampling distribution \sphinxhyphen{} the distribution of the estimator across samples.

\sphinxAtStartPar
In subsequent sessions we will see how the sampling distribution allows us to
\begin{itemize}
\item {} 
\sphinxAtStartPar
construct confidence intervals for population parameters (intervals within which we believe the true value is likely to lie)

\item {} 
\sphinxAtStartPar
conduct hypothesis tests for population parameters

\end{itemize}


\section{4.5 Summary}
\label{\detokenize{04.f. Population.and.samples:summary}}\label{\detokenize{04.f. Population.and.samples::doc}}\begin{itemize}
\item {} 
\sphinxAtStartPar
In this session we have introduced the concept of populations and samples, and how we use statistical inference to make statements about the unknown population parameters.

\item {} 
\sphinxAtStartPar
These unknown popultion parameters are estimated by sample statistics. Variability in the sample and any associated statistics are to be expected.

\item {} 
\sphinxAtStartPar
The variability in the sample statistics can be quantified by estimating the standard error.

\end{itemize}


\section{Appendix: Additional Reading}
\label{\detokenize{04.g. Population.and.samples:appendix-additional-reading}}\label{\detokenize{04.g. Population.and.samples::doc}}
\sphinxAtStartPar
This appendix section contains additional information which will deepen your understanding. However, it is not examinable and is completely optional reading.


\subsection{A1 More on populations}
\label{\detokenize{04.g. Population.and.samples:a1-more-on-populations}}
\sphinxAtStartPar
There are additional issues related to the definition of the population, that should be considered.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Is the population well defined?

\end{itemize}

\sphinxAtStartPar
Loosely speaking, we think about the population as being the wider group (often of people or patients) who we can generalise the results to. For some research questions the population of interest is well defined. For instance, suppose we undertake a study where we are attempting to estimate the proportion of adults (18 years and above) in the UK with hypertension in 2020. The population is well defined. Conversely, suppose we undertake a study to estimate the effect of a blood\sphinxhyphen{}pressure\sphinxhyphen{}lowering treatment among a sample of 50 patients in the UK in 2020. In this case, the population of interest can be difficult to pin down. Who can we generalize our results to? Is the population restricted in time and space? Can we generalise to patients in other countries? Can we generalise to future patients?
\begin{itemize}
\item {} 
\sphinxAtStartPar
Is the sample representative of the population?

\end{itemize}

\sphinxAtStartPar
Clearly a sample can be chosen in may ways, and the way in which we are able to make inferences about the population depends critically on the way in which the sample is selected: it is hard to over\sphinxhyphen{}emphasize the importance and relevance of the sampling process to the meaning and validity of the subsequent inferences. In this module, we will assume that sampling units (in this case, people) are randomly sampled from the population.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Is the population finite, or (effectively or potentially) infinite?

\end{itemize}

\sphinxAtStartPar
For example, a study of a new treatment for a disease may wish to generalise to all potential patients.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Have we sampled all the population?

\end{itemize}

\sphinxAtStartPar
For example, a study of leukemia in the years following a leak from a nuclear power station may sample all subjects developing leukemia within the relevant time period in the vicinity of the power station. In such an example it is not clear how to define a wider population from which the sample can be considered to have been drawn. In these and other cases one approach is to consider a notional or counterfactual population, which can only have a conceptual existence.

\sphinxAtStartPar
In general the issues can be complex and will not be considered further here.


\subsection{A2 Bias of estimators}
\label{\detokenize{04.g. Population.and.samples:a2-bias-of-estimators}}
\sphinxAtStartPar
Using statistical theory it is possible to show that the sample mean, \(\bar{X}\), is an unbiased estimator of the population mean, \(\mu\). One of the simplest examples is when our random variables follow the \sphinxstyleemphasis{Bernoulli} distribution.

\sphinxAtStartPar
\sphinxstylestrong{Example} Let \(X_1, X_2,.., X_n\) be Bernoulli trials with success parameter \(p\).

\sphinxAtStartPar
Our estimate of \(p\) is the sample mean,
\begin{equation*}
\begin{split}
\hat{p} = \bar{X} = \frac{X_1 + X_2 + ... + X_n}{n}
\end{split}
\end{equation*}
\sphinxAtStartPar
We will now show that the expected value of this estimator is equal to the population mean, \(p\).
\begin{equation*}
\begin{split}
\begin{align*}
E(\bar{X}) & = E\left[\frac{X_1 + X_2 + ... + X_n}{n}\right] \\
& = \frac{1}{n} E[X_1 + X_2 + ... + X_n] \qquad  \mbox{(we can take constants out of expectations)} \\
&= \frac{1}{n} (E(X_1) + E(X_2) + ... + E(X_n)) \\
&= \frac{1}{n} (p + p + ... + p) = p \\
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
This simple use of algebra illustrates that \(\bar{X}\) is an unbiased estimator for \(p\).

\sphinxAtStartPar
\sphinxstylestrong{Exercise:} You can use similar logic to demonstrate that the sample mean is an unbiased estimator for the population mean when the random variables \(X\) follow a normal distribution with known variance.


\subsection{A3 CLT for binary data}
\label{\detokenize{04.g. Population.and.samples:a3-clt-for-binary-data}}
\sphinxAtStartPar
We will return to the emotional distress study again, using the sub\sphinxhyphen{}sample of 10 people, but this time measure a binary characteristic for each person \sphinxhyphen{} the presence of emotional distress.

\sphinxAtStartPar
We suppose that, in the population, the true proportion is 28\%. Under this assumption, we can simulate (draw) different samples,  each containing 10 people. If we do this a very large number of times, say 10,000, then the distribution of the different sample proportions we obtain will give us a very good picture of the true sampling distribution of the proportion. (Of course, remember that in practice we cannot do this because we won’t know the true population proportion).

\sphinxAtStartPar
The code below obtains the approximate sampling distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Population parameters}
\PYG{k+kc}{pi} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{0.28}
\PYG{n}{n\PYGZus{}in\PYGZus{}study} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{10}

\PYG{c+c1}{\PYGZsh{} Simulate data from multiple studies}
\PYG{n}{different\PYGZus{}studies} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{10000}
\PYG{n+nf}{set.seed}\PYG{p}{(}\PYG{l+m}{1042}\PYG{p}{)}
\PYG{n}{study\PYGZus{}measurements\PYGZus{}ed} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{list}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nf}{for }\PYG{p}{(}\PYG{n}{i} \PYG{n}{in} \PYG{l+m}{1}\PYG{o}{:}\PYG{n}{different\PYGZus{}studies}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{n}{study\PYGZus{}measurements\PYGZus{}ed}\PYG{p}{[[}\PYG{n}{i}\PYG{p}{]]} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rbinom}\PYG{p}{(}\PYG{n}{n\PYGZus{}in\PYGZus{}study}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{k+kc}{pi}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Calculate the proportion in each study}
\PYG{n}{sample.props}   \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n}{study\PYGZus{}measurements\PYGZus{}ed}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw graphs}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{sample.props}\PYG{p}{[}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{10000}\PYG{p}{]}\PYG{p}{,} 
     \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,} \PYG{n}{breaks}\PYG{o}{=}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}0.05}\PYG{p}{,} \PYG{l+m}{0.95}\PYG{p}{,} \PYG{l+m}{0.05}\PYG{p}{)}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{cornflowerblue\PYGZdq{}}\PYG{p}{,}
     \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sample proportion with emotional distress\PYGZdq{}}\PYG{p}{,} 
     \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution of the sample \PYGZbs{}n proportion (samples of n=10)\PYGZdq{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} the \PYGZdq{}\PYGZbs{}n\PYGZdq{} makes a newline}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{04.g. Population.and.samples_5_0}.png}

\sphinxAtStartPar
The graph above shows us a reasonably accurate picture of the sampling distribution. Unlike the sample mean, the sampling distribution of the sample proportion is not quite symmetric. It is also not continuous \sphinxhyphen{} the sample statistic can only take 10 different values, with a sample size of \(n=10\).

\sphinxAtStartPar
Below, the code shows that the mean of the sample means is approximately 0.28. (The discrepancy is just random error due to the fact that our “sampling distribution” does not come from an infinite number of samples. If we simulated a sufficiently large number of samples, this number would become closer to the true value of 0.28.)

\sphinxAtStartPar
The final line of code below lists the (approximate) probability density function, which gives us the whole sampling distribution for the sample proportion in this example.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} Summarise the approximate sampling distribution}

\PYG{c+c1}{\PYGZsh{} The mean value of the different sample means}
\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{sample.props}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} The whole sampling distribution (i.e. the PDF)}
\PYG{n+nf}{table}\PYG{p}{(}\PYG{n}{sample.props}\PYG{p}{)}\PYG{o}{/}\PYG{n}{different\PYGZus{}studies}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.27911\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
sample.props
     0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9 
0.0365 0.1439 0.2630 0.2592 0.1793 0.0876 0.0257 0.0043 0.0004 0.0001 
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we will explore what happens to the sampling distribution as we take larger samples. So instead of 10 people per sample, suppose we had 100 or 10,000 people in each sample. The central limit theorem tells us we expect the distribution to become more normal as we increase the sample size.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x10n} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{10}\PYG{p}{,}\PYG{l+m}{1000}\PYG{p}{)}\PYG{p}{,}\PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{n+nf}{rbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{l+m}{0.28}\PYG{p}{)}\PYG{p}{)} 
\PYG{n}{x10mean} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{colMeans}\PYG{p}{(}\PYG{n}{x10n}\PYG{p}{)}
                            
\PYG{n}{x50n} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{50}\PYG{p}{,}\PYG{l+m}{1000}\PYG{p}{)}\PYG{p}{,}\PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{n+nf}{rbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{l+m}{0.28}\PYG{p}{)}\PYG{p}{)} 
\PYG{n}{x50mean} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{colMeans}\PYG{p}{(}\PYG{n}{x50n}\PYG{p}{)}

\PYG{n}{x100n} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{100}\PYG{p}{,}\PYG{l+m}{1000}\PYG{p}{)}\PYG{p}{,}\PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{n+nf}{rbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{l+m}{0.28}\PYG{p}{)}\PYG{p}{)} 
\PYG{n}{x100mean} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{colMeans}\PYG{p}{(}\PYG{n}{x100n}\PYG{p}{)}

\PYG{n}{x1000n} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{1000}\PYG{p}{,}\PYG{l+m}{1000}\PYG{p}{)}\PYG{p}{,}\PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{n+nf}{rbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{l+m}{0.28}\PYG{p}{)}\PYG{p}{)} 
\PYG{n}{x1000mean} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{colMeans}\PYG{p}{(}\PYG{n}{x1000n}\PYG{p}{)}
                                
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{2}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{x10mean}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{cornflowerblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution of the sample \PYGZbs{}n proportion (samples of n=10)\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{x50mean}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{cornflowerblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution of the sample \PYGZbs{}n proportion (samples of n=50)\PYGZdq{}}\PYG{p}{)}        
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{x100mean}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{cornflowerblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution of the sample \PYGZbs{}n proportion (samples of n=100)\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{x1000mean}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{cornflowerblue\PYGZdq{}}\PYG{p}{,}\PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution of the sample \PYGZbs{}n proportion (samples of n=1,000)\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{04.g. Population.and.samples_9_0}.png}

\sphinxAtStartPar
As predicted by the CLT, even though the original data are binary (each person has emotional distress or does not), the sample proportion (which is the mean of the binary outcomes) has a distribution which becomes approximately normal with sufficiently large samples.


\chapter{5. Likelihood}
\label{\detokenize{05.a. Likelihood:likelihood}}\label{\detokenize{05.a. Likelihood::doc}}
\sphinxAtStartPar
In statistical inference, our task is to make statements about the underlying parameter(s) of our proposed model given the observed data. In particular, we typically wish to obtain the best estimate of the unknown parameters.  We also wish to know how well we have estimated the unknown parameter(s). The concept of likelihood provides the best single framework for this task. We will see that the likelihood function, often simply called the likelihood, plays a fundamental role in both frequentist and Bayesian inference.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain the concepts of likelihood and maximum likelihood estimation

\item {} 
\sphinxAtStartPar
derive a likelihood in a simple situation

\item {} 
\sphinxAtStartPar
explain the connection between maximising the likelihood and maximising the log\sphinxhyphen{}likelihood

\item {} 
\sphinxAtStartPar
describe and apply the process of obtaining a maximum likelihood estimator

\end{itemize}



\sphinxAtStartPar
The next sub\sphinxhyphen{}sections introduce the idea of maximum likelihood estimation, define the likelihood and log\sphinxhyphen{}likelihood functions and illustrate the process of obtaining the maximum likelihood estimator.


\section{5.1 Maximum likelihood estimation}
\label{\detokenize{05.b. Likelihood:maximum-likelihood-estimation}}\label{\detokenize{05.b. Likelihood::doc}}
\sphinxAtStartPar
Suppose we are interested in the probability that a single patient will experience a particular side effect from a particular drug. We decide to run a small clinical study including 8 patients. The observed data consist of the number, of those 8 patients, who experience a side effect. Suppose that we conduct the study and observe that 2 patients experience a side effect. We wish to use these observed data to make statements \sphinxhyphen{} inferences \sphinxhyphen{} about the unknown probability of experiencing a side effect from that drug.

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:} We begin by defining a model for the data. Here, we define \(X\) as the random variable representing the total number of the 8 patients who experience a side effect. Our model is that
\begin{equation*}
\begin{split}
X \sim binomial(8, \pi)
\end{split}
\end{equation*}
\sphinxAtStartPar
which \sphinxhyphen{} we remember from the probability sessions \sphinxhyphen{} involves the assumptions that each Bernoulli event (whether or not each individual patient experiences a side effect) is independent and has the same probability of occurring.

\sphinxAtStartPar
This model involves the unknown parameter \(\pi\).

\sphinxAtStartPar
\sphinxstylestrong{Data:} We have observed a realisation from this model, \(X=2\). These are often called our observed data.

\sphinxAtStartPar
Under our proposed statistical model, the probability that 2 out of 8 patients experience a side effect is:
\begin{equation*}
\begin{split}
P(X=2) = {8 \choose 2} \pi^2 (1-\pi)^6
\end{split}
\end{equation*}
\sphinxAtStartPar
Since \(\pi\) is unknown, it is natural to consider how the probability of observing these data varies with different values of \(\pi\):


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\pi\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
P(\(X\)=\(2\))
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
0.25
&
\sphinxAtStartPar
0.311
\\
\hline
\sphinxAtStartPar
0.5
&
\sphinxAtStartPar
0.109
\\
\hline
\sphinxAtStartPar
0.75
&
\sphinxAtStartPar
0.004
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Suppose that, in truth, the unknown probability of a patient experiencing a side effect from this drug was 0.75. The probability of then observing 2 from 8 patients experiencing a side effect is 0.004. This is a very low probability, so this would be an unusual or perhaps unexpected event, although not strictly impossible.

\sphinxAtStartPar
Suppose that, conversely, the unknown probability of a patient experiencing a side effect from this drug was actually 0.25. Then the probability of observing 2 from 8 patients experiencing a side effect would be 0.31 (\(31\%\)). If this were the case, there would be nothing unusual or unexpected about our observed data.

\sphinxAtStartPar
We do not know which value of \(\pi\) is the true value. But a sensible strategy to obtain a ‘best guess’, or estimate, of \(\pi\), might be to pick the value which maximises the probability of observing the data that we observed. We will see below that this probability is in fact the likelihood, leading to the concept of maximising the likelihood or maximum likelihood. This is a term that you will encounter frequently in statistics.

\sphinxAtStartPar
Following these ideas, we can extend the table above by considering a finer range of possible values for \(\pi\) between 0 and 1, and plot the probability of observing \(X=2\), assuming that that value of \(\pi\) were true. This gives the graph below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define a range of values for pi}
\PYG{k+kc}{pi} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{n}{by} \PYG{o}{=} \PYG{l+m}{0.05}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate the likelihood for each value, given n=8 and x=2}
\PYG{n}{L\PYGZus{}pi} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{choose}\PYG{p}{(}\PYG{l+m}{8}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{o}{*}\PYG{k+kc}{pi}\PYG{o}{\PYGZca{}}\PYG{l+m}{2}\PYG{o}{*}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{k+kc}{pi}\PYG{p}{)}\PYG{o}{\PYGZca{}}\PYG{p}{(}\PYG{l+m}{8}\PYG{l+m}{\PYGZhy{}2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the output}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{k+kc}{pi}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{L\PYGZus{}pi}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add a line to indicate the value which yields the highest likelihood}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{k+kc}{pi}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{L\PYGZus{}pi}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{05.b. Likelihood_1_0}.png}

\sphinxAtStartPar
We see that \(\pi=0.25\) is the value that leads to the highest probability of observing the data that we did indeed observe (i.e \(X=2\)) so we choose this value as our best guess for \(\pi\). We will see that this value is called the maximum likelihood estimator. We write \(\hat{\pi} = 0.25\), where we have added a hat to indicate that this is being viewed as an estimate of an unknown parameter.

\sphinxAtStartPar
The likelihood when \(\pi = 0\) is exactly zero, as is the likelihood when \(\pi = 1\). This makes sense because these two probabilities would make the observed data impossible \sphinxhyphen{} they imply that patients would either \sphinxstyleemphasis{never} or \sphinxstyleemphasis{always} experience side effects. Informally, we could say that these values are \sphinxstyleemphasis{inconsistent} with the data.

\sphinxAtStartPar
Note that, our estimate of the probability of a patient experiencing a side effect is intuitively a sensible one: it is the sample proportion, \(\frac{2}{8}\).

\sphinxAtStartPar
We will see later on that estimators obtained in this way (by maximising a likelihood) have very nice statistical properties.


\section{5.2 The likelihood}
\label{\detokenize{05.c. Likelihood:the-likelihood}}\label{\detokenize{05.c. Likelihood::doc}}
\sphinxAtStartPar
The function that we maximised above to find our estimate for the unknown parameter \(\pi\) took the same algebraic appearance as the probability distribution function, evaluated at the value of the observed data. We will see below that this function is called the likelihood. The likelihood looks like a probability distribution function. It has a probabilistic interpretation for any particular value of \(\pi\): it’s the probability of seeing the observed data assuming that is the true value of \(\pi\). However, in contrast to the probability distribution function, which is a function of \(x\) and sums to 1 over all possible values of \(x\), the likelihood function is a function of \(\pi\). So, for example, this does not sum to 1 over all possible values of \(\pi\).

\sphinxAtStartPar
A general definition of the likelihood is as follows.

\sphinxAtStartPar
For a probability model with parameter \(\theta\), the likelihood of the parameter \(\theta\) given the observed data \(x\) is defined as
\begin{equation*}
\begin{split}
L(\theta | x) = P(x | \theta)
\end{split}
\end{equation*}
\sphinxAtStartPar
On the right hand side of this equation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
This is either a probability distribution function or a density function

\item {} 
\sphinxAtStartPar
If our distribution is discrete, as above, this is: \(P(x | \theta) = P(X=x)\)

\item {} 
\sphinxAtStartPar
If our distribution is continuous, this becomes: \(P(x | \theta) = f(x)\)

\item {} 
\sphinxAtStartPar
\(P(x | \theta)\) is a probability statement. It is the probability of seeing the observed data, under the assumed model, assuming that the true parameter value is equal to \(\theta\).

\end{itemize}

\sphinxAtStartPar
And on the left hand side of this equation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(L(\theta | x)\) is the likelihood function, often just called the likelihood.

\end{itemize}

\sphinxAtStartPar
In an informal sense the likelihood conveys the \sphinxstyleemphasis{consistency} of different values of the parameter with the observed data.

\sphinxAtStartPar
We often just write the likelihood as \(L(\theta)\). The additional notation (writing “\(| x\)”) is merely to remind ourselves that the likelihood function involves the observed data, but it is not a function of these: \(x\) is treated as a fixed quantity in the likelihood.


\subsection{5.2.1 Example: the Binomial model}
\label{\detokenize{05.c. Likelihood:example-the-binomial-model}}
\sphinxAtStartPar
Consider a diabetes clinic at which patients present following initial diagnosis. The first line of intervention for diabetes is lifestyle change, and the clinician wants to determine what proportion of patients will respond to this intervention. She decides to conduct a study by following up twenty patients who present to the clinic in one day.

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:} We assume that a binomial model is appropriate for the number of patients who will respond to lifestyle changes out of the twenty patients in total.
\begin{equation*}
\begin{split} X \sim binomial(20, \pi) \end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Data:} Out of the twenty patients sampled, she found that twelve of them had responded well after six weeks of recommended lifestyle changes. Our observed data are \(x = 12\).

\sphinxAtStartPar
\sphinxstylestrong{Probability distribution function:} As we described before, the likelihood of \(\pi\) given these data is the probability of observing the data for different values for \(\pi\). Remember the probability distribution function for a binomial distribution of size 20 is
\begin{equation*}
\begin{split} 
P(X = x|\pi) = {20 \choose x} \pi^{x} (1-\pi)^{20 - x} 
\end{split}
\end{equation*}
\sphinxAtStartPar
for a given value of \(\pi\).

\sphinxAtStartPar
\sphinxstylestrong{Likelihood:} The likelihood has this same form but is viewed as a function of \(\pi\), rather than a function of \(x\). For our observed data of 12 out of 20 patients,
\begin{equation*}
\begin{split} 
L(\pi | x = 12) = {20 \choose 12} \pi^{20} (1-\pi)^{20 - 12} 
\end{split}
\end{equation*}
\sphinxAtStartPar
As before, we can identify the value of \(\pi\) which gives the maximum likelihood by plotting the likelihood for a range of values of \(\pi\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define a range of values for pi}
\PYG{k+kc}{pi} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{n}{by} \PYG{o}{=} \PYG{l+m}{0.01}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate the likelihood for each value, this time given n=20 and x=12}
\PYG{n}{L\PYGZus{}pi} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{choose}\PYG{p}{(}\PYG{l+m}{20}\PYG{p}{,}\PYG{l+m}{12}\PYG{p}{)}\PYG{o}{*}\PYG{k+kc}{pi}\PYG{o}{\PYGZca{}}\PYG{l+m}{12}\PYG{o}{*}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{k+kc}{pi}\PYG{p}{)}\PYG{o}{\PYGZca{}}\PYG{p}{(}\PYG{l+m}{20}\PYG{l+m}{\PYGZhy{}12}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the output}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{k+kc}{pi}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{L\PYGZus{}pi}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Find the value of pi for which L\PYGZus{}pi is highest}
\PYG{n}{pi\PYGZus{}max} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{k+kc}{pi}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{L\PYGZus{}pi}\PYG{p}{)}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Add a line to the plot at pi\PYGZus{}max}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{n}{pi\PYGZus{}max}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add a title specifying the value of pi\PYGZus{}max}
\PYG{n+nf}{title}\PYG{p}{(}\PYG{n+nf}{paste}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Maximum at\PYGZdq{}}\PYG{p}{,} \PYG{n}{pi\PYGZus{}max}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{05.c. Likelihood_2_0}.png}

\sphinxAtStartPar
The value which maximises this function is 0.6, the observed sample proportion; we’ll call this value \(\hat{\pi}\) to indicate that it is an estimate of \(\pi\). Notice that the likelihood for values of \(\pi\) smaller than 0.3 or greater than 0.9 is very small \sphinxhyphen{} much smaller than that of values around 0.6 \sphinxhyphen{} suggesting that these values are inconsistent with the observed data.


\subsection{5.2.2 Example: the Exponential model}
\label{\detokenize{05.c. Likelihood:example-the-exponential-model}}
\sphinxAtStartPar
Suppose we wish to estimate how long patients usually wait in reception before their GP appointment. At one practice, a patient walks through the door and the receptionist records the time until they get called through.

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:} The waiting time in minutes, \(Y\), is a continuous random variable which must be non\sphinxhyphen{}negative. It is common to use an exponential distribution to model waiting times, so we will assume it’s a reasonable choice for this example.
\begin{equation*}
\begin{split} 
Y \sim Exp(\lambda) 
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstyleemphasis{Remember that the mean of this distribution is equal to one over the rate parameter \(\lambda\), i.e. \(E(Y) = \frac{1}{\lambda}\).}

\sphinxAtStartPar
\sphinxstylestrong{Data:} The receptionist observes that the patient waits for eight minutes and forty\sphinxhyphen{}five seconds, so \(y = 8.75\).

\sphinxAtStartPar
\sphinxstylestrong{Probability density function:} The PDF for an exponential distribution is
\begin{equation*}
\begin{split}
f_Y(y|\lambda) = \lambda e^{-y\lambda}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Likelihood:} We write down the likelihood for \(\lambda\) based on the exponential PDF above.
\begin{equation*}
\begin{split} 
L(\lambda | y = 8.75) = \lambda e^{-8.75\lambda}
\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define a range of values for lambda, equating to mean waiting times from 1 to 100 minutes}
\PYG{n}{lam} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0.01}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{n}{by} \PYG{o}{=} \PYG{l+m}{0.01}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate the likelihood for each value, given y=8.75}
\PYG{n}{L\PYGZus{}lam} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{lam}\PYG{o}{*}\PYG{n+nf}{exp}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}8.75}\PYG{o}{*}\PYG{n}{lam}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Find the value of lambda for which L\PYGZus{}lam is highest}
\PYG{n}{lam\PYGZus{}max} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{lam}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{L\PYGZus{}lam}\PYG{p}{)}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Plot the likelihood and indicate the maximum value}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{lam}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{L\PYGZus{}lam}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{n}{lam\PYGZus{}max}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{title}\PYG{p}{(}\PYG{n+nf}{paste}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Maximum at\PYGZdq{}}\PYG{p}{,} \PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{lam\PYGZus{}max}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{05.c. Likelihood_5_0}.png}

\sphinxAtStartPar
If we evaluate over a fine enough range of values for \(\lambda\), we find that the value which maximises this exponential likelihood is equal to \(\frac{1}{8.75}\), i.e. one over the observed waiting time. This defines an exponential distribution with mean equal to the observed waiting time.

\sphinxAtStartPar
As with the binomial example, the estimate obtained by maximising the likelihood is intuitively sensible based on the data we’ve observed.


\section{5.3 Log likelihood}
\label{\detokenize{05.d. Likelihood:log-likelihood}}\label{\detokenize{05.d. Likelihood::doc}}
\sphinxAtStartPar
We have discussed the idea that finding the maximum value of a likelihood gives us sensible estimates for the unknown parameters. For the examples above it is relatively clear from calculating a few values of the likelihood where the maximum lies, but this will not always be the case.

\sphinxAtStartPar
A theoretical result which will come in handy is that a value which maximises the likelihood also maximises the log\sphinxhyphen{}transform of the likelihood, or the \sphinxstyleemphasis{log\sphinxhyphen{}likelihood}. This is because the log is a \sphinxstyleemphasis{concave} function, so when we use it to transform the likelihood, any maximum or minimum stays in the same place on the x\sphinxhyphen{}axis. We will denote the log\sphinxhyphen{}likelihood by \(l(\theta) = \log(L(\theta))\).

\sphinxAtStartPar
This result is evident when plotting the transformation of the likelihoods for the binomial and exponential distributions.

\sphinxAtStartPar
For the binomial example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{12}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{)}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow} \PYG{o}{=} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} likelihood L(pi)}
\PYG{k+kc}{pi} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{n}{by} \PYG{o}{=} \PYG{l+m}{0.01}\PYG{p}{)}
\PYG{n}{L\PYGZus{}pi} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{choose}\PYG{p}{(}\PYG{l+m}{20}\PYG{p}{,}\PYG{l+m}{12}\PYG{p}{)}\PYG{o}{*}\PYG{k+kc}{pi}\PYG{o}{\PYGZca{}}\PYG{l+m}{12}\PYG{o}{*}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{k+kc}{pi}\PYG{p}{)}\PYG{o}{\PYGZca{}}\PYG{p}{(}\PYG{l+m}{20}\PYG{l+m}{\PYGZhy{}12}\PYG{p}{)}
\PYG{n}{pi\PYGZus{}max} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{k+kc}{pi}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{L\PYGZus{}pi}\PYG{p}{)}\PYG{p}{]}

\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{k+kc}{pi}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{L\PYGZus{}pi}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{n}{pi\PYGZus{}max}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{title}\PYG{p}{(}\PYG{n+nf}{paste}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Maximum at\PYGZdq{}}\PYG{p}{,} \PYG{n}{pi\PYGZus{}max}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} log\PYGZhy{}likelihood l(pi)}
\PYG{n}{l\PYGZus{}pi} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{log}\PYG{p}{(}\PYG{n}{L\PYGZus{}pi}\PYG{p}{)}

\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{k+kc}{pi}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{l\PYGZus{}pi}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{k+kc}{pi}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{l\PYGZus{}pi}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{title}\PYG{p}{(}\PYG{n+nf}{paste}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Maximum at\PYGZdq{}}\PYG{p}{,} \PYG{k+kc}{pi}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{l\PYGZus{}pi}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{05.d. Likelihood_1_0}.png}

\sphinxAtStartPar
For the exponential example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{12}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{)}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow} \PYG{o}{=} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} likelihood L(beta)}
\PYG{n}{lam} \PYG{o}{=} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0.01}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{n}{by} \PYG{o}{=} \PYG{l+m}{0.01}\PYG{p}{)}
\PYG{n}{L\PYGZus{}lam} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{lam}\PYG{o}{*}\PYG{n+nf}{exp}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}8.75}\PYG{o}{*}\PYG{n}{lam}\PYG{p}{)}
\PYG{n}{lam\PYGZus{}max} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{lam}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{L\PYGZus{}lam}\PYG{p}{)}\PYG{p}{]}

\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{lam}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{L\PYGZus{}lam}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{n}{lam\PYGZus{}max}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{title}\PYG{p}{(}\PYG{n+nf}{paste}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Maximum at\PYGZdq{}}\PYG{p}{,} \PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{lam\PYGZus{}max}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} log\PYGZhy{}likelihood l(beta)}
\PYG{n}{l\PYGZus{}lam} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{log}\PYG{p}{(}\PYG{n}{L\PYGZus{}lam}\PYG{p}{)}

\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{lam}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{l\PYGZus{}lam}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v} \PYG{o}{=} \PYG{n}{lam}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{l\PYGZus{}lam}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{title}\PYG{p}{(}\PYG{n+nf}{paste}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Maximum at\PYGZdq{}}\PYG{p}{,} \PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{lam}\PYG{p}{[}\PYG{n+nf}{which.max}\PYG{p}{(}\PYG{n}{l\PYGZus{}lam}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{05.d. Likelihood_3_0}.png}


\subsection{5.3.1 Why use the log likelihood?}
\label{\detokenize{05.d. Likelihood:why-use-the-log-likelihood}}
\sphinxAtStartPar
Log\sphinxhyphen{}transformed likelihoods are generally “better\sphinxhyphen{}behaved” and easier to work with than the original form. Remember the rules of logs for products and powers \sphinxhyphen{} we’ll see in the next section how these make computation with the log\sphinxhyphen{}likelihood very convenient.


\section{5.4 Finding the MLE}
\label{\detokenize{05.e. Likelihood:finding-the-mle}}\label{\detokenize{05.e. Likelihood::doc}}
\sphinxAtStartPar
So far we have obtained the maximum likelihood estimate (MLE) by plotting the likelihood for different parameter values and looking for the value which yields the maximum. Of course, the estimate obtained in this way depends on how many parameter values we evaluate.

\sphinxAtStartPar
A more formal way is to determine the location of that maximal point algebraically, from the likelihood function itself. In this way, we can directly obtain the general form for the MLE in terms of the data.

\sphinxAtStartPar
This is where the log\sphinxhyphen{}likelihood comes into its own; we know that a value which maximises the log\sphinxhyphen{}likelihood also maximises the likelihood, and the impact of logs on products and powers make the algebra much simpler.

\sphinxAtStartPar
We find the maximum likelihood estimator of a parameter from the log\sphinxhyphen{}likelihood function through the following steps:



\sphinxAtStartPar
 Method for finding MLEs:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Obtain the derivative of the log\sphinxhyphen{}likelihood: \(\frac{d l(\theta \mid {x})}{d \theta}\)

\item {} 
\sphinxAtStartPar
Set \(\frac{d l(\theta \mid {x})}{d \theta}=0\) and solve for \(\theta\)

\item {} 
\sphinxAtStartPar
Verify that it is a maximum by showing that the second derivative \(\frac{d ^2 l(\theta \mid  {x})}{d \theta ^2 }\) is negative when the MLE is substituted for \(\theta\).

\end{enumerate}




\subsection{5.4.1 Binomial model}
\label{\detokenize{05.e. Likelihood:binomial-model}}
\sphinxAtStartPar
We will derive the MLE for the binomial example described earlier. In general, the likelihood given observed data of \(x\) responders out of \(n\) patients is
\begin{equation*}
\begin{split} 
L(\pi|x) = {n \choose x} \pi^{x} (1-\pi)^{n - x}
\end{split}
\end{equation*}
\sphinxAtStartPar
and so the log\sphinxhyphen{}likelihood is
\begin{equation*}
\begin{split} 
\begin{align*}
l(\pi|x) & = \log\left({n \choose x} \pi^{x} (1-\pi)^{n - x}\right) \\ 
& = \log {n \choose x} + x \log \pi + (n-x)\log (1-\pi)
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can now obtain the maximum likelihood estimate from this function.

\sphinxAtStartPar
\sphinxstylestrong{Step 1}: Differentiate the log\sphinxhyphen{}likelihood with respect to our parameter \(\pi\)
\begin{equation*}
\begin{split}
\begin{equation}
\frac{d l \left( \pi \mid x \right)}{d \pi} =  \frac{x}{\pi}  -\frac{(n-x)}{(1-\pi)}
\end{equation}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Step 2}: We set the derivative equal to zero and solve for \(\pi\)
\begin{equation*}
\begin{split}
\begin{align*}
0 &=  \frac{x}{\hat{\pi}}  -\frac{(n-x)}{(1-\hat{\pi})} \\
\frac{(n-x)}{(1-\hat{\pi})} &=  \frac{x}{\hat{\pi}}   \\
(n-x)\hat{\pi} &=  x(1-\hat{\pi}) \\
n\hat{\pi}-x\hat{\pi} &=  x-x\hat{\pi} \\
\hat{\pi} &=  \frac{x}{n} \\
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Having solved the equation, we get that the maximum likelihood estimator for \(\pi\) is \(\hat{\pi} =  \frac{x}{n}\) (note that we put a hat to indicate that it is an estimator).

\sphinxAtStartPar
There is one thing left for us to check: we have found that \(\frac{x}{n}\) is the point where the derivative of the log\sphinxhyphen{}likelihood is zero, but that could mean that it is a maximum or a minimum of the log\sphinxhyphen{}likelihood function. To verify that this is indeed a maximum, we need to compute the second derivative of the log\sphinxhyphen{}likelihood and check that it takes a negative value when \({\pi} =  \frac{x}{n}\).

\sphinxAtStartPar
\sphinxstylestrong{Step 3}: Find the second derivative:
\begin{equation*}
\begin{split}
\begin{equation}
\frac{d l^2 \left( \pi \mid x \right)}{d \pi ^2} =  -\frac{x}{\pi^2}  -\frac{(n-x)}{(1-\pi)^2}
\end{equation}
\end{split}
\end{equation*}
\sphinxAtStartPar
This second derivative must be negative if we plug in \(\frac{x}{n}\) for \({\pi}\). Both fractions on the right hand side have a squared number in the denominator which is positive, so we only have to think about the numerators. We have that \(-x \leq 0 \) since \(x \geq 0\) and also \(-(n-x) \leq 0 \) since \(n \geq x \geq 0\). Therefore, the value \(\frac{x}{n}\) is indeed a maximum.

\sphinxAtStartPar
Thus the MLE for \(\pi\) is \(\frac{x}{n}\).


\subsection{5.4.2 Exponential model}
\label{\detokenize{05.e. Likelihood:exponential-model}}
\sphinxAtStartPar
The likelihood function for the exponential example is
\begin{equation*}
\begin{split}
L(\lambda | y) = \lambda e^{-\lambda y}
\end{split}
\end{equation*}
\sphinxAtStartPar
Therefore we have
\begin{equation*}
\begin{split} 
l(\lambda | y) = \log \left( \lambda e^{-\lambda y} \right) = \log \lambda - y \lambda 
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Step 1}: Differentiate the log\sphinxhyphen{}likelihood with respect to our parameter \(\lambda\)
\begin{equation*}
\begin{split}
\frac{d l \left( \lambda \mid y \right)}{d \lambda} = \frac{1}{\lambda} - y
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Step 2}: Set the derivative to zero and solve for \(\lambda\)
\begin{equation*}
\begin{split} 
\begin{align*}
\frac{1}{\lambda} - y &= 0 \\
\hat{\lambda} & = \frac{1}{y} 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Step 3}: Verify that this is a maximum rather than a minimum by considering the second derivative
\begin{equation*}
\begin{split} 
\frac{d l^2 \left(\lambda \mid y \right)}{d \lambda ^2} = -\frac{1}{\lambda^2}
\end{split}
\end{equation*}
\sphinxAtStartPar
This is negative for any value of \(\lambda\). So the MLE for \(\lambda\) is \(\hat{\lambda} = \frac{1}{y}\), one over the observed waiting time.


\section{5.5 Summary}
\label{\detokenize{05.f. Likelihood:summary}}\label{\detokenize{05.f. Likelihood::doc}}
\sphinxAtStartPar
Likelihood is a fundamental concept in statistical inference. In this session we have introduced the definition of likelihood and demonstrated how it can be used to estimate an unknown parameter, through maximum likelihood estimation. For two examples, we have seen that estimates obtained by MLE are intuitively sensible for the parameter of interest and have derived them algebraically via the log\sphinxhyphen{}likelihood.

\sphinxAtStartPar
In the next session, we will find out about the specific mathematical properties which make the MLE a “good” estimator, and extend to the situation where our data consist of more than one observation.


\chapter{6. Maximum Likelihood}
\label{\detokenize{06.a. Maximum Likelihood:maximum-likelihood}}\label{\detokenize{06.a. Maximum Likelihood::doc}}
\sphinxAtStartPar
In inferential statistics, the problem we are often faced is this: we have collected some data, and we have a statistical model for how this data was generated. However, we do not know what the values of the parameters of this model are. We need to find a way to estimate these parameters. In the previous session, we were introduced to the likelihood function, which measures how consistent different values of the parameter are with the data that we have observed. Extending this concept, we used calculus to obtain the maximum likelihood estimator for the parameter.

\sphinxAtStartPar
So far, we have only looked at examples where our data consists of one observation \sphinxhyphen{} surely, this is not a sufficient sample size!

\sphinxAtStartPar
We will now consider the more realistic scenario, where we have a random sample of observations from a particular distribution \sphinxhyphen{} in this case, we say that the sample is independently and identically distributed (i.i.d).



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Derive the likelihood and log\sphinxhyphen{}likelihood functions given an i.i.d. sample

\item {} 
\sphinxAtStartPar
Derive maximum likelihood estimator from single and multi\sphinxhyphen{}parameter distributions given an i.i.d. sample

\item {} 
\sphinxAtStartPar
Describe the main properties of MLEs

\end{itemize}



\sphinxAtStartPar
The following subsections define the likelihood function for \(n\) i.i.d observations and describe the process of obtaining the maximum likelihood estimator in this setting. The session ends with a demonstration of some important properties of maximum likelihood estimators.


\section{6.1 Likelihood with independent observations}
\label{\detokenize{06.b. Maximum Likelihood:likelihood-with-independent-observations}}\label{\detokenize{06.b. Maximum Likelihood::doc}}
\sphinxAtStartPar
Suppose that the observed data consiste of a sample of \(n\) observations. If these observations are independent, then the joint likelihood function from these \(n\) observations has a very convenient form; it is the product of the likelihood from each observation.

\sphinxAtStartPar
Suppose that the random variables \(X_1,..., X_n\) are i.i.d., and that our observed data are \(\mathbf{x} = \left\{ x_1, x_2, ..., x_n \right\}\). Then the likelihood function is given by:
\begin{equation*}
\begin{split}
\begin{align*}
L \left( \theta \mid \mathbf{x} \right) &=  L\left( \theta \mid x_1 \right) L\left( \theta \mid x_2 \right) ...  L \left( \theta \mid x_n \right) \\
 &= \prod_{i=1}^n  L\left( \theta \mid x_i \right).
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Recall that we often prefer to work with the log\sphinxhyphen{}likelihood function, as it simplifies the algebra when it comes to finding the MLE. The log\sphinxhyphen{}likelihood function for \(n\) independent observations is given by:
\begin{equation*}
\begin{split}
\begin{align*}
l \left( \theta \mid \mathbf{x} \right) 
 &= log \prod_{i=1}^n L\left( \theta \mid x_i \right) \\
  &= \sum_{i=1}^n log L\left( \theta \mid x_i \right) \\
   &= \sum_{i=1}^n l\left( \theta \mid x_i \right) 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Finding the MLE involves the same three steps as we saw in the previous session, but the log\sphinxhyphen{}likelihood function is now a joint function for the \(n\) observations:



\sphinxAtStartPar
 Method for finding MLEs:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Obtain the derivative of the log\sphinxhyphen{}likelihood: \(\frac{d l(\theta \mid \mathbf{x})}{d \theta}\)

\item {} 
\sphinxAtStartPar
Set \(\frac{d l(\theta \mid \mathbf{x})}{d \theta}=0\) and solve for \(\theta\)

\item {} 
\sphinxAtStartPar
Verify that it is a maximum by showing that the second derivative \(\frac{d ^2 l(\theta \mid  \mathbf{x})}{d \theta ^2 }\) is negative when the MLE is substituted for \(\theta\).

\end{enumerate}




\subsection{6.1.1 Example: Exponential distribution}
\label{\detokenize{06.b. Maximum Likelihood:example-exponential-distribution}}
\sphinxAtStartPar
Recall the example from the previous session, investigating the time that patients wait until their GP appointment in a particular practice. The receptionist records the time that elapses between when a patient walks through the door, and when they are called through for their appointment for a random sample of 8 people. These times (in minutes) are: 8.75, 10.20, 15.29, 7.89, 7.04, 12.04, 19.04, 17.50.

\sphinxAtStartPar
As a reminder, we can model the waiting time until a specific event using the exponential distribution with parameter \(\lambda\), which has a probability density function given by:
\begin{equation*}
\begin{split}
\begin{equation}  
f _X\left(x \mid \lambda \right)=\lambda e^{-x\lambda} , x > 0, \lambda > 0  
\end{equation}
\end{split}
\end{equation*}\begin{quote}

\sphinxAtStartPar
Recall that the mean of this distribution is equal to one over the rate parameter \(\lambda\), i.e. \(E(X) = \frac{1}{\lambda}\).
\end{quote}

\sphinxAtStartPar
We have that the log\sphinxhyphen{}likelihood is:
\begin{equation*}
\begin{split}
\begin{align}
\log L\left( \lambda \mid \mathbf{x} \right) &= \sum_{i=1}^n \log  L(\lambda \mid x_i) \\ 
&= \sum_{i=1}^n \log \left( \lambda e^{-x_i \lambda } \right) \\
&= \sum_{i=1}^n \log \lambda -x_i\lambda \\  
&= n \log \lambda -\lambda \sum_{i=1}^n x_i 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can make a plot of this log\sphinxhyphen{}likelihood, using the data from our example with eight observations.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}six independent observations for waiting times }
\PYG{n}{obs} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{8.75}\PYG{p}{,} \PYG{l+m}{10.20}\PYG{p}{,} \PYG{l+m}{15.29}\PYG{p}{,} \PYG{l+m}{7.89}\PYG{p}{,} \PYG{l+m}{7.04}\PYG{p}{,} \PYG{l+m}{12.04}\PYG{p}{,} \PYG{l+m}{19.04}\PYG{p}{,} \PYG{l+m}{17.50}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{length}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}possible values for the parameter lambda}
\PYG{n}{lambda} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{0.01}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}plot the log\PYGZhy{}likelihood}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{lambda}\PYG{p}{,} \PYG{n}{n}\PYG{o}{*}\PYG{n+nf}{log}\PYG{p}{(}\PYG{n}{lambda}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{lambda}\PYG{o}{*}\PYG{n+nf}{sum}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{,} 
     \PYG{n}{xlab}\PYG{o}{=} \PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{lambda}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}100}\PYG{p}{,}\PYG{l+m}{0}\PYG{p}{)}\PYG{p}{,}
     \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Log\PYGZhy{}likelihood\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{06.b. Maximum Likelihood_2_0}.png}

\sphinxAtStartPar
Graphically, we observe that the maximum is between 0 and 0.25. We will use the three steps, as before, to derive the MLE algebraically:

\sphinxAtStartPar
\sphinxstylestrong{Step1}: Taking the derivative of the log\sphinxhyphen{}likelihood with respect to \(\lambda\):
\begin{equation*}
\begin{split}
\begin{equation}
\frac{d log L\left( \lambda \mid x_1 ,..., x_n \right) }{d \lambda} = \frac{n}{\lambda}- \sum_{i=1}^n x_i 
\end{equation}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Step2:} Set the derivative equal to zero and solve for \(\lambda\):
\begin{equation*}
\begin{split}
\begin{align*}
0 &= \frac{n}{\lambda}- \sum_{i=1}^n x_i \\
\hat{\lambda} &= \frac{n }{\sum_{i=1}^n x_i} = \frac{1}{\bar{x}}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
The MLE is \(\hat{\lambda}= \frac{1}{\bar{x}}\). And to check that this provides a maximum, we go on to the next step:

\sphinxAtStartPar
\sphinxstylestrong{Step3:} Find the second derivative:
\begin{equation*}
\begin{split}
\begin{equation}
\frac{d l^2 \left( \lambda \mid \boldsymbol{x} \right)}{d \lambda ^2} 
= - \frac{n}{\lambda^2}
\end{equation}
\end{split}
\end{equation*}
\sphinxAtStartPar
When \({\lambda}=\frac{1}{\bar{x}}\), we have:
\begin{equation*}
\begin{split}
\begin{align}
 \frac{d l^2 \left( \lambda \mid \boldsymbol{x} \right)}{d \lambda ^2}  
 &=-n \bar{x}^2
 \end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
which is negative. This verifies that we found the maximum likelihood estimate.

\sphinxAtStartPar
Going back to our example of eight patients waiting for their GP appointment, the maximum likelihood estimate \(\lambda\) is given by one over the average of the eight waiting times:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m}{1}\PYG{o}{/}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.0818414322250639\end{split}
\end{equation*}
\sphinxAtStartPar
We have that \(\hat{\lambda}=0.0818\) minutes.


\subsection{6.1.2 Example: Normal distribution}
\label{\detokenize{06.b. Maximum Likelihood:example-normal-distribution}}
\sphinxAtStartPar
We will now consider the normal distribution. Remember that the normal distribution has two parameters, \(\mu\) and \(\sigma^2\). We will first obtain the MLE for \(\mu\) (treating \(\sigma^2\) as a constant), and in the practical, we will obtain the MLE for \(\sigma^2\) (treating \(\mu\) as a constant).

\sphinxAtStartPar
Recall that normal distribution has probability density function given by*:
\begin{equation*}
\begin{split}
\begin{equation}  
f_X \left( x \mid \mu, \sigma^2 \right)= (2 \pi \sigma^2)^{-\frac{1}{2}} \exp \left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}
\end{equation}
\end{split}
\end{equation*}
\sphinxAtStartPar
(* note that the notation here is slightly different to section 3. Here we are more prescriptive; on the left hand side the notation says that the random variable \(X\) is sampled from parameters \(\mu\) and \(\sigma^2\), where the distribution is defined on the right hand side. Both versions of notation are acceptible. Another notation style is to use a semi\sphinxhyphen{}colon instead, ie. \(f_X( x ; \mu, \sigma^2)\)).

\sphinxAtStartPar
We have that the log\sphinxhyphen{}likelihood given an i.i.d. sample of size \(n\) is:
\begin{equation*}
\begin{split}
\begin{align}
l \left(\mu, \sigma^2 \mid  \boldsymbol{x}  \right) &=  \sum_{i=1}^n \log \left\{ (2 \pi \sigma^2)^{-\frac{1}{2}} \exp \left\{-\frac{(x_i-\mu)^2}{2\sigma^2} \right\} \right\} \\
&= \sum_{i=1}^n \left\{ \log (2 \pi \sigma^2)^{-\frac{1}{2}}+ \log \exp  \left\{-\frac{(x_i-\mu)^2}{2\sigma^2} \right\}  \right\} \\
&= \sum_{i=1}^n \left\{ -\frac{1}{2} \log (2 \pi \sigma^2) - \frac{(x_i-\mu)^2}{2\sigma^2}  \right\} \\
&=  {-\frac{n}{2}}\log (2 \pi \sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
We will first find the MLE for the parameter \(\mu\).

\sphinxAtStartPar
\sphinxstylestrong{Step1}: Take the derivative of the log\sphinxhyphen{}likelihood with respect to  \(\mu\). Note that this requires use of the chain rule:
\begin{equation*}
\begin{split}
\begin{align}  
\frac{d l \left(\mu, \sigma^2 \mid  \mathbf{x}  \right) }{d \mu}
&=  -\frac{2}{2\sigma^2}(-1) \sum_{i=1}^n (x_i-\mu) \\
&=  \frac{ \sum_{i=1}^n (x_i-\mu)}{\sigma^2} \\
&=  \frac{ \sum_{i=1}^n x_i-n\mu}{\sigma^2}
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Step2:} Setting the derivative equal to zero and solving for \(\mu\):
\begin{equation*}
\begin{split}
\begin{align}  
0 &=  \frac{ \sum_{i=1}^n x_i-n\mu}{\sigma^2} \\
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Since \(\sigma^2 > 0\), we have that:
\begin{equation*}
\begin{split}
\begin{equation}  
0 = \sum_{i=1}^n x_i-n\mu 
\end{equation}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\begin{equation}  
\hat{\mu} =\frac{ \sum_{i=1}^n x_i}{n} = \bar{x}
\end{equation}
\end{split}
\end{equation*}
\sphinxAtStartPar
We have that the MLE for \(\mu\) is the sample mean, \(\bar{x}\).

\sphinxAtStartPar
\sphinxstylestrong{Step3:} Find the second derivative:
\begin{equation*}
\begin{split}
\begin{align}  
\frac{d^2 l \left(\mu, \sigma^2 \mid  \mathbf{x}  \right) }{d \mu^2 }
&=  -\frac{n}{\sigma^2},
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
since both \(n>0\) and \(\sigma^2 >0\), we have that the second derivative is negative, verifying that we have found the maximum.

\sphinxAtStartPar
In the practical, we will find the MLE for \(\sigma^2\).


\section{6.2 Properties of maximum likelihood estimators}
\label{\detokenize{06.c. Maximum Likelihood:properties-of-maximum-likelihood-estimators}}\label{\detokenize{06.c. Maximum Likelihood::doc}}
\sphinxAtStartPar
Maximum likelihood estimators can be shown to have some very useful properties. In particular, there are some very important asymptotic properties (properties that we observe as the sample size of our data gets very very large).

\sphinxAtStartPar
To explore these properties, have a look at the simulation below. We generate a sample of size 8 from the exponential distribution where \(\lambda=12.22\). The MLE is calculated from this the observed mean of the sample. We repeat this 100 times, and we plot a histogram of the 100 MLEs that we obtain.

\sphinxAtStartPar
Change the sample size, \(n\), to larger numbers and see what you notice about the histogram.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{8}  \PYG{c+c1}{\PYGZsh{}  make this sample size bigger, and see what happens to the histogram! }

\PYG{c+c1}{\PYGZsh{} MLEs will be stored in this vector}
\PYG{n}{mle} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{100}\PYG{p}{)}

\PYG{n+nf}{for }\PYG{p}{(}\PYG{n}{i} \PYG{n}{in} \PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{100}\PYG{p}{)}\PYG{p}{\PYGZob{}}
  \PYG{c+c1}{\PYGZsh{} Generate a sample of size n from an exponential distribution with lambda=0.0818}
  \PYG{n}{sample} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rexp}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{rate}\PYG{o}{=}\PYG{l+m}{0.0818}\PYG{p}{)}
  \PYG{c+c1}{\PYGZsh{} Calculate the MLE (the reciprocal mean of the sample) and store it }
  \PYG{n}{mle}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{1}\PYG{o}{/}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Plot a histogram of the 100 MLEs }
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{mle}\PYG{p}{,} \PYG{n}{breaks}\PYG{o}{=}\PYG{l+m}{20}\PYG{p}{,} 
     \PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.3}\PYG{p}{)}\PYG{p}{,} 
     \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Histogram of MLE\PYGZdq{}}\PYG{p}{,} 
     \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{MLE\PYGZdq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Add red line to indicate true lambda }
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{12.22}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{06.c. Maximum Likelihood_1_0}.png}

\sphinxAtStartPar
You may notice that, as \(n\) becomes large, the distribution of the MLE becomes more and more concentrated around the true value, and the histrogram appears to look more bell\sphinxhyphen{}shaped.

\sphinxAtStartPar
Suppose we denote the parameter of interest as \(\theta\) and its MLE as \(\hat{\theta}\). The tabs below show some important properties of MLEs.
\subsubsection*{Bias}



\sphinxAtStartPar
The MLE is \sphinxstylestrong{asymptotically unbiased}, i.e. on average we obtain the correct answer as samples become large.

\sphinxAtStartPar
\(\mathbb{E}(\hat{\theta}) \rightarrow \theta\) as \(n \rightarrow \infty\).


\subsubsection*{Consistency}



\sphinxAtStartPar
The MLE is \sphinxstylestrong{consistent}, i.e. the MLE converges towards the correct answer as samples become large.

\sphinxAtStartPar
\(\hat{\theta} \rightarrow \theta\) in probability as \(n \rightarrow \infty\).


\subsubsection*{Normality}



\sphinxAtStartPar
The MLE is \sphinxstylestrong{asymptotically normal}.

\sphinxAtStartPar
\(\hat{\theta} \sim N(\theta ,Var(\hat{\theta} ))\) as \(n \rightarrow \infty\).

\sphinxAtStartPar
The approximate normal distribution of the MLE means that confidence intervals and hypothesis tests for the parameters can be constructed easily.


\subsubsection*{Efficiency}



\sphinxAtStartPar
The MLE is \sphinxstylestrong{asymptotically efficient}.

\sphinxAtStartPar
\(Var(\hat{\theta})\) is the smallest variance amongst all unbiased estimators as \(n \rightarrow \infty\).

\sphinxAtStartPar
This means that, for example, confidence intervals constructed around the MLE will be the narrowest amongst confidence intervals of estimators that are linear and unbiased.


\subsubsection*{Transformation invariance}



\sphinxAtStartPar
The MLE is \sphinxstylestrong{transformation invariant}.

\sphinxAtStartPar
If \(\hat{\theta}\) is the MLE for \(\theta\), \(g(\hat{\theta})\) is the MLE of \(g(\theta)\) for any function \(g\).



\sphinxAtStartPar
You might question to what extent these asymptotic properties are useful in practical examples where the sample size is relatively small.

\sphinxAtStartPar
Further, in the cases that we have covered so far, it is fairly straightforward to compute the likelihood function and to find the value that maximizes it, but in many situations, this will be a complex task that requires numerical approaches.

\sphinxAtStartPar
In the subsequent sessions on Bayesian Statistics, we will see a different paradigm for making inference which can address some of these issues.


\section{6.3 Summary}
\label{\detokenize{06.d. Maximum Likelihood:summary}}\label{\detokenize{06.d. Maximum Likelihood::doc}}
\sphinxAtStartPar
We now know how to obtain the likelihood and log\sphinxhyphen{}likelihood functions when you have an i.i.d. sample of observations. We can then obtain the maximum likelihood estimators of the parameters of the distribution. The MLE is an important tool as it has a number of important asymptotic properties, as we demonstrated using a simulation in R. Finally, we introduced the idea of a log\sphinxhyphen{}likelihood ratio, which is a way of comparing estimates of a parameter with the maximum likelihood estimate.

\sphinxAtStartPar
You may be wondering how you might measure the precision of your estimator. We will return to this question in our session about confidence intervals.

\sphinxAtStartPar
Note that the maximum likelihood estimator, and confidence intervals, are tools from the “frequentist” or “classical” approach to statistics. In later sessions, you will meet the Bayesian approach to statistics, where the Likelihood will also play an important role.


\section{Appendix: Additional Reading}
\label{\detokenize{06.e. Maximum Likelihood:appendix-additional-reading}}\label{\detokenize{06.e. Maximum Likelihood::doc}}
\sphinxAtStartPar
This appendix section contains additional information which will deepen your understanding. However, it is not examinable and is completely optional reading.


\subsection{A1: Log\sphinxhyphen{}likelihood ratios}
\label{\detokenize{06.e. Maximum Likelihood:a1-log-likelihood-ratios}}
\sphinxAtStartPar
So far we have used the MLE to find an estimate of a parameter. Typically, the estimate is computed from a sample, so if we were to \sphinxstyleemphasis{sample again} we would expect the estimate to vary a little. But what about others values; what steps are involved to compare other estimates to the MLE? How much would the sample estimates vary? The \sphinxstylestrong{log\sphinxhyphen{}likelihood ratio (LLR)} is a useful approach. The LLR gives a measure of consistency of a value of \(\theta\) relative to the most likely value.

\sphinxAtStartPar
The LLR is defined as,
\begin{equation*}
\begin{split} log\frac{L(\theta)}{L(\hat\theta)} \end{split}
\end{equation*}
\sphinxAtStartPar
where \(L(\theta)\) is the likelihood evaluated at any value, and \(L(\hat\theta)\) is the likelihood evaluated at the MLE.

\sphinxAtStartPar
Alternatively, and especially when evaluating in software, the following is used;
\begin{equation*}
\begin{split} LLR(\theta) = l(\theta)-l(\hat\theta) \end{split}
\end{equation*}
\sphinxAtStartPar
Let’s explore the LLR and its properties with an small example.

\sphinxAtStartPar
For a simple coin\sphinxhyphen{}flipping example, from a trial of 10 coin\sphinxhyphen{}flips, 4 were heads (\(X=4\)) and the remainder were tails. From this experiment, we know that the MLE (ie. \(\hat\theta\)) is 0.4, but we also want to use the LLR to compare other estimates of \(\theta\). Looking at the LLR graphically we note that the LLR is a negative value, the further away from zero the less consistent the parameter value with to the MLE.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
 \PYG{n}{x} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{4}\PYG{p}{;} \PYG{n}{n}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{10}
 \PYG{n}{theta\PYGZus{}vals} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}
 \PYG{n}{LLR} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{n}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{n}\PYG{p}{,}\PYG{n}{x}\PYG{o}{/}\PYG{n}{n}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR}\PYG{p}{,}\PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{l\PYGZsq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} add additional things}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{n+nf}{length}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{06.e. Maximum Likelihood_1_0}.png}

\sphinxAtStartPar
In the above experiment, for a \sphinxstyleemphasis{fair} coin it would not be unusual to observe 4 heads from 10 trials. The MLE is 0.4 but we \sphinxstyleemphasis{know} for a fair coin that the true parameter \(\pi\) will be 0.5. The MLE is a \sphinxstyleemphasis{sample} of the distribution for \(\pi\). So let’s zoom in on the figure previously generated;

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
 \PYG{n}{x} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{4}\PYG{p}{;} \PYG{n}{n}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{10}
 \PYG{n}{theta\PYGZus{}vals} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}
 \PYG{n}{LLR} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{n}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{n}\PYG{p}{,}\PYG{n}{x}\PYG{o}{/}\PYG{n}{n}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR}\PYG{p}{,}\PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{l\PYGZsq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}5}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} add additional things}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{n+nf}{length}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0.5}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}10}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{red\PYGZsq{}}\PYG{p}{,}\PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{06.e. Maximum Likelihood_3_0}.png}

\sphinxAtStartPar
We can see in the figure (red line) that when \(\theta=0.5\) the LLR is very close to 0. Values of \(\theta\) further away from the MLE than 0.5 will have a even lower LLR. So we can make qualitative statements using the LLR in relation to the MLE.

\sphinxAtStartPar
Let’s increase the sample size and observe what happens to the LLR.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
 \PYG{n}{ns}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{10}\PYG{p}{,}\PYG{l+m}{20}\PYG{p}{,}\PYG{l+m}{40}\PYG{p}{,}\PYG{l+m}{80}\PYG{p}{,}\PYG{l+m}{160}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} assume we have an \PYGZsq{}unfair coin\PYGZsq{} with heads being more likely than tails}
\PYG{c+c1}{\PYGZsh{} rather than taking a random sample, assume a sample consistent with the null hypothesis \PYGZhy{} this is so the MLE remains 0.5}
 \PYG{n}{x1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{o}{*}\PYG{l+m}{0.4}\PYG{p}{,}\PYG{l+m}{0}\PYG{p}{)}
 \PYG{n}{x2}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{o}{*}\PYG{l+m}{0.4}\PYG{p}{,}\PYG{l+m}{0}\PYG{p}{)}
 \PYG{n}{x3}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}\PYG{o}{*}\PYG{l+m}{0.4}\PYG{p}{,}\PYG{l+m}{0}\PYG{p}{)}
 \PYG{n}{x4}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{4}\PYG{p}{]}\PYG{o}{*}\PYG{l+m}{0.4}\PYG{p}{,}\PYG{l+m}{0}\PYG{p}{)} 
 \PYG{n}{x5}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{round}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{5}\PYG{p}{]}\PYG{o}{*}\PYG{l+m}{0.4}\PYG{p}{,}\PYG{l+m}{0}\PYG{p}{)} 
 \PYG{n}{theta\PYGZus{}vals} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}
 \PYG{n}{LLR01} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x1}\PYG{o}{/}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n}{LLR02} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x2}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x2}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x2}\PYG{o}{/}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n}{LLR03} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x3}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x3}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x3}\PYG{o}{/}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n}{LLR04} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x4}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{4}\PYG{p}{]}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x4}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{4}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x4}\PYG{o}{/}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{4}\PYG{p}{]}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n}{LLR05} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x5}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{5}\PYG{p}{]}\PYG{p}{,}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{dbinom}\PYG{p}{(}\PYG{n}{x5}\PYG{p}{,}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{5}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x5}\PYG{o}{/}\PYG{n}{ns}\PYG{p}{[}\PYG{l+m}{5}\PYG{p}{]}\PYG{p}{,}\PYG{n}{log}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}
 \PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR01}\PYG{p}{,}\PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{l\PYGZsq{}}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{0.5}\PYG{p}{,}\PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}5}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} compare to large sample sizes}
 \PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR02}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{)}
 \PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR03}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{1.5}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR04}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{LLR05}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{blue\PYGZsq{}}\PYG{p}{,}\PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{2.5}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{n+nf}{length}\PYG{p}{(}\PYG{n}{theta\PYGZus{}vals}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0.5}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}10}\PYG{p}{,}\PYG{l+m}{0.01}\PYG{p}{)}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{red\PYGZsq{}}\PYG{p}{,}\PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{06.e. Maximum Likelihood_5_0}.png}

\sphinxAtStartPar
At smaller sample sizes the LLR is slightly left skewed when the sample mean is 0.4. As the sample size increases you can see that the LLR becomes more symmetrical about the sample mean, and that the slope of the LLR at values away from the sample mean is steeper. We can start to see the relationship between sample size and the precision of the sample mean. Qualitatively, if we wanted to test whether the coin was fair, it is clear that a larger sample would enable us to have more confidence in our assessment.

\sphinxAtStartPar
Returning to the fact that the data are a sample from a population distribution, we can explore what happens when multiple samples of the same size are drawn. MLE is a sample of the true parameter we can perform the above experiment multiple times and identify the parameters space where the LLR will be zero.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
 \PYG{n}{x} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{4}\PYG{p}{;} \PYG{n}{n}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{10}
 \PYG{n}{sampl} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rbinom}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m}{1000}\PYG{p}{,}\PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,}\PYG{n}{prob}\PYG{o}{=}\PYG{l+m}{0.5}\PYG{p}{)}
\PYG{n}{mles} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{sampl}\PYG{o}{/}\PYG{n}{n} 
 \PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{mles}\PYG{p}{,}\PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{,}\PYG{n}{breaks}\PYG{o}{=}\PYG{l+m}{30}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{06.e. Maximum Likelihood_8_0}.png}

\sphinxAtStartPar
From the histogram above you can see that repeating the experiment (different samples) will return different values of the MLE and corresponding LLR. The LLR ratio can be used to assess how consistent different values of the parameter are with the MLE.

\sphinxAtStartPar
The principles behind the LLR also relate to construction of confidence intervals, an issue which we will return to when we meet logistic regression and other models estimated using maximum likelihood estimation.


\chapter{7. Frequentist I: Confidence Intervals}
\label{\detokenize{07.a. Frequentist I:frequentist-i-confidence-intervals}}\label{\detokenize{07.a. Frequentist I::doc}}
\sphinxAtStartPar
In previous sessions we considered the concept of estimating population parameters using information from a sample from the population. When we present an estimate of a population quantity, it is important to also provide a measure of how precise that estimate is. Do we believe it is close to the true value? Can we provide a range of values within which we believe the true value lies?

\sphinxAtStartPar
This is the purpose of a confidence interval, often abbreviated by CI. Loosely speaking, a confidence interval provides a range of values for the population parameter which our observed data are consistent with.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
correctly interpret a 95\% confidence interval interval

\item {} 
\sphinxAtStartPar
describe properties of a 95\% confidence interval over repeated sampling

\item {} 
\sphinxAtStartPar
calculate a 95\% confidence interval for the mean

\item {} 
\sphinxAtStartPar
use resampling (bootstrapping) approaches to obtain percentile confidence intervals

\end{itemize}




\section{7.1 Confidence intervals}
\label{\detokenize{07.b. Frequentist I:confidence-intervals}}\label{\detokenize{07.b. Frequentist I::doc}}
\sphinxAtStartPar
To explore the concept of confidence intervals, we will return to the example of emotional distress among violence researchers.

\sphinxAtStartPar
We will again consider the smaller subsample of 10 researchers and focus on estimating the population mean age, \(\mu\). Among our 10 sampled violence researchers, the sample mean age and the sample proportion suffering from emotional distress are:
\begin{quote}

\sphinxAtStartPar
Sample mean age \(\bar{x}= 29.57\); sample standard deviation of age \(SD = 4.95\)
\end{quote}

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:} As before, we will let \(X_1, ...,X_{10}\) be  random variables representing the ages of 10 sampled researchers . For simplicity, we will assume that we know the true value of the population standard deviation, \(\sigma = 4.8\). We assume the following model
\begin{equation*}
\begin{split} 
X_i \overset{\small{iid}}{\sim} N(\mu, 4.8^2), \qquad i=1,2,...,10
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Data:} The realised values of the random variables are \(x_1, ..., x_{10}\) (i.e. the observed ages).

\sphinxAtStartPar
\sphinxstylestrong{Estimator and estimate:} The best estimator of the population mean age is the sample mean age.

\sphinxAtStartPar
From our sample of data, the estimate is \(\hat{\mu} = 29.57\). But how good an estimate is this? In order to answer that question, we will construct a 95\% confidence interval around the estimate.

\sphinxAtStartPar
\sphinxstylestrong{Sampling distribution of the estimator:} Recall that the sampling distribution of the sample mean is the distribution we would see if we repeatedly sampled 10 researchers a very large number of times, each time calculating the sample mean age, and drew a histogram of the sample means. We obtained the sampling distribution algebraically:
\begin{equation*}
\begin{split}
\hat{\mu} \sim N(\mu, 1.52^2)
\end{split}
\end{equation*}
\sphinxAtStartPar
Recall that when we are talking about the sampling distribution (i.e. the distribution of an \sphinxstyleemphasis{estimator}), we call the standard deviation the \sphinxstylestrong{standard error}. So the sample mean age follows a normal distribution, under repeated sampling, centred around the population mean \(\mu\) with standard error given by \(SE(\hat{\mu}) = 1.52\).

\sphinxAtStartPar
We do not quite have sufficient information to plot the sampling distribution, because we still do not know where the central value \(\mu\) is. However, otherwise we can draw the exact shape. The graph below draws the sampling distribution around an unknown population mean \(\mu\).

\sphinxAtStartPar
{[}The code used to generate the graph is suppressed, since it is not our focus here, but if you wish to see it you can click the button to the right.{]}

\noindent\sphinxincludegraphics{{07.b. Frequentist I_1_0}.png}


\subsection{7.1.1 Confidence interval for the mean}
\label{\detokenize{07.b. Frequentist I:confidence-interval-for-the-mean}}
\sphinxAtStartPar
We now use a general fact about normal distributions:
\begin{quote}

\sphinxAtStartPar
For a normal distribution,  95\% of the observations lie within 1.96 standard deviations of the mean.
\end{quote}

\sphinxAtStartPar
For the sampling distribution above, the “observations” are the different sample means we would see under (hypothetical) repeated sampling. Recall that when the we talk about a distribution of an estimator, we call the standard deviation the standard error. Thus the standard deviation of these observations (sample means) is the standard error of the mean, which takes a value of 1.52 here.

\sphinxAtStartPar
Therefore, 95\% of the sample means lie within \(1.52 \times 1.96 = 2.98\) of the population mean \(\mu\).

\sphinxAtStartPar
Imagine taking each (hypothetical) sample mean and “stretching out” a distance of 2.98 either way to give a range of values around that sample mean.

\noindent\sphinxincludegraphics{{07.b. Frequentist I_3_0}.png}

\sphinxAtStartPar
What proportion of such intervals would we expect to contain the true value \(\mu\)? Have a think about it and then click the button to the right.

\sphinxAtStartPar
From the plot above we can see that if we created intervals for each sample mean by stretching a distance of 1.96 standard errors in either direction then most of such intervals would cross the true value \(\mu\).
\begin{itemize}
\item {} 
\sphinxAtStartPar
For the 2.5\% of sample means that lie to the right of the right\sphinxhyphen{}hand dashed green line (which is at 1.96 standard errors above the mean), these intervals will miss the true value \(\mu\).

\item {} 
\sphinxAtStartPar
For the 2.5\% of sample means that lie to the left of the left\sphinxhyphen{}hand dashed green line (which is at 1.96 standard errors above the mean), these intervals will miss the true value \(\mu\).

\item {} 
\sphinxAtStartPar
Thus, 95\% of the intervals constructed in such a way will include the true value \(\mu\).

\end{itemize}

\sphinxAtStartPar
These intervals are called \sphinxstylestrong{95\% confidence intervals}.


\section{7.2 Confidence intervals for the mean}
\label{\detokenize{07.c. Frequentist I:confidence-intervals-for-the-mean}}\label{\detokenize{07.c. Frequentist I::doc}}

\subsection{7.2.1 Example}
\label{\detokenize{07.c. Frequentist I:example}}
\sphinxAtStartPar
In the sample of 10 researchers, the estimate of the population mean age is \(\hat{\mu} = 29.75\), the sample mean age. The standard error of the mean is
\begin{equation*}
\begin{split}
SE(\hat{\mu}) = \frac{\sigma}{\sqrt{n}} = \frac{4.8}{\sqrt{10}} = 1.52
\end{split}
\end{equation*}
\sphinxAtStartPar
We have seen that the 95\% confidence interval for the mean is calculated as
\begin{equation*}
\begin{split}
\hat{\mu} \pm 1.96 \times SE(\hat{\mu})
\end{split}
\end{equation*}
\sphinxAtStartPar
Substituting in the sample mean and the standard error gives
\begin{equation*}
\begin{split}
29.57 \pm 1.96 \times 1.52 
\end{split}
\end{equation*}
\sphinxAtStartPar
This gives the 95\% confidence interval for the population mean age: \((26.6, 32.5)\).

\sphinxAtStartPar
The code below reads in the data, prints the sample mean age and then calculates the 95\% confidence interval for the population mean age.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Our sample of data (ages for 10 sampled researchers)}
\PYG{n}{ages} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{28.1}\PYG{p}{,}\PYG{l+m}{27.5}\PYG{p}{,}\PYG{l+m}{25}\PYG{p}{,}\PYG{l+m}{29.9}\PYG{p}{,}\PYG{l+m}{29.7}\PYG{p}{,}\PYG{l+m}{29.9}\PYG{p}{,}\PYG{l+m}{39.9}\PYG{p}{,}\PYG{l+m}{33.6}\PYG{p}{,}\PYG{l+m}{21.3}\PYG{p}{,}\PYG{l+m}{30.8}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Sample mean (estimate of the population mean)}
\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the lower and upper limits of the confidence interval}
\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m}{1.96}\PYG{o}{*}\PYG{l+m}{1.52}
\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{)} \PYG{o}{+} \PYG{l+m}{1.96}\PYG{o}{*}\PYG{l+m}{1.52}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}29.57\end{split}
\end{equation*}\begin{equation*}
\begin{split}26.5908\end{split}
\end{equation*}\begin{equation*}
\begin{split}32.5492\end{split}
\end{equation*}

\subsection{7.2.2  95\% confidence interval for a mean}
\label{\detokenize{07.c. Frequentist I:confidence-interval-for-a-mean}}
\sphinxAtStartPar
For random variables \(𝑋_1,...,𝑋_n\), with \(𝑋_i \overset{\small{iid}}{\sim} N (\mu, \sigma^2)\) for \(i=1,...,n\) and \(\sigma\) is a known value, a 95\% confidence interval for \(\mu\) is given by:
\begin{equation*}
\begin{split}
\hat{\mu} \pm 1.96 \ SE(\hat{\mu})
\end{split}
\end{equation*}
\sphinxAtStartPar
where the standard error of \(\hat{\mu}\) is given by
\begin{equation*}
\begin{split}
SE(\hat{\mu}) = \frac{\sigma}{\sqrt{n}}
\end{split}
\end{equation*}
\sphinxAtStartPar
The calculation of this confidence interval relies on the assumptions that
\begin{itemize}
\item {} 
\sphinxAtStartPar
the original random variables follow a normal distribution

\item {} 
\sphinxAtStartPar
the value of  \(\sigma\)  is known

\end{itemize}

\sphinxAtStartPar
However, if these assumptions are not true, we can still obtain valid confidence intervals:
\begin{itemize}
\item {} 
\sphinxAtStartPar
If the original random variables do not follow a normal distribution but the sample size is large, then the Central Limit Theorem tells us that the sampling distribution of the mean is approximately normal. So this formula for the confidence interval is still valid.

\item {} 
\sphinxAtStartPar
If \(\sigma\) is unknown (which is typically the case), there is a modified confidence interval based on the t\sphinxhyphen{}distribution which provides a correct interval. Essentially, we replace the number 1.96 above by a slightly larger number to compensate for the estimation of the standard deviation. For large sample sizes (\(n>30\) or so), the substitution of the estimated standard deviation makes little difference. More detail is provided later in this session.

\end{itemize}


\section{7.3 Interpretation of confidence intervals}
\label{\detokenize{07.d. Frequentist I:interpretation-of-confidence-intervals}}\label{\detokenize{07.d. Frequentist I::doc}}
\sphinxAtStartPar
For our emotional distress sub\sphinxhyphen{}sample, our estimated mean age is \(\hat{\mu}= 29.75\), with a 95\% confidence interval of \((26.6, 32.5)\). Having calculated this confidence interval for our unknown population age, \(\mu\), how do we interpret it?


\subsection{7.3.1 Operational definition}
\label{\detokenize{07.d. Frequentist I:operational-definition}}
\sphinxAtStartPar
For the 95\% confidence interval calculated above \((26.6, 32.5)\), it is tempting to say that the probability that the population mean age, \(\mu\), is between 26.6 and 32.5 is 95\%. However, this is incorrect, because this  implies that \(\mu\) has a probability distribution, rather than being a fixed unknown number. Either the true value of \(\mu\) lies within the interval 26.6 to 32.5, or it does not.

\sphinxAtStartPar
Strictly, the interpretation of a confidence interval has to be with respect to the process of repeated sampling: if we repeated the study an infinite number of times, 95\% of the 95\% confidence intervals calculated would include the true population mean \(\mu\).

\sphinxAtStartPar
This operational definition is long\sphinxhyphen{}winded and can be confusing. In practice,  we often use looser interpretations to aid communication of results, as described below.


\subsection{7.3.2 Looser interpretation (practical)}
\label{\detokenize{07.d. Frequentist I:looser-interpretation-practical}}
\sphinxAtStartPar
In practice,  we often loosely interpret a 95\% confidence interval by saying
\begin{itemize}
\item {} 
\sphinxAtStartPar
that we are 95\% confident that the true population mean lies within the 95\% confidence interval calculated.

\item {} 
\sphinxAtStartPar
that our data are consistent with values of the population mean within the 95\% confidence interval calculated.

\end{itemize}

\sphinxAtStartPar
It is important, however, to bear the strict operational definition of the confidence interval in mind when we use these types of interpretations.


\subsection{7.3.3 Confidence intervals under repeated sampling}
\label{\detokenize{07.d. Frequentist I:confidence-intervals-under-repeated-sampling}}
\sphinxAtStartPar
In order to see how confidence intervals behave under repeated sampling, we will now randomly draw 100 different samples of 10 people. Within each sample, we calculate the sample mean and the 95\% confidence interval (assuming the population value \(\sigma\) is known).  The graph below shows the 95\% confidence intervals from the 100 samples. {[}Click to view the code that generates the graph{]}.

\noindent\sphinxincludegraphics{{07.d. Frequentist I_4_0}.png}

\sphinxAtStartPar
In the figure above, what do you notice? Approximately what proportion of intervals include the true value of \(\mu\)? Have a think about this and then click to see some comments about the graph.

\sphinxAtStartPar
We see that:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the point estimates tend to cluster around the true value of \(\mu\), and fall symmetrically either side

\item {} 
\sphinxAtStartPar
93 out of the 100 confidence intervals include the true value

\item {} 
\sphinxAtStartPar
3 out of 100 of the intervals to lie entirely above the true value

\item {} 
\sphinxAtStartPar
4 out of 100 of the intervals to lie entirely below the true value

\end{itemize}

\sphinxAtStartPar
If we were to do simulate a much larger number of confidence intervals, we would see that:
\begin{itemize}
\item {} 
\sphinxAtStartPar
95\% of the confidence intervals would include the true value

\item {} 
\sphinxAtStartPar
2.5\% of the intervals would lie entirely above the true value

\item {} 
\sphinxAtStartPar
2.5\% of the intervals would lie entirely below the true value

\end{itemize}


\section{7.4 Approximate confidence intervals for parameters estimated using large samples}
\label{\detokenize{07.e. Frequentist I:approximate-confidence-intervals-for-parameters-estimated-using-large-samples}}\label{\detokenize{07.e. Frequentist I::doc}}
\sphinxAtStartPar
You will encounter many different confidence intervals during your studies. Many of these rely on an asymptotic normal distribution, as described below.

\sphinxAtStartPar
There are many different confidence intervals and many approaches to calculating confidence intervals. We do not aim to give you a comprehensive list here. Below, we describe a few commonly used confidence intervals to give you a flavour. \sphinxstylestrong{Please note:} We do not expect you to memorise these formulae.


\subsection{7.4.1 Normal\sphinxhyphen{}based confidence intervals}
\label{\detokenize{07.e. Frequentist I:normal-based-confidence-intervals}}
\sphinxAtStartPar
The Central Limit Theorem tells us that the mean of independent identically distributed random variables, with finite expectation and variance, tends to a normal distribution as the sample size tends to infinity.

\sphinxAtStartPar
In fact, the Central Limit Theorem means that most typically encountered parameter estimators tends to normal as the sample sizes tend to infinity. So we can follow a very similar approach to the one above to construct confidence intervals for any parameter estimators that follow an approximate normal distribution when sample sizes are large, giving a confidence interval of the form
\begin{equation*}
\begin{split}
\mbox{Estimate} \pm 1.96 \times SE(\mbox{Estimator})
\end{split}
\end{equation*}

\subsection{7.4.2 Proportions and rates}
\label{\detokenize{07.e. Frequentist I:proportions-and-rates}}
\sphinxAtStartPar
First, we need some notation.

\sphinxAtStartPar
Proportion

\sphinxAtStartPar
We are estimating a population proportion from a single observation from a binomial distribution.

\sphinxAtStartPar
Our observed data consist of one observation from \(X \sim binomial(n, \pi)\), with the realised (observed) value being \(X=k\).

\sphinxAtStartPar
Rate

\sphinxAtStartPar
We are estimating a population rate (per person\sphinxhyphen{}year), from the total number of events out of \(P\) person\sphinxhyphen{}years of observation.

\sphinxAtStartPar
Our observed data consist of one observation from \(X \sim Poisson(\lambda P)\). The realised (observed) value is \(X=d\).

\sphinxAtStartPar
Logarithm of rate

\sphinxAtStartPar
For the rate, we may wish to perform our calculations on the log scale. These confidence intervals are approximate; the approximation can work better following a transformation (e.g. the log). This is one example of that approach.

\sphinxAtStartPar
To do this, we need to define the log\sphinxhyphen{}rate, \(\nu = log(\lambda)\)

\sphinxAtStartPar
Using this notation, we can write down the estimate of the parameter of interest, it’s standard error and an approximate 95\% confidence interval. These are shown in the table below.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Estimate of parameter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Standard Error
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Approximate 95\% Confidence Interval
\\
\hline
\sphinxAtStartPar
Proportion
&
\sphinxAtStartPar
\(\hat{\pi} = \frac{k}{n}\)
&
\sphinxAtStartPar
\(\sqrt{\frac{\pi (1-\pi)}{n}}\)
&
\sphinxAtStartPar
\(\hat{\pi} \pm 1.96 \times \sqrt{\frac{\hat{\pi} (1-\hat{\pi})}{n}}\)
\\
\hline
\sphinxAtStartPar
Rate
&
\sphinxAtStartPar
\(\hat{\lambda} = \frac{d}{P}\)
&
\sphinxAtStartPar
\(\frac{\lambda}{\sqrt{d}}\)
&
\sphinxAtStartPar
\(\hat{\lambda} \pm 1.96 \times \frac{\hat{\lambda}}{\sqrt{d}}\)
\\
\hline
\sphinxAtStartPar
Log Rate
&
\sphinxAtStartPar
\(\hat{\nu} = log\left(\frac{d}{P}\right)\)
&
\sphinxAtStartPar
\(\sqrt{\frac{1}{\lambda P}}\)
&
\sphinxAtStartPar
\(\hat{\nu} \pm 1.96 \times \sqrt{\frac{1}{e^{\hat{\nu}} P}}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The three tabs below provide examples of using the formulae above to obtain approximate 95\% confidence intervals for proportions and rates.
\subsubsection*{Proportion}
\begin{itemize}
\item {} 
\sphinxAtStartPar
We want to estimate the population proportion of patients who experience a side effect from a particular drug.

\item {} 
\sphinxAtStartPar
In a clinical study of \(80\) patients given the drug, \(X=20\) experience a side effect.

\item {} 
\sphinxAtStartPar
Our estimate of the population proportion experiencing a side effect is \(\hat{\pi} = 0.25\)

\item {} 
\sphinxAtStartPar
Our 95\% confidence interval for this proportion is:

\end{itemize}
\begin{equation*}
\begin{split}
0.25 \pm 1.96 \times \sqrt{\frac{0.25 (1-0.25)}{80}}
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
This gives a range of 0.155 to 0.349.

\item {} 
\sphinxAtStartPar
So our estimate of the proportion of patients who experience a side effect is: 0.25 (95\% CI 0.155 to 0.349). Our best guess is that 25\% of patients experience a side\sphinxhyphen{}effect from this drug. We are 95\% confident that the true proportion lies between 15.5\% and 34.9\%.

\end{itemize}
\subsubsection*{Rate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
We want to estimate the rate of panic attacks among adults with a mild anxiety disorder.

\item {} 
\sphinxAtStartPar
Suppose we observe \(80\) patients with the disorder for 1 year, so \(P=80\). In total, these patients experience \(d=2\) panic attacks during the year.

\item {} 
\sphinxAtStartPar
Our estimate of the annual rate of panic attacks per person is \(\hat{\lambda} = 2/80 = 0.025\).

\item {} 
\sphinxAtStartPar
Our 95\% confidence interval for the rate is:

\end{itemize}
\begin{equation*}
\begin{split}
0.025 \pm 1.96 \times \frac{0.025}{\sqrt{2}}
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
This gives a range of \sphinxhyphen{}0.0096 to 0.0596. This illustrates an important point \sphinxhyphen{} approximate confidence intervals sometimes contain impossible parameter values (the rate \(\lambda\) cannot be negative). To resolve this problem, we will re\sphinxhyphen{}do the calculations on the log scale.

\end{itemize}
\subsubsection*{Logarithm of the rate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The logarithm of the observed rate is \(\nu = log(0.025) = -3.689\). So this is our estimate of the log rate.

\item {} 
\sphinxAtStartPar
Our 95\% confidence interval for the log\sphinxhyphen{}rate is:

\end{itemize}
\begin{equation*}
\begin{split}
-3.689 \pm 1.96 \times \sqrt{\frac{1}{0.025 \times 80}}
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
This gives a range of \sphinxhyphen{}5.075 to \sphinxhyphen{}2.303. This is an interval within which we are confident the log\sphinxhyphen{}rate lies. To obtain an interval on the original scale, we take the exponential transformation of each of these values:

\end{itemize}
\begin{equation*}
\begin{split}
(e^{-5.075}=0.006, e^{-2.303}=0.0999).
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
So our estimated rate and 95\% confidence interval is: 0.025 (95\% CI 0.006 to 0.0999). We are 95\% confident that the true rate of panic attacks per year per person lies between 0.006 and 0.0999.

\end{itemize}


\subsection{7.4.2 The mean}
\label{\detokenize{07.e. Frequentist I:the-mean}}
\sphinxAtStartPar
In this subsection we consider estimating a population mean. Our observed data comprise \(n\) independent observations, \(x_1, x_2, ..., x_n\). We consider two possibilities:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data are normally distributed, \(X_i \sim Normal(\mu, \sigma^2),\) for \(i=1,..,n\)

\item {} 
\sphinxAtStartPar
Data are not normally distributed.

\end{itemize}

\sphinxAtStartPar
In each case, the population mean and variance (the square of the population standard deviation) are:
\begin{equation*}
\begin{split}
E[X] = \mu, \ \ Var(X) = \sigma^2
\end{split}
\end{equation*}
\sphinxAtStartPar
The sample mean is \(\bar{x}\) and the sample standard deviation is \(s\). Our estimate of the population mean is just the sample mean: \(\hat{\mu} = \bar{x}\). And, as we have seen, the standard error is
given by \(\frac{\sigma}{\sqrt{n}}\).

\sphinxAtStartPar
There are various ways of constructing a 95\% confidence interval, depending on the situation. These are shown in the table below.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Approximate 95\% Confidence Interval
\\
\hline
\sphinxAtStartPar
Small samples
&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal distribution, known \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}\)
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal distribution, unknown \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm t_{n-1} \times \frac{s}{\sqrt{n}}\)
\\
\hline
\sphinxAtStartPar
Large samples
&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal or not, known \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}\)
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal or not, unknown \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm 1.96 \times \frac{s}{\sqrt{n}}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
where \(t_{n-1}\) is a number obtained from the t\sphinxhyphen{}distribution, which is similar to the standard normal distribution. This number is around 2 for small \(n\) and becomes very close to \(1.96\) for large samples.

\sphinxAtStartPar
For small samples, where the data are not normally distributed, the confidence interval assuming data are normally distributed often has reasonable performance, but other methods (e.g. bootstrap confidence intervals) may be advisable.


\subsection{7.4.3 Comparing two groups}
\label{\detokenize{07.e. Frequentist I:comparing-two-groups}}
\sphinxAtStartPar
There are many ways of comparing outcomes between two groups. Two popular options are the difference in proportions for binary outcomes and the difference in means for continuous outcomes. Confidence intervals for other measures (e.g. the risk ratio, the odds ratio, the difference in medians, etc.) also can be obtained.
\subsubsection*{Difference in proportions}

\sphinxAtStartPar
We are interested in the population difference in proportions from two observations from two binomial distributions. Suppose our observed data consist of two observations from \(X_1 \sim binomial(n_1, \pi_1)\), with the realised values being \(X_1=k_1\) and \(X_2 = k_2\). We want to estimate the difference \(\delta = \pi_1 - \pi_2\).

\sphinxAtStartPar
We estimate the proportion in the first group by \(\hat{\pi}_1 = \frac{k_1}{n_1}\). Similarly, we estimate the proportion in the second group by \(\hat{\pi}_2 = \frac{k_2}{n_2}\). Then our estimate of the difference in proportions is \(\hat{\delta} = \hat{\pi}_1 - \hat{\pi}_2\).

\sphinxAtStartPar
The standard error for the difference in proportions is:
\begin{equation*}
\begin{split}
\sqrt{\frac{\pi_1 (1-\pi_1)}{n_1} + \frac{\pi_2 (1-\pi_2)}{n_2}}
\end{split}
\end{equation*}
\sphinxAtStartPar
And we can obtain an approximate 95\% confidence interval as:
\begin{equation*}
\begin{split}\hat{\delta} \pm 1.96 \times \sqrt{\frac{\hat{\pi}_1 (1-\hat{\pi}_1)}{n_1} + \frac{\hat{\pi}_2 (1-\hat{\pi}_2)}{n_2}}\end{split}
\end{equation*}\subsubsection*{Difference in means}

\sphinxAtStartPar
We are interested in the difference in population means between two groups from \(n_1\) iid observations from a normal distribution from group 1 and \(n_2\) from group 2. Suppose our observed data are \(n_1\) observations drawn from \(X_i \sim Normal(\mu_1, \sigma^2),\) for \(i=1,..,n_1\) and \(n_2\) observations drawn from \(X_i \sim Normal(\mu_2, \sigma^2),\) for \(i=1,..,n_2\). We want to estimate the difference \(\delta = \mu_1 - \mu_2\)

\sphinxAtStartPar
The sample means are \(\bar{x}_1\) and \(\bar{x}_2\) and the sample standard deviations are \(s_1\) and \(s_2\).

\sphinxAtStartPar
We can obtain a pooled estimate of the standard deviation, if we’re happy to assume that these are equal, as follows
\begin{equation*}
\begin{split} 
s = \sqrt{\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_1^2 }{n_1 + n_2 - 2}}
\end{split}
\end{equation*}
\sphinxAtStartPar
Our estimate of the difference in population means is: \(\hat{\delta} = \hat{\mu}_1 - \hat{\mu}_2\). This has standard error:
\begin{equation*}
\begin{split} 
SE(\hat{\delta}) =  \sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
\end{split}
\end{equation*}
\sphinxAtStartPar
Various confidence intervals can be obtained, depending on the setting. These are shown in the table below.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Approximate 95\% Confidence Interval
\\
\hline
\sphinxAtStartPar
Small samples
&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal distribution, known \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm 1.96 \times \sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal distribution, unknown \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm t_{n_1+n_2-2} \times s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)
\\
\hline
\sphinxAtStartPar
Large samples
&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal or not, known \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm 1.96 \times s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)
\\
\hline
\sphinxAtStartPar
\sphinxhyphen{} Normal or not, unknown \(\sigma\)
&
\sphinxAtStartPar
\(\hat{\mu} \pm 1.96 \times s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
where \(t_{n_1+n_2-2}\) is a number obtained from the t\sphinxhyphen{}distribution with \(n_1 + n_2 - 2\) degrees of freedom. This will give a number that takes a value of around 2 for smaller samples and approximately 1.96 for larger samples.

\sphinxAtStartPar
Modified intervals that do not assume equality of standard deviation in the two groups also exist.


\section{7.5 Confidence Intervals using resampling}
\label{\detokenize{07.f. Frequentist I:confidence-intervals-using-resampling}}\label{\detokenize{07.f. Frequentist I::doc}}
\sphinxAtStartPar
We saw that we can often create an approximate sampling distribution by resampling from our sample data. This is particularly useful in situations where there is no algebraic derivation for the sampling distribution.

\sphinxAtStartPar
We have seen that the important connection between sampling distributions and confidence intervals. So we would intuitively expect to be able to construct a confidence interval from the approximate sampling distribution we obtained using resampling. This is indeed possible. There are many ways of doing this, but the simplest and most intuitive method is the \sphinxstylestrong{bootstrap percentile confidence interval}.

\sphinxAtStartPar
The basic idea is very simple. We construct an approximate sampling distribution using bootstrap samples, as we did previously. Then we take the 2.5th and 97.5th percentiles of that distribution (the value such that 2.5\% of the observations \sphinxhyphen{} the estimates across bootstrap samples \sphinxhyphen{} lie below the value; and the value such that 2.5\% of observations lie above the value, respectively). These form the limits of our 95\% confidence interval.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{set.seed}\PYG{p}{(}\PYG{l+m}{78234}\PYG{p}{)} 

\PYG{c+c1}{\PYGZsh{} Read in the sample of 10 ages}
\PYG{n}{ages} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{28.1}\PYG{p}{,}\PYG{l+m}{27.5}\PYG{p}{,}\PYG{l+m}{25}\PYG{p}{,}\PYG{l+m}{29.9}\PYG{p}{,}\PYG{l+m}{29.7}\PYG{p}{,}\PYG{l+m}{29.9}\PYG{p}{,}\PYG{l+m}{39.9}\PYG{p}{,}\PYG{l+m}{33.6}\PYG{p}{,}\PYG{l+m}{21.3}\PYG{p}{,}\PYG{l+m}{30.8}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw bootstrap samples }
\PYG{n}{bootstrap\PYGZus{}samples} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{lapply}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{1039}\PYG{p}{,} \PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)} \PYG{n+nf}{sample}\PYG{p}{(}\PYG{n}{ages}\PYG{p}{,} \PYG{n}{replace} \PYG{o}{=} \PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate sample means in each bootstrap sample}
\PYG{n}{r.mean} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{sapply}\PYG{p}{(}\PYG{n}{bootstrap\PYGZus{}samples}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Obtain the 2.5th and 97.5th percentiles of the sample means across bootstrap samples}
\PYG{p}{(}\PYG{n}{q}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{quantile}\PYG{p}{(}\PYG{n}{r.mean}\PYG{p}{,} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.025}\PYG{p}{,} \PYG{l+m}{0.975}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
         
\PYG{c+c1}{\PYGZsh{} Draw the approximate sampling distribution with the percentile confidence limits marked in red}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{4.5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4.5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{r.mean}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution for mean \PYGZbs{}n with percentile 95\PYGZpc{} confidence limits\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sample mean\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{n}{q}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}\begin{description*}
\item[2.5\textbackslash{}\%] 26.798
\item[97.5\textbackslash{}\%] 32.501
\end{description*}\end{split}
\end{equation*}
\noindent\sphinxincludegraphics{{07.f. Frequentist I_1_1}.png}

\sphinxAtStartPar
The approximate 95\% confidence interval for the mean age obtained by using the algebric approximation to the sampling distribution was: 26.6 to 32.5. The bootstrap percentile 95\% confidence interval is: 26.8 to 32.5. We see that these intervals are very similar to one another, as we would expect.


\section{7.6 Summary: Use of confidence intervals}
\label{\detokenize{07.g. Frequentist I:summary-use-of-confidence-intervals}}\label{\detokenize{07.g. Frequentist I::doc}}
\sphinxAtStartPar
In this session we have discussed the concepts underlying confidence intervals and different interpretations of 95\% confidence intervals.

\sphinxAtStartPar
We can construct a confidence interval for any estimate, including:
\begin{itemize}
\item {} 
\sphinxAtStartPar
means

\item {} 
\sphinxAtStartPar
proportions

\item {} 
\sphinxAtStartPar
differences in means

\item {} 
\sphinxAtStartPar
risk ratios

\item {} 
\sphinxAtStartPar
regression coefficients

\item {} 
\sphinxAtStartPar
etc.

\end{itemize}

\sphinxAtStartPar
The way we construct confidence intervals can vary but the basic interpretation of the confidence interval remains the same.

\sphinxAtStartPar
While we have focused on 95\% confidence intervals, we can construct other intervals, e.g. 99\% confidence intervals. The use of 95\% confidence intervals is largely convention.


\section{Further resources}
\label{\detokenize{07.h. Frequentist I:further-resources}}\label{\detokenize{07.h. Frequentist I::doc}}
\sphinxAtStartPar
Note: further resources are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. All examinable material is contained within the notes.

\sphinxAtStartPar
Ashley I Naimi, Brian W Whitcomb, Can Confidence Intervals Be Interpreted?, American Journal of Epidemiology, Volume 189, Issue 7, July 2020, Pages 631–633, \sphinxurl{https://doi.org/10.1093/aje/kwaa004}


\chapter{8. Frequentist II: Hypothesis tests}
\label{\detokenize{08.a. Frequentist II:frequentist-ii-hypothesis-tests}}\label{\detokenize{08.a. Frequentist II::doc}}
\sphinxAtStartPar
In this session we continue with frequentist inference, exploring the concept of hypothesis testing and p\sphinxhyphen{}values. We discuss the general principle underlying a p\sphinxhyphen{}value, connections to confidence intervals and common misinterpretations and misuses of p\sphinxhyphen{}values.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
describe the meaning of the terms null and alternative hypotheses

\item {} 
\sphinxAtStartPar
describe what a p\sphinxhyphen{}value is

\item {} 
\sphinxAtStartPar
correctly interpret a p\sphinxhyphen{}value

\item {} 
\sphinxAtStartPar
explain the connection between 95\% confidence intervals and p\sphinxhyphen{}values

\item {} 
\sphinxAtStartPar
describe the calculation of a p\sphinxhyphen{}value

\end{itemize}



\sphinxAtStartPar
This session does not cover the mathematical derivation underlying hypothesis tests and common test statistics. Our intention is not to equip you with the ability to construct novel hypothesis tests. The purpose of this session is rather to convey an understanding of what a hypothesis test is, what a p\sphinxhyphen{}value is and how to interpret p\sphinxhyphen{}values correctly.


\section{8.1 Evidence against hypotheses}
\label{\detokenize{08.b. Frequentist II:evidence-against-hypotheses}}\label{\detokenize{08.b. Frequentist II::doc}}
\sphinxAtStartPar
This session considers the concept of testing hypotheses.


\subsection{8.1.1 Proving and disproving hypotheses}
\label{\detokenize{08.b. Frequentist II:proving-and-disproving-hypotheses}}
\sphinxAtStartPar
Let’s consider a very simple example. Suppose we believe that all men are over 120cm tall. We could:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Prove} the hypothesis by finding every man and showing they are more than 120cm tall

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Disprove} the hypothesis by finding a single man less than 120cm tall

\end{itemize}
\begin{quote}

\sphinxAtStartPar
It is easier to find evidence against a hypothesis than to prove it to be correct.
\end{quote}

\sphinxAtStartPar
The general approach we will take is as follows. We start with a \sphinxstylestrong{null hypothesis}, which is typically a statement about the population value of parameters. This will often be a statement of “no difference”. Some examples might be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Exposure to passive smoking is not associated with subsequent risk of lung cancer.

\item {} 
\sphinxAtStartPar
Treatment A does not improve survival compared with placebo

\item {} 
\sphinxAtStartPar
The mean body mass index (BMI) in England is the same as the mean BMI in Scotland.

\end{itemize}

\sphinxAtStartPar
We assume that our null hypothesis holds, i.e. that our sample of data came from a population in which our null hypothesis is true. We then look for evidence, in our sample data, against the null hypothesis (i.e. to falsify the hypothesis).

\sphinxAtStartPar
For example, suppose our null hypothesis is that the mean BMI is the same in England and Scotland and that we have a random sample of adults from England and Scotland. If we assume our null hypothesis is true (the two populations have the same mean BMI), then we would expect our two samples to have similar means. If, in fact, we observed very different sample means in the two sample groups then we would take this as \sphinxstyleemphasis{evidence against our null hypothesis}.


\subsection{8.1.2 Example}
\label{\detokenize{08.b. Frequentist II:example}}
\sphinxAtStartPar
To explore the concept of hypothesis testing, we will return to the example of emotional distress among violence researchers. The researchers were randomly assigned to receive an intervention (group debriefing aimed at reducing emotional distress) or control (nothing). At the end of the intervention, 22 researchers in the intervention group and 26 researchers in the control group filled in a questionnaire measuring emotional distress. The score gives a value of 0\sphinxhyphen{}20, with higher scores indicating higher distress.

\sphinxAtStartPar
The sample mean scores and their standard deviations in the two groups are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Control group (\(n_0\)=26), sample mean emotional distress score (sample standard deviation):  \(\bar{x}_0 = 6.35\), (SD = 1.87)

\item {} 
\sphinxAtStartPar
Intervention group (\(n_1\)=22), sample mean emotional distress score (sample standard deviation):  \(\bar{x}_1 = 5.45\), (SD = 1.87)

\end{itemize}

\sphinxAtStartPar
The research question we consider in this session is:
\begin{quote}

\sphinxAtStartPar
Is the true mean emotional distress score is different in the intervention and control group?
\end{quote}

\sphinxAtStartPar
The population parameter of interest is therefore the difference between the population mean emotional distress score in the intervention and control groups.

\sphinxAtStartPar
Aside: as is often the case, the population is a bit hard to define here. We can think about it as being the wider population of people who could be given the intervention (or not).

\sphinxAtStartPar
The code below reads in the data, obtains the sample means and SDs and draws histograms of the scores in each group.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Read in data (emotional distress scores in control and intervention group)}
\PYG{n}{dist0} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{5}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{8}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{10}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{)}
\PYG{n}{dist1}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,} \PYG{l+m}{10}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{3}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{8}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{3}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate sample means }
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sample means: \PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dist0}\PYG{p}{)}
\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dist1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate sample standard deviations }
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sample SDs: \PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{n+nf}{var}\PYG{p}{(}\PYG{n}{dist0}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{n+nf}{var}\PYG{p}{(}\PYG{n}{dist1}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Sample difference in means}
\PYG{n+nf}{print}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Difference in sample means:\PYGZdq{}}\PYG{p}{)}
\PYG{p}{(}\PYG{n}{delta.hat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dist1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dist0}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw histograms of the scores in each group}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{dist0}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Distress score, control\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{12}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{dist1}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Distress score, intervention\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{12}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Sample means: \PYGZdq{}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}6.34615384615385\end{split}
\end{equation*}\begin{equation*}
\begin{split}5.45454545454545\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Sample SDs: \PYGZdq{}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}1.87493589634009\end{split}
\end{equation*}\begin{equation*}
\begin{split}1.8702501163843\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1] \PYGZdq{}Difference in sample means:\PYGZdq{}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}-0.891608391608392\end{split}
\end{equation*}
\noindent\sphinxincludegraphics{{08.b. Frequentist II_3_8}.png}

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:}  We will let \(Y_{0,1}, ...,Y_{0,26}\) be random variables representing the emotional distress scores of the 26 sampled researchers in the control group and \(Y_{1,1}, ...,Y_{1,22}\) be random variables representing the emotional distress scores of the 22 sampled researchers in the intervention group. So the first subscript denotes the group (0=control, 1=intervention) and the second is an index for the person (i=1, …, 26 in the control group; i=1,…,22 in the intervention group).

\sphinxAtStartPar
We will assume that all random variables are independent of each other. The emotional distress scores in the control group are all drawn from the same normal distribution, with population mean \(\mu_0\) and population standard deviation  \(\sigma\). For now, we suppose \(\sigma\) is a known value, with \(\sigma = 1.75\). The emotional distress scores in the intervention group are assumed to be drawn from a normal distribution with population mean \(\mu_1\) and the same population standard deviation.

\sphinxAtStartPar
This model can be compactly written as follows
\begin{equation*}
\begin{split} 
Y_{j,i} \overset{\small{iid}}{\sim} N(\mu_j, 1.75^2), \qquad i=1,2,...,n_j
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Data:} We will let \(y_{0,1}, ...,y_{0,26}\) and \(y_{1,1}, ...,y_{1,22}\) represent the realised values of these random variables (i.e. the observed emotional distress scores).

\sphinxAtStartPar
\sphinxstylestrong{Estimand, estimator and estimate:} The population parameter (estimand) we are interested in is:
\begin{equation*}
\begin{split}
\delta = \mu_1 - \mu_0
\end{split}
\end{equation*}
\sphinxAtStartPar
The obvious estimator for this is the sample difference in means:
\begin{equation*}
\begin{split}
\hat{\delta} = \bar{Y_1} - \bar{Y_0} = \frac{1}{n_1} \sum_{i=1}^{n_1} Y_{1,i} - \frac{1}{n_0} \sum_{i=1}^{n_0} Y_{0,i}
\end{split}
\end{equation*}
\sphinxAtStartPar
And the sample estimate is:
\begin{equation*}
\begin{split}
\hat{\delta} =  \bar{y_1} - \bar{y_0} = \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1,i} - \frac{1}{n_0} \sum_{i=1}^{n_0} y_{0,i} = 5.4545 - 6.3462 = -0.892
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Null and alternative hypotheses:} The null hypothesis is that there is no difference in the population mean emotional distress score in the intervention and control groups. Formally, we write:
\begin{equation*}
\begin{split}
H_0:  \delta = 0
\end{split}
\end{equation*}
\sphinxAtStartPar
The alternative hypothesis (sometimes written \(H_1\) or \(H_A\)) is that the null hypothesis is not true:
\begin{equation*}
\begin{split}
H_1: \delta \neq 0
\end{split}
\end{equation*}
\sphinxAtStartPar
In our sample, we have seen that \(\hat{\delta} = -0.892\). So the sample mean emotional distress score is lower in the intervention group (which is the direction we might be hoping for, since this group have received a form of counselling to reduce their emotional distress). However, the two sample means are very unlikely to be exactly equal, even if the true mean emotional distress score is the same in the two groups, due to sampling variability (i.e. due to random chance). So how should we interpret this sample difference in means? Does it constitute evidence against our null hypothesis?

\sphinxAtStartPar
In order to answer this question, we need to consider the sampling distribution of the difference in means. Unlike in previous sessions, where we used the sampling distribution to obtain confidence intervals, we are now interested in a subtly different sampling distribution: the sampling distribution that we would see \sphinxstyleemphasis{if the null hypothesis were true}.


\subsection{8.1.3 Sampling distribution for the difference in sample means}
\label{\detokenize{08.b. Frequentist II:sampling-distribution-for-the-difference-in-sample-means}}
\sphinxAtStartPar
Under the statistical models above, if \(\sigma\) is a known value it is straightforward to derive the sampling distribution of the estimator (the difference in sample means between groups).
\begin{quote}

\sphinxAtStartPar
Linear combinations of independent normal distributions are also normal
\end{quote}

\sphinxAtStartPar
Thus the distribution of \(\hat{\delta}\) is normal. We can then calculate its expectation and variance using techniques from the \sphinxhref{https://lshtm-hds.github.io/Math-Refresher}{Refresher} to obtain:
\begin{equation*}
\begin{split}
\hat{\delta} \sim N\left(\delta, \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_0} \right) \right)
\end{split}
\end{equation*}
\sphinxAtStartPar
Substituting in the values \(\sigma = 1.75\), \(n_0 = 26\) and \(n_1 = 22\), we have
\begin{equation*}
\begin{split}
\hat{\delta} \sim N\left(\delta, 1.75^2 \left(\frac{1}{22} + \frac{1}{26} \right) \right) = N(\delta,  0.507^2)  
\end{split}
\end{equation*}
\sphinxAtStartPar
So the expectation of the sampling distribution is the true population value \(\delta\) and the standard error is \(0.507\). Remember that because we are considering the distribution of an estimator, we call the standard deviation of the estimator (the square root of the variance) the standard error.

\sphinxAtStartPar
\sphinxstylestrong{Sampling distribution under the null hypothesis}

\sphinxAtStartPar
We are interested in the distribution of the difference in sampling means would look like under repeated sampling \sphinxstyleemphasis{if the null hypothesis were true}. The null hypothesis states that \(\delta = 0\). Therefore, under the null hypothesis,
\begin{equation*}
\begin{split}
\hat{\delta} \sim  N(0,  0.507^2) 
\end{split}
\end{equation*}
\sphinxAtStartPar
The graph below shows this distribution. The code to draw the graph is suppressed, since it is not the focus here, but can be viewed by clicking the button.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Sample difference in means}
\PYG{n}{delta.hat} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dist1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{dist0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Randomly generate 10000 sample differences in means (following the sampling distribution under the null hypothesis)}
\PYG{n}{sample.diff.means} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rnorm}\PYG{p}{(}\PYG{l+m}{10000}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.507}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw the approximate sampling distribution with the percentile confidence limits marked in red}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{sample.diff.means}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution for \PYGZbs{}n difference in sample means, \PYGZbs{}n UNDER THE NULL HYPOTHESIS\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Difference in sample means\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}2.5}\PYG{p}{,} \PYG{l+m}{2.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.8}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}2.5}\PYG{p}{,} \PYG{l+m}{2.5}\PYG{p}{,} \PYG{l+m}{0.025}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}2.5}\PYG{p}{,} \PYG{l+m}{2.5}\PYG{p}{,} \PYG{l+m}{0.025}\PYG{p}{)}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.507}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{0}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{n}{delta.hat}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{green\PYGZdq{}}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}

\PYG{n+nf}{text}\PYG{p}{(}\PYG{l+m}{1.5}\PYG{p}{,} \PYG{l+m}{0.2}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Null value\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{text}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}1.7}\PYG{p}{,} \PYG{l+m}{0.4}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Observed value\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1.5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.005}\PYG{p}{,} \PYG{l+m}{0.18}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}1.8}\PYG{p}{,} \PYG{l+m}{\PYGZhy{}0.9}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.38}\PYG{p}{,} \PYG{l+m}{0.28}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The sampling distribution above shows us the distribution of the differences in sample means that we could have seen under repeated sampling, i.e. if we had done the same study a very large number of times. The question we must ask now is: is the value we have seen consistent with this sampling distribution? Or is it surprising? A “surprising” result is taken as evidence against the null hypothesis. In order to clarify these ideas, consider two scenarios that could have happened.

\sphinxAtStartPar
Scenario 1: Suppose we had done exactly the same study, but had seen a difference in sample means of \(\hat{\delta} = -3.5\) (i.e. the intervention group sample mean score was 3.5 units lower than the control group mean).

\sphinxAtStartPar
Scenario 2: Suppose we had done this study, but had actually seen a difference in sample means of \(\hat{\delta} = 0.02\).

\sphinxAtStartPar
What would we conclude in these scenarios? The graph below superimposes these two scenarios on the sampling distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Draw the approximate sampling distribution with the percentile confidence limits marked in red}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{6}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{hist}\PYG{p}{(}\PYG{n}{sample.diff.means}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Sampling distribution for \PYGZbs{}ndifference in sample means, \PYGZbs{}nUNDER THE NULL HYPOTHESIS\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Difference in sample means\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}3.5}\PYG{p}{,} \PYG{l+m}{3.5}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}3.5}\PYG{p}{,} \PYG{l+m}{3.5}\PYG{p}{,} \PYG{l+m}{0.025}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}3.5}\PYG{p}{,} \PYG{l+m}{3.5}\PYG{p}{,} \PYG{l+m}{0.025}\PYG{p}{)}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{0.507}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{0}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{n}{delta.hat}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{green\PYGZdq{}}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{\PYGZhy{}3.5}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{orange\PYGZdq{}}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{0.02}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}

\PYG{n+nf}{text}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}2.5}\PYG{p}{,} \PYG{l+m}{0.42}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Scenario 1: \PYGZbs{}nObserved value\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{text}\PYG{p}{(}\PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{0.22}\PYG{p}{,}  \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Scenario 2: \PYGZbs{}nObserved value\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{text}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}1.9}\PYG{p}{,} \PYG{l+m}{0.62}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Actual data: \PYGZbs{}nObserved value\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.05}\PYG{p}{,} \PYG{l+m}{1.5}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.005}\PYG{p}{,} \PYG{l+m}{0.15}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}2.5}\PYG{p}{,} \PYG{l+m}{\PYGZhy{}3.4}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.35}\PYG{p}{,} \PYG{l+m}{0.2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}1.8}\PYG{p}{,} \PYG{l+m}{\PYGZhy{}0.9}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0.55}\PYG{p}{,} \PYG{l+m}{0.48}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Scenario 1} We can see from the histogram that, under the null hypothesis, the probability of seeing a difference in sample means of \sphinxhyphen{}3.5 or less is incredibly low. In fact, this probability is less than 1 in 10,000. So if we did 10,000 studies we would expect only one of them to have a difference in sample means of \sphinxhyphen{}3.5 or less.
\begin{itemize}
\item {} 
\sphinxAtStartPar
So, have we been very unlucky and picked a very very unusual sample by chance?

\item {} 
\sphinxAtStartPar
Or is our initial premise incorrect? Is the null hypothesis wrong?

\end{itemize}

\sphinxAtStartPar
This particular sample mean difference appears to be inconsistent with our null hypothesis. We interpret these “surprising” sample statistics as evidence against the null hypothesis.

\sphinxAtStartPar
\sphinxstylestrong{Scenario 2} Again, the histogram shows quite clearly that, under the null hypothesis, many of the samples that we could have obtained would give us a sample mean difference close to zero. So this sample difference is completely consistent with the null hypothesis.

\sphinxAtStartPar
In this case, we would conclude that there is no evidence against the null hypothesis.

\sphinxAtStartPar
\sphinxstylestrong{Our actual observed data} Our observed sample mean difference (\sphinxhyphen{}0.892) is somewhere in between. In fact, we can calculate the probability of observing a sample mean difference of \sphinxhyphen{}0.892 or lower (i.e. the proportion of the area of the histogram that lies to the left of \sphinxhyphen{}0.89): this turns out to be 4\%. So under repeated sampling, if our null hypothesis is true and there is truly no difference between the mean emotional distress score in the intervention and control groups, then we would expect to see a difference at least this big 4\% of the time.

\sphinxAtStartPar
In fact, we typically consider the proportion of samples in which we would get an estimate at least as extreme as the one we did get \sphinxstyleemphasis{in either direction}. In our case, this is the probability of seeing a sample mean difference of less than \sphinxhyphen{}0.892 or greater than +0.892. Under the null hypothesis, approximately 8\% of samples would produce a sample mean difference at least as extreme as the one we have seen in our sample.

\sphinxAtStartPar
So we had around a 1 in 13 chance of ending up with this result, if the null hypothesis is true. We interpret this as  weak evidence against the null hypothesis.


\section{8.2 The p\sphinxhyphen{}value}
\label{\detokenize{08.c. Frequentist II:the-p-value}}\label{\detokenize{08.c. Frequentist II::doc}}
\sphinxAtStartPar
The p\sphinxhyphen{}value is defined as the probability of observing the sample estimate or a more extreme one (in either direction) given that the null hypothesis is true.

\sphinxAtStartPar
The smaller the p\sphinxhyphen{}value, the lower the chance of getting a difference as big  as the one observed if the null hypothesis is true.

\sphinxAtStartPar
Therefore, the smaller the p\sphinxhyphen{}value,  the stronger the evidence against the null hypothesis.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{pvalues}.png}
\caption{Interpretation of p\sphinxhyphen{}values \sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1119478/}{(taken from Sterne \& Davey\sphinxhyphen{}Smith)}}\label{\detokenize{08.c. Frequentist II:inference}}\end{figure}

\sphinxAtStartPar
The value of 0.05 has historically been used as a cut\sphinxhyphen{}off, with values of \(p<0.05\) deemed “statistically significant” and values of \(p\geq 0.05\) “not significant”. As discussed further in a later sub\sphinxhyphen{}section, we do not recommend dichotomising p\sphinxhyphen{}values in this way.
\begin{quote}

\sphinxAtStartPar
Note that: \sphinxhyphen{} We have described what is called a \sphinxstyleemphasis{two\sphinxhyphen{}sided test}. Occasionally, a \sphinxstyleemphasis{one\sphinxhyphen{}sided test} might be used, where the p\sphinxhyphen{}value is the probability of results that are as extreme, or more extreme, \sphinxstyleemphasis{in the same direction} as the observed result. However, it is rare that it is justifiable to ignore sample statistics in one direction, so we will focus on two\sphinxhyphen{}sided tests.
 \sphinxhyphen{} When the sampling distribution is not symmetric, it can be hard to define what is \sphinxstyleemphasis{as extreme as} the estimate we have seen. In this case, there are various ways of obtaining the two\sphinxhyphen{}sided p\sphinxhyphen{}value. We do not pursue this further.
\end{quote}


\section{8.3 Connection between p\sphinxhyphen{}values and confidence intervals}
\label{\detokenize{08.d. Frequentist II:connection-between-p-values-and-confidence-intervals}}\label{\detokenize{08.d. Frequentist II::doc}}
\sphinxAtStartPar
Recall that we previously used the following fact:
\begin{quote}

\sphinxAtStartPar
For a normal distribution, approximately 95\% of observations are contained within 1.96 standard deviations of the mean.
\end{quote}

\sphinxAtStartPar
Which, applied to sampling distributions, tells us that:
\begin{quote}

\sphinxAtStartPar
For a normally distributed sampling distribution that is centred around the true population value, 95\% of the estimates obtained under repeated sampling would be contained within 1.96 standard errors of the true population value
\end{quote}

\sphinxAtStartPar
Applying this to the estimator \(\hat{\delta}\), this leads to a 95\% confidence interval of
\begin{equation*}
\begin{split}
\hat{\delta} \pm  1.96 \times SE(\delta)
\end{split}
\end{equation*}
\sphinxAtStartPar
The graph below shows some possible values of \(\hat{\delta}\), along with their 95\% confidence intervals. We see that:
\begin{itemize}
\item {} 
\sphinxAtStartPar
if \(\hat{\delta}\) is exactly equal to the number \(1.96 \times  SE(\delta)\) then the 95\% confidence interval just touches zero.

\item {} 
\sphinxAtStartPar
if \(\hat{\delta} > 1.96 \times  SE(\delta)\) then the 95\% confidence interval does not include zero \sphinxhyphen{} the whole interval lies above zero.

\item {} 
\sphinxAtStartPar
if \(0 < \hat{\delta} < 1.96\)  then the 95\% confidence interval does include zero.

\end{itemize}

\sphinxAtStartPar
So what p\sphinxhyphen{}values would these values of \(\hat{\delta}\) result in?
\begin{itemize}
\item {} 
\sphinxAtStartPar
if \(\hat{\delta} = 1.96 \times  SE(\delta)\) then we know that 2.5\% of the estimates lie above that point, so p=0.05.

\item {} 
\sphinxAtStartPar
if \(\hat{\delta} > 1.96 \times  SE(\delta)\) then fewer than 2.5\% of estimates lie above \(\hat{\delta}\), so p<0.05

\item {} 
\sphinxAtStartPar
if \(0 < \hat{\delta} < 1.96 \times  SE(\delta)\) then more than 2.5\% of estimates lie above \(\hat{\delta}\), so p>0.05

\end{itemize}

\sphinxAtStartPar
This leads us to the connection between 95\% confidence intervals and p\sphinxhyphen{}values. When a 95\% confidence interval and p\sphinxhyphen{}value are obtained from the same sampling distribution (which is typically the case when both are presented),


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
P\sphinxhyphen{}value
&\sphinxstyletheadfamily 
\sphinxAtStartPar
95\% confidence interval
\\
\hline
\sphinxAtStartPar
\(<0.05\)
&
\sphinxAtStartPar
Excludes the null value
\\
\hline
\sphinxAtStartPar
\(\geq 0.05\)
&
\sphinxAtStartPar
Contains the null value
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\noindent\sphinxincludegraphics{{08.d. Frequentist II_1_0}.png}


\section{8.4 Other (mis\sphinxhyphen{})interpretations of p\sphinxhyphen{}values}
\label{\detokenize{08.e. Frequentist II:other-mis-interpretations-of-p-values}}\label{\detokenize{08.e. Frequentist II::doc}}

\subsection{8.4.1 P\sphinxhyphen{}values as decision rules}
\label{\detokenize{08.e. Frequentist II:p-values-as-decision-rules}}
\sphinxAtStartPar
Traditionally, hypothesis tests have been thought of as a means to make decisions. In this paradigm, a cut\sphinxhyphen{}off (typically p<0.05) is chosen. If the p\sphinxhyphen{}value is smaller than the chosen cut\sphinxhyphen{}off, the null hypothesis is rejected. If the p\sphinxhyphen{}value is above the cut\sphinxhyphen{}off then the null hypothesis is accepted. This leads to the terminology of:
\begin{itemize}
\item {} 
\sphinxAtStartPar
“Type I error”, rejecting the null hypothesis when it is true

\item {} 
\sphinxAtStartPar
“Type II error”, accepting the null hypothesis when it is false

\end{itemize}

\sphinxAtStartPar
Linked to this approach is the habit of labelling p\sphinxhyphen{}values < 0.05 as “significant” and those larger as “non\sphinxhyphen{}significant”.

\sphinxAtStartPar
There are some instances where this decision\sphinxhyphen{}making paradigm seems appropriate. Some health data science research is indeed concerned with decision making. For example, we may wish to carry out a trial to assess whether a particular clinical decision support system improves the clinicians’ ability to detect malignant tumours. However much health data science research is not, at least directly, concerned with decision making. For example if we carry out an epidemiological study in which we relate risk of a particular disease to gender, we do this because we are interested in understanding the aetiology of the disease, not because we want to assess whether to modify gender! For this reason many researchers regard p\sphinxhyphen{}values as a measure of strength of evidence against the null hypothesis, rather than as an aid to decision
making.

\sphinxAtStartPar
In general, we do not advocate any approach which dichotomises p\sphinxhyphen{}values. There is very little difference, in terms of the information contained about the population parameter, between the two p\sphinxhyphen{}values of \(p=0.049\) and \(p=0.051\). Therefore it seems counter\sphinxhyphen{}intuitive to make very different decisions based on these p\sphinxhyphen{}values.

\sphinxAtStartPar
P\sphinxhyphen{}values represent an area of substantial philosophical controversy in statistics. We choose to interpret the p\sphinxhyphen{}value as a measure of strength of evidence against the null hypothesis. It should, however, be pointed out that some statisticians advocate strongly against this interpretation.

\sphinxAtStartPar
In much health data science research, we are interested in knowing more about a particular population parameter. Many health data scientists, therefore, choose to focus on obtaining and interpreting estimates and confidence intervals rather than calculating p\sphinxhyphen{}values.


\subsection{8.4.2 Misinterpretations of p\sphinxhyphen{}values}
\label{\detokenize{08.e. Frequentist II:misinterpretations-of-p-values}}
\sphinxAtStartPar
The p\sphinxhyphen{}value is the subject of a lot of argument, debate and controversy, both within the statistical world and beyond. The following warn against some common misinterpretations and mis\sphinxhyphen{}uses of p\sphinxhyphen{}values:
\begin{quote}

\sphinxAtStartPar
Do not:  \sphinxhyphen{} believe that an association or effect exists just because it was statistically significant.
 \sphinxhyphen{} conclude that an association or effect is absent just because it was not statistically significant.
 \sphinxhyphen{} base conclusions solely on whether an association or effect was statistically significant or not.
 \sphinxhyphen{} conclude anything about scientific or practical importance based on statistical significance (or lack thereof).
 \sphinxhyphen{} interpret a p\sphinxhyphen{}value as the probability that chance alone produced the observed association or effect or the probability that the null hypothesis is true.
\end{quote}

\sphinxAtStartPar
Importantly, statistical significance was never meant to imply scientific or clinical importance. As well as the p\sphinxhyphen{}value, always consider the estimated effect of the population parameter of interest and its confidence interval. These will often provide more insight than the p\sphinxhyphen{}value alone.


\section{8.5 Calculating p\sphinxhyphen{}values}
\label{\detokenize{08.f. Frequentist II:calculating-p-values}}\label{\detokenize{08.f. Frequentist II::doc}}

\subsection{8.5.1 Example: Calculation of the p\sphinxhyphen{}value}
\label{\detokenize{08.f. Frequentist II:example-calculation-of-the-p-value}}
\sphinxAtStartPar
In the emotional distress example, our difference in sample means is \(\hat{\delta} = -0.892\).
We are interested in the distribution of the difference in sampling means would look like under repeated sampling \sphinxstyleemphasis{if the null hypothesis were true}. The null hypothesis states that \(\delta = 0\). Therefore, under the null hypothesis,
\begin{equation*}
\begin{split}
\hat{\delta} \sim  N(0,  0.507^2) 
\end{split}
\end{equation*}
\sphinxAtStartPar
The easiest way to do this calculation is to standardise the estimator to follow a standard normal distribution, i.e.
\begin{equation*}
\begin{split}
Z = \frac{\hat{\delta}}{0.507} \sim  N(0, 1) 
\end{split}
\end{equation*}
\sphinxAtStartPar
In our sample, we get a value of \(Z=-0.892/0.507 = -1.76\). The p\sphinxhyphen{}value is defined as
\begin{equation*}
\begin{split}
p = Pr( | \hat{\delta} | \geq  -0.892) = Pr( | Z | \geq   1.76)
\end{split}
\end{equation*}
\sphinxAtStartPar
The standard normal distribution is symmetric, so this is equal to \(2 \times P(Z \geq 1.76)\). This probability can be looked up using pre\sphinxhyphen{}calculated tables stored in all standard statistical software.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Manual calculation of p\PYGZhy{}value: }
\PYG{l+m}{2}\PYG{o}{*}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{l+m}{1.76}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.0784078065749654\end{split}
\end{equation*}

\subsection{8.5.2 Approximate tests in large samples}
\label{\detokenize{08.f. Frequentist II:approximate-tests-in-large-samples}}
\sphinxAtStartPar
More generally, suppose that the random variable used to calculate our p\sphinxhyphen{}value (above, the random variable was the  difference in sample means) is denoted by \(R\) and that it has an expected value and variance (under the null hypothesis) denoted by \(E(R)\) and \(Var(R)\). Then define:
\begin{equation*}
\begin{split}
Z = \frac{R - E[R]}{\sqrt{Var(R)}} = \frac{R - E[R]}{SE(R)} 
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(SE(R)\) is the standard error of \(R\) (the standard deviation of the sampling distribution; alternatively the square root of the variance of \(R\)). To simplify this even further, in many cases, as for the difference in sample means, \(E(R) = 0\).

\sphinxAtStartPar
Thanks to the Central Limit Theorem, in almost all situations, as the sample size \(n\) becomes large, the distribution of \(Z\) tends towards a standard normal distribution.
\begin{equation*}
\begin{split}
lim_{n\rightarrow \infty} \ \  Z \sim N(0, 1). 
\end{split}
\end{equation*}
\sphinxAtStartPar
The standard normal distribution can then be used to calculate the two\sphinxhyphen{}sided p\sphinxhyphen{}value, as above.


\subsection{8.5.3 The two\sphinxhyphen{}sample t\sphinxhyphen{}test}
\label{\detokenize{08.f. Frequentist II:the-two-sample-t-test}}
\sphinxAtStartPar
Let us return to the comparison in population means between two groups. When, as is more typical, we do not know the value of \(\sigma\), we need to replace it with an estimate from our sample, \(\hat{\sigma}\). Typically we use an estimate based on the sample standard deviations in the two groups, \(s_1\) and \(s_0\):
\begin{equation*}
\begin{split}
\hat{\sigma}^2 = \frac{(n_1 - 1) s_1^2 + (n_0 - 1) s_0^2}{n_1 + n_0 - 2}
\end{split}
\end{equation*}
\sphinxAtStartPar
For our sample of data, \(\hat{\sigma} = 1.873\). The sampling distribution we used above involves the true population standard deviation
\begin{equation*}
\begin{split}
\hat{\delta} \sim N\left(\delta, \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_0} \right) \right)
\end{split}
\end{equation*}
\sphinxAtStartPar
Similarly, the equivalent version of the sampling distribution (which we will find it easier to modify for our current purposes), is also no longer exactly true:
\begin{equation*}
\begin{split}
\frac{\hat{\delta} - \delta}{\sigma \sqrt{\left(\frac{1}{n_1} + \frac{1}{n_0}\right) }}\sim N(0,1)
\end{split}
\end{equation*}
\sphinxAtStartPar
This is only approximately true if we substitute the sample estimate \(\hat{\sigma}\) into the equation. A little more algebra (not shown here), however, gives us an exact distribution.
\begin{equation*}
\begin{split}
\frac{\hat{\delta} -\delta}{\hat{\sigma} \sqrt{\frac{1}{n_1} + \frac{1}{n_0}}} \sim t_{n_1 + n_0 - 2}
\end{split}
\end{equation*}
\sphinxAtStartPar
Under the null hypothesis, \(\delta = 0\), giving
\begin{equation*}
\begin{split}
T = \frac{\hat{\delta}}{\hat{\sigma} \sqrt{\frac{1}{n_1} + \frac{1}{n_0}}} \sim t_{n_1 + n_0 - 2}
\end{split}
\end{equation*}
\sphinxAtStartPar
Substituting in the numbers from our sample of data,
\begin{equation*}
\begin{split}
T = \frac{-0.892}{1.873 \sqrt{\frac{1}{22} + \frac{1}{26}}} 
\end{split}
\end{equation*}
\sphinxAtStartPar
gives \(t = -1.644\) (remembering that \(T\) is the random variable and \(t\) here is the realised (observed) value of that statistic). T\sphinxhyphen{}distributions are symmetric around zero, so we take \sphinxstyleemphasis{at least as extreme as} to mean less than \sphinxhyphen{}1.64 or greater than +1.64, which in turn is twice the probability of being less than \sphinxhyphen{}1.64. We simply need to calculate this probability for a t\sphinxhyphen{}distribution with 46 degrees of freedom (where we obtained 46 as \(n_1 + n_0 - 2\)).

\sphinxAtStartPar
The code below performs this calculation and then uses an inbuilt R package to obtain the same p\sphinxhyphen{}value.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Manual calculation of p\PYGZhy{}value (two equivalent calculations)}
\PYG{l+m}{2}\PYG{o}{*}\PYG{n+nf}{pt}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}1.644}\PYG{p}{,} \PYG{l+m}{46}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Read in data (emotional distress scores in control and intervention group)}
\PYG{n}{dist0} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{5}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{8}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{10}\PYG{p}{,}  \PYG{l+m}{9}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{)}
\PYG{n}{dist1}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,} \PYG{l+m}{10}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{3}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{8}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{7}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{4}\PYG{p}{,}  \PYG{l+m}{6}\PYG{p}{,}  \PYG{l+m}{3}\PYG{p}{,}  \PYG{l+m}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} T\PYGZhy{}test using inbuilt R package}
\PYG{n}{dist} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{n}{dist0}\PYG{p}{,} \PYG{n}{dist1}\PYG{p}{)} 
\PYG{n}{gp} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{c}\PYG{p}{(}\PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{26}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{rep}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,} \PYG{l+m}{22}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nf}{t.test}\PYG{p}{(}\PYG{n}{dist}\PYG{o}{\PYGZti{}}\PYG{n}{gp}\PYG{p}{,} \PYG{n}{var.equal}\PYG{o}{=}\PYG{k+kc}{TRUE}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.106994541315052\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
	Two Sample t\PYGZhy{}test

data:  dist by gp
t = 1.6435, df = 46, p\PYGZhy{}value = 0.1071
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 \PYGZhy{}0.2004223  1.9836391
sample estimates:
mean in group 0 mean in group 1 
       6.346154        5.454545 
\end{sphinxVerbatim}

\sphinxAtStartPar
Rounding to 2 decimal places, the p\sphinxhyphen{}value is 0.11.

\sphinxAtStartPar
In the output from the R package, the line

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{t = 1.6435, df = 46, p\sphinxhyphen{}value = 0.1071}}

\sphinxAtStartPar
tells us that the value of the statistic \(T\) above is \(t=1.64\) in this sample, the degrees of freedom tell us that we are looking at a t\sphinxhyphen{}distribution on 46 degrees of freedom. We are also given a 95\% confidence interval for the population difference in means: (\sphinxhyphen{}0.20 to 1.98). As we noted above, when the p\sphinxhyphen{}value is >0.05 then the null value (here, zero) will be included in the 95\% confidence interval.


\subsection{8.5.4 Other hypothesis tests}
\label{\detokenize{08.f. Frequentist II:other-hypothesis-tests}}
\sphinxAtStartPar
You will meet many types of hypothesis tests over your statistical studies. Many, like the t\sphinxhyphen{}test above, are constructed around a particular estimator and so there is a nice connection between the estimate, the 95\% confidence interval and the p\sphinxhyphen{}value from the hypothesis test. Where this is the case, it is good practice to present the estimate and confidence interval alongside the p\sphinxhyphen{}value, since they contain much more information than the p\sphinxhyphen{}value alone.

\sphinxAtStartPar
In other cases, tests can be constructed without a specific parameter being estimated. The chi\sphinxhyphen{}squared test is a very commonly\sphinxhyphen{}used test. It tests the null hypothesis of no association between two unordered categorical variables. This test does not directly invoke the sampling distribution of an estimator, so typically only the p\sphinxhyphen{}value is presented, rather than also presenting an estimate and confidence interval.

\sphinxAtStartPar
In general, hypothesis testing is a controversial and widely misunderstood area of frequentist statistics. Where possible, focusing on estimating parameters along with confidence intervals can avoid some of the more damaging misuses of p\sphinxhyphen{}values.


\section{Further resources}
\label{\detokenize{08.g. Frequentist II:further-resources}}\label{\detokenize{08.g. Frequentist II::doc}}
\sphinxAtStartPar
Note: further resources are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. All examinable material is contained within the notes.

\sphinxAtStartPar
Stang A, Poole C, Kuss O. The ongoing tyranny of statistical significance testing in biomedical research. Eur J Epidemiol. 2010;25(4):225\sphinxhyphen{}230. doi:10.1007/s10654\sphinxhyphen{}010\sphinxhyphen{}9440\sphinxhyphen{}x

\sphinxAtStartPar
\sphinxhref{https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108}{Ronald L. Wasserstein \& Nicole A. Lazar (2016) The ASA Statement on p\sphinxhyphen{}Values: Context, Process, and Purpose, The American Statistician, 70:2, 129\sphinxhyphen{}133, DOI: 10.1080/00031305.2016.1154108}

\sphinxAtStartPar
\sphinxhref{https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913}{Ronald L. Wasserstein, Allen L. Schirm \& Nicole A. Lazar (2019) Moving to a World Beyond “p < 0.05”, The American Statistician, 73:sup1, 1\sphinxhyphen{}19, DOI: 10.1080/00031305.2019.1583913}

\sphinxAtStartPar
\sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1119478/}{Sterne JA, Davey Smith G. Sifting the evidence\sphinxhyphen{}what’s wrong with significance tests? BMJ. 2001;322(7280):226\sphinxhyphen{}231. doi:10.1136/bmj.322.7280.226}


\chapter{9. Bayesian Statistics I}
\label{\detokenize{09.a. Bayesian Statistics I:bayesian-statistics-i}}\label{\detokenize{09.a. Bayesian Statistics I::doc}}
\sphinxAtStartPar
So far in this module, we have looked at frequentist or classical statistical ideas, such as maximum likelihood estimation, hypothesis testing and p\sphinxhyphen{}values. Underlying the frequentist approach is the belief that there is a true state of reality, and that parameters have a fixed and true value. Probabilities are long\sphinxhyphen{}run frequencies; for example, the probability of a driver in London having a car accident is a fixed value between 0 and 1. A typical way of estimating this value is to take a sample of observations, construct a likelihood function for these observations, and to obtain the parameter value that maximizes the likelihood. When we take a Bayesian approach, the parameter we wish to estimate is considered to be a random variable, and probabilities may represent a subjective belief about the state of uncertainty, or there may be a data generating distribution underlying the random parameter. For example, you may have a prior belief about the probability of a driver in London having a car accident, and after collecting a sample of data, you combine your prior beliefs with the likelihood for those observations to construct an updated belief \sphinxhyphen{} the posterior. Your belief may change in light of the data.

\sphinxAtStartPar
Bayesian methods sometimes require numerical integration, and cheaper computing has made Bayesian approaches more feasible in the last 20 years. Bayesian approaches are likely to be an important part of working in Health Data Science. In the next two sessions, we introduce the fundamental principles.

\sphinxAtStartPar
The current session introduces the basic concepts underlying Bayesian inference and then applies the basic principles to a simple example using proportions.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
compare the notions of probability and likelihood in Bayesian and Frequentist paradigms

\item {} 
\sphinxAtStartPar
explain the notions of prior and posterior distributions

\item {} 
\sphinxAtStartPar
apply Bayes Theorem in the discrete case

\item {} 
\sphinxAtStartPar
Understand and apply the basic principles of Bayesian analysis using proportions, specifically:
\begin{itemize}
\item {} 
\sphinxAtStartPar
use the beta distribution as a prior and derive the posterior distribution

\item {} 
\sphinxAtStartPar
obtain credible HPD intervals for the parameter

\item {} 
\sphinxAtStartPar
obtain prior and posterior predictive distributions

\item {} 
\sphinxAtStartPar
explain the concept of conjugate priors

\end{itemize}

\end{itemize}




\section{9.1 Introduction to Bayesian Inference}
\label{\detokenize{09.b. Bayesian Statistics I:introduction-to-bayesian-inference}}\label{\detokenize{09.b. Bayesian Statistics I::doc}}

\subsection{9.1.1 Probability}
\label{\detokenize{09.b. Bayesian Statistics I:probability}}
\sphinxAtStartPar
In Session 2, we learned about probability in the frequentist sense: the proportion of times an event occurs in the long\sphinxhyphen{}run. Let’s have a look at the following two scenarios:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
A research group wishes to know the probability that a baby who is born in a particular hospital ward has cystic fibrosis. They look at the records on screening tests done at birth to investigate.

\item {} 
\sphinxAtStartPar
A 34 year old woman attends her GP practice, worried that she has cancer because she has had feelings of “fullness” and “bloating” as well as mild nausea for the last 2 weeks. The patient mentions ovarian, bowel and pancreatic cancer as concerns having read about her symptoms on the internet. The rest of the history as well as physical examination are unremarkable. If the GP’s assessment of the risk were above a certain level, the GP might refer the patient for tests (collect more data). In this case, the GP concludes that the current information about the patient suggests there is a very low risk that the patient has cancer.

\end{enumerate}
\begin{quote}

\sphinxAtStartPar
What is the quantity that we trying to estimate in each scenario?What is the frequentist definition of probability in each of these settings? Does it make sense?
\end{quote}

\sphinxAtStartPar
A key problem with the frequentist paradigm is that the “long\sphinxhyphen{}run” frequency definition is not always relevant, or even appropriate, as we see in the second example above. Further, notice that the GP uses information from different sources to draw his/her conclusion about the probability that the patient has cancer. This synthesis of information can be incorporated into a Bayesian framework. A frequentist, in contrast, would tackle this problem by thinking about:
\begin{quote}

\sphinxAtStartPar
a) the probability of the patient having these symptoms, given that she has cancer;b) the probability of the patient having these symptoms, given that she does not have cancer;
\end{quote}

\sphinxAtStartPar
and comparing the two probabilities. Note that this does not take into account the extra information about the context.


\subsection{9.1.2 Bayesian Inference}
\label{\detokenize{09.b. Bayesian Statistics I:bayesian-inference}}
\sphinxAtStartPar
The underlying concept for Bayesian inference essentially works as follows. We have some population parameter \(\theta\) which we wish to make inference on, and the likelihood \(p(y|\theta)\) which tells us how likely different values of \(y\) are, conditional on different parameter values \(\theta\). In the frequentist approach, \(\theta\) is considered to be a fixed, but unknown, constant. Inference is then based on the likelihood \(p(\mathbf{y}|\theta)\), where \(\mathbf{y} = \left\{y_1, . . . ,y_n\right\}\) is a sample of observations from the population. The frequentist approach looks at the distribution of the data given \(\theta\) to estimate \(\theta\) by using, for example, the maximum likelihood approach which we covered in Session 6.

\sphinxAtStartPar
In the Bayesian paradigm, we no longer assume that the parameters have a fixed true value, but consider \(\theta\) to be a random quantity with an unknown distribution, which we wish to estimate. This distribution is denoted by \(p(\theta|y)\), and so we look at the distribution of the parameter, having seen data \(y\). To achieve this, we will have to specify a prior probability distribution, denoted \(p(\theta)\), which represents our initial beliefs about the distribution of \(\theta\)
prior to observing any data. In some situations, when we are trying to estimate a parameter \(\theta\) we have some knowledge, about the possible value of \(\theta\) before we take into account the data that we observe.

\sphinxAtStartPar
For example, consider the way a physician makes diagnostic decisions. A patient presents with a set of symptoms, concerned that they might have a certain disease. The physician assesses the probability that this patient has this disease, based on symptoms, family history, alternative explanations of symptoms and prevalence of the disease (their prior view that the patient has the disease). The physician might send the patient for a diagnostic test (collects some data) if her prior assessment of risk is above some threshold. Then the physician re\sphinxhyphen{}assesses the chance that the patient has this disease, taking account of the results and reliability of the diagnostic test (updates their prior in light of the data to get a posterior view on whether the patient has the disease). Depending on their certainty, the physician may then send the patient for further diagnostic tests. This thought process can be represented by the figure below and is analogous to Bayesian thinking.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{Physician}.png}
\end{figure}

\sphinxAtStartPar
In this example, the physician is assessing the probability that the patient has the disease. It is the physician’s prior probability based on their own training, knowledge and experience; a colleague may have a different prior probability. Here, prior probability is being defined subjectively. The size of the probability represents the physician’s degree of belief about the occurrence of an event, i.e. their own personal assessment of how likely an event is, based on the evidence available to them before the test results are given. This definition corresponds more closely to the everyday, intuitive  usage of probability than a frequentist interpretation (where the probability of a particular event occurring can be interpreted as the proportion of times the event would/does occur in a large number of similar trials or situations). The prior probability of the event might come from direct data, known prevalance of disease in a population, or data from related populations. If such prior information does not exist, then it can be formally elicited from experts, but we would want to acknowledge the uncertainty in the experts’ knowledge.


\section{9.2 Bayes Theorem (recap)}
\label{\detokenize{09.c. Bayesian Statistics I:bayes-theorem-recap}}\label{\detokenize{09.c. Bayesian Statistics I::doc}}
\sphinxAtStartPar
Let’s remind ourselves of Bayes theorem for discrete events, which we met in Session 2 (probability):

\sphinxAtStartPar
If \(A\) and \(B\) are events, then
\begin{equation*}
\begin{split}
P(A|B) = \frac{ P(B|A) P(A) } {P(B)} \propto P(B|A) P(A),
\end{split}
\end{equation*}
\sphinxAtStartPar
or in words:
\begin{equation*}
\begin{split}
\mbox{posterior probability of A given B} \propto \mbox{the likelihood of B given A} \times \mbox{the prior probability of A}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Also, if \(A_i\) is a set of mutually exclusive and exhaustive events, i.e. \( p( \bigcup\limits_i A_i ) = \sum\limits_i p(A_i) = 1\) and \(A_i \cap A_j = \emptyset\) for \(i \neq j\), then
\begin{equation*}
\begin{split}
p(A_i|B) = \frac{ p(B|A_i) p(A_i) } {\sum\limits_j p(B|A_j) p(A_j) }.
\end{split}
\end{equation*}
\sphinxAtStartPar
The calculation of the denominator is more difficult if we have continuous parameters as it requires integration over A; we will discuss this in the next section.

\sphinxAtStartPar
We will illustrate Bayes Theorem further with the diagnostic test example for Covid\sphinxhyphen{}19 below. We see Bayesian reasoning is purely probabilistic. Bayes theorem gives us a principled way to update prior probabilities on the basis of new data.


\subsection{9.2.1 Example}
\label{\detokenize{09.c. Bayesian Statistics I:example}}
\sphinxAtStartPar
\sphinxhref{https://www.bmj.com/content/bmj/369/bmj.m1808.full.pdf}{Watson (2020)} discusses some interesting issues around the interpretation of Covid\sphinxhyphen{}19 diagnostic tests. Typically, a clinician estimates a pre\sphinxhyphen{}test probability (a prior probability) of having Covid\sphinxhyphen{}19 for a particular area, which is derived from knowledge about local rates of Covid\sphinxhyphen{}19. Then, given a patient’s test result, the post\sphinxhyphen{}test probability (the posterior probability) of having Covid\sphinxhyphen{}19 is obtained. The posterior probability depends on the pre\sphinxhyphen{}test probability, as well as the sensitivity and specificity of the test, which are difficult to estimate; often, sensitivity is over\sphinxhyphen{}estimated. The article discusses how one can be fairly confident about a positive test result, but more caution is needed for a negative test result, as there may still be quite a high chance that a person has Covid\sphinxhyphen{}19. We illustrate this with Bayes’ theorem.

\sphinxAtStartPar
Suppose that, in a student hall of residence, the prevalence of Covid\sphinxhyphen{}19 if you have a persistent cough is \(75\%\). Suppose we assume that the test will be positive in Covid\sphinxhyphen{}19 patients \(70\%\) of the time (sensitivity is 0.7), and it will be negative in non\sphinxhyphen{}Covid\sphinxhyphen{}19 patients \(95\%\) of the time (specificity is 0.95). Given that a student in this hall with a persistent cough tests negative, what is the probability that they have Covid\sphinxhyphen{}19? In other words, what is the probability of a false negative?

\sphinxAtStartPar
Let us denote by \(C+\) the event that a person has Covid\sphinxhyphen{}19, and \(C-\) the event that a person does not have Covid\sphinxhyphen{}19.  Further we denote by \(T+\) and \(T-\) the events that a person has a positive and a negative test, respectively. The information we are given is that:
\begin{equation*}
\begin{split}
p(C+)=0.75, \qquad p(T+|C+)=0.70, \qquad p(T-|C-)=0.95
\end{split}
\end{equation*}
\sphinxAtStartPar
Now, what we want is:
\begin{equation*}
\begin{split}
\begin{align}
p(\mbox{false negative}) &= p(C+|T-) = \frac{p(T-|C+)p(C+)}{p(T-)} \\
&= \frac{p(T-|C+)p(C+)}{p(T-|C+)p(C+) + p(T-|C-)p(C-)} \\
&= \frac{(1-0.7) \times 0.75}{(1-0.7) \times 0.75 + 0.95 \times 0.25} \\
&= \frac{0.225}{0.4625} \\
&= 0.4864
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
You can see that, despite the negative test result, due to the very high prevalence of Covid\sphinxhyphen{}19 in the hall of residence and the relatively low sensitivity rate, there is still a 48.64\% chance that a person has Covid\sphinxhyphen{}19.

\sphinxAtStartPar
Suppose a different student has no symptoms. The prevalence of Covid\sphinxhyphen{}19 in asymptomatic people is 0.1. They use the same diagnostic test and the test result is positive. What is the probability that this student with a positive test result has Covid\sphinxhyphen{}19? In other words, what is \(p(C+|T+)\)?

\sphinxAtStartPar
Solution:
\begin{equation*}
\begin{split}
\begin{align}
p(C+|T+) &= \frac{p(T+|C+)p(C+)}{p(T+)} \\ & = \frac{p(T+|C+)p(C+)}{p(T+|C+)p(H+) + p(T+|C-)p(C-)}  \\
&= \frac{0.7 \times 0.1}{0.7 \times 0.1 + (1-0.95) \times 0.9} \\
&= \frac{0.07}{0.115} \\
&= 0.609
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
This means that, amongst all the people who test positive, \(60.9\%\) will actually have the disease. After a positive result from a test, the probability that you have Covid\sphinxhyphen{}19 increase from \(10\%\) to \(61\%\).

\sphinxAtStartPar
Note that these results are specific to the the prevalence of Covid\sphinxhyphen{}19 in the area, as well as the sensitivity and specificity of the diagnostic test. The code below reproduces the leaf\sphinxhyphen{}plot from \sphinxhref{https://www.bmj.com/content/bmj/369/bmj.m1808.full.pdf}{Watson (2020)}. The \(x\)\sphinxhyphen{}axis is the pre\sphinxhyphen{}test probability of having Covid\sphinxhyphen{}19. The corresponding \(y\)\sphinxhyphen{}values on the lower curve (lower leaf) are the post\sphinxhyphen{}test probabilities of having Covid\sphinxhyphen{}19, following a negative test result. The corresponding \(y\)\sphinxhyphen{}values on the upper curve (upper leaf) are the post\sphinxhyphen{}test probabilities of having Covid\sphinxhyphen{}19, following a positive test result. The correponding values on the diagonal (\(y=x\)) line represent probabilities if no test is carried out.

\sphinxAtStartPar
In our first example, the prevalence in symptomatic people is 0.75, so we follow the orange arrows to find that the post\sphinxhyphen{}test probability after a negative result 0.4864. In the second example, the prevalence in asymptomatic people is 0.1. We follow the purple arrows to find that the post\sphinxhyphen{}test probability after a positive result is 0.609. How do you think the shape of the lower and upper leaves would change, if sensitivity was higher? If specificity was lower? Re\sphinxhyphen{}run the code with different values to check.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Function takes as arguments the sensitivitiy of the test (sensi) }
\PYG{c+c1}{\PYGZsh{} and the specificity (speci)}

\PYG{n}{leafplot} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{function}\PYG{p}{(}\PYG{n}{sensi}\PYG{p}{,} \PYG{n}{speci}\PYG{p}{)}\PYG{p}{\PYGZob{}}
  
  \PYG{n}{pretest} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{l+m}{0.01}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}possible pre\PYGZhy{}test probabilities }
  
  \PYG{c+c1}{\PYGZsh{}probability of having Covid\PYGZhy{}19 after a positive test result }
  \PYG{n}{pos.test} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{sensi}\PYG{o}{*}\PYG{n}{pretest}\PYG{o}{/}\PYG{p}{(}\PYG{n}{sensi}\PYG{o}{*}\PYG{n}{pretest}\PYG{o}{+}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{speci}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{pretest}\PYG{p}{)}\PYG{p}{)}
  
  \PYG{c+c1}{\PYGZsh{}probability of having Covid\PYGZhy{}19 after a negative test result }
  \PYG{n}{neg.test} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{p}{(}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sensi}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sensi}\PYG{p}{)}\PYG{o}{*}\PYG{n}{pretest}\PYG{o}{+}\PYG{n}{speci}\PYG{o}{*}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{pretest}\PYG{p}{)}\PYG{p}{)}
  
  \PYG{c+c1}{\PYGZsh{}plot leaves}
  \PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{,} \PYG{n}{pos.test}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{darkgreen\PYGZdq{}}\PYG{p}{,} 
     \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Pre\PYGZhy{}test Probability\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Post\PYGZhy{}test Probability\PYGZdq{}}\PYG{p}{)}
  \PYG{n+nf}{points}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{,} \PYG{n}{neg.test}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{darkgreen\PYGZdq{}}\PYG{p}{)}
  \PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{l+m}{0}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{darkgreen\PYGZdq{}}\PYG{p}{)}
  \PYG{n+nf}{legend}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{topleft\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Positive Test\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Negative Test\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{col}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Purple\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Orange\PYGZdq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{,} \PYG{n}{bg}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{transparent\PYGZdq{}}\PYG{p}{)}
  
  \PYG{c+c1}{\PYGZsh{}plot arrows }
    \PYG{c+c1}{\PYGZsh{}we use pretest[11] to get the prevalence value of 0.1, and }
    \PYG{c+c1}{\PYGZsh{}pretest[76] to get the prevalence value of 0.75 in the vector \PYGZdq{}pretest\PYGZdq{}}
    
  \PYG{n+nf}{arrows}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{[}\PYG{l+m}{11}\PYG{p}{]}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{pretest}\PYG{p}{[}\PYG{l+m}{11}\PYG{p}{]}\PYG{p}{,} \PYG{n}{pos.test}\PYG{p}{[}\PYG{l+m}{11}\PYG{p}{]}\PYG{p}{,} \PYG{n}{angle}\PYG{o}{=}\PYG{l+m}{15}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{purple\PYGZdq{}}\PYG{p}{)}
  \PYG{n+nf}{arrows}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{[}\PYG{l+m}{11}\PYG{p}{]}\PYG{p}{,} \PYG{n}{pos.test}\PYG{p}{[}\PYG{l+m}{11}\PYG{p}{]}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{pos.test}\PYG{p}{[}\PYG{l+m}{11}\PYG{p}{]}\PYG{p}{,} \PYG{n}{angle}\PYG{o}{=}\PYG{l+m}{15}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{purple\PYGZdq{}}\PYG{p}{)}
  \PYG{n+nf}{arrows}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{[}\PYG{l+m}{76}\PYG{p}{]}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{pretest}\PYG{p}{[}\PYG{l+m}{76}\PYG{p}{]}\PYG{p}{,} \PYG{n}{neg.test}\PYG{p}{[}\PYG{l+m}{76}\PYG{p}{]}\PYG{p}{,} \PYG{n}{angle}\PYG{o}{=}\PYG{l+m}{15}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{orange\PYGZdq{}}\PYG{p}{)}
  \PYG{n+nf}{arrows}\PYG{p}{(}\PYG{n}{pretest}\PYG{p}{[}\PYG{l+m}{76}\PYG{p}{]}\PYG{p}{,} \PYG{n}{neg.test}\PYG{p}{[}\PYG{l+m}{76}\PYG{p}{]}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{neg.test}\PYG{p}{[}\PYG{l+m}{76}\PYG{p}{]}\PYG{p}{,} \PYG{n}{angle}\PYG{o}{=}\PYG{l+m}{15}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{orange\PYGZdq{}}\PYG{p}{)}
  
  \PYG{p}{\PYGZcb{}}

\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{6.5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5.5}\PYG{p}{)}
\PYG{n+nf}{leafplot}\PYG{p}{(}\PYG{n}{sensi}\PYG{o}{=}\PYG{l+m}{0.7}\PYG{p}{,} \PYG{n}{speci}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{}See what happens to the plot when you change sensitivity and specificity! }
\PYG{c+c1}{\PYGZsh{}leafplot(0.95, 0.8)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{09.c. Bayesian Statistics I_3_0}.png}


\section{9.3 The Bayesian paradigm in Health data science problems.}
\label{\detokenize{09.d. Bayesian Statistics I:the-bayesian-paradigm-in-health-data-science-problems}}\label{\detokenize{09.d. Bayesian Statistics I::doc}}
\sphinxAtStartPar
In this section we discuss the Bayesian approach in Health data science problems. Some features of the Bayesian paradigm are particularly useful in this context:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Bayes theorem provides a statistically principled method for combining data. Thus, we can take into account the context within which the data are generated. For example, results of a diagnostic test may have a different interpretation/consequence if used in a symptomatic patient than in a general screening programme. The prior probability of disease would be higher in the former than the latter. Priors can then be updated by the test result to give an assessment of disease risk specific to the local prevalence.

\item {} 
\sphinxAtStartPar
For problems where there are multiple or diverse sources of data which must be combined, the Bayesian framework provides a natural environment for doing so. Examples where Bayesian synthesis of information is common are:•	models of biological systems, for example genetic and genomic pathways,•	models of the natural history of diseases over time and relationships with clinical events,•	economic models of disease trajectories and cost\sphinxhyphen{}effect trade\sphinxhyphen{}offs for interventions that interrupt the trajectories,•	ecological studies of pollutant emissions and effects on population health,•	demographic studies, for example to study migration,•	speech recognition software,•	other pattern recognition models such as medical imaging or search engines,•	epidemic modelling.In all these examples complex data is synthesised and/or used to update outputs.

\item {} 
\sphinxAtStartPar
Bayesian models fit well into decision theory methodology, providing we can also specify consequences of model outputs.

\item {} 
\sphinxAtStartPar
In many examples, especially those that aim to model complicated processes, some of the data inputs are very sparse, or even non\sphinxhyphen{}existent. In such cases, prior data may be formally elicited from an expert panel and incorporated in a Bayesian analysis. Examples include multiple evidence synthesis and identification of latent groups.

\item {} 
\sphinxAtStartPar
Bayesians are allowed to make direct probability statements about unknown quantities. Frequentists cannot make these direct probability statements because the unknown model parameters are assumed fixed.

\item {} 
\sphinxAtStartPar
In recent years the resources available to complete Bayesian analysis have increased, including bespoke software and packages within commercial statistical software.

\end{enumerate}

\sphinxAtStartPar
But Bayesian methods are not that widely used in statistics compared with more classical approaches because they have some limitations.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Sometimes the need for a prior distribution is a barrier if little is known about a parameter and researchers fall back on priors that are weakly informative. In that case, it is not easy to see how much benefit comes from a Bayesian analysis.

\item {} 
\sphinxAtStartPar
Because of the need to use Bayesian updating via a prior distribution, the analysis almost always requires a parametric approach. This limits the structure of the analysis models. Although non\sphinxhyphen{}parametric Bayesian methods are available for some situations, they often have underlying parametric assumptions.

\item {} 
\sphinxAtStartPar
The numerical integration methods usually required for realistic problems are often computationally expensive. This is especially true if there are multiple sources of evidence to be combined.

\item {} 
\sphinxAtStartPar
Many statisticians are unfamiliar with the methods and associated software.

\end{enumerate}


\section{9.4 Bayes thorem for discrete and continous data}
\label{\detokenize{09.e. Bayesian Statistics I:bayes-thorem-for-discrete-and-continous-data}}\label{\detokenize{09.e. Bayesian Statistics I::doc}}
\sphinxAtStartPar
So far this session, we have looked at Bayes theorem in the discrete case. We turn to the more general case of Bayes thorem to make inference about an unknown parameter \(\theta\), which could be discrete or continuous.

\sphinxAtStartPar
The probability distribution for \(\theta\) reflects our uncertainty about it before seeing the data, \sphinxstylestrong{prior distribution}, \(p(\theta)\). Once the data data \(y\) is known, we condition on it. Using Bayes theorem we obtain a conditional probability distribution for unobserved quantities of interest given the data. If \(\theta\) is continuous, we have:
\begin{equation*}
\begin{split}
p(\theta \mid y)= \frac{ p(\theta)\, p(y \mid \theta)}{\int  p(\theta)\,p(y \mid \theta)\,d\theta},
\end{split}
\end{equation*}
\sphinxAtStartPar
and \(\theta\) is discrete and takes values in the set \(\Theta\), we have:
\begin{equation*}
\begin{split}
p(\theta \mid y)= \frac{ p(\theta)\, p(y \mid \theta)}{\sum_{\theta \in \Theta}  p(\theta) p(y \mid \theta) }.
\end{split}
\end{equation*}
\sphinxAtStartPar
We call \(p(\theta \mid y)\) the \sphinxstylestrong{posterior distribution}.

\sphinxAtStartPar
Note that the Bayesian approach is naturally synthetic in that it allows data from different sources to be combined, according to Bayes principles. This approach is most useful when there is informative prior information. We note that the Bayesian approach can be recursive, so \(p(\theta \mid y)\) may be used as a prior when calculating \(p(\theta \mid y, z)\) for a second data set \(z\).

\sphinxAtStartPar
The denominator, \({\int  p(\theta)\,p(y \mid \theta)\,d\theta}\) or \(\sum_{\theta \in \Theta}  p(\theta) p(y \mid \theta)\), is a constant with respect to \(\theta\). One of the challenges of using Bayesian approaches is that the integration can be analytically intractable, so that numerical methods are needed (for example, numerical integration or Markov Chain Monte Carlo methods). These methods are beyond the scope of the current module. In this introductory course, we will only look at examples where this constant need not be calculated, since the form of the posterior can be inferred by inspection once observing that the posterior is proportional to the product of the prior and likelihood:
\begin{equation*}
\begin{split}p(\theta \mid y) \propto p(\theta)\,p(y \mid \theta).\end{split}
\end{equation*}
\sphinxAtStartPar
We will see how this works for the inference of proportions.


\section{9.5 Bayesian inference on proportions}
\label{\detokenize{09.f. Bayesian Statistics I:bayesian-inference-on-proportions}}\label{\detokenize{09.f. Bayesian Statistics I::doc}}
\sphinxAtStartPar
Consider a new drug being developed for the relief of chronic pain. To find out about its efficacy, we propose to run a single\sphinxhyphen{}arm early\sphinxhyphen{}phase clinical trial in which we give this drug to a number \(n\) of randomly selected patients. Because patients are independent of each other, so it seems reasonable to model the data using the Binomial distribution, \(Y\sim Bin(n,\theta).\) We have that \(\theta\in [0,1]\) is the probability of pain relief (success) in each patient, and this is unknown. We then make the observation that there are \(y\) successes out of \(n\) independent trials. As a reminder, the probability distribution function of the Binomial distribution is:
\begin{equation*}
\begin{split}
p \left(y \mid \theta \right) = {n \choose y} \theta^y (1-\theta)^{n-y}.
\end{split}
\end{equation*}
\sphinxAtStartPar
To proceed, we need to have a prior distribution for \(\theta\). Let us consider three possible prior distributions:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
An uninformative prior, where all values of \(\theta\) are equally probable.You essentially have no prior information about the effectiveness of the drug.

\item {} 
\sphinxAtStartPar
A symmetrical, concave prior that is centered at 0.5.You think that the drug is likely to be effective for patients around half of the time.

\item {} 
\sphinxAtStartPar
An asymmetrical prior with a spike at 0.1.You think that the drug is generally ineffective, and feel quite strongly about it.

\end{enumerate}

\sphinxAtStartPar
Now, the Beta distribution is a flexible distribution that can represent each of these prior beliefs by appropriate choice of its parameters. It is also convenient because it has a similar form to the Binomial distribution.


\subsection{9.5.1 The Beta prior}
\label{\detokenize{09.f. Bayesian Statistics I:the-beta-prior}}
\sphinxAtStartPar
The Beta distribution is a flexible two parameter distribution that is restricted to the interval between 0 and 1, and so it is a reasonable form for a probability distribution for a proportion. The two parameters, \(a\) and \(b\), are often called  “shape” parameters. Given \(\theta \sim \hbox{Beta}(a,b)\), the probability density function, expectation and variance of the distribution are as follows:
\begin{equation*}
\begin{split}
\begin{align}
p(\theta|a, b)  &=  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \,\,
\theta^{a-1} \; (1-\theta)^{b-1}  \mbox{  where  } \theta \in (0,1) \\
{E}( \theta |a, b) &=    \frac{a}{a+ b }     \\
{Var}( \theta |a, b) &=    \frac{a b}{ (a+ b)^2 (a+ b+1)} 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
The \sphinxstyleemphasis{Gamma function} \(\Gamma(x)\) is defined for positive integers as \(\Gamma (x)=(x-1)!\), and has a more complex form for real numbers.

\sphinxAtStartPar
This prior distribution is very flexible. For example:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(a=1, b=1\) results in the uniform distribution

\item {} 
\sphinxAtStartPar
\(a=2, b=2\) results in a symmetrical distribution centered on \(p=0.5\)

\item {} 
\sphinxAtStartPar
\(a=2, b=9\) results in an asymmetrical distribution with a spike at \(p=0.1\).

\end{enumerate}

\sphinxAtStartPar
These are the priors we specified earlier; they are plotted below. Note that the higher the values of \(a, b,\) the
smaller the variance of the distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{7}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}
\PYG{n}{theta} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{l+m}{0.01}\PYG{p}{)}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(1,1) Distribution\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(2,2) Distribution\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{9}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(2,9) Distribution\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{09.f. Bayesian Statistics I_1_0}.png}


\subsection{9.5.2 Posterior}
\label{\detokenize{09.f. Bayesian Statistics I:posterior}}
\sphinxAtStartPar
Now, we apply Bayes theorem to obtain the posterior distribution using a \(Beta(a,b)\) distribution for the prior:
\begin{equation*}
\begin{split}
\begin{align}
p(\theta \mid y) &= \frac{ p(\theta)\, p(y \mid \theta)} {\int  p(\theta)\,p(y \mid \theta)\,d\theta}\\
         &\propto  p(\theta)\, p(y \mid \theta)
         \end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Substituting in the appropriate distributions gives
\begin{equation*}
\begin{split}
\begin{align} 
p(\theta \mid y) &= \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1}  {n \choose y} \theta^y (1-\theta)^{n-y} \\
         &\propto \theta^{a-1} (1-\theta)^{b-1} \theta^y (1-\theta)^{n-y} \\
         &\propto \theta^{a+y-1} (1-\theta)^{b+n-y-1} 
         \end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Now by inspection, we can see that this is in the form of a Beta distribution: we have that the posterior is proportional to \(\theta^{a+y-1} (1-\theta)^{b+n-y-1}\). In other words, the posterior is \(Beta(a+y, b+n-y).\) This distribution has mean given by: \(\frac{a+y}{a+b+n}\) and variance \(\frac{(a+y)(b+n-y)}{(a+b+n)^2(a+b+n+1)}.\)

\sphinxAtStartPar
Suppose the data we observe is \(y=4\) successes out of a total of \(10\) patients. Then:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
With the uniform \(Beta(1,1)\) prior, our posterior is \(Beta(5, 7)\).

\item {} 
\sphinxAtStartPar
With the symmetrical \(Beta(2, 2)\) prior, our posterior is \(Beta(6, 8)\).

\item {} 
\sphinxAtStartPar
With the asymmetrical \(Beta(2, 9)\) prior, our posterior is \(Beta(6, 15)\).

\end{enumerate}

\sphinxAtStartPar
We plot the possible distibutions below:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{l+m}{0.01}\PYG{p}{)}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{l+m}{5}\PYG{p}{,} \PYG{l+m}{7}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,}  \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(5, 7) Distribution\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(6, 8) Distribution\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(6, 15) Distribution\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{09.f. Bayesian Statistics I_3_0}.png}

\sphinxAtStartPar
We can see that the uninformative prior leads to the posterior with the highest variance amongst the three. The narrow prior in the third example shifts the posterior distribution to the right. We can see that different choices of prior lead to different results. For this reason, it is often recommended to repeat analyses with different priors to see how much the results change: this is called \sphinxstyleemphasis{sensitivity analysis}.


\section{9.6 Summarising Posteriors}
\label{\detokenize{09.g. Bayesian Statistics I:summarising-posteriors}}\label{\detokenize{09.g. Bayesian Statistics I::doc}}
\sphinxAtStartPar
We often display the posterior distribution graphically to get a sense of the information that we have about the parameter. However, other ways to summarize the distribution can be helpful. We may also wish to summarise the posterior distribution by a credible interval.

\sphinxAtStartPar
Remember that a classical 95\%  confidence interval is defined such that, if the data collection process is repeated again and again, then in the long run, 95\% of the confidence intervals formed would contain the true parameter value.

\sphinxAtStartPar
A Bayesian 95\% \sphinxstylestrong{credible interval} is an interval which contains 95\% of the posterior distribution of the parameter.

\sphinxAtStartPar
There may be several different credible intervals such that the interval contains 95\% of the distribution. The 95\%  \sphinxstylestrong{Highest Posterior Density (HPD)} interval is the credible interval with the smallest range of values for \(\theta\) (providing the posterior is concave). Algebraically, this is the region \([\theta_L, \theta_U]\) that contains \(95\%\) of the probability, such that:
\begin{equation*}
\begin{split}
P(\theta \in [\theta_L,\theta_U])= 0.95 \mbox{ such that for all } \theta_O \notin  [\theta_L,\theta_U] \mbox{ and all  } \theta_I\in[\theta_L,\theta_U], p(\theta_O|y) < p(\theta_I|y).
\end{split}
\end{equation*}
\sphinxAtStartPar
In our previous example, when we used the asymmetrical \(Beta(2, 9)\) prior, our posterior was \(Beta(6, 15)\). The posterior mean is  \(\frac{6}{6+15}=0.286\). The 95\% HPDI is (0.107,0.475). We plot the distribution below and check that the area between these two values gives us 0.95. Now, note that the interval (0.09, 0.465) also gives us an area of 0.95, but this interval is wider. In a sense, the HPDI is the “tightest” interval so that the area under the posterior distribution is 0.95.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{,} \PYG{l+m}{0.01}\PYG{p}{)}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{7}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{n+nf}{dbeta}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Beta(6, 15) Distribution  \PYGZbs{}n with 95\PYGZpc{} credible interval\PYGZdq{}}\PYG{p}{,}  \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{density\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{0.475}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{dashed\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{v}\PYG{o}{=}\PYG{l+m}{0.107}\PYG{p}{,}  \PYG{n}{lty}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{dashed\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Area under the 95\PYGZpc{} HDPI}
\PYG{n+nf}{pbeta}\PYG{p}{(}\PYG{l+m}{0.475}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{15}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{pbeta}\PYG{p}{(}\PYG{l+m}{0.107}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{15}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}The interval (0.09, 0.465) also a 95\PYGZpc{} credible interval }
\PYG{n+nf}{pbeta}\PYG{p}{(}\PYG{l+m}{0.465}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{15}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{pbeta}\PYG{p}{(}\PYG{l+m}{0.09}\PYG{p}{,} \PYG{l+m}{6}\PYG{p}{,} \PYG{l+m}{15}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.949975144544822\end{split}
\end{equation*}\begin{equation*}
\begin{split}0.951266598161814\end{split}
\end{equation*}
\noindent\sphinxincludegraphics{{09.g. Bayesian Statistics I_1_2}.png}
\begin{quote}

\sphinxAtStartPar
Note:   We have phrased the above discussion in terms of 95\% confidence and credible intervals. However, there is nothing special about the level 95\%. We can make the discussion more general by talking about \(100(1−𝛼)\%\)  confidence or credible intervals instead, with \(\alpha \in (0,1)\) (where \(\alpha = 0.05\) for 95\% confidence or credible intervals but e.g. \(\alpha = 0.01\) for 99\% intervals).
\end{quote}


\section{9.7 Prior Predictions}
\label{\detokenize{09.h. Bayesian Statistics I:prior-predictions}}\label{\detokenize{09.h. Bayesian Statistics I::doc}}
\sphinxAtStartPar
Before observing a quantity \(y\), we can provide its predictive distribution by integrating out the unknown parameter,
\begin{equation*}
\begin{split}
p(y) = \int p(y|\theta) p(\theta) d\theta.
\end{split}
\end{equation*}
\sphinxAtStartPar
Predictions are useful in many settings, for example forecasting, cost\sphinxhyphen{}effectiveness models and design of
studies. In the trial described earlier in this section, we had 10 patients. Suppose we are interested in predicting the number of patients who will have a positive response. Recall that the Beta distribution is a suitable prior distribution for \(\theta\), the proportion of positive responses. We have:
\begin{equation*}
\begin{split}
\begin{align*}
\theta &\sim \hbox{Beta}(a,b) \\
	y &\sim \hbox{Binomial}(\theta,n)
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
The exact predictive distribution \(p(y)\) can be computed analytically and is known as the \sphinxstyleemphasis{Beta\sphinxhyphen{}Binomial} distribution. It has the complex form with three parameters,  number of trials \(n\) and shape parameters, \(a\) and \(b\):
\begin{equation*}
\begin{split}
\begin{align*}
p(y) &=  \frac{ \Gamma (a+ b)}{ \Gamma (a) \Gamma (b) }  {n \choose y}  \frac{\Gamma (a+ y) \Gamma (b+n-y)}{\Gamma (a+b+n)} \\
E(y) &=  n \frac{a}{a+b}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Given that we use the asymmetrical \(Beta(2, 9)\) prior, our predictive distribution would be:
\begin{equation*}
\begin{split}
p(y) =  \frac{ \Gamma (11)}{ \Gamma (2) \Gamma (9) }  {10 \choose y}  \frac{\Gamma (2+ y) \Gamma (19-y)}{\Gamma (21)},
\end{split}
\end{equation*}
\sphinxAtStartPar
with \(E(y) =  10 \, \frac{2}{11} = 1.81\). So, before observing any data, we would predict around 2 patients to have a positive response out of 10.


\subsection{9.7.1 Posterior Prediction}
\label{\detokenize{09.h. Bayesian Statistics I:posterior-prediction}}
\sphinxAtStartPar
Suppose that have observed \(y\), and we want to predict future observations \(z\), assuming that \(z\) and \(y\) are independent, conditional on \(\theta\). The posterior predictive distribution for \(z\) is given by,
\begin{equation*}
\begin{split}
\begin{align*}
p(z|y) &= \int p(z, \theta | y) d \theta \\
     &= \int p(z |y, \theta) p(\theta |y ) d \theta \\ 
      &= \int p(z | \theta) p(\theta |y ) d \theta 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
We are now weighting the probability distribution function for \(z\) with our posterior belief after having observed \(y\).

\sphinxAtStartPar
For our example, we found that the posterior distribution \(p(\theta |y ) \) is a Beta(\(a+y, b+n-y\)) distribution. Thus our posterior predictive distribution is a Beta\sphinxhyphen{}binomial distribution with the number of trials \(n_p\) and shape parameters \(a+y, b+n-y\).

\sphinxAtStartPar
Now, given that we use the asymmetrical \(Beta(2, 9)\) prior, and then observe that \(y=4\) patients out of \(n=10\) had a successful result, and we wish to predict how many sucesses \(z\) out of \(n_p=20\) to expect, our posterior predictive distribution is a Beta\sphinxhyphen{}binomial with parameters \(20\) and shape parameters \(6\) and \(15\). The expectation of this distribution is \(E(y) =  20 \frac{6}{21} \approx 6\) patients.


\section{9.8 Conjugacy}
\label{\detokenize{09.i. Bayesian Statistics I:conjugacy}}\label{\detokenize{09.i. Bayesian Statistics I::doc}}
\sphinxAtStartPar
In the example with the Beta\sphinxhyphen{}Binomial model, we found that using the Beta distribution for the prior lead us to a posterior distribution that is also a Beta distribution. This is not a coincidence. Often, a particular distributional family is chosen for the prior, so that the resulting posterior distribution belongs to the same family. This is called a conjugate prior. Below are the conjugate priors for some common likelihood models.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Likelihood
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Conjugate Prior
\\
\hline
\sphinxAtStartPar
Bernoulli
&
\sphinxAtStartPar
Beta
\\
\hline
\sphinxAtStartPar
Binomial
&
\sphinxAtStartPar
Beta
\\
\hline
\sphinxAtStartPar
Poisson
&
\sphinxAtStartPar
Gamma
\\
\hline
\sphinxAtStartPar
Geometric
&
\sphinxAtStartPar
Beta
\\
\hline
\sphinxAtStartPar
Normal
&
\sphinxAtStartPar
Normal, Gamma and a few others
\\
\hline
\sphinxAtStartPar
Exponential
&
\sphinxAtStartPar
Gamma
\\
\hline
\sphinxAtStartPar
Gamma
&
\sphinxAtStartPar
Gamma
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{9.8.1 Exercise}
\label{\detokenize{09.i. Bayesian Statistics I:exercise}}
\sphinxAtStartPar
Suppose that there is an experiment where \(n\) patients are asked to try different treatments each time they get a headache. We are interested in the number of different treatments a patient takes before they find one that is successful. For patient \(i\), for \(1 \leq i \leq n\), we denote by \(y_i\) the number of treatments tried before the first success. Note that \(\left\{ y_1, y_2, ..., y_n \right\}\) are a sample from a Geometric distribution: \(y_i \sim Geom(\theta)\). The probability density function of a geometric distribution is:
\begin{equation*}
\begin{split}p(y | \theta) = \theta (\theta -1)^{y-1}\end{split}
\end{equation*}
\sphinxAtStartPar
Suppose we wish to make inference on \(\theta\). By specifying a Beta prior for \(\theta\): \(\theta \sim Beta(a, b)\), derive the posterior distribution of \(\theta\).

\sphinxAtStartPar
Try the exercise and then click the button to reveal the solution.

\sphinxAtStartPar
Solution:
\begin{equation*}
\begin{split}
\begin{align*}
p(\theta \mid y_1, ..., y_n) 
         &\propto  p(\theta) \prod_{i=1}^n p(y_i \mid \theta)\\
                &\propto  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1} \prod_{i=1}^n \theta (\theta -1)^{y-1}\\
        &\propto  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1}  \theta^n (\theta -1)^{\sum_{i=1}^n y_i-n}\\
        &\propto  \theta^{a+n-1} (\theta -1)^{\sum_{i=1}^n y_i -n +b-1}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
This is a Beta distribution with parameters \(a+n\) and \(\sum_{i=1}^n y_i-n+b\).


\chapter{10. Bayesian Statistics II: Normal data}
\label{\detokenize{10.a. Bayesian Statistics II:bayesian-statistics-ii-normal-data}}\label{\detokenize{10.a. Bayesian Statistics II::doc}}
\sphinxAtStartPar
In the previous session, we looked at Bayesian inference for proportions. We now consider continuous data and explore Bayesian inference for data when they are assumed to follow a Normal distribution.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Find the posterior for a Normally distributed mean when the variance of the data is known

\item {} 
\sphinxAtStartPar
Find credible and HPD intervals for a Normally distributed mean

\item {} 
\sphinxAtStartPar
Find the Bayesian predictive distributions for Normal data and data summaries.

\end{itemize}



\sphinxAtStartPar
The next sessions calculate the posterior for the mean of a Normal distribution, obtain HPD credible intervals and use the posterior to make predictions.


\section{10.1 Example: CD4 cell counts}
\label{\detokenize{10.b. Bayesian Statistics II:example-cd4-cell-counts}}\label{\detokenize{10.b. Bayesian Statistics II::doc}}
\sphinxAtStartPar
In this session, we will use a dataset on CD4 cell counts which is available in R through the \sphinxstyleemphasis{boot} package. CD4 cells are in our blood as part of our immune system. Since these cells die in people who have HIV, CD4 cell counts are used in HIV patients to determine the health of their immune system and susceptibility to opportunistic infections.

\sphinxAtStartPar
In this dataset, there are 20 patients with HIV. Their CD4 cell counts are recorded before and after they were put on treatment. We wish to investigate whether this treatment increased their CD4 cell counts.

\sphinxAtStartPar
We install the \sphinxstyleemphasis{boot} package where the data is stored and we look at the data. Note that the unit of CD4 cell count is 100 \(cells/mm^3\). We are interested in the difference in CD4 cell counts before and after treatment. We look at the summary statistics of the difference.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{library}\PYG{p}{(}\PYG{n}{boot}\PYG{p}{)}
\PYG{n}{ydata} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n}{cd4}\PYG{o}{\PYGZdl{}}\PYG{n}{oneyear} \PYG{o}{\PYGZhy{}} \PYG{n}{cd4}\PYG{o}{\PYGZdl{}}\PYG{n}{baseline}
\PYG{n}{data} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{cbind}\PYG{p}{(}\PYG{n}{cd4}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{ydata}\PYG{p}{)}
\PYG{n}{data}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{ydata}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A data.frame: 20 × 3
\begin{tabular}{r|lll}
  & baseline & oneyear & y\\
  & <dbl> & <dbl> & <dbl>\\
\hline
	1 & 2.12 & 2.47 &  0.35\\
	2 & 4.35 & 4.61 &  0.26\\
	3 & 3.39 & 5.26 &  1.87\\
	4 & 2.51 & 3.02 &  0.51\\
	5 & 4.04 & 6.36 &  2.32\\
	6 & 5.10 & 5.93 &  0.83\\
	7 & 3.77 & 3.93 &  0.16\\
	8 & 3.35 & 4.09 &  0.74\\
	9 & 4.10 & 4.88 &  0.78\\
	10 & 3.35 & 3.81 &  0.46\\
	11 & 4.15 & 4.74 &  0.59\\
	12 & 3.56 & 3.29 & -0.27\\
	13 & 3.39 & 5.55 &  2.16\\
	14 & 1.88 & 2.82 &  0.94\\
	15 & 2.56 & 4.23 &  1.67\\
	16 & 2.96 & 3.23 &  0.27\\
	17 & 2.49 & 2.56 &  0.07\\
	18 & 3.03 & 4.31 &  1.28\\
	19 & 2.66 & 4.37 &  1.71\\
	20 & 3.00 & 2.40 & -0.60\\
\end{tabular}\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
\PYGZhy{}0.6000  0.2675  0.6650  0.8050  1.3775  2.3200 
\end{sphinxVerbatim}

\sphinxAtStartPar
In the classical framework, we could use a paired t\sphinxhyphen{}test to see if the mean change in CD4 cell counts is significantly different from the null hypothesis value of zero (\(H_0: \mu = E[Y]=0)\).

\sphinxAtStartPar
For our Bayesian analysis, we will assume these measurements come from a Normal distribution with an unknown mean \(\mu\),
which represents the mean change in CD4 counts. We will assume that the variance is known to be \(\sigma^2 = 0.7\). This is slightly artificial as, in a real example, we may not know what the true variance is; however, we might be able to infer the variability of CD4 counts from earlier studies. Having both \(\mu\) and \(\sigma^2\) unknown requires a more complicated analysis which we will not cover in this course.

\sphinxAtStartPar
The Bayesian analysis involves constructing a likelihood for the data, specifying an appropriate prior distribution and combining them to obtain a posterior distribution. We will then describe how credible intervals for \(\mu\), and prior and posterior predictive distributions can be found.


\section{10.2 Calculating the posterior for the mean of a Normal distribution}
\label{\detokenize{10.c. Bayesian Statistics II:calculating-the-posterior-for-the-mean-of-a-normal-distribution}}\label{\detokenize{10.c. Bayesian Statistics II::doc}}
\sphinxAtStartPar
In this section, we obtain the posterior for the mean of a Normal distribution with known variance, \(\sigma^2\).

\sphinxAtStartPar
Suppose we have \(n\) observed independent data points, each assumed to come from the Normal distribution: \(y_1,\dots,y_n \sim N(\mu,\sigma^2)\). Recall that the Normal distribution has probability density function given by
\begin{equation*}
\begin{split}
p(y \mid \mu, \sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{1/2} 
\exp\left\{-\frac{1}{2\sigma^2}(y-\mu)^2\right\}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Note that some authors will parameterize the Normal distribution with the \sphinxstyleemphasis{precision} instead of the variance: \(\eta=\frac{1}{\sigma^2}\).


\subsection{10.2.1 Likelihood}
\label{\detokenize{10.c. Bayesian Statistics II:likelihood}}
\sphinxAtStartPar
For convenience, we will drop the conditioning on \(\sigma^2\), since we are assuming this is a known number. Since we assume all observations are independent, the likelihood is the product of the \(n\) individual p.d.f.s:
\begin{equation*}
\begin{split}
\begin{align*}
p(y_1,\dots,y_n \mid \mu) 
&= p(y_1 \mid \mu) p(y_2 \mid \mu) \dots p(y_n \mid \mu) \\
&= \prod_{i=1}^n p(y_i \mid \mu) \\
&=
\left( \frac{1}{2\pi \sigma^2}\right)^{n/2} \exp\left\{
-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2 \right\}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Notice that
\begin{equation*}
\begin{split}
\begin{align*}
\sum_{i=1}^n (y_i - \mu)^2  &= \sum_{i=1}^n (y_i - \bar
y + \bar y  -
\mu)^2 \\
& = \sum_{i=1}^n (y_i-\bar y)^2 + n(\bar y - \mu)^2, \mbox{ since }
\sum_{i=1}^n (y_i-\bar y)=0,\\
&= (n-1)s^2 + n(\bar y - \mu)^2,
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
where (as usual) \(s^2 = \sum_{i=1}^n (y_i - \bar y)^2 /(n-1).\)

\sphinxAtStartPar
Thus the Likelihood can be written:
\begin{equation*}
\begin{split}
p(y_1,\dots,y_n \mid \mu) = \left(\frac{1}{2\pi \sigma^2}\right)^{n/2} \exp\left\{
-\frac{1}{2\sigma^2}\left[(n-1)s^2 + n(\bar y - \mu)^2 \right]   \right\}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Since we are interested in the posterior for \(\mu,\) we can drop all terms not involving \(\mu,\) so the likelihood is proportional to
\begin{equation*}
\begin{split}
p(y_1,\dots,y_n \mid \mu) \propto \exp\left\{ -\frac{n}{2\sigma^2} (\bar y - \mu)^2 \right\}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Notice that this also has the same form of a Normal distribution for the mean \(\bar{y}\), specifically, \(\bar{y} \sim N(\mu, \frac{\sigma^2}{n})\).


\subsection{10.2.2 Prior}
\label{\detokenize{10.c. Bayesian Statistics II:prior}}
\sphinxAtStartPar
We noted in the previous session that the Normal distribution is a conjugate prior when the likelihood is a Normal distribution. Thus, for convenience, we will use a Normal distribution as a prior for \(\mu\):
\begin{equation*}
\begin{split}
\mu  \sim N(\phi, \tau^{2}),
\end{split}
\end{equation*}
\sphinxAtStartPar
as the posterior distribution will conveniently be a Normal distribution as well. The prior parameters \(\phi\) and \(\tau^2\) should be specified based on prior knowledge of \(\mu\) and the uncertainty around this prior knowledge. It may come from previous research or formally elicited from investigators. If no prior evidence is available, we assign an appriopriately large value to \(\tau\).


\subsection{10.2.3 Posterior}
\label{\detokenize{10.c. Bayesian Statistics II:posterior}}
\sphinxAtStartPar
To derive the posterior for the mean \(\mu\), we need to find the  distribution of that parameter conditional on the data (both the empirical data and prior distribution). In the following calculation, we are only interested in the parts of the p.d.f. that depend on \(\mu\). Any terms not involving \(\mu\) are part of the \sphinxstyleemphasis{normalisation constant}. This is part of the p.d.f., but does not affect the shape of the density.

\sphinxAtStartPar
The posterior is given by
\begin{equation*}
\begin{split}
\begin{align*}
p(\mu \mid y_1,\dots,y_n) 
&\propto p(y_1,\dots,y_n \mid \mu) p(\mu) \\
&\propto \exp\left\{ -\frac{n}{2\sigma^2} (\bar y - \mu)^2 \right\}
\exp\left\{-\frac{1}{2\tau^2}(\mu-\phi)^2\right\} \\ 
 & = \exp\left\{ -\frac{n}{2\sigma^2}(\bar{y}-\mu)^2
-\frac{1}{2\tau^2}(\mu-\phi)^2\right\} \\
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Expanding the brackets and retaining only terms containing \(\mu\):
\begin{equation*}
\begin{split}
p(\mu \mid y_1,\dots,y_n) 
 \propto \exp\left\{ -\frac{n}{2\sigma^2 \tau^2}  (-2n\bar{y}\mu\tau^2 - n\mu^2\tau^2+\mu^2\sigma^2-2\mu\phi\sigma^2)
\right\} 
\end{split}
\end{equation*}
\sphinxAtStartPar
Completing the squared term for \(\mu\):
\begin{equation*}
\begin{split}
p(\mu \mid y_1,\dots,y_n) 
\propto \exp\left\{ -\frac{\tau^2 n + \sigma^2}{2 \sigma^2 \tau^2}\left(\mu - \frac{ \tau^2 n\bar{y} -\sigma^2\phi}{\tau^2n+\sigma^2}\right)^2\right\}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can recognise this has the form the p.d.f. of the Normal distribution, therefore we see that
\begin{equation*}
\begin{split}
\mu \vert y_1,\dots,y_n \sim N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} \right\}.
\end{split}
\end{equation*}
\sphinxAtStartPar
We see that:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
the Normal prior is \sphinxstyleemphasis{conjugate} for a Normal Likelihood, as the posterior is also Normal.

\item {} 
\sphinxAtStartPar
The posterior mean, \(\frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}\) is a weighted average of the data \(\bar y \) and the prior mean \(\phi\): we can write it as \(w \bar{y} + (1-w) \phi\), where \(w= \frac{\tau^2 n}{\tau^2 n + \sigma^2}\) . Hence the posterior combines the information from the likelihood (data) and prior (a priori belief).

\item {} 
\sphinxAtStartPar
The variance of the posterior is  \(\frac{\sigma^2\tau^2}{\tau^2n+\sigma^2}\). In a larger study, since \(n\) becomes very large, we have \(\tau^2 >> \frac{\sigma^2}{n}\), so the posterior variance tends to zero.

\item {} 
\sphinxAtStartPar
In smaller studies, \(\tau^2 << \frac{\sigma^2}{n}\), the posterior mean is closer to \(\phi\) and the posterior variance depends both on the prior and sampling variance \(\frac{\sigma^2\tau^2}{\tau^2n+\sigma^2}\).

\end{enumerate}


\section{10.3 Credible Intervals}
\label{\detokenize{10.d. Bayesian Statistics II:credible-intervals}}\label{\detokenize{10.d. Bayesian Statistics II::doc}}
\sphinxAtStartPar
We saw in the previous session that a Bayesian \(95\%\) credible interval is an interval which contains \(95\% \) of the posterior distribution of the parameter, and the \(95 \%\) Highest Posterior Density (HPD) interval is the credible interval with the smallest range of values for \(\theta\).

\sphinxAtStartPar
Given that the posterior distribution has mean \(\psi\) and variance \(\gamma^{2}\), the \(95\%\) HPD interval is given by
\(\psi \pm 1.96 \times \gamma\). Thus, for a standard Normal posterior, the 95\% HPD interval is \((-1.96,1.96).\)


\subsection{10.3.1 CD4 cell counts example:}
\label{\detokenize{10.d. Bayesian Statistics II:cd4-cell-counts-example}}
\sphinxAtStartPar
In the CD4 cell count example, suppose that we have very strong prior information that suggests the treatment is not effective, and we expect that the difference in cell counts is approximately zero. Let us denote by \(y\) the difference in CD4 cell counts. We set \(\mu \sim N(0, 0.1)\) to reflect that there is only about \(2.5\%\) chance that the treatment increases mean CD4 counts by more than 0.62 (1.96 \(\times \sqrt{0.1}\)) and a \(50\%\) chance that it will actually decrease the mean CD4 count).

\sphinxAtStartPar
Summarizing the information we have:
\begin{quote}

\sphinxAtStartPar
sample size \(n = 20\)mean of data \(\bar{y} = 0.805\)variance of data (assumed known) \(\sigma^2 = 0.7\)prior mean \( \phi = 0\)prior variance \(\tau^2= 0.1\)
\end{quote}

\sphinxAtStartPar
We find the posterior distribution:
\begin{equation*}
\begin{split}
\begin{align*}
\mu \vert y_1,\dots,y_n &\sim N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} \right\} \\
 &\sim N\left\{ \frac{ 0.1 \times 20 \times 0.805 + 0 }{0.1 \times 20 + 0.7}, \frac{0.7 \times 0.1}{0.1 \times 20 +0.7 } \right\} \\
  &\sim N\left\{ 0.596, 0.0259 \right\}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
We plot below the prior distribution (in blue), the distribution of \(\bar{y}\) (red) and the posterior distribution (purple). We observe that the mean of the posterior distribution is in between the mean of the prior and that of the likelihood. Note that in R, the Normal distribution is parameterized by the standard deviation rather than the variance.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{7}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{seq}\PYG{p}{(}\PYG{l+m}{\PYGZhy{}2}\PYG{p}{,} \PYG{l+m}{2}\PYG{p}{,} \PYG{l+m}{0.01}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}plot the prior }
\PYG{n}{y1} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{mean}\PYG{o}{=}\PYG{l+m}{0}\PYG{p}{,} \PYG{n}{sd}\PYG{o}{=}\PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{l+m}{0.1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y1}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Density\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{n+nf}{expression}\PYG{p}{(}\PYG{n}{mu}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{legend}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{topleft\PYGZdq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Prior distribution\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Distribution of mean of y\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Posterior distribution\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
       \PYG{n}{col}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{purple\PYGZdq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lty}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}plot the observed distribution }
\PYG{n}{y2} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{mean}\PYG{o}{=}\PYG{l+m}{0.805}\PYG{p}{,} \PYG{n}{sd}\PYG{o}{=}\PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{l+m}{0.7}\PYG{o}{/}\PYG{l+m}{20}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y2}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{)}
\PYG{n}{y3} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{dnorm}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{mean}\PYG{o}{=}\PYG{l+m}{0.596}\PYG{p}{,} \PYG{n}{sd}\PYG{o}{=}\PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{l+m}{0.0259}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y3}\PYG{p}{,} \PYG{n}{type}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{l\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd}\PYG{o}{=}\PYG{l+m}{1}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{purple\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{10.d. Bayesian Statistics II_2_0}.png}

\sphinxAtStartPar
The  \(95\%\) HPD interval can be calculated as \(0.596 \pm 1.96 \times \sqrt{0.0259} = (0.281, 0.911)\). This interval lies wholly above zero, so we can state that we have a strong posterior belief that there is an increase in CD4 cell counts.


\section{10.4 Predictions}
\label{\detokenize{10.e. Bayesian Statistics II:predictions}}\label{\detokenize{10.e. Bayesian Statistics II::doc}}

\subsection{10.4.1 Prior predictive distributions}
\label{\detokenize{10.e. Bayesian Statistics II:prior-predictive-distributions}}
\sphinxAtStartPar
Finding the predictive distribution for a new patient \(y\) before making any observations involves finding the following distribution:
\begin{equation*}
\begin{split}
p(y | \sigma^2, \phi, \tau^2) = \int p(y, \mu | \sigma^2, \phi, \tau^2) d \mu\\
= \int p(y | \mu, \sigma^2, \phi, \tau^2) p(\mu |  \phi, \tau^2) d \mu
\end{split}
\end{equation*}
\sphinxAtStartPar
This calculation involves a lot of algebra. We instead use a different approach: note that we can write the observation as \(y = \mu + \epsilon\), where \(\mu \sim N(\phi, \tau^2)\) and \(\epsilon \sim N(0, \sigma^2)\). Then, since \(\mu\) and \(\epsilon\) are independent, we can use this result:
\begin{quote}

\sphinxAtStartPar
If X and Y be independent random variables that are Normally distributed, \(X\sim N(\mu _{X},\sigma _{X}^{2})\) and \(Y\sim N(\mu _{Y},\sigma _{Y}^{2})\), then their sum is also Normally distributed: \(X + Y \sim N(\mu _{X}+\mu _{Y},\sigma _{X}^{2}+\sigma _{Y}^{2})\).
\end{quote}

\sphinxAtStartPar
Thus we have that \(y \sim N(\phi, \tau^2 + \sigma^2)\).

\sphinxAtStartPar
In our example, before collecting any data, suppose we wish to predict the probability that the difference in cell counts is greater than 0.3 (30 \(cells/mm^3\)). We have that \(y \sim N(0, 0.1 + 0.7)\). We compute \(p(y > 0.3)\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m}{1}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{l+m}{0.3}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{l+m}{0.8}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.368657838608209\end{split}
\end{equation*}
\sphinxAtStartPar
Given our prior distribution alone, the probability that the change in CD4 count for a new patient will exceed 0.3 (30 \(cells/mm^3\)) is approximately 0.369.


\subsection{10.4.2 Posterior predictive distributions}
\label{\detokenize{10.e. Bayesian Statistics II:posterior-predictive-distributions}}
\sphinxAtStartPar
Suppose that have observed \(y_1, ..., y_n \), and we want to predict future observations \(z\), assuming that \(z\) and \(y_i\) are independent for all \(1 \leq i \leq n\), conditional on \(\mu\). The posterior predictive distribution for \(z\) is given by,
\begin{equation*}
\begin{split}
\begin{align*}
p(z| y_1, ..., y_n,  \sigma^2, \phi, \tau^2) &= \int p(z, \mu | y_1, ..., y_n,  \sigma^2, \phi, \tau^2) d \mu \\
     &= \int p(z | y_1, ..., y_n,\mu,  \sigma^2) p(\mu |y_1, ..., y_n,\sigma^2, \phi, \tau^2  ) d \mu. \\ 
     \end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Again, this involves some fiddly algebra but we can use a similar method to that we used for the prior predictive distribution. We wish to know what the predictive distribution of a new patient \(z\) is, given the previous observations \(y_1, ..., y_n\). We can write \(z  = \mu + \epsilon\). We have that \(\mu \vert y_1,\dots,y_n \sim N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} \right\}, \) and \(\epsilon \sim N(0, \sigma^2)\).

\sphinxAtStartPar
Using the result for the sum of two independent Normal distributions, the posterior predictive distribution has the form \( N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} + \sigma ^2\right\}\)

\sphinxAtStartPar
In our example, based on both prior and observed data, the predictive distribution for cell counts in a new patient being greater than 0.3 (30 \(cells/mm^3\)) is \(N(0.596, 0.0259 + 0.7)\). We can compute \(f(z | y_1, ..., y_n > 0.3)\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m}{1}\PYG{o}{\PYGZhy{}} \PYG{n+nf}{pnorm}\PYG{p}{(}\PYG{l+m}{0.3}\PYG{p}{,} \PYG{l+m}{0.596}\PYG{p}{,} \PYG{n+nf}{sqrt}\PYG{p}{(}\PYG{l+m}{0.7259}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.635861643314828\end{split}
\end{equation*}
\sphinxAtStartPar
After having observed the data, the predictive probability that the next patient will have a difference in CD4 cell counts of greater than 0.3 (30 \(cells/mm^3\)) has increased substantially to 0.636.


\section{10.5 Multiparameter models}
\label{\detokenize{10.f. Bayesian Statistics II:multiparameter-models}}\label{\detokenize{10.f. Bayesian Statistics II::doc}}
\sphinxAtStartPar
Suppose now that our likelihood has two unknown parameters, \((\mu,\sigma^2).\) In this case, we would need a prior distribution for both parameters, and our posterior distribution will now be bivariate. If desired we can summarise this by the mean and covariance matrix or by HPD contour maps. However, often in applications, interest focusses only on one parameter, say \(\mu;\) the other parameter is usually referred to as a nuisance parameter. In Bayesian inference, we typically use simulation to draw from the posterior distribution of \((\mu,\sigma^2).\) For marginal inference for \(\mu,\) we summarise the draws from \(\mu\) in the usual way, across all simulated values of \(\sigma^2\). Analytically, this is equivalent to integrating the posterior over \(\sigma^2\):
\begin{equation*}
\begin{split}
p(\mu | y) = \int p(\mu,\sigma^2 |y) \, d\sigma^2,
\end{split}
\end{equation*}
\sphinxAtStartPar
where we have used Bayes’ theorem to obtain the posterior, i.e.\(p(\mu,\sigma^2 |y).\) This integral may be intractable (hence the preference for simulation approaches).


\section{Further Resources}
\label{\detokenize{10.g. Bayesian Statistics II:further-resources}}\label{\detokenize{10.g. Bayesian Statistics II::doc}}
\sphinxAtStartPar
Note: further resources are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. All examinable material is contained within the notes.


\subsection{Resources for learning}
\label{\detokenize{10.g. Bayesian Statistics II:resources-for-learning}}
\sphinxAtStartPar
These textbooks are recommended for further learning and examples:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{http://www.stat.columbia.edu/~gelman/book/}{Bayesian data analysis by Gelman et. al} can be downloaded in PDF format.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-the-bugs-book/}{The Bugs Book by Lunn et. al.} is available at the LSHTM library.

\item {} 
\sphinxAtStartPar
\sphinxhref{http://jim-stone.staff.shef.ac.uk/BookBayes2012/HTML\_BayesRulev5EbookHTMLFiles/ops/xhtml/ch01BayesJVSone.html}{an introductory book by Jim Stone with nice examples} The first chapter is freely available online.

\end{itemize}


\subsection{Examples of applications}
\label{\detokenize{10.g. Bayesian Statistics II:examples-of-applications}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.nature.com/articles/nrg2615}{Article on Nature providing guidelines Bayesian analyses for genetic association studies}.

\item {} 
\sphinxAtStartPar
The potential benefits of incorporating prior information in the context of health care evaluation is discussed by \sphinxhref{https://projecteuclid.org/euclid.ss/1089808280}{David Spiegelhalter in this article}.

\item {} 
\sphinxAtStartPar
We mentioned earlier that Bayesian approaches can be helpful for overcoming challenges with small sample sizes in clinical trials for rare diseases; you can read more about this \sphinxhref{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2551510/pdf/bmj00623-0045.pdf}{in an article by Lilford et. al.}

\end{itemize}


\part{Statistical modelling}


\chapter{Investigations and the role of regression modelling}
\label{\detokenize{11. Investigation.Intro:investigations-and-the-role-of-regression-modelling}}\label{\detokenize{11. Investigation.Intro::doc}}
\sphinxAtStartPar
Data scientists don’t do statistics just for fun (although, clearly statistics is indeed fun!).  At the heart of each data science project is a question to be answered.

\sphinxAtStartPar
This section of the notes begins by thinking about the different types of investigation that might be carried out within a data science project. We consider three important classes of investigation type: \sphinxstylestrong{description}, \sphinxstylestrong{prediction} and \sphinxstylestrong{causal}.

\sphinxAtStartPar
We then move on to consider a commonly used family of statistical analysis: \sphinxstylestrong{regression modelling}.  Three sessions introduce linear regression, beginning with the simplest type which we call \sphinxstylestrong{simple linear regression} involving a single explanatory variable. We then extend this to incorporate multiple explanatory variables, through \sphinxstylestrong{multivariable linear regression} modelling. We explore how to model various types of explanatory variables, including continuous, binary and categorical covariates and discover how to include interactions and higher\sphinxhyphen{}order terms (which are need to model non\sphinxhyphen{}linear relationships) in the regression model. The last of the linear regression sessions explores diagnostics to assess whether the underlying assumptions of the linear model hold in a particular dataset.

\sphinxAtStartPar
These ideas are then extended to other settings in the remaining two sessions. First, we meet \sphinxstylestrong{logistic regression}, an extension of linear regression modelling to settings where the outcome variable is binary. Finally, we define the \sphinxstylestrong{Generalised Linear Model (GLM)}, which is a generalisation of linear regression to a wide range of settings and can be seen as a way of unifying linear, logistic and Poisson regression models, as well as many other types of regression model. We explore \sphinxstylestrong{Poisson regression} as an important example of a GLM.

\sphinxAtStartPar
We conclude this section of the notes by returning to the idea of investigations and – armed with our new knowledge about regression modelling – consider the role of regression modelling in different types of investigations.


\chapter{11. Types of Investigation}
\label{\detokenize{11.a. Types of Investigation:types-of-investigation}}\label{\detokenize{11.a. Types of Investigation::doc}}
\sphinxAtStartPar
This session introduces how to set up a research question, explores the different types of investigation and brings in key concepts like prediction, causality and confounding. This session demonstrates how methods learnt in previous sessions are applied in research.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Describe three different types of investigations that arise in medical statistics and health data science.

\item {} 
\sphinxAtStartPar
Link a research question to an investigation type and compare the properties of different investigation types.

\item {} 
\sphinxAtStartPar
Explain how and why explanatory variables are used differently in prediction studies and in causal investigations.

\end{itemize}




\section{11.1 Specifying research questions}
\label{\detokenize{11.b. Types of Investigation:specifying-research-questions}}\label{\detokenize{11.b. Types of Investigation::doc}}
\sphinxAtStartPar
Specifying the research question or questions is a crucial starting point for an investigation. In some cases the research question will be highly specific, and in others could be more wide ranging with several components. The research question then informs the subsequent stages of the investigation, ranging from choice of study population; study design; data collection; monitoring and quality control; data analysis; presentation of conclusions; interpretation. Figure 11.1 illustrates one
way of representing the whole process of an investigation (which we saw earlier in the Introduction).

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=400\sphinxpxdimen]{{01_intro_PPDAC_adapt}.png}
\end{figure}

\sphinxAtStartPar
The statistician/data scientist plays an important role at all stages of an investigation, not just at
the data analysis phase. It is perhaps most usual for collaborators who are subject\sphinxhyphen{}matter experts
(e.g. clinicians) to pose the initial research question. However, the statistician very often plays a
key part in refining these initial ideas in order to translate them into something formal and clearly
specified.


\section{11.2 Different types of investigation}
\label{\detokenize{11.c. Types of Investigation:different-types-of-investigation}}\label{\detokenize{11.c. Types of Investigation::doc}}

\subsection{11.2.1 Classification}
\label{\detokenize{11.c. Types of Investigation:classification}}
\sphinxAtStartPar
The research question informs what type of investigation is required. Investigations can be divided
broadly into the following types:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Description

\item {} 
\sphinxAtStartPar
Prediction

\item {} 
\sphinxAtStartPar
Causality and explanation

\end{enumerate}

\sphinxAtStartPar
Hernan, Hsu \& Healy (Chance, 2019) set out to classify data science tasks and used three classifications: \sphinxstyleemphasis{Description}, \sphinxstyleemphasis{Prediction}, and \sphinxstyleemphasis{Counterfactual prediction} (meaning causality). Schmueli (Statistical Science, 2010) also described similar classifications: \sphinxstyleemphasis{Descriptive modelling}, \sphinxstyleemphasis{Predictive modelling}, and \sphinxstyleemphasis{Explanatory modelling}. See also Hand (Harvard Data Science Review, 2019) for a nice discussion on this topic.


\subsection{11.2.2 Implications of investigation type}
\label{\detokenize{11.c. Types of Investigation:implications-of-investigation-type}}
\sphinxAtStartPar
The distinction between the different types of investigation is crucial because it has a fundamental impact on the steps of the analysis and beyond. For example, the investigation type influences:
\begin{itemize}
\item {} 
\sphinxAtStartPar
How we decide what variables are to be included in the analysis

\item {} 
\sphinxAtStartPar
What analysis methods to use

\item {} 
\sphinxAtStartPar
How we assess the fit/performance of the model or oter analysis approach used

\item {} 
\sphinxAtStartPar
How we present the results from the analysis

\item {} 
\sphinxAtStartPar
How the findings might be used in practice

\item {} 
\sphinxAtStartPar
How we need to work with other experts at different stages

\end{itemize}


\subsection{11.2.3 The role of study design}
\label{\detokenize{11.c. Types of Investigation:the-role-of-study-design}}
\sphinxAtStartPar
The different types of investigation may be performed using data from studies of different design. Having posed a research question, we can consider (with input from collaborators) what data are required to answer it robustly, including whether new data collection is needed, or whether there are existing data that could be used to address the question. This process needs to take into account considerations of cost, timeliness, feasibility and ethics. For example, for some questions our ideal study could be a randomized controlled trial, but to perform one would require such long followup that it would be infeasible and unethical, and so we would turn to observational data to address the research question. There is a major emphasis in the recent biostatistical and epidemiological literature on the use of ‘found’ data from sources such as electronic health records, which present great opportunities to answer research questions using data on a large number of individuals, but also present challenges for analysis and interpretation. All three types of investigation may make use of observational data. Randomized controlled trials are designed to estimate treatment effects (i.e. for causal investigations), but secondary analyses of trial data can be used in other types of investigation, such as to develop a prediction model.


\section{11.3 Properties of different types of investigation}
\label{\detokenize{11.d. Types of Investigation:properties-of-different-types-of-investigation}}\label{\detokenize{11.d. Types of Investigation::doc}}

\subsection{11.3.1 Description}
\label{\detokenize{11.d. Types of Investigation:description}}
\sphinxAtStartPar
In a descriptive investigation the data are used to provide a quantitative summary of features of the
population of interest, or in other words the data are summarised in a compact way.

\sphinxAtStartPar
Simple descriptive analyses involve calculating proportions of individuals with a particular characteristic (e.g. males and females; smokers and non\sphinxhyphen{}smokers), or estimating features of the distribution of continuous variables (e.g. mean and variance of weight or blood pressure). The resulting information is then presented using tables and data visualisation.

\sphinxAtStartPar
Some descriptive analyses may extend to use of more complex methods of analysis. For example, the research question may concern how individuals within a population cluster together interms of their dietary habits, requiring clustering methods. It may be of interest to estimate theexpected survival time post\sphinxhyphen{}disease diagnosis in the presence of censored survival times, which
would require survival analysis techniques.

\sphinxAtStartPar
All investigations should start with some basic descriptive analysis to gain understanding of the features of the data at hand. It is at this stage that we can uncover challenges such as missing data, gain insights into how certain variables are distributed, and, where relevant, gain understanding of correlations between key variables, including to identify collinearities. Some investigations then go on to the main research question, which goes beyond description, and others may be entirely descriptive and not proceed onto other questions.

\sphinxAtStartPar
Huebner et al. (2019) provide useful guidance on ‘initial data analysis’. See also Spiegelhalter
(2019) for an accessible discussion of summarising and communicating descriptions of data.


\subsection{11.3.2 Prediction}
\label{\detokenize{11.d. Types of Investigation:prediction}}
\sphinxAtStartPar
Prediction is about using data on some features of individuals to predict other features with the aim of predicting the outcome for new or future observations. More formally, prediction is concerned with mapping data on variables \(X_{1}\), \(X_{2}\), … , \(X_{p}\) to an outcome \(Y\) . The prediction model could be developed using statistical models such as regression, or approaches that would be described as machine learning algorithms.

\sphinxAtStartPar
Results from prediction investigations are used for a range of purposes: to inform people of their risk or prognosis; to identify people at high risk of an adverse event and hence take action such as more frequent screening (though the investigation will not tell us whether such screening would be effective).

\sphinxAtStartPar
Prediction models are typically developed using observational data. A well known example is the Framingham Risk Score, which provides predictions of a person’s 10\sphinxhyphen{}year of developing coronary heart disease (D’Agostino et al 2008).

\sphinxAtStartPar
There is a huge literature on prediction in the medical setting. See for example the books by Riley et al. (2019) and Steyerberg (2019).


\subsection{11.3.3 Causality and explanation}
\label{\detokenize{11.d. Types of Investigation:causality-and-explanation}}
\sphinxAtStartPar
In causal investigations we seek to understand the causal effect of one or more variables on an outcome. Hernan et al. (2019) describe this as “Using data to predict certain features of the world as if the world had been different”. For a simple example of a causal investigation, consider a continuous outcome \(Y\) (e.g. blood pressure) and a binary treatment variable \(X\), where \(X = 1\) denotes treated and \(X = 0\) denotes untreated. A causal investigation asks how the mean of Y would be different if all individuals had \(X = 1\) compared with if all individuals had \(X = 0\). In other words, if we could change \(X\) what would be the expected change in \(Y\) ?

\sphinxAtStartPar
Questions such as this can be arguably simple to answer using a randomized controlled trial, where there is no confounding of the treatment\sphinxhyphen{}outcome association. However, issues of drop\sphinxhyphen{}out and non\sphinxhyphen{}compliance are important to consider. Historically, some have considered answering causal questions to lie only in the domain of randomized experiments. However, randomized experiments are not feasible or ethical to address many important questions. It is now recognised that causality is often the goal of investigations using observational data. See for example the paper of Hernan (2018), who wrote “being explicit about the causal objective of a study reduces ambiguity in the scientific question, errors in the data analysis, and excesses in the interpretation of the results”. The field of ‘causal inference’ has developed in recent decades, with particular advances in recent years, to enable this.

\sphinxAtStartPar
Schmeuli (2010) equates causality with ‘explanation’, meaning explanation of mechanisms of how one (or more) variable affects another. However, Hernan et al. (2019) make the point that we may be able to say that \(X\) causes \(Y\) without understanding the underlying mechanism. For example we may find strong evidence from a trial that a drug is effective for a given outcome, but the precise biological mechanisms through which the effect is transmitted are not well understood.

\sphinxAtStartPar
The variable of interest in a causal investigation could be use of a medical treatment (a drug) or application of a procedure. More generally it could be an ‘exposure’ such as ‘smoking’ or ‘exercising for at least 30 minutes per day’. The ‘hypothetical intervention’ of interest should be (reasonably) well defined, even if we could never in reality intervene on it in the real world (e.g. it would be impractical, not to say unethical, to intervene on smoking status). See Hernan (2016) for a discussion of related issues.


\subsection{11.3.4 Is there a fourth investigation type?}
\label{\detokenize{11.d. Types of Investigation:is-there-a-fourth-investigation-type}}
\sphinxAtStartPar
There is arguably a fourth investigation type which is concerned with exploring how several explanatory variables \(X_{1}\), … , \(X_{p}\) are associated with an outcome \(Y\). This might be described as an “exploration of risk factors” investigation. It may involve univariable analyses, looking at the association of each explanatory variable (“risk factor”) individually with the outcome, and multivariable analyses which look at association of several variables with the outcome in a single model. These types of analysis are typically carried out using observational data, and many (or perhaps most) epidemiological studies are investigations of this type, at least historically.

\sphinxAtStartPar
These types of investigation can be useful for understanding associations between variables in the population of interest and, as such, some may consider these analyses to be descriptive. However, as we all know, association is not causation! These types of investigation often do not consider the relative temporal ordering of explanatory variables, which means that interpretation of estimated associations as causal effects can be misleading. There is recent emphasis in the epidemiological literature on more principled investigations which are more explicit about the aim of the investigation.

\sphinxAtStartPar
Like in a prediction investigation, the interest is in several explanatory variables. However, unlike in a prediction investigation, the aim is to actually explore quantitatively the unconditional and conditional associations of the explanatory variables with \(Y\), rather than being purely on predicting \(Y\). Unlike in a causal investigation, there is not a particular focus on a single variable. However, there is often an attempt to discuss the associations as though they may be causal even though an explicit causal question has not been posed.

\sphinxAtStartPar
Investigators should be wary of over\sphinxhyphen{}interpreting findings from “exploration of risk factors” investigations. And if we are really interested in addressing a causal question we should be explicit about that and carry out our analysis and interpretations accordingly.


\section{11.4 An example: stroke in women}
\label{\detokenize{11.e. Types of Investigation:an-example-stroke-in-women}}\label{\detokenize{11.e. Types of Investigation::doc}}
\sphinxAtStartPar
Table 1 provides an example of the features of different investigation types. The overall topic is stroke in women. The table (taken from Hernan et al. 2019) provides an example research question, the features of data that would be required to answer it, and the types of analysis that could be used for investigations of three types: Description, Prediction and Causal inference.

\sphinxAtStartPar
Table 1: From Hernan, Hsu \& Healy 2019. Examples of Tasks Conducted by Data Scientists Working with Electronic Health Records


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Prediction
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Causal inference
\\
\hline
\sphinxAtStartPar
Example of scientific question
&
\sphinxAtStartPar
How can women aged 60\sphinxhyphen{}80 years with stroke history be partitioned in classes defined by their characteristics?
&
\sphinxAtStartPar
What is the probability of having a stroke next year for women with certain characteristics?
&
\sphinxAtStartPar
Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?
\\
\hline
\sphinxAtStartPar
 Data
&
\sphinxAtStartPar
 \sphinxhyphen{} Eligibility criteria  \sphinxhyphen{} Features (symptoms, clinical parameters … )
&
\sphinxAtStartPar
 \sphinxhyphen{} Eligibility criteria \sphinxhyphen{} Output (diagnosis of stroke over the next year) \sphinxhyphen{} Inputs (age, blood pressure, history of stroke, diabetes at baseline)
&
\sphinxAtStartPar
 \sphinxhyphen{} Eligibility criteria  \sphinxhyphen{} Outcome (diagnosis of stroke over the next year)  \sphinxhyphen{}Treatment (initiation of statins at baseline)  \sphinxhyphen{} Confounders  \sphinxhyphen{} Effect modifiers (optional)
\\
\hline
\sphinxAtStartPar
 Example of analytics
&
\sphinxAtStartPar
 Cluster analysis
&
\sphinxAtStartPar
 Regression  Decision trees  Random forests  Support vector machines  Neural networks
&
\sphinxAtStartPar
 Regression  Matching  Inverse probability weighting  G\sphinxhyphen{}formula  G\sphinxhyphen{}estimation  Instrumental variable estimation
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{11.5 Role of explanatory variables in different types of investigation}
\label{\detokenize{11.f. Types of Investigation:role-of-explanatory-variables-in-different-types-of-investigation}}\label{\detokenize{11.f. Types of Investigation::doc}}
\sphinxAtStartPar
The role of explanatory variables in different types of investigation differs. We focus here on prediction investigations and causal investigations.


\subsection{11.5.1 Prediction}
\label{\detokenize{11.f. Types of Investigation:prediction}}
\sphinxAtStartPar
In prediction investigations the aim is to use \(X_{1}\), … , \(X_{p}\) to predict \(Y\) . In this setting the \(X_{1}\), … , \(X_{p}\) are often referred to as the ‘predictors’ for obvious reasons. For a prediction problem we may well use all of the explanatory variables \(X_{1}\), … , \(X_{p}\) in the prediction model or algorithm. Crucially, in prediction we are not interested in the inter\sphinxhyphen{}relationships between the explanatory variables \(X_{1}\), … , \(X_{p}\) and their temporal ordering. The only aim is to achieve a good prediction of the outcome \(Y\). It may be desirable to reduce the number of explanatory variables, particularly in settings where the number of potential predictors \(p\) is very large. Various principled procedures are available for reducing the number of predictor variables.


\subsection{11.5.2 Causality and explanation}
\label{\detokenize{11.f. Types of Investigation:causality-and-explanation}}
\sphinxAtStartPar
In investigations of causality, one of the explanatory variables is designated as the treatment or exposure of interest. Let’s suppose this is variable \(X_{1}\) and the research question is about how \(X_{1}\) affects \(Y\). Or, in other words, if \(X_{1}\) had been different, how would \(Y\) have been different? Let’s consider the setting of an Randomized Controlled Trials and an observational study separately and think of the situation where \(X_{1}\) is a binary treatment variable

\sphinxAtStartPar
\sphinxstyleemphasis{Randomized controlled trials (RCT)}

\sphinxAtStartPar
Suppose individuals are randomized to receive treatment \((X_{1} = 1)\) or not \((X_{1} = 0)\), and the outcome \(Y\) is observed after some period of follow\sphinxhyphen{}up. It is straightforward to estimate the treatment effect in this setting because of the randomization. For a continuous outcome, we would quantify the treatment effect using a difference in the mean outcome in the two treatment groups \((E(Y |X_{1} = 1) − E(Y |X_{1} = 0))\). For a binary outcome we could quantify the treatment effect in terms of a risk difference \((Pr(Y = 1|X_{1} = 1) − Pr(Y = 1|X_{1} = 0))\), risk ratio \((Pr(Y = 1|X_{1} = 1)/Pr(Y = 1|X_{1} = 0))\) or odds ratio \(((Pr(Y = 1|X_{1} = 1)/Pr(Y = 0|X_{1} = 1))/(Pr(Y = 1|X_{1} = 0)/Pr(Y = 0|X_{1} = 0)))\), for example.

\sphinxAtStartPar
Some of the other explanatory variables \(X_{2}\), … , \(X_{p}\) are likely to be associated with \(Y\), but we do not need to use them to estimate the treatment effect due to the study design. Sometimes investigators will adjust for baseline variables, measured at the start of the trial prior to treatment. By the study design, baseline variables are not associated with the treatment. There can be advantages of adjusting for baseline variables that are predictors of the outcome. Though there are particular nuances to the interpretation of the resulting estimates depending on the types of outcome (continuous, binary, etc) and on how the treatment effect is quantified.

\sphinxAtStartPar
Of course, there are many important considerations surrounding the validity and interpretation of
treatment effects estimated using RCTs, such as whether the effect is a ‘per\sphinxhyphen{}protocol’ or ‘intentionto\sphinxhyphen{}treat’ effect, whether there is drop\sphinxhyphen{}out, non\sphinxhyphen{}adherence or treatment switching.

\sphinxAtStartPar
\sphinxstyleemphasis{Observational studies}

\sphinxAtStartPar
Suppose we have available observational data on the treatment variable \(X_{1}\) and the outcome \(Y\), for example from electronic health records. In this setting the treatment in non\sphinxhyphen{}randomized, and there are very likely to be confounders of the association between the treatment and the outcome.

\sphinxAtStartPar
A confounder is a variable that affects both the treatment and the outcome. Confounding variables occur prior in time to both the treatment/exposure and the outcome. See VanderWeele and Schpitser (2013) for a formal statistical discussion of confounding.

\sphinxAtStartPar
To estimate the causal effect of \(X_{1}\) on \(Y\) requires us to control for confounding. Consider a simple setting in which there is only one other variable at play, \(X_{2}\), which in the observational setting affects whether a person gets the treatment \(X_{1}\) and also affects their outcome \(Y\). For example, if \(X_{1}\) is a blood pressure\sphinxhyphen{}lowering medication and \(Y\) is blood pressure 1 year later, then \(X_{2}\) could be the person’s blood pressure at the time origin. The assumed relationships between the three variables \(X_{1}\), \(X_{2}\) and \(Y\) are illustrated in Figure 2 using directed acyclic graphs (DAGs), contrasting the relationships in an RCT and in an observational study.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=250\sphinxpxdimen]{{Session_11_Figure_2}.jpg}
\end{figure}

\sphinxAtStartPar
DAGs, also called ‘causal diagrams’, are used to graphically describe mechanistic relationships between variable using uni\sphinxhyphen{}directional arrows. An arrow connecting two variables indicates (potential) causation in the direction of the arrow and the absence of an arrow indicates an assumption that there is no direct causal effect of the first variable on the second. See Greenland et al. (Epidemiology, 1999) and Shrier and Platt (2008) for introductions to causal diagrams. Some other useful more recent articles on this are from Etminan et al. (2020) and Tenant et al. (2019). In simple situation such as this example, we don’t need a DAG to tell us that we need to account for the confounding by \(X_{2}\) in our analysis in order to estimate the effect of \(X_{1}\) on \(Y\) . However, when there are lots of variables at play DAGs become very useful, and have formal theory attached.

\sphinxAtStartPar
In summary, in a causal investigation the variables on which the research question focuses are \(X_{1}\) and \(Y\) . However, depending on the study design, we may need to account for other variables in the analysis, though those other variables are not our main focus. The concept of confounding is not relevant in prediction investigations.


\section{11.6 Summary}
\label{\detokenize{11.g. Types of Investigation:summary}}\label{\detokenize{11.g. Types of Investigation::doc}}
\sphinxAtStartPar
We have placed some emphasis on how the investigation type affects what variables should be included in the analysis and on how the results might be interpreted. There are naturally many other things to consider which are beyond the scope of this session. The above example focused on regression. The next few sessions in this module will focus on regression models of different types. They are a fundamental part of the statistician’s toolbox and are used in investigations of different types. However, there are many other specialised methods available for specific tasks. For example, in descriptive analyses we may use clustering methods and principal components analysis. In prediction tasks, machine learning methods not based on regression are increasingly used. In studies of causal effects many specialised methods have been developed over recent years. Some of these involve regression and others not.

\sphinxAtStartPar
The type of investigation affects how we should assess the performance and assumptions of a model/analysis. For example, in prediction tasks we should assess how well the prediction model performs in terms of predicting the outcome for a new individual. This requires tools such as cross validation, and measures of predictive performance such as \(R^2\) , area under the curve, sensitivity and specificity. In causal analyses we are concerned with whether the assumptions of the models used are valid and whether the model is correctly specified, alongside the validity of untestable assumptions such as whether there are any important confounders that have not been accounted for in the analysis.

\sphinxAtStartPar
This session aimed to provide a broad overview of different types of investigation used in medical statistics/health data science, and which you are likely to encounter in your future careers. This topic has seen some recent emphasis in the literature. The statistical and epidemiological community is increasingly emphasising the need for researchers to ensure they conduct meaningful studies and interpret findings appropriately, particularly relating to the use of observational data. It is a wide topic, and we have only touched on some aspects here.


\section{References}
\label{\detokenize{11.h. Types of Investigation:references}}\label{\detokenize{11.h. Types of Investigation::doc}}
\sphinxAtStartPar
NOTE: You are not expected to read all of these references! It is intended as a list of resources that you may find useful in the future or if you wish to follow\sphinxhyphen{}up on some of the topics discussed in more detail.

\sphinxAtStartPar
Bandoli G., Palmsten K., Chambers C.D., et al. Revisiting the Table 2 fallacy: A motivating example examining preeclampsia and preterm birth. Pediatric and Perinatal Epidemiology 2018; 32: 390\sphinxhyphen{}397.

\sphinxAtStartPar
D’Agostino R.B., Vasan R.S., Pencina M.J., et al. General Cardiovascular Risk Profile for Use in Primary Care: The Framingham Heart Study. Circulation 2008; 117: 743–753.

\sphinxAtStartPar
Etminan M, Collins GS, Mansournia MA. Using Causal Diagrams to Improve the Design and Interpretation of Medical Research. CHEST 2020; 158: Supplement S21\sphinxhyphen{}S28.

\sphinxAtStartPar
Greenland S., Pearl J., Robins J.M. Causal diagrams for epidemiological research. Epidemiology 1999; 10:37–48.

\sphinxAtStartPar
Hand D. What is the Purpose of Statistical Modelling? Harvard Data Science Review 2019 \sphinxurl{https://doi.org/10.1162/99608f92.4a85af74}

\sphinxAtStartPar
Hernan M.A. Does water kill? A call for less casual causal inferences. Annals of Epidemiology 2016; 26: 674\sphinxhyphen{}680.

\sphinxAtStartPar
Hernan M.A. The C\sphinxhyphen{}Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data. Am J Public Health. 2018;108: 616–619.

\sphinxAtStartPar
Hernan M.A., Hsu J., Healy B.. A second chance to get causal inference right: a classification of data science tasks. Chance 2019; 32: 42\sphinxhyphen{}49.

\sphinxAtStartPar
Huebner M., le Cessie S., Schmidt C., Wach W. A Contemporary Conceptual Framework for Initial Data Analysis. Observational Studies 2019; 4: 171\sphinxhyphen{}192.

\sphinxAtStartPar
Riley R.D. et al. Prognosis Research in Healthcare: Concepts, Methods, and Impact. 2019. Oxford University Press.

\sphinxAtStartPar
Schmueli. To explain or to predict? Statistical Science 2010; 25: 289\sphinxhyphen{}310.

\sphinxAtStartPar
Schooling CM, Jones H. Clarifying questions about “risk factors”: predictors versus explanation. Emerging Themes in Epidemiology 2018; 15: 10.

\sphinxAtStartPar
Schrier I., Platt R.W. Reducing bias through directed acyclic graphs. BMC Medical Research Methodology 2008; 8: 70.

\sphinxAtStartPar
Spiegelhalter D. The Art of Statistics: Learning from Data. 2019. Penguin.

\sphinxAtStartPar
Steyerberg E. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. 2nd Edition. 2019. Springer.

\sphinxAtStartPar
Tennant PWG, Harrison WJ, Murray EJ, et al. Use of directed acyclic graphs (DAGs) in applied health research: review and recommendations. MedRxiv 2019.
\sphinxurl{https://www.medrxiv.org/content/10.1101/2019.12.20.19015511v1}

\sphinxAtStartPar
VanderWeele T.J., Shpitser I. On the definition of a confounder. Annals of Statistics 2013; 41: 196\sphinxhyphen{}220.

\sphinxAtStartPar
Westreich D., Greenland S. The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients. American Journal of Epidemiology 2013; 177: 292\sphinxhyphen{}298.


\chapter{12. Linear Regression I}
\label{\detokenize{12.a. Linear Regression I:linear-regression-i}}\label{\detokenize{12.a. Linear Regression I::doc}}
\sphinxAtStartPar
This is the first of three sessions that explore linear regression modelling. These are models where the outcome of interest is a continuous variable.



\sphinxAtStartPar
By the end of this session, you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain, in general, the rationale behind parametric statistical models;

\item {} 
\sphinxAtStartPar
fit and interpret a linear regression model;

\item {} 
\sphinxAtStartPar
describe the main properties of ordinary least squares estimators;

\item {} 
\sphinxAtStartPar
explain confidence intervals and hypothesis testing for regression coefficients

\end{itemize}



\sphinxAtStartPar
\sphinxstylestrong{Acknowledgements:}  Thank you to Jennifer Nicholas and Chris Frost whose notes on linear regression were particularly useful in the development of the current lesson.


\section{12.1 Introduction}
\label{\detokenize{12.b. Linear Regression I:introduction}}\label{\detokenize{12.b. Linear Regression I::doc}}
\sphinxAtStartPar
A parametric statistical model is an algebraic description of how one or more \sphinxstylestrong{outcome} variables are influenced by \sphinxstylestrong{covariates}. Such models are widely used in medical research. Some examples of questions that we can investigate using statistical models include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Does birthweight increase with length of pregnancy?

\item {} 
\sphinxAtStartPar
Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?

\item {} 
\sphinxAtStartPar
Can we predict the risk of heart disease for our patients?

\end{itemize}

\sphinxAtStartPar
In the above examples, the outcome variables are birthweight, inflammation and heart disease. In the first two examples, the length of pregnancy and drug use are covariates. In the third example, no covariates are explicitly mentioned. However, when answering the third question, researchers may want to consider a range of patient characteristics that are associated with the risk of heart disease as covariates in their model, for example: diet, exercise, comorbodities, medications etc.

\sphinxAtStartPar
Recall that statistical models contain \sphinxstylestrong{population parameters} and representations of \sphinxstylestrong{uncertainty}. The population parameters are unknown quantities that we want to estimate from our sample and the uncertainty is a measure of the variability in the outcome variable that is not explained by the covariates.

\sphinxAtStartPar
This is the first ofthe sessions on linear regression. In this session, we will learn how to define linear regression models, how to estimate their population parameters and how to estimate measures of uncertainty. We begin by introducing the \sphinxstylestrong{simple linear regression model} which includes one outcome and one covariate. In the second session, we introduce the \sphinxstylestrong{multivariable linear regression model}, which is an extension of the simple linear regression model to situations with multiple covariates. We explore linear regression models with categorical variables, interactions and non\sphinxhyphen{}linear terms. In the third session, we discuss the key assumptions underlying linear regression models and important model diagnostics. The optional material to the session explores how to conduct an \sphinxstylestrong{analysis of variance} of statistical models.

\sphinxAtStartPar
Before delving in, it is worth making a note of the different terminologies that you may come across in the medical literature. Here, I have already used the terms: outcome and covariates. Table 1 summarises alternatives terms that may be used to describe the same concepts.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Outcome
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Covariates
\\
\hline
\sphinxAtStartPar
\(Y\)\sphinxhyphen{}variable
&
\sphinxAtStartPar
\(x\)\sphinxhyphen{}variables
\\
\hline
\sphinxAtStartPar
Dependent variable
&
\sphinxAtStartPar
Independent variables
\\
\hline
\sphinxAtStartPar
Response variable
&
\sphinxAtStartPar
Regressors
\\
\hline
\sphinxAtStartPar
Output variable
&
\sphinxAtStartPar
Input variables
\\
\hline
\sphinxAtStartPar
(no direct analogy)
&
\sphinxAtStartPar
Explanatory variables
\\
\hline
\sphinxAtStartPar
(no direct analogy)
&
\sphinxAtStartPar
Predictor variables
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Table 1: Different terminology used for outcome and covariates

\sphinxAtStartPar
Finally, it is important to understand that statistical models make \sphinxstylestrong{assumptions} about the form of relationships between outcomes and covariates. Although we can examine our data to investigate the validity of these assumptions (using methods covered in the next session), we can never be certain that the model is correct.


\section{12.2 Data used in our examples}
\label{\detokenize{12.c. Linear Regression I:data-used-in-our-examples}}\label{\detokenize{12.c. Linear Regression I::doc}}
\sphinxAtStartPar
For our examples we will use data on babies and their mothers. The data contains a random sample of 1,174 mothers and their newborn babies. The column Birth Weight contains the birth weight of the baby, in ounces; Gestational Days is the number of gestational days, that is, the number of days the baby was in the womb. There is also data on maternal age, maternal height, maternal pregnancy weight, and whether or not the mother was a smoker.

\sphinxAtStartPar
The following code can be used to download and look at the data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Load data}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Look at the first 10 rows of the data}
\PYG{n+nf}{head}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A data.frame: 6 × 6
\begin{tabular}{r|llllll}
  & Birth.Weight & Gestational.Days & Maternal.Age & Maternal.Height & Maternal.Pregnancy.Weight & Maternal.Smoker\\
  & <int> & <int> & <int> & <int> & <int> & <chr>\\
\hline
	1 & 120 & 284 & 27 & 62 & 100 & False\\
	2 & 113 & 282 & 33 & 64 & 135 & False\\
	3 & 128 & 279 & 28 & 64 & 115 & True \\
	4 & 108 & 282 & 23 & 67 & 125 & True \\
	5 & 136 & 286 & 25 & 62 &  93 & False\\
	6 & 138 & 244 & 33 & 62 & 178 & False\\
\end{tabular}\end{split}
\end{equation*}

\section{12.2.1 Exploratory analyses}
\label{\detokenize{12.c. Linear Regression I:exploratory-analyses}}
\sphinxAtStartPar
The simple linear regression model is used to model the relationship between one single variable (\(X\)) and a single outcome (\(Y\)). For example, suppose we are interested in investigating the following relationships in our birthweight data:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Association between the length of pregnancy (i.e. number of gestational days) and birthweight.

\item {} 
\sphinxAtStartPar
Association between mother’s smoking status and birthweight.

\end{enumerate}

\sphinxAtStartPar
An important first step in an analysis is to summarise and display the data. Below is a scatterplot and boxplot displaying the relevant data for Examples 1 and 2 respectively.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set the plot area into a 1x2 array}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Example 1: Scatter Plot}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Example 1\PYGZdq{}}\PYG{p}{,} 
     \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Gestational Days\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight (oz)\PYGZdq{}}\PYG{p}{,} \PYG{n}{pch}\PYG{o}{=}\PYG{l+m}{19}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Example 2: Box plot}
\PYG{n+nf}{boxplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Example 2\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Mother smokes\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight (oz)\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{12.c. Linear Regression I_3_0}.png}

\sphinxAtStartPar
\sphinxstyleemphasis{Example 1:} Birthweight and gestational days appear to be highly correlated, where an increase in gestational days is associated with increased birthweight.

\sphinxAtStartPar
\sphinxstyleemphasis{Example 2:} It appears that mothers who do not smoke give birth to heavier babies, on average, than mothers who do smoke.


\section{12.2.2 Determining the dependent and independent variables}
\label{\detokenize{12.c. Linear Regression I:determining-the-dependent-and-independent-variables}}
\sphinxAtStartPar
Before defining a regression model, we have to decide which is the independent variable and which is the outcome (i.e. the dependent variable). In this context, it is natural to consider birthweight as the outcome: conceptually, it makes little sense to investigate how birthweight influences length of pregnancy or the mother’s smoking status.  However, it is not necessarily always as straightforward. Suppose we were investigating the association between age and weight. It is possible that we might be interested in age as a predictor of weight, or in weight as a predictor of age. The aim of the analysis will guide the choice of outcome.

\sphinxAtStartPar
While the outcome is the same in our two examples, an important difference is the type of independent variable. In Example 1, the independent variable (length of pregnancy) is a continuous variable, whereas in Example 2, the independent variable (mother’s smoking status) is binary (yes or no). Using these examples, we will later see how the two different types of variables are modelled differently in linear regression.


\section{12.3 The simple linear regression model}
\label{\detokenize{12.d. Linear Regression I:the-simple-linear-regression-model}}\label{\detokenize{12.d. Linear Regression I::doc}}
\sphinxAtStartPar
The equation for the simple linear regression model, relating \(X\) and \(Y\) is:
\begin{equation*}
\begin{split}
Y = \beta_0 + \beta_1 X + \epsilon 
\end{split}
\end{equation*}
\sphinxAtStartPar
There are two components of this model: the \sphinxstylestrong{linear predictor} and the \sphinxstylestrong{error term}. The linear predictor represents the variation in \(Y\) that can be predicted using the model: \(\beta_0 + \beta_1 X\). The error term, denoted by \(\epsilon\), represents the variation in \(Y\) that cannot be predicted (by a linear relationship with \(X\)). This variation is sometimes referred to as the \sphinxstylestrong{random error} or \sphinxstylestrong{noise}.

\sphinxAtStartPar
The subsequent two sections take a closer look at the linear predictor and error term, respectively.


\subsection{12.3.1 The linear predictor}
\label{\detokenize{12.d. Linear Regression I:the-linear-predictor}}
\sphinxAtStartPar
The linear predictor is an additive function of the independent variables. With a single variable, it is simply:
\begin{equation*}
\begin{split}
\beta_0 + \beta_1 X
\end{split}
\end{equation*}
\sphinxAtStartPar
In linear regression, the linear predictor represents the algebraic relationship between the mean of the outcome and the independent variable. When \(X\) takes a particular value, \(X=x\), the value of the linear predictor, \(\beta_0+\beta_1x\), is interpreted as the expected value of \(Y\) when \(X\) takes the value \(x\):
\begin{equation*}
\begin{split}
E(Y|X=x) = \beta_0 + \beta_1 x.
\end{split}
\end{equation*}
\sphinxAtStartPar
The specification of the linear predictor has two parameters: \(\beta_0\) and \(\beta_1\). These are interpreted as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\) is the intercept. It is the expected value of \(Y\) when \(X\) takes the value 0.

\item {} 
\sphinxAtStartPar
\(\beta_1\) is the slope (or gradient). It is the expected change in \(Y\) per one unit increase in \(X\).

\end{itemize}

\sphinxAtStartPar
It is worth emphasising that this model assumes that \sphinxstylestrong{the relationship between \(X\) and \(Y\) is linear}. It is important to note that it is possible to have more complex relationships between variables that do not meet this assumption (see examples in the plots below). When this is the case, simple linear regression would not be an appropriate method to use but we might be able to model the relationship well by including non\sphinxhyphen{}linear terms. We will pursue these ideas further in the next session.

\noindent\sphinxincludegraphics{{12.d. Linear Regression I_1_0}.png}


\subsection{12.3.2 The error term}
\label{\detokenize{12.d. Linear Regression I:the-error-term}}
\sphinxAtStartPar
The error term, \(\epsilon\) represents the variance in \(Y\) that cannot be predicted by the model. Individual values of the errors can be written as (Y \sphinxhyphen{}  (\(\beta_0 + \beta_1X\))). These errors cannot be observed, since they involve the unknown population parameters \(\beta_0\) and \(\beta_1\).

\sphinxAtStartPar
We assume that \(\epsilon\) has a normal distribution with mean 0 and variance \(\sigma^2\), where \(\sigma^2\) is termed the \sphinxstylestrong{residual variance} (i.e. the variance of the residuals):
\begin{equation*}
\begin{split}
\epsilon \sim N(0,\sigma^2)
\end{split}
\end{equation*}
\sphinxAtStartPar
Importantly, note that the errors must be independent of the independent variable \(X\).


\subsection{12.3.3 Different ways of expressing the simple linear regression model}
\label{\detokenize{12.d. Linear Regression I:different-ways-of-expressing-the-simple-linear-regression-model}}
\sphinxAtStartPar
Suppose we have a sample size of \(n\) and we let \(y_i\) and \(x_i\) \((i=1,...,n)\) denote the observed outcome and value of \(X\) for the \(i^{th}\) observation, respectively. Then, we can write the simple linear regression model as:
\begin{equation*}
\begin{split}
y_i = \beta_0 + \beta_1 x_{i}+ \epsilon_i \text{ where } \epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2).
\end{split}
\end{equation*}
\sphinxAtStartPar
Recall that \(iid\) means “identically, independently distributed”. A key assumption of linear regression model is that all of the observations are independent.

\sphinxAtStartPar
This relationship can equivalently be expressed using matrix algebra:
\begin{equation*}
\begin{split}
\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon} \text{ where }\epsilon \sim N(0,\mathbf{I}\sigma^2)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\begin{vmatrix}y_1\\y_2 \\. \\. \\. \\y_n \end{vmatrix}=\begin{vmatrix}1 & x_1 \\ 1 & x_2 \\1 & . \\1 & .  \\ 1& . \\1 & x_n \end{vmatrix}\begin{vmatrix} \beta_0 \\ \beta_1 \end{vmatrix}+\begin{vmatrix}\epsilon_1\\ \epsilon_2 \\ . \\ . \\. \\ \epsilon_n \end{vmatrix} 
\end{split}
\end{equation*}
\sphinxAtStartPar
In this formulation, \(\mathbf{X}\) is an \(n \times 2\) matrix, \(Y\) and \(\epsilon\) are vectors of length \(n\) whilst \(\beta\) is a vector of length 2.


\subsection{12.3.4 Assumptions}
\label{\detokenize{12.d. Linear Regression I:assumptions}}
\sphinxAtStartPar
It is worth emphasising the four key assumptions that we have made in the simple linear regression model:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linearity}: The relationship between \(X\) and the mean of \(Y\) is linear.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normality}: The errors follow a normal distribution.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Homoscedasticity}: The variance of error terms are constant across all values of \(X\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Independence}: All observations are independent of each other.

\end{enumerate}


\section{12.4 Estimation of the population parameters}
\label{\detokenize{12.e. Linear Regression I:estimation-of-the-population-parameters}}\label{\detokenize{12.e. Linear Regression I::doc}}
\sphinxAtStartPar
In the specification of the simple linear regression model there are three population parameters (\(\beta_0\), \(\beta_1\), and \(\sigma\)). Since we do not know these parameters, we need to estimate them based on a sample from our population. We will use the symbols \(\hat{\beta}_0\) \(\hat{\beta}_1\), and \(\hat{\sigma}\) to represent the sample estimates of the true population parameters.

\sphinxAtStartPar
There are many different methods available for obtaining estimates of the parameters \(\beta_0\) and \(\beta_1\). In this section, we focus on an approach that works by minimising the amount of error in the model. These estimates are called the \sphinxstylestrong{ordinary least squares estimates} (the reason for this name will become clear in the next section).


\subsection{12.4.1 Fitted values and residuals}
\label{\detokenize{12.e. Linear Regression I:fitted-values-and-residuals}}
\sphinxAtStartPar
\sphinxstylestrong{Fitted values:} Once we have estimates \(\hat{\beta}_0\) and \(\hat{\beta}_1\), the fitted value for the \(i{th}\) observation (in other words, the predicted value of the outcome for that individual) is:
\begin{equation*}
\begin{split}
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Residuals:} The residual for the \(i{th}\) observation is
\begin{equation*}
\begin{split}
\hat{\epsilon}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i) = y_i - \hat{y}_i
\end{split}
\end{equation*}
\sphinxAtStartPar
Sometimes, the word residual is used to refer to both the residual we have defined here and the error term, in which case it is necessary to distinguish between the true and fitted/estimated residual. Here, we use the term error to refer to the deviation of the observed value from the true mean outcome and we use the term residual to refer to the deviation of the observed value from the fitted value, as defined above.


\subsection{12.4.2 Ordinary least squares estimates}
\label{\detokenize{12.e. Linear Regression I:ordinary-least-squares-estimates}}
\sphinxAtStartPar
The ordinary least square (OLS) estimates are those which minimise the sum of squared deviations from the fitted regression line. The residuals, \(\hat{\epsilon}\), measure deviations of the observed outcomes from the fitted regression line. Therefore This sum is sometimes called the \sphinxstylestrong{residual sum of squares}. It is often denoted by \(SS_{RES}\) (where “SS” stands for Sum of Squares and “RES” is shorthand for RESiduals).

\sphinxAtStartPar
Formally, the OLS estimators are the values of \(\hat{\beta}_0\) and \(\hat{\beta}_1\) that minimise:
\begin{equation*}
\begin{split}
SS_{RES} = \sum_{i=1}^n \hat{\epsilon}_i^2 = \sum_{i=1}^n (y_i - \hat{\beta_0} -\hat{\beta_1}x_i)^2.
\end{split}
\end{equation*}
\sphinxAtStartPar
The ordinary least squares estimates of \(\beta_0\) and \(\beta_1\) are given by the following:
\begin{equation*}
\begin{split}
\begin{align}
\hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta_1} &= \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\bar{y}=\frac{\sum_{i=1}^n y_i}{n}\) and \(\bar{x} = \frac{\sum_{i=1}^n x_i}{n}\). A proof of this result is given at the end of this session.


\subsection{12.4.3 Estimation of the error variance}
\label{\detokenize{12.e. Linear Regression I:estimation-of-the-error-variance}}
\sphinxAtStartPar
The residual sum of squares can be thought of as the remaining unexplained variation in the outcome. Therefore, an intuitively appealing estimator of \(\sigma^2\) is given by dividing the residual sum of squares by the number of observations:
\begin{equation*}
\begin{split}
\hat{\sigma}^2 = \sum_{i=1}^n \frac{\hat{\epsilon}_i^2}{n} = \sum_{i=1}^n \frac{(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2}{n}
\end{split}
\end{equation*}
\sphinxAtStartPar
However, this is a biased estimator. The bias arises because the observed values tend, on average, to lie closer to the fitted line (defined by \(\hat{\beta}_0\) and \(\hat{\beta}_1\)) than they do to the true regression line (defined by \(\beta_0\) and \(\beta_1\)). This is an exact parallel to the way the variablility of a sample around its mean underestimates the variability around the population mean.

\sphinxAtStartPar
It can be shown that an unbiased estimator of the residual variance in the simple linear regression model is given by:
\begin{equation*}
\begin{split}
\hat{\sigma}^2  = \sum_{i=1}^n \frac{\hat{\epsilon_i}^2}{n-2}=\sum_{i=1}^n \frac{(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2}{n-2}
\end{split}
\end{equation*}
\sphinxAtStartPar
This quantity is referred to as the residual mean square. It is often denoted by \(MS_{RES}\), where “MS” stands for Mean Square and “RES” is shorthand for residual. The denominator is \((n-2)\) because fitting the model first requires the estimation of two parameters (\(\beta_0\) and \(\beta_1\)) and the estimation of these parameters is said to reduce the information about the variance by two degrees of freedom.


\subsection{12.4.4 Maximum likelihood estimation}
\label{\detokenize{12.e. Linear Regression I:maximum-likelihood-estimation}}
\sphinxAtStartPar
An alternative approach to estimating the model parameters is maximum likelihood estimation. This approach selects the estimates which maximise the likelihood (or equivalently, the log\sphinxhyphen{}likelihood) of the parameter values. It can be shown that the ordinary least square estimates for \(\beta_0\) and \(\beta_1\) are also the maximum likelihood estimates (a proof of this result is at the end of the session).

\sphinxAtStartPar
The maximum likelihood estimate of \(\sigma^2\) is equal to the biased estimate given above, obtained by dividing the residual sum of squares by the number of observations.


\section{12.5 Example: continuous independent variable}
\label{\detokenize{12.f. Linear Regression I:example-continuous-independent-variable}}\label{\detokenize{12.f. Linear Regression I::doc}}
\sphinxAtStartPar
We now return to our first example, where we are interested in investigating the association between birthweight and length of pregnancy. We will fit a linear model to explore this association.


\subsection{12.5.1 The model}
\label{\detokenize{12.f. Linear Regression I:the-model}}
\sphinxAtStartPar
The outcome is birthweight, which is measured in ounces (oz). The independent variable is length of pregnancy, \(L\) (i.e. number of gestational days).The following model defines our assumed relationship between the length of pregnancy (\(L\)) and a baby’s birthweight (\(Y\)):
\begin{equation*}
\begin{split}
\text{Model 1: }y_i = \beta_0 + \beta_1 l_i +  \epsilon_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
We will use the \sphinxcode{\sphinxupquote{lm()}} to perform simple linear regressions in R. Click \sphinxhref{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm}{here} for details of how this command works.

\sphinxAtStartPar
The following code can be used to perform this linear regression in R:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Model 1: Investigating the relationship between birthweight and length of pregancy}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}49.348 \PYGZhy{}11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)      \PYGZhy{}10.75414    8.53693   \PYGZhy{}1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 \PYGZsq{}***\PYGZsq{} 0.001 \PYGZsq{}**\PYGZsq{} 0.01 \PYGZsq{}*\PYGZsq{} 0.05 \PYGZsq{}.\PYGZsq{} 0.1 \PYGZsq{} \PYGZsq{} 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1661,	Adjusted R\PYGZhy{}squared:  0.1654 
F\PYGZhy{}statistic: 233.4 on 1 and 1172 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
There is a lot of information contained in this output. For the moment, we will focus on the estimates of the intercept and slope. These can be found under the column heading \sphinxcode{\sphinxupquote{Estimate}}.
\begin{itemize}
\item {} 
\sphinxAtStartPar
The estimated intercept, \(\hat{\beta}_0\) is equal to \sphinxhyphen{}10.75. This is interpreted as: the estimated mean birthweight of a child born after 0 gestational days is \sphinxhyphen{}10.75oz. Since there are no observations with 0 gestational days in the study, this is an extrapolation based on the observed data and an assumption of linearity. Estimates based on extrapolation should be interpreted with caution and in this case, the results make little sense because a negative birthweight is estimated. Moreover, no child is born after 0 gestational days and so this intercept is of little interest. Later on, we will discuss a technique called \sphinxstylestrong{centering} which is often used to make the intercept term more interpretable.

\item {} 
\sphinxAtStartPar
The estimated slope, \(\hat{\beta}_1\) is equal to 0.47. This is interpreted as: the mean birthweight of a baby is estimated to increase by 0.47oz for each daily increase in the gestational period.

\item {} 
\sphinxAtStartPar
The estimated residual standard error, \(\hat{\sigma}\) is equal to 16.74 (the residual variance is equal to \(16.74^2\)). This means that the observed outcomes are scattered around the fitted regression line with a standard deviation of 16.74oz.

\end{itemize}

\sphinxAtStartPar
It is always useful to look at the data. The code below graphs the data and superimposes the fitted regression line.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{with}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{Birth.Weight}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{12.f. Linear Regression I_4_0}.png}


\section{12.6 Inference for the slope}
\label{\detokenize{12.g. Linear Regression I:inference-for-the-slope}}\label{\detokenize{12.g. Linear Regression I::doc}}
\sphinxAtStartPar
Most commonly, we wish to conduct statistical inference on the estimated slope. Consequently, we focus our attention here on \(\beta_1\), but it is possible to apply the same methods to the intercept, \(\beta_0\).

\sphinxAtStartPar
For our statistical inference, we will view the values of the independent variable (\(x_1, x_2, ..., x_n\)) as fixed quantities.


\subsection{12.6.1 Sampling distribution of estimated slope}
\label{\detokenize{12.g. Linear Regression I:sampling-distribution-of-estimated-slope}}
\sphinxAtStartPar
Knowing the sampling distribution of \(\hat{\beta_1}\) allows us to perform hypothesis tests and construct confidence intervals for \(\beta_1\). Therefore, we now obtain that sampling distribution. The estimated slope, \(\hat{\beta}_1\), is a linear combination of the observed outcome values:
\begin{equation*}
\begin{split}
\begin{align}
\hat{\beta}_1 &= \sum_{i=1}^n \left(\frac{(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}(y_i-\bar{y})\right) 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can simplify this by substituting in \((y_i - \bar{y})=\beta_1(x_i-\bar{x})+(\epsilon_i-\bar{\epsilon})\), allowing us to write the estimated slope as a function of the random error (\(\epsilon\)):
\begin{equation*}
\begin{split}
\begin{align}
\hat{\beta}_1  &=\beta_1 + \sum_{i=1}^n \left(\frac{(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}(\epsilon_i-\bar{\epsilon})\right)
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Since the estimated parameter is a linear combination of the \(\epsilon_i\), and \(\epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2)\), the estimated parameter itself is normally distributed, with a distribution centred around the true value, \(\beta_1\). More specifically,
\begin{equation*}
\begin{split}
\begin{align}
\hat{\beta}_1  \sim N\left(\beta_1, \frac{\sigma^2}{SS_{xx}}\right), \qquad \mbox{with} \qquad  SS_{xx} = \sum_{i=1}^n(x_i-\bar{x})^2
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
So the standard error of the slope is \(SE(\hat{\beta}_1) = \sqrt{\frac{\sigma^2}{SS_{xx}}}\). The standard error (and therefore also the variance) of \(\hat{\beta}_1\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
increases with the size of the error variance (as might be expected intuitively),

\item {} 
\sphinxAtStartPar
decreases with increasing sample size (larger sample sizes give more precise estimates) and

\item {} 
\sphinxAtStartPar
decreases as \({SS}_{xx}^2\) increases (a wider range of \(x\) values leads to more precision in the slope estimate).

\end{itemize}

\sphinxAtStartPar
We have seen that the sampling distrbution of \(\hat{\beta}_1\) follows a normal distribution. We can convert this to a standard normal distribution:
\begin{equation*}
\begin{split}
\begin{align}
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\sigma^2/SS_{xx}}} \sim N(0,1) 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Of course, we do not know the true value of the error variance, \(\sigma\). When we replace this with our sample estimate, this changes the distribution above from a normal to a t\sphinxhyphen{}distribution. For large samples, these two distributions are very similar but for smaller samples the t\sphinxhyphen{}distribution has larger tails (suggesting that in smaller samples needing to estimate \(\sigma\) leads to more variability in the estimated slope, which makes sense intuitively).

\sphinxAtStartPar
Therefore, replacing \(\sigma\) by the estimate \(\hat{\sigma}\), we have:
\begin{equation*}
\begin{split}
\begin{align}
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\hat{\sigma}^2/SS_{xx}}} \sim t_{n-2}
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
This t\sphinxhyphen{}distribution is the one we will use to obtain p\sphinxhyphen{}values and confidence intervals for the slope parameter.


\subsection{12.6.2 Hypothesis testing}
\label{\detokenize{12.g. Linear Regression I:hypothesis-testing}}
\sphinxAtStartPar
Typically, we are interested in assessing whether there is a relationship, or association, between the independent variable \(X\) and outcome \(Y\). Recall our model: \(E[Y | X=x] = \beta_0 + \beta_1 X\). Within the framework of this linear model, no association between \(X\) and \(Y\) would be reflected by a value of \(\beta_1 = 0\).

\sphinxAtStartPar
Therefore, we are typically interested in testing the \sphinxstylestrong{null hypothesis} \(H_0: \beta_1=0\) against the alternative \(H_1: \beta_1 \neq 0\).

\sphinxAtStartPar
Under our null hypothesis, the following test statistic follows a t\sphinxhyphen{}distribution, as we saw above:
\begin{equation*}
\begin{split}
T = \frac{\hat{\beta}_1 - 0}{\sqrt{\hat{\sigma}^2/SS_{xx}}} \sim t_{n-2}
\end{split}
\end{equation*}
\sphinxAtStartPar
We now follow the familiar process from the session in hypothesis testing. We evaluate \(T\) in our particular sample and then calculate the probability of obtaining that value or one more extreme for a \(t\)\sphinxhyphen{}distribution with \(n-2\) degrees of freedom.

\sphinxAtStartPar
Notes
\begin{quote}

\sphinxAtStartPar
\(T\) is simply the estimate divided by its standard error. This is a familiar form of test statistic which we saw in the session about hypothesis testing.   
Although typically we are interested in the null hypothesis \(H_0: \beta_1=0\) we could use the same approach to test other null hypotheses, such as \(H_0: \beta_1=5\). However, it is rare that we are interested in values other than 0. Therefore, p\sphinxhyphen{}values outputted by statistical software arise from testing the null hypothesis value of 0 by default.
\end{quote}


\subsubsection{Example}
\label{\detokenize{12.g. Linear Regression I:example}}
\sphinxAtStartPar
Returning to our linear model relating birthweight to gestational days in the baby dataset, we will now test the null hypothesis that there is no association between gestational days and birthweight, i.e. \(H_0: \beta_1 = 0\) in Model 1.

\sphinxAtStartPar
First, we rerun the code to reproduce the linear model output.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}49.348 \PYGZhy{}11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)      \PYGZhy{}10.75414    8.53693   \PYGZhy{}1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1661,	Adjusted R\PYGZhy{}squared:  0.1654 
F\PYGZhy{}statistic: 233.4 on 1 and 1172 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
In the above output, the column \sphinxcode{\sphinxupquote{Std.Error}} gives the standard errors of the estimated intercept and slope.

\sphinxAtStartPar
The columns \sphinxcode{\sphinxupquote{t value}} and \sphinxcode{\sphinxupquote{Pr(>|t|)}} give the test statistic and associated \(p\)\sphinxhyphen{}value for a hypothesis test, testing the null hypothesis that \(H_0: \beta_0 = 0\) in the top row and \(H_0: \beta_1 = 0\) in the bottom row.

\sphinxAtStartPar
Note that the t\sphinxhyphen{}value for the slope (which we call \(T\) in the discussion above) is 15.28. You can check that this is equal to the estimate divided by the standard error (0.46656/0.03054 = 15.277014).

\sphinxAtStartPar
To test the null hypothesis that \(H_0: \beta_1=0\) against the alternative \(H_1:\beta_1 \neq  0\), the test statistic is 15.28 and the associated \(p\)\sphinxhyphen{}value is \(<2\times10^{-16}\). This is a very small \(p\)\sphinxhyphen{}value and therefore the data provide strong evidence against the null hypothesis. Based on these results, we can conclude that birthweight is associated with length of pregnancy.


\subsection{12.6.3 Confidence intervals for the regression coefficients}
\label{\detokenize{12.g. Linear Regression I:confidence-intervals-for-the-regression-coefficients}}
\sphinxAtStartPar
We saw above that:
\begin{equation*}
\begin{split}
\begin{align}
\frac{\hat{\beta}_1 - \beta_1}{\hat{SE}(\hat{\beta}_1) } \sim t_{n-2}
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\hat{SE}(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2/SS_{xx}}\). This leads to the following 95\% confidence interval for \(\beta_1\):
\begin{equation*}
\begin{split}
\hat{\beta}_1 \pm t_{0.025, n-2} \hat{SE}(\hat{\beta}_1)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(t_{0.025, n-2}\) is the 97.5\(^{th}\) percentile of a \(t\)\sphinxhyphen{}distribution with \(n-2\) degrees of freedom. For large samples, this value will be approximately 1.96. For smaller samples it will be a slightly larger number (reflecting additional imprecision in our estimate).

\sphinxAtStartPar
If \(n\) is sufficiently large, the t\sphinxhyphen{}distribution is well approximated by a normal distribution. In this case, a 95\% confidence interval can be found by:
\begin{equation*}
\begin{split}
\hat{\beta}_1 \pm 1.96 \times \hat{SE}(\hat{\beta}_1)
\end{split}
\end{equation*}
\sphinxAtStartPar
Example: Calculate a 95\% for  \(\hat{\beta}_1\)  (using the values given in the R output above):

\sphinxAtStartPar
Click the button to reveal the solution.

\sphinxAtStartPar
Solution:
\begin{equation*}
\begin{split}
\hat{\beta}_1 \pm 1.96 \times SE(\hat{\beta}_1) \\
0.46656 \pm 1.96 \times 0.03054
\end{split}
\end{equation*}
\sphinxAtStartPar
which gives a 95\% CI from 0.407 to 0.526.

\sphinxAtStartPar
The data are consistent with an increase in birthweight per daily increase in gestational age between 0.41oz and 0.52oz.

\sphinxAtStartPar
Since 0 does not lie within the interval, we conclude that there is evidence of an association between birthweight and length of pregancy (as also indicated by the results of the hypothesis test).

\sphinxAtStartPar
Alternatively, we can obtain confidence intervals using \sphinxcode{\sphinxupquote{confint}} in R. The option \sphinxcode{\sphinxupquote{parm}} tells R which regression coefficients to provide confidence intervals for. Try omitting this option or changing it to value 1 to see what happens.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Confidence intervals for the slope, beta\PYGZus{}1}
\PYG{n+nf}{confint}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{parm}\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 1 × 2 of type dbl
\begin{tabular}{r|ll}
  & 2.5 \% & 97.5 \%\\
\hline
	Gestational.Days & 0.4066435 & 0.5264702\\
\end{tabular}\end{split}
\end{equation*}

\section{12.7 Example: binary independent variable}
\label{\detokenize{12.h. Linear Regression I:example-binary-independent-variable}}\label{\detokenize{12.h. Linear Regression I::doc}}
\sphinxAtStartPar
We now return to our second example, where we are interested in the association between birthweight and the mother’s smoking status. In exploratory analyses, we saw that  mothers who do not smoke give birth to heavier babies, on average, than mothers who do smoke. We will now use a simple linear regression model to further explore this association.

\sphinxAtStartPar
The outcome variable is the same as for our previous example. A key difference, however, is that the independent variable is a binary variable.


\subsection{12.7.1 The model}
\label{\detokenize{12.h. Linear Regression I:the-model}}
\sphinxAtStartPar
We have a continuous outcome variable and a binary independent variable. To include this binary variable in the model, we create a \sphinxstylestrong{dummy} variable that takes the value 1 if the mother smokes and 0 if the mother doesn’t smoke:
\begin{equation*}
\begin{split} s_{i} = 
\begin{cases}
    1 & \text{ if the $i^{th}$ baby's mother smokes} \\
    0 & \text{ if the $i^{th}$ baby's mother does not smoke}
\end{cases} \end{split}
\end{equation*}
\sphinxAtStartPar
We then define the following linear regression model:
\begin{equation*}
\begin{split} 
\text{Model 2: } y_i = \beta_0 + \beta_1 s_i + \epsilon_i
\end{split}
\end{equation*}
\sphinxAtStartPar
When including binary (or categorical) variables in a linear regression in R, we can tell R to treat it as a factor variable using \sphinxcode{\sphinxupquote{factor()}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Example 2: Investigating the relationship between birthweight and mother\PYGZsq{}s smoking status.}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model2}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n+nf}{factor}\PYG{p}{(}\PYG{n}{Maternal.Smoker}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} factor(Maternal.Smoker), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}68.085 \PYGZhy{}11.085   0.915  11.181  52.915 

Coefficients:
                            Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)                 123.0853     0.6645 185.221   \PYGZlt{}2e\PYGZhy{}16 ***
factor(Maternal.Smoker)True  \PYGZhy{}9.2661     1.0628  \PYGZhy{}8.719   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 17.77 on 1172 degrees of freedom
Multiple R\PYGZhy{}squared:  0.06091,	Adjusted R\PYGZhy{}squared:  0.06011 
F\PYGZhy{}statistic: 76.02 on 1 and 1172 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\hat{\beta}_0 = 123.09\). This is interpreted as the estimated mean birthweight (in oz) of a baby with “dummy” variable equal to 0, i.e. it is the estimated mean birthweight of babies whose mothers do not smoke.

\item {} 
\sphinxAtStartPar
\(\hat{\beta}_1=-9.23\). The mean birthweight is estimated to decrease by 9.23oz per unit increase in the “dummy” variable. A unit increase in the dummy variable equates to moving from the non\sphinxhyphen{}smoking group to the smoking group, so we can interpret this as the difference in mean birthweights between the two groups.

\item {} 
\sphinxAtStartPar
\(\hat{\sigma}=17.77\). The observed outcomes are scattered around the fitted regression line with a standard deviation of 17.77oz.

\end{itemize}


\subsubsection{Exercises}
\label{\detokenize{12.h. Linear Regression I:exercises}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Perform a hypothesis test of the null hypothesis that there is no association between maternal smoking and birthweight. Write down the null hypothesis, the test statistic and the p\sphinxhyphen{}value. interpret your p\sphinxhyphen{}value.

\item {} 
\sphinxAtStartPar
Calculate (manually or using R) a 95\% confidence interval for the difference in mean birthweight between the group whose mothers smoke and those who don’t.

\end{enumerate}

\sphinxAtStartPar
Try the exercise and then click the button to reveal the solution.

\sphinxAtStartPar
Solution:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The null hypothesis is \(H_0: \beta_1 = 0\). The test statistic (from the R output) is \(T=-8.72\). We can check this is the ratio of the estimated slope to its standard error (\sphinxhyphen{}9.2661/1.0628 = \sphinxhyphen{}8.72). The p\sphinxhyphen{}value is \(p<2e_16\). In health applications, we typically write \(p<0.001\). We interpret this as strong evidence against the null hypothesis. Therefore, we can conclude there is an association between birthweight and maternal smoking.

\item {} 
\sphinxAtStartPar
This is a large sample, so an approxiate 95\% confidence interval can be obtained by the estimate plus\sphinxhyphen{}or\sphinxhyphen{}minus 1.96 times the standard error. This is:

\end{enumerate}
\begin{equation*}
\begin{split}
\begin{align*}
-9.2661 \pm 1.96 \times 1.0628  = 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
This gives an interval from \sphinxhyphen{}11.35 to \sphinxhyphen{}7.18.

\sphinxAtStartPar
Alternatively, in R:  \sphinxcode{\sphinxupquote{confint(model2, parm=2, level=0.95)}}


\section{12.8 Additional material}
\label{\detokenize{12.i. Linear Regression I:additional-material}}\label{\detokenize{12.i. Linear Regression I::doc}}
\sphinxAtStartPar
This section contains additional material concerning confidence intervals for a fitted value and reference ranges. These are useful topics in regression, but will not be examinable.


\subsection{12.8.1 Confidence intervals for a fitted value}
\label{\detokenize{12.i. Linear Regression I:confidence-intervals-for-a-fitted-value}}
\sphinxAtStartPar
So far we have only discussed conducting inference on the estimated regression coefficients. However, it may also be of interest to determine \sphinxstylestrong{confidence intervals for the fitted outcomes}, or \sphinxstylestrong{prediction intervals}. The subsequent two sections describe and illustrate these two concepts, respectively.

\sphinxAtStartPar
Rather than focusing on associations between variables and the outcome, we are sometimes interested in the expected value of the outcome at particular values of \(X\), i.e. \(y_x = E[Y | X=x] = \beta_0+\beta_1 x\).

\sphinxAtStartPar
The fitted value is our estimate of this quantity,
\begin{equation*}
\begin{split}
\hat{y}_x = \hat{\beta}_0 + \hat{\beta}_1 x.
\end{split}
\end{equation*}
\sphinxAtStartPar
The variance of the fitted value is given by:
\begin{equation*}
\begin{split}
V(\hat{y}_x) = \sigma^2 \left(\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}\right)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(SS_{xx}=\sum_{i=1}^n(x_i-\bar{x})^2\), i.e. the sum of squares of \(X\).

\sphinxAtStartPar
The 95\% confidence interval for the fitted value is given by:
\begin{equation*}
\begin{split}
\hat{y_x} \pm t_{n-2, 0.975}\hat{\sigma} \sqrt{\frac{1}{n}+ \frac{(x-\bar{x})^2}{SS_{xx}}}
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
95\% confidence intervals can be obtained for values of the independent variable that do not arise in the data. However, the width of the confidence interval increases with the distance from the mean (as can be seem from the formula and figure given below).

\item {} 
\sphinxAtStartPar
Care must be taken when extrapolating outside the range of the observed data as this makes an un\sphinxhyphen{}testable assumption that linearity continues outside the observed data range.

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Example}. The R code below calculates a 95\% confidence interval for the fitted value of birthweight of a baby born after 280 gestational days.


\subsubsection{Example}
\label{\detokenize{12.i. Linear Regression I:example}}
\sphinxAtStartPar
We return to our first example, exploring the association between birthweight and length of pregnancy (gestational days).Suppose we are interested in the expected birthweight for a baby who is born at 280 days’ gestation.

\sphinxAtStartPar
The code below refits our model and uses it to estimate the expected birthweight for this gestational age. It also provides a 95\% confidence interval around that estimate.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Refit model}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Confidence interval for a fitted value }
\PYG{n}{new.data}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{data.frame}\PYG{p}{(}\PYG{n}{Gestational.Days}\PYG{o}{=}\PYG{l+m}{280}\PYG{p}{)}
\PYG{n+nf}{predict}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{newdata}\PYG{o}{=}\PYG{n}{new.data}\PYG{p}{,} \PYG{n}{interval}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{confidence\PYGZdq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 1 × 3 of type dbl
\begin{tabular}{r|lll}
  & fit & lwr & upr\\
\hline
	1 & 119.8818 & 118.9215 & 120.8421\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
We estimate that the expected (average) birthweight for babies born at 280 days’ gestation is 119.9oz. The 95\% confidence interval for this estimate is is (118.9, 120.8). Informally, we can interpret this as: it is plausible that the true value of the expected birthweight, for babies born at 280 days’ gestation, lies between 118.9oz and 120.8oz.

\sphinxAtStartPar
We can extend this idea to graph the fitted values \sphinxhyphen{} estimated expected birthweight \sphinxhyphen{} and their confidence intervals across the range of gestational days. The code below does this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the fitted regression line (the fitted values)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Gestational days\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight (oz)\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{95\PYGZpc{} Confidence intervals\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add the confidence intervals for the fitted regression line}
\PYG{n}{conf\PYGZus{}interval}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{predict}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{newdata}\PYG{o}{=}\PYG{n}{data}\PYG{p}{,} \PYG{n}{interval}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{confidence\PYGZdq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{conf\PYGZus{}interval}\PYG{p}{[}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{conf\PYGZus{}interval}\PYG{p}{[}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{12.i. Linear Regression I_6_0}.png}

\sphinxAtStartPar
Notice how the confidence interval around the fitted line is narrowest in the centre of the x\sphinxhyphen{}axis, where most of our data are concentrated, and widest at the extremes.


\subsection{12.8.2 Prediction intervals}
\label{\detokenize{12.i. Linear Regression I:prediction-intervals}}
\sphinxAtStartPar
The 95\% confidence interval around the fitted line describes our certainty about where the fitted line is (i.e. where the expected value is).

\sphinxAtStartPar
Sometimes, we are not interested in the average outcome at a particular point, but the likely spread of values arount the average. In this case, we are interested in obtaining what is called a \sphinxstylestrong{prediction interval}, or \sphinxstylestrong{reference range}.

\sphinxAtStartPar
A 95\% prediction interval, or 95\% reference range, is an interval within which 95\% of future observations are expected to lie.

\sphinxAtStartPar
The predicted value for an individual with \(X=x\) is the fitted value, as above. However, there are now two sourcces of uncertainty to take into account. (1) There is uncertainty about the fitted value (the expected value), as above. (2) There is random error around that point (\(\sigma^2\)). Thus, the variance in the individual prediction is given by:
\begin{equation*}
\begin{split}
\begin{align*}
V(\hat{y_x}) + \sigma^2 &= \sigma^2 \left(\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}\right)+ \sigma^2 \\
 & = \sigma^2\left(1+\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}\right)
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
A 95\% prediction interval is then given by:
\begin{equation*}
\begin{split}
\hat{y_x} \pm t_{n-2, 0.975} \hat{\sigma} \sqrt{1+ \frac{1}{n}+ \frac{(x-\bar{x})^2}{S_{xx}}}
\end{split}
\end{equation*}

\subsubsection{Example}
\label{\detokenize{12.i. Linear Regression I:id1}}
\sphinxAtStartPar
The R code below calculates a 95\% prediction interval for the birthweight of babies who are born at 280 days’ gestation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Prediction interval}
\PYG{n+nf}{predict}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{newdata}\PYG{o}{=}\PYG{n}{new.data}\PYG{p}{,} \PYG{n}{interval}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{prediction\PYGZdq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 1 × 3 of type dbl
\begin{tabular}{r|lll}
  & fit & lwr & upr\\
\hline
	1 & 119.8818 & 87.01496 & 152.7486\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
The 95\% prediction interval for babies born at 280 days’ gestation is (87.0, 152.7). This means that we would expect 95\% of babies born after 280 gestational days to weigh between 87 and 152.7 ounces.


\subsection{12.8.3 Comparing intervals}
\label{\detokenize{12.i. Linear Regression I:comparing-intervals}}
\sphinxAtStartPar
The code below produces two scatterplots of gestational days against birthweight with the linear regression line of best fit (obtained from Model 1) superimposed. The blue lines on the left\sphinxhyphen{}hand side plot represent the 95\% confidence intervals for the fitted values across the entire range of gestational days. The blue lines on the right\sphinxhyphen{}hand side plot represent the 95\% prediction intervals.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Set the graphical space so that two plots are shown side\PYGZhy{}by\PYGZhy{}side in one row}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{8}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Confidence intervals for predicted values}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Gestational days\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight (oz)\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{95\PYGZpc{} Confidence intervals\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}

\PYG{n}{conf\PYGZus{}interval}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{predict}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{newdata}\PYG{o}{=}\PYG{n}{data}\PYG{p}{,} \PYG{n}{interval}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{confidence\PYGZdq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{conf\PYGZus{}interval}\PYG{p}{[}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{conf\PYGZus{}interval}\PYG{p}{[}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Reference ranges}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Gestational days\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight (oz)\PYGZdq{}}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{95\PYGZpc{} Prediction intervals\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}

\PYG{n}{conf\PYGZus{}interval}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{predict}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{newdata}\PYG{o}{=}\PYG{n}{data}\PYG{p}{,} \PYG{n}{interval}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{prediction\PYGZdq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{conf\PYGZus{}interval}\PYG{p}{[}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{lines}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{conf\PYGZus{}interval}\PYG{p}{[}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{12.i. Linear Regression I_13_0}.png}

\sphinxAtStartPar
As expected, we see that the prediction interval is much wider. Loosely speaking, the plot on the left shows a range of uncertainty about where the \sphinxstyleemphasis{average} line is. The plot on the right shows a range of uncertainty about where individual measurements will lie.


\chapter{13. Linear Regression II}
\label{\detokenize{13.a. Linear Regression II:linear-regression-ii}}\label{\detokenize{13.a. Linear Regression II::doc}}
\sphinxAtStartPar
This is the second of three sessions that explore linear regression modelling. These are models where the outcome of interest is a continuous variable.



\sphinxAtStartPar
By the end of this session, you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain the difference between a univariable and multivariable linear regression model

\item {} 
\sphinxAtStartPar
fit and interpret a multivariable linear regression

\item {} 
\sphinxAtStartPar
describe the principles of centering

\item {} 
\sphinxAtStartPar
interpret categorical variables, quadratic terms and interaction terms included in a linear regression model

\end{itemize}



\sphinxAtStartPar
\sphinxstylestrong{Acknowledgements:}  Thank you to Jennifer Nicholas and Chris Frost whose notes on linear regression were particularly useful in the development of the current lesson.


\section{13.1 Categorical independent variables}
\label{\detokenize{13.b. Linear Regression II:categorical-independent-variables}}\label{\detokenize{13.b. Linear Regression II::doc}}
\sphinxAtStartPar
We have explored simple linear regression with a continuous independent variable and with a binary variable. We now extend these ideas to include a categorical independent variable.

\sphinxAtStartPar
We will return to the baby example and use linear regression to explore the association between maternal Body Mass Index (BMI) category and the baby’s birthweight.


\subsection{13.1.1 Dummy variables}
\label{\detokenize{13.b. Linear Regression II:dummy-variables}}
\sphinxAtStartPar
We have height measured in inches and weight measured in pounds. BMI is obtained using the formula \(BMI = 703 \times weight (lb) / height (in)^2\). We will then categorise BMI according to the World Health Organisation’s classification.

\sphinxAtStartPar
We will define a categorical variable \(c_{i}\) denoting the \(i^{th}\) mother’s BMI category, defined as follows:
\begin{equation*}
\begin{split} 
c_{i} =
\begin{cases}
    1 & \text{ if the mother's BMI is less than 18.5 (underweight)} \\
    2 & \text{ if the mother's BMI is at least 18.5 and less than 25 (normal)} \\
    3 & \text{ if the mother's BMI is at least 25 and less than 30 (overweight)} \\
    4 & \text{ if the mother's BMI is 30 or more (obese)} 
\end{cases} 
\end{split}
\end{equation*}
\sphinxAtStartPar
Our BMI categorical variable has four categories. To distinguish between all four categories we need \sphinxstyleemphasis{three} dummy variables.  We choose a \sphinxstyleemphasis{baseline} or \sphinxstyleemphasis{reference} group, which for us will be the underweight category. For each of the other categories, we create a dummy variable which indicates that the woman is in (or not) that category. Specifically, we define our dummy variables as:
\begin{equation*}
\begin{split} 
w_{1i} =
\begin{cases}
    1 & \text{ if } c_{i}=2\\
    0 & \text{ if } c_{i} \neq 2
\end{cases} 
\end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split}
w_{2i} =
\begin{cases}
    1 & \text{ if } c_{i}=3\\
    0 & \text{ if } c_{i} \neq 3
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split} 
w_{3i} =
\begin{cases}
    1 & \text{ if } c_{i}=4\\
    0 & \text{ if } c_{i} \neq 4
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
The R code below read in the baby data and create variables containing the mother’s BMI (\sphinxcode{\sphinxupquote{Maternal.BMI}}) and the mother’s BMI category (\sphinxcode{\sphinxupquote{Maternal.BMIcat}}).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Calculate maternal BMI (with conversion factor due to measurement in lb and in)}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMI} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{l+m}{703}\PYG{o}{*}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Pregnancy.Weight}\PYG{o}{/}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m}{2}

\PYG{c+c1}{\PYGZsh{} Categorise the BMI values}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMIcat} \PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{1}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMIcat}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMI}\PYG{o}{\PYGZgt{}=}\PYG{l+m}{18.5} \PYG{o}{\PYGZam{}} \PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMI}\PYG{o}{\PYGZlt{}}\PYG{l+m}{25}\PYG{p}{]}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{2}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMIcat}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMI}\PYG{o}{\PYGZgt{}=}\PYG{l+m}{25}   \PYG{o}{\PYGZam{}} \PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMI}\PYG{o}{\PYGZlt{}}\PYG{l+m}{30}\PYG{p}{]}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{3}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMIcat}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMI}\PYG{o}{\PYGZgt{}=}\PYG{l+m}{30}\PYG{p}{]}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{4}

\PYG{c+c1}{\PYGZsh{} Tabulate the BMI categories}
\PYG{n+nf}{table}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.BMIcat}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  1   2   3   4 
 84 932 124  34 
\end{sphinxVerbatim}


\subsection{13.1.1 The model}
\label{\detokenize{13.b. Linear Regression II:the-model}}
\sphinxAtStartPar
A linear regression model relating birthweight (\(Y\), the outcome) to the three dummy variables (\(W_1, W_2, W_3\)) representing the mother’s BMI category (\(C\), the categorical independent variable) is defined as:
\begin{equation*}
\begin{split}
\text{Model 3: } y_i = \beta_0 + \beta_{1}w_{1,i} + \beta_{2}w_{2,i} + \beta_{3}w_{3,i}  + \epsilon_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
The equation above can also be written as follows:
\begin{equation*}
\begin{split}
\begin{align}
y_i &= \beta_0 + \epsilon_i \text{ if } c_i=1 \text{ (underweight mothers) }  \\
y_i &= \beta_0 + \beta_1 +  \epsilon_i \text{ if } c_i=2 \text{ (normal weight mothers) } \\
y_i &= \beta_0 + \beta_2 + \epsilon_i \text{ if } c_i=3 \text{ (overweight mothers) } \\
y_i &= \beta_0 + \beta_3 + \epsilon_i \text{ if } c_i=4 \text{ (obese mothers) }  \\
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
This makes explicit the interpretation of the parameters in the model.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\) is the expectation of \(Y\) when \(C=1\)

\item {} 
\sphinxAtStartPar
\(\beta_0 + \beta_1\) is the expectation of \(Y\) when \(C=2\). Hence \(\beta_1\) is the difference in the expectation of \(Y\) between groups defined by \(C=1\) and \(C=2\).

\item {} 
\sphinxAtStartPar
\(\beta_0 + \beta_2\) is the expectation of \(Y\) when \(C=3\). Hence \(\beta_2\) is the difference in the expectation of \(Y\) between groups defined by \(C=1\) and \(C=3\).

\item {} 
\sphinxAtStartPar
\(\beta_0 + \beta_3\) is the expectation of \(Y\) when \(C=4\). Hence \(\beta_3\) is the difference in the expectation of \(Y\) between groups defined by \(C=1\) and \(C=4\).

\end{itemize}

\sphinxAtStartPar
Notes
\begin{quote}

\sphinxAtStartPar
In this parameterisation of the model, the group defined by \(C=0\) is often referred to as the baseline group. There is no statistical reason why one group rather than another should be chosen as the baseline group. It can sometimes be desirable to re\sphinxhyphen{}parameterise a model of this type to estimate parameters representating differences in mean levels from a particular baseline group. In this example, for instance, we might instead want the group with normal weight to be our baseline group, in which case we would need to redefine our first dummy variable.  
Note that, in contrast to the models that we have met so far, this has more than one variable in it (even though the three dummy variables together measure a single characteristic). Therefore, this is no longer strictly a simple linear regression model. It is an example of a multivariable linear regression model. We will discuss general theory for this model later. Broadly speaking, the ideas we have met in the context of simple linear regression extend to this more general model very naturally.
\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Model 3: Relating birthweight to length of pregnancy and mother\PYGZsq{}s height group. }
\PYG{n}{model3}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n+nf}{factor}\PYG{p}{(}\PYG{n}{Maternal.BMIcat}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} factor(Maternal.BMIcat), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}64.828 \PYGZhy{}10.828   0.172  11.172  57.677 

Coefficients:
                         Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)               115.286      1.996  57.752   \PYGZlt{}2e\PYGZhy{}16 ***
factor(Maternal.BMIcat)2    4.543      2.084   2.180   0.0295 *  
factor(Maternal.BMIcat)3    3.037      2.585   1.175   0.2404    
factor(Maternal.BMIcat)4    8.626      3.719   2.320   0.0205 *  
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 18.3 on 1170 degrees of freedom
Multiple R\PYGZhy{}squared:  0.006152,	Adjusted R\PYGZhy{}squared:  0.003604 
F\PYGZhy{}statistic: 2.414 on 3 and 1170 DF,  p\PYGZhy{}value: 0.06511
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\hat{\beta}_0 = 115.29\). This is interpreted as the estimated mean birthweight (in oz) of a baby with all “dummy” variables equal to 0, i.e. it is the estimated mean birthweight of babies in our baseline category (those with mothers who are underweight).

\item {} 
\sphinxAtStartPar
\(\hat{\beta}_1= 4.543\). The mean birthweight is estimated to increase by 4.5 oz per unit increase in the first “dummy” variable. A unit increase in the first dummy variable equates to moving from the underweight group to the normal weight group. So we can interpret this as the difference in mean birthweights between the group whose mothers have normal BMI and those whose mothers are underweight.

\item {} 
\sphinxAtStartPar
\(\hat{\beta}_2= 3.037\). The mean birthweight is estimated to increase by 3.0 oz per unit increase in the second “dummy” variable. A unit increase in the second dummy variable equates to moving from the underweight group to the overweight group. So we can interpret this as the difference in mean birthweights between the group whose mothers are overweight and those whose mothers are underweight.

\item {} 
\sphinxAtStartPar
\(\hat{\beta}_3= 8.626\). The mean birthweight is estimated to increase by 8.6 oz per unit increase in the third “dummy” variable. A unit increase in the first dummy variable equates to moving from the underweight group to the obese group. So we can interpret this as the difference in mean birthweights between the group whose mothers are obese and those whose mothers are underweight.

\end{itemize}

\sphinxAtStartPar
Overall, we see a pattern of higher maternal BMI being associated with higher birthweights, particularly for the group with obese mothers.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\hat{\sigma}=18.3\). The observed outcomes are scattered around the fitted regression line with a standard deviation of 18.3oz.

\end{itemize}

\sphinxAtStartPar
We can obtain confidence intervals around the three BMI estimates as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{confint}\PYG{p}{(}\PYG{n}{model3}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m}{0.95}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 4 × 2 of type dbl
\begin{tabular}{r|ll}
  & 2.5 \% & 97.5 \%\\
\hline
	(Intercept) & 111.3691529 & 119.202276\\
	factor(Maternal.BMIcat)2 &   0.4533602 &   8.631864\\
	factor(Maternal.BMIcat)3 &  -2.0356770 &   8.109410\\
	factor(Maternal.BMIcat)4 &   1.3296865 &  15.922414\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
The R output from the model provides p\sphinxhyphen{}values for each of the three coefficients relating maternal BMI to birth weight. However, we are typically interested in the broad question of whether maternal BMI is related to birth weight, rather than whether an individual dummy variable is related to the outcome.

\sphinxAtStartPar
Therefore, when we have a categorical variable in a regression model, the hypothesis of interest usually relates to the combination of all dummy variables representing the categorical variable. An appropriate hypothesis test jointly tests the hypothesis that all coefficients for dummy variables are zero, i.e.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \beta_1 = 0, \beta_2 = 0, \beta_3 = 0\)

\item {} 
\sphinxAtStartPar
\(H_1: \text{at least one of } \ \beta_1, \beta_2, \beta_3 \neq 0\)

\end{itemize}

\sphinxAtStartPar
We use a partial \(F\)\sphinxhyphen{}test to test this hypothesis. Details are beyond the scope of the course, but are outlined in the appendix to this session. The R code to obtain the joint p\sphinxhyphen{}value is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Remove maternal BMI from the model (i.e. a constant\PYGZhy{}only model)}
\PYG{n}{model3\PYGZus{}without}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{l+m}{1}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}

\PYG{n+nf}{anova}\PYG{p}{(}\PYG{n}{model3}\PYG{p}{,} \PYG{n}{model3\PYGZus{}without}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A anova: 2 × 6
\begin{tabular}{r|llllll}
  & Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\
  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\
\hline
	1 & 1170 & 391633.5 & NA &        NA &       NA &         NA\\
	2 & 1173 & 394057.9 & -3 & -2424.344 & 2.414232 & 0.06510651\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
In this case we have a p\sphinxhyphen{}value of p=0.065, indicating some evidence against the null hypothesis of no association bewteen maternal BMI category and the baby’s birthweight.


\subsection{13.1.2 Categorising continuous variables}
\label{\detokenize{13.b. Linear Regression II:categorising-continuous-variables}}
\sphinxAtStartPar
In the above example, we have categorised an continuous variable (BMI) in order to demonstrate how a categorical variable should be included in a linear regression model. This is important to know, since there are many variables that are categorical by definition and may be required for a statistical analysis. For example: cancer stage, ethnicity, education level, etc. While these examples should be included as a categorical variable in a linear regression model, it is not, in general, recommended to categorise a continuous variable in a linear model. We did so above, purely for pedagogical reasons.

\sphinxAtStartPar
One of the problems with categorising continuous variables is that it is difficult to decide what the cut\sphinxhyphen{}off for each category should be. In the example above, however, there are widely used categorisations.

\sphinxAtStartPar
We often lose information by categorising continous variables. We can often obtain a better and more parsimonious fit (using fewer parameters to describe the relationship) by modelling the continuous variable without categorisation. We will return to these ideas later.


\section{13.2 Multivariable linear regression}
\label{\detokenize{13.c. Linear Regression II:multivariable-linear-regression}}\label{\detokenize{13.c. Linear Regression II::doc}}
\sphinxAtStartPar
Multivariable linear regression extends the simple linear regression model to situations in which we wish to relate two or more independent variables to one outcome. Where there are multiple independent variables, we will refer to them as \sphinxstylestrong{covariates}.

\sphinxAtStartPar
There can be a number of different reasons why we would want to add more covariates in our linear regression model. Recall these two examples of questions we might want to answer using statistical models (given at the beginning of this lesson):
\begin{itemize}
\item {} 
\sphinxAtStartPar
Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?

\item {} 
\sphinxAtStartPar
Can we predict the risk of heart disease for our patients?

\end{itemize}

\sphinxAtStartPar
In the first example, we could use a statistical model with inflammation as the outcome and drug use as the independent variable of interest, but we may need to \sphinxstylestrong{control} (or \sphinxstylestrong{adjust}) for the \sphinxstylestrong{confounding} effects of other patient characteristics (age, gender, other medication use, etc.). In the second example, there are many different factors that could be associated with risk of heart disease (age, gender, lifestyle choices, etc.) and we may wish to include all such factors in a statistical model to predict heart disease.

\sphinxAtStartPar
Here, we introduce the multivariable linear regression model and describe how to estimate and interpret the parameters in the model using an example from the birthweight data.

\sphinxAtStartPar
Note
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{A note on notation.} There can be some confusion between the terms \sphinxstylestrong{multivariable} models and \sphinxstylestrong{multivariate} models. Multivariate models are those which have more than one outcome variable. Such models are beyond the scope of this module; we focus our attention on \sphinxstylestrong{univariate} models which have only one outcome. Both simple linear regression models and multivariable linear regression models are considered as univariate.
\end{quote}


\subsection{13.2.1 The multivariable linear regression model}
\label{\detokenize{13.c. Linear Regression II:the-multivariable-linear-regression-model}}
\sphinxAtStartPar
Suppose we wish to relate an outcome (\(Y\)) to \(p\) predictor variables \((X_1, X_2, ..., X_p)\). The appropriate multivariable linear regression model is a straightforward extension of the simple linear regression model:
\begin{equation*}
\begin{split} 
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ,..., \beta_p x_{ip}+\epsilon_i \text{ with } \epsilon_i \overset{iid}{\sim} N(0,\sigma^2).
\end{split}
\end{equation*}
\sphinxAtStartPar
where, \(y_i\) is the value of the dependent variable for the ith participant and \(x_{ji}\) is the value of the jth predictor variable for the ith participant.

\sphinxAtStartPar
The parameters in the model are interpreted as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\) is the intercept. It is the expectation of \(Y\) when all the \(X_j\)’s are zero.

\item {} 
\sphinxAtStartPar
\(\beta_j\) is the expected change in \(Y\) for a 1 unit increase in \(X_j\) \sphinxstyleemphasis{with all the other covariates held constant}.

\end{itemize}

\sphinxAtStartPar
The \(\beta_j\)’s are the \sphinxstylestrong{regression coefficients} (otherwise known as \sphinxstylestrong{partial regression coefficients}). Each one measures the effect of one covariate controlled (or adjusted) for all of the others.


\subsection{13.2.2 The multivariable linear regression model in matrix notation}
\label{\detokenize{13.c. Linear Regression II:the-multivariable-linear-regression-model-in-matrix-notation}}
\sphinxAtStartPar
Similarly to the simple linear regression model, the multivariable linear regression model can be expressed using matrix algebra.
\begin{equation*}
\begin{split}
\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon} \text{ where }\epsilon \sim N(0,\mathbf{I}\sigma^2)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\begin{vmatrix}y_1\\y_2 \\. \\. \\. \\y_n \end{vmatrix}=\begin{vmatrix}1 & x_{11} & x_{12} & ... & x_{1p} \\ 1 & x_{21} & x_{22} & ... & x_{2p}  \\1 & . \\1 & .  \\ 1& . \\1 & x_{p1} & x_{p} & ... & x_{pn} \end{vmatrix}\begin{vmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ ... \\ \beta_p \end{vmatrix}+\begin{vmatrix}\epsilon_1\\ \epsilon_2 \\ . \\ . \\. \\ \epsilon_n \end{vmatrix} 
\end{split}
\end{equation*}
\sphinxAtStartPar
In this formulation, \(\mathbf{X}\) is an \(n \times (p+1)\) matrix, \(Y\) and \(\epsilon\) are vectors of length \(n\) whilst \(\mathbf{\beta}\) is a vector of length \((p+1)\).


\subsection{13.2.3 Estimation of the parameters}
\label{\detokenize{13.c. Linear Regression II:estimation-of-the-parameters}}
\sphinxAtStartPar
The regression coefficients in multivariable linear regression can be estimated by minimising the residual sum of squares:
\begin{equation*}
\begin{split}
\begin{align}
SS_{RES} &= \sum_{i=1}^n \hat{\epsilon}_i^2 = \sum_{i=1}^n (y_i-\hat{y})^2 \\
&= \sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}_1x_{1i}-...-\hat{\beta}_px_{pi})^2 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
The closed form solution, obtained by solving the \((p+1)\) simultaneous equations that result from setting the partial derivatives of the above equation with respect to each parameter estimate to zero, can be written succinctly using matrix notation:
\begin{equation*}
\begin{split}
\mathbf{\hat{\beta}}= (\mathbf{X'X})^{-1}X'Y
\end{split}
\end{equation*}
\sphinxAtStartPar
\(\mathbf{\hat{\beta}}\) is an unbiased estimator of \(\mathbf{\beta}\). Its distribution is as follows:
\begin{equation*}
\begin{split}
\mathbf{\hat{\beta}} \sim \mathbf{N(\beta, (X'X)^{-1}\sigma^2)}.
\end{split}
\end{equation*}
\sphinxAtStartPar
This expresses the fact that the elements of \(\mathbf{\hat{\beta}}\) follow a multivariate normal distribution whose variances and covariances are given by \(\mathbf{(X'X)^{-1}\sigma^2}\).

\sphinxAtStartPar
It can also be shown that the following is an unbiased estimator for \(\sigma^2\):
\begin{equation*}
\begin{split}
\begin{align}
\hat{\sigma}^2 &= \sum_{i=1}^n \frac{\hat{\epsilon_i}^2}{(n-(p+1))}\\
              &=\sum_{i=1}^n \frac{(y_i - \hat{\beta_0} - \hat{\beta}_1x_{1i}- ... - \hat{\beta_p}x_{ip})^2}{(n-(p+1))}
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
While it is useful to know how these parameters are estimated, in practice they are often obtained using statistical software. Next, we demonstrate how to perform multivariable regression in R using the birthweight data and discuss the interpretation of the estimated regression coefficients.


\section{13.3 Including multiple covariates}
\label{\detokenize{13.d. Linear Regression II:including-multiple-covariates}}\label{\detokenize{13.d. Linear Regression II::doc}}
\sphinxAtStartPar
We are interested in investigating a model that relates birthweight to length of pregnancy and mother’s height. We will use the following multivariable linear regression model:
\begin{equation*}
\begin{split}
\text{Model 4: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i  + \epsilon_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
The outcome \(y_i\) denotes the birthweight (in oz) for the \(i^{th}\) baby. The predictors \(l_i\) and \(h_i\) denote the length of pregnancy (i.e. number of gestational days), and the height of the mother (in inches), for the \(i^{th}\) baby, respectively.

\sphinxAtStartPar
The linear regression can be conducted in R using the \sphinxcode{\sphinxupquote{lm()}} command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Model 4: Relating birthweight to length of pregnancy and mother\PYGZsq{}s height}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model4}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{o}{+}\PYG{n}{Maternal.Height}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days + Maternal.Height, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}53.829 \PYGZhy{}10.589   0.246  10.254  54.403 

Coefficients:
                  Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)      \PYGZhy{}88.51993   14.31910  \PYGZhy{}6.182 8.73e\PYGZhy{}10 ***
Gestational.Days   0.45237    0.03006  15.051  \PYGZlt{} 2e\PYGZhy{}16 ***
Maternal.Height    1.27598    0.19049   6.698 3.27e\PYGZhy{}11 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1969,	Adjusted R\PYGZhy{}squared:  0.1955 
F\PYGZhy{}statistic: 143.5 on 2 and 1171 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Interpretation of the regression coefficients}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\hat{\beta}_1=0.45\). This is the estimated regression coefficient for number of gestational days. It is interpreted as: the expected increase in a baby’s birthweight for each gestational day, \sphinxstyleemphasis{amongst babies whose mothers were of the same height}, is 0.45 ounces.

\end{itemize}

\sphinxAtStartPar
It may be tempting to make causal inference from regression models such as Model 4, i.e. “longer pregnancies \sphinxstylestrong{cause} an increase in birthweight”. However, this is far from straightforward. Based on the results presented above, it would be reasonable to say that “birthweight increases with length of pregnancy”. However, it is much less reasonable to claim that higher birthweight is caused by longer pregnancies (based on these results alone), because there may be an unobserved third variable that is the “real” cause of both increased length of pregancy and birthweight. Causal statements require more than just the results of a statistical model to make them plausible; this is a topic that we return to in the next lesson.

\sphinxAtStartPar
\sphinxstyleemphasis{Excerise}: What is the interpretation of \(\hat{\beta}_2?\)

\sphinxAtStartPar
\sphinxstylestrong{Interpretation of the intercept}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\hat{\beta}_0=-88.52\). The interpretation is that the estimated mean birthweight for a child who was born after 0 gestastional days and whose mother’s height is 0 inches is \sphinxhyphen{}88.52 ouces. Clearly this is an absurd value to estimate because no babies are born that quickly and no mothers are that short. If we wish to obtain a more reasonable intercept, we can use a technique called \sphinxstylestrong{centering}.

\end{itemize}


\section{13.4 Centering}
\label{\detokenize{13.e. Linear Regression II:centering}}\label{\detokenize{13.e. Linear Regression II::doc}}
\sphinxAtStartPar
In many analyses, interpreting the intercept is not as important as interpreting the estimated regression coefficients and so it does not matter if our intercept is an absurd value (as in the example above). However, if we do wish to obtain an interpretable intercept, we can \sphinxstylestrong{center} the independent variables.

\sphinxAtStartPar
Centering a variable means subtracting a constant from every value of the  variable. This essentially shifts the scale of the predictor (the point 0 is shifted to the chosen constant), but does not affect the units of the variable. Consequently, the new interpretation of the intercept would be the mean of \(Y\) when the independent variable is equal to the constant. The estimated regression coefficient of the independent variable is not affected.

\sphinxAtStartPar
As as example, we will repeat the analysis above, but center each of the covariates on their mean value.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} What are the mean gestational days and mothers height in our data?}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create new (centered) variables in our data}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days.Centered}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height.Centered}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Redefine Model 4 using the centered variables}
\PYG{n}{model4}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days.Centered}\PYG{o}{+}\PYG{n}{Maternal.Height.Centered}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  148.0   272.0   280.0   279.1   288.0   353.0 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  53.00   62.00   64.00   64.05   66.00   72.00 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days.Centered + Maternal.Height.Centered, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}53.829 \PYGZhy{}10.589   0.246  10.254  54.403 

Coefficients:
                           Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)               119.46252    0.47980 248.983  \PYGZlt{} 2e\PYGZhy{}16 ***
Gestational.Days.Centered   0.45237    0.03006  15.051  \PYGZlt{} 2e\PYGZhy{}16 ***
Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e\PYGZhy{}11 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 \PYGZsq{}***\PYGZsq{} 0.001 \PYGZsq{}**\PYGZsq{} 0.01 \PYGZsq{}*\PYGZsq{} 0.05 \PYGZsq{}.\PYGZsq{} 0.1 \PYGZsq{} \PYGZsq{} 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1969,	Adjusted R\PYGZhy{}squared:  0.1955 
F\PYGZhy{}statistic: 143.5 on 2 and 1171 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
Now the intercept (\(\hat{\beta}_0\)) is equal to 119.46. This is interpreted as: the estimated mean birthweight for a child who was born after 279.1 gestastional days and whose mother’s height is 64.05 inches is 119.46 ouces. Additionally, notice that the estimated regression coefficients for gestational days and mother’s height, and their associated standard errors have not changed.


\section{13.6  Including higher\sphinxhyphen{}order terms}
\label{\detokenize{13.f. Linear Regression II:including-higher-order-terms}}\label{\detokenize{13.f. Linear Regression II::doc}}
\sphinxAtStartPar
As we have already discussed, linear regression assumes that the relationship between the outcome and the independent variables is linear. As we already know, this is not always the case in real data. For example, suppose we are interested in the association between weight and age. On average, the weight of young adults will increase with age. However, at a certain age, the average weight may start to decrease. In this case, the association between weight and age would follow a non\sphinxhyphen{}linear (upside\sphinxhyphen{}down) \(u\)\sphinxhyphen{}shape. It could still be possible to model this relationship within the linear regression framework, by adding a \sphinxstylestrong{second\sphinxhyphen{}order term} to the model. This procedure is known as \sphinxstylestrong{quadratic regression}.


\subsection{13.6.1 The quadratic regression model}
\label{\detokenize{13.f. Linear Regression II:the-quadratic-regression-model}}
\sphinxAtStartPar
The quadratic regression model is a multivariable regression model with two independent variables where the second variable is the square of the first variable. Algebraically:
\begin{equation*}
\begin{split}
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon \text{ where }  \epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2).
\end{split}
\end{equation*}
\sphinxAtStartPar
Despite the fact that one of the variables is the square of the other, this is still a linear regression model because the expectation of the outcome is a linear function of both parameters.

\sphinxAtStartPar
The figure below shows two scatter plots of the data used in Scenario A above. The plot on the left\sphinxhyphen{}hand side includes the fitted values of a linear regression model (with no higher\sphinxhyphen{}order terms included) and the right\sphinxhyphen{}hand side plot includes the fitted values of a quadratic regression model. By comparing the plots, we can see that the quadratic regression model does have a better fit, particularly at the extreme values of \(X\).

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=600\sphinxpxdimen]{{quadratic_example}.png}
\end{figure}

\sphinxAtStartPar
Unfortunately, interpreting \(\beta_1\) and \(\beta_2\) is not as straightforward as in most linear models. The reason for this is that it is not possible to change \(X^2\) by 1 unit whilst holding \(X\) constant.


\subsection{13.6.2 Example}
\label{\detokenize{13.f. Linear Regression II:example}}
\sphinxAtStartPar
Suppose the outcome, birthweight, is denoted \(Y\) and length of pregnancy (gestational days) is denoted by \(L\). The original linear model we considered was:
\begin{equation*}
\begin{split}
\text{Model 1: } y_i = \beta_0 + \beta_1 l_i + \epsilon_i
\end{split}
\end{equation*}
\sphinxAtStartPar
We now extend this to allow a quadratic relationship between \(L\) and \(Y\):
\begin{equation*}
\begin{split}
\text{Model 5: } y_i = \beta_0 + \beta_1 l_i + \beta_2 l^2_i + \epsilon_i
\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} What are the mean gestational days and mothers height in our data?}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add quadratic term:}
\PYG{n}{model5}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{o}{+}\PYG{n+nf}{I}\PYG{p}{(}\PYG{n}{Gestational.Days}\PYG{o}{*}\PYG{o}{*}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model5}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days + I(Gestational.Days\PYGZca{}2), 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}49.527 \PYGZhy{}10.980   0.190   9.973  69.655 

Coefficients:
                        Estimate Std. Error t value Pr(\PYGZgt{}|t|)  
(Intercept)           \PYGZhy{}6.901e+01  5.043e+01  \PYGZhy{}1.368    0.171  
Gestational.Days       8.962e\PYGZhy{}01  3.678e\PYGZhy{}01   2.436    0.015 *
I(Gestational.Days\PYGZca{}2) \PYGZhy{}7.890e\PYGZhy{}04  6.731e\PYGZhy{}04  \PYGZhy{}1.172    0.241  
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 \PYGZsq{}***\PYGZsq{} 0.001 \PYGZsq{}**\PYGZsq{} 0.01 \PYGZsq{}*\PYGZsq{} 0.05 \PYGZsq{}.\PYGZsq{} 0.1 \PYGZsq{} \PYGZsq{} 1

Residual standard error: 16.74 on 1171 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1671,	Adjusted R\PYGZhy{}squared:  0.1656 
F\PYGZhy{}statistic: 117.4 on 2 and 1171 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
The estimated coefficient for the quadratic term is very small (\sphinxhyphen{}0.000789). The p\sphinxhyphen{}value, testing the null hypothesis that \(H_0: \beta_2 = 0\) is p=0.241, suggesting no evidence against the null hypothesis. Therefore, we conclude there is no evidence of a quadratic relationship.


\subsection{13.6.3 More complex models}
\label{\detokenize{13.f. Linear Regression II:more-complex-models}}
\sphinxAtStartPar
Quadratic regression models are limited in terms of describing relationships between variables in most medical applications. Quadratic functions either increase to a maximum and the decline, or fall to a minimum and then increase. Further, the behaviour of a quadratic is symmetric about the turning point. Such relationships in medical research are rarely plausible.

\sphinxAtStartPar
There are a number of alternative approaches that can be used to model complex relationships between continuous independent variables and the outcome within a linear regression model. Some are discussed below.

\sphinxAtStartPar
The quadratic regression model belongs to a family of \sphinxstylestrong{polynomial regression models} and is the simplest model in that family. Further power terms can be added to the regression model in order to increase complexity. For example, a cubic regression model is one which includes a cubic term as well as a squared term.

\sphinxAtStartPar
An even more flexible approach is to use a \sphinxstylestrong{piecewise polynomial model}, which allows for a different polynomial function in different ranges of the observed values of \(X\), defined according to specified \sphinxstylestrong{knots}. The flexibility of the model (and therefore its ability to model more complex relationships) can be increased by increasing the degree of polynomial and/or the number of knots. However, highly flexible models may overfit the data and make the model difficult to interpret. In general, it is a good idea to consider an appropriate trade\sphinxhyphen{}off between flexibility and interpretability

\sphinxAtStartPar
A related idea is to use \sphinxstylestrong{splines} to flexibly model the relationship. These are a type of piecewise polynomial model, where the adjacent polynomials are constrained to meet at the join points (the knots).


\section{13.7  Modelling interaction terms}
\label{\detokenize{13.g. Linear Regression II:modelling-interaction-terms}}\label{\detokenize{13.g. Linear Regression II::doc}}
\sphinxAtStartPar
Suppose we fit a multivariable linear regression model relating the outcome of weight to the covariates age, sex and height, for adults in the general population. In this case, the estimated regression coefficient for height represents the effect of a unit increase in height on weight in people of the same age and sex. The model assumes that the coefficient relating weight to height is the same for all people of all ages and sexes. For example, that it is the same for twenty year old men as in ninety\sphinxhyphen{}three year old women. But this is not necessarily true! It could be that the slope of the association between weight and height differs by sex and by age. If this is the case, we say there is an \sphinxstylestrong{interaction} between height and sex and between height and age.

\sphinxAtStartPar
The term \sphinxstylestrong{interaction} is used to describe situations in which the relationship between \(Y\) and \(X\) differs according to the level of one or more other covariates.


\subsection{13.7.1 Linear regression with an interaction term}
\label{\detokenize{13.g. Linear Regression II:linear-regression-with-an-interaction-term}}
\sphinxAtStartPar
Suppose we wish to relate an outcome (\(Y\)) to two covariates (\(X_1\) and \(X_2\)), but we want to allow the association between \(Y\) and \(X_1\) to differ according to the value of \(X_2\). To allow for this we fit an interaction model that contains an additional variable (\(X_3\)) that is the product of \(X_1\) and \(X_2\):
\begin{equation*}
\begin{split}
y_i =  \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i \text{ where }  \epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2).
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
\(y_i\) = value of the outcome for the \(i^{th}\) observation

\item {} 
\sphinxAtStartPar
\(x_{1i}\) = value of the first covariate for the \(i^{th}\) observation

\item {} 
\sphinxAtStartPar
\(x_{2i}\) =  value of the second covariate for the \(i^{th}\) observation

\item {} 
\sphinxAtStartPar
\(x_{3i}\) = \(x_{1i} \times x_{2i}\)

\end{itemize}

\sphinxAtStartPar
To understand why this model allows the association between \(Y\) and \(X_1\) to vary according to \(X_2\), we can consider the form of the equation when we fix \(X_2\) to have a particular value, say \(X_2=k\). In this situation the relationship between \(Y\) and \(X_1\) is as follows:
\begin{equation*}
\begin{split}
y_i = (\beta_0 + \beta_2k) + (\beta_1 + \beta_3k)x_{1i} + \epsilon_i.
\end{split}
\end{equation*}
\sphinxAtStartPar
In other words, when \(x_2=k\) the relationship between \(Y\) and \(X_1\) is a linear one with both slope and intercept dependent upon \(k\). The intercept is \(\beta_0+\beta_2k\) and the slope is \(\beta_1 +  \beta_3k\).

\sphinxAtStartPar
By allowing the association between \(Y\) and \(X_1\) to vary according to \(X_2\), we have also allowed the slope for the association between \(Y\) and \(X_2\) to vary according to \(X_1\). If we look at the form of the model when \(X_1\) takes particular value, say \(X_1=m\), we find:
\begin{equation*}
\begin{split}
y_i = (\beta_0+\beta_1m) + (\beta_2+\beta_3m)x_{2i} + \epsilon_i.
\end{split}
\end{equation*}
\sphinxAtStartPar
Again, the relationship between \(Y\) and \(X_2\) is a linear one with both slope and intercept dependent upon \(m\).


\subsection{13.7.2 Interaction between a continuous variable and a binary variable}
\label{\detokenize{13.g. Linear Regression II:interaction-between-a-continuous-variable-and-a-binary-variable}}
\sphinxAtStartPar
The interaction model is particularly easy to interpret when one of the covariates (say \(X_2\)) is a binary, taking the values 0 and 1 (i.e. a dummy variable). The linear regression model then becomes:
\begin{equation*}
\begin{split}
\begin{align}
&y_i = \beta_0 + \beta_1x_{1i} + \epsilon_i \text{ when } x_2=0\\ 
&y_i = (\beta_0 + \beta_2) + (\beta_1+\beta_3)x_{1i} + \epsilon_i \text{ when } x_2=1 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
The interpretation of each of the parameters is as follows.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\) is the intercept when \(X_2=0\)

\item {} 
\sphinxAtStartPar
\(\beta_0 + \beta_2\) is the intercept when \(X_2=1\)

\item {} 
\sphinxAtStartPar
\(\beta_2\) is the difference in intercepts between the two groups defined by \(X_2\).

\item {} 
\sphinxAtStartPar
\(\beta_1\) is the slope when \(X_2=0\)

\item {} 
\sphinxAtStartPar
\(\beta_1+\beta_3\) is the slope when \(X_2=1\)

\item {} 
\sphinxAtStartPar
\(\beta_3\) is the difference in slopes between the two groups defined by \(X_2\).

\end{itemize}

\sphinxAtStartPar
\sphinxstyleemphasis{Example.} To demonstrate the impact of adding an interaction term, we will consider two models: (1) relating birthweight (\(Y\)) to length of pregnancy (\(X_1\)) and mother’s smoking status (\(X_2\)) and (2) relating birthweight (\(Y\)) to length of pregnancy (\(X_1\)), mother’s smoking status (\(X_2\)) and their interaction (\(X_3\)). In these models, \(X_2=1\) indicates that the mother smokes and \(X_2=0\) indicates that the mother does not smoke.

\sphinxAtStartPar
We first consider the model with no interaction term. The code below defines the model in R, summarises the results and produces a scatter plot of birthweight against gestational days, with the fitted values superimposed. The blue points (and line) on the scatter plot are observations in the group of babies whose mothers do not smoke and the red points (and line) are observations in the group of babies whose mothers do smoke.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}Define a dummy variable for maternal.smoker}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{0}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker}\PYG{o}{==}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{True\PYGZdq{}}\PYG{p}{]}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{1}

\PYG{c+c1}{\PYGZsh{}Model without the interaction term}
\PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{o}{+}\PYG{n+nf}{factor}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Scatterplot}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{0}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{0}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} 
     \PYG{n}{pch} \PYG{o}{=} \PYG{l+m}{19}\PYG{p}{,} \PYG{n}{xlab} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Gestational days\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight\PYGZdq{}}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{n+nf}{rgb}\PYG{p}{(}\PYG{n}{red} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{green} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{blue} \PYG{o}{=} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{alpha} \PYG{o}{=} \PYG{l+m}{0.25}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{a} \PYG{o}{=} \PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{o}{\PYGZdl{}}\PYG{n}{coefficients}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{o}{\PYGZdl{}}\PYG{n}{coefficients}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd} \PYG{o}{=} \PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{points}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{1}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{1}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} 
       \PYG{n}{col} \PYG{o}{=} \PYG{n+nf}{rgb}\PYG{p}{(}\PYG{n}{red} \PYG{o}{=} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{green} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{blue} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{alpha} \PYG{o}{=} \PYG{l+m}{0.25}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pch} \PYG{o}{=} \PYG{l+m}{19}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{a} \PYG{o}{=} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]} \PYG{o}{+} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{no\PYGZus{}int\PYGZus{}model}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,} 
       \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd} \PYG{o}{=} \PYG{l+m}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = data\PYGZdl{}Birth.Weight \PYGZti{} data\PYGZdl{}Gestational.Days + factor(data\PYGZdl{}Maternal.Smoker2))

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}50.789 \PYGZhy{}11.035  \PYGZhy{}0.211  10.053  52.412 

Coefficients:
                               Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)                    \PYGZhy{}3.18492    8.32945  \PYGZhy{}0.382    0.702    
data\PYGZdl{}Gestational.Days           0.45117    0.02968  15.200   \PYGZlt{}2e\PYGZhy{}16 ***
factor(data\PYGZdl{}Maternal.Smoker2)1 \PYGZhy{}8.37440    0.97346  \PYGZhy{}8.603   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.25 on 1171 degrees of freedom
Multiple R\PYGZhy{}squared:  0.2157,	Adjusted R\PYGZhy{}squared:  0.2143 
F\PYGZhy{}statistic:   161 on 2 and 1171 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{13.g. Linear Regression II_3_1}.png}

\sphinxAtStartPar
As can be seen from the above figure, the fitted values from the model with no interaction term form two straight lines with a common slope 0.45 ounces and intercepts \sphinxhyphen{}3.18 ounces for the non\sphinxhyphen{}smoking group and \sphinxhyphen{}3.18\sphinxhyphen{}8.37=\sphinxhyphen{}11.55 ounces for the smoking group. This type of model (no interactions) is sometimes known as a \sphinxstylestrong{parallel lines} regression model, because it restricts the lines to be parallel. It permits adjustment of the effect of one covariate for the effects of others, but forces the effects of a unit change in each covariate to be constant, whatever the level of the other covariate. This restriction is not appropriate if the slope effect of one covariate depends on the value of another covariate. Adding an interaction term removes this restriction.

\sphinxAtStartPar
Below, we fit the second model which includes an interaction term, and produce a second scatter plot with the fitted values from our new model superimposed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Create the interaction term}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{int1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{o}{*}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2}

\PYG{c+c1}{\PYGZsh{}Include the interaction term in our model}
\PYG{n}{int\PYGZus{}model1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{o}{+}\PYG{n+nf}{factor}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2}\PYG{p}{)}\PYG{o}{+}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{int1}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{int\PYGZus{}model1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Scatter plot}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{0}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{0}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} 
     \PYG{n}{pch} \PYG{o}{=} \PYG{l+m}{19}\PYG{p}{,} \PYG{n}{xlab} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Gestational days\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Birthweight\PYGZdq{}}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{n+nf}{rgb}\PYG{p}{(}\PYG{n}{red} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{green} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{blue} \PYG{o}{=} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{alpha} \PYG{o}{=} \PYG{l+m}{0.25}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{a} \PYG{o}{=} \PYG{n}{int\PYGZus{}model1}\PYG{o}{\PYGZdl{}}\PYG{n}{coefficients}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{n}{int\PYGZus{}model1}\PYG{o}{\PYGZdl{}}\PYG{n}{coefficients}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{blue\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd} \PYG{o}{=} \PYG{l+m}{2}\PYG{p}{)}
\PYG{n+nf}{points}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{1}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Smoker2} \PYG{o}{==} \PYG{l+m}{1}\PYG{p}{,} \PYG{p}{]}\PYG{o}{\PYGZdl{}}\PYG{n}{Birth.Weight}\PYG{p}{,} 
       \PYG{n}{col} \PYG{o}{=} \PYG{n+nf}{rgb}\PYG{p}{(}\PYG{n}{red} \PYG{o}{=} \PYG{l+m}{1}\PYG{p}{,} \PYG{n}{green} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{blue} \PYG{o}{=} \PYG{l+m}{0}\PYG{p}{,} \PYG{n}{alpha} \PYG{o}{=} \PYG{l+m}{0.25}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pch} \PYG{o}{=} \PYG{l+m}{19}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{a} \PYG{o}{=} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{int\PYGZus{}model1}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]} \PYG{o}{+} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{int\PYGZus{}model1}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{int\PYGZus{}model1}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{2}\PYG{p}{]} \PYG{o}{+} \PYG{n+nf}{coef}\PYG{p}{(}\PYG{n}{int\PYGZus{}model1}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{4}\PYG{p}{]}\PYG{p}{,} 
       \PYG{n}{col} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{red\PYGZdq{}}\PYG{p}{,} \PYG{n}{lwd} \PYG{o}{=} \PYG{l+m}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = data\PYGZdl{}Birth.Weight \PYGZti{} data\PYGZdl{}Gestational.Days + factor(data\PYGZdl{}Maternal.Smoker2) + 
    data\PYGZdl{}int1)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}51.023 \PYGZhy{}11.078  \PYGZhy{}0.084   9.995  50.499 

Coefficients:
                                Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)                     19.63964   10.29098   1.908 0.056580 .  
data\PYGZdl{}Gestational.Days            0.36962    0.03671  10.069  \PYGZlt{} 2e\PYGZhy{}16 ***
factor(data\PYGZdl{}Maternal.Smoker2)1 \PYGZhy{}72.68713   17.23243  \PYGZhy{}4.218 2.65e\PYGZhy{}05 ***
data\PYGZdl{}int1                        0.23085    0.06176   3.738 0.000194 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.16 on 1170 degrees of freedom
Multiple R\PYGZhy{}squared:  0.2249,	Adjusted R\PYGZhy{}squared:  0.2229 
F\PYGZhy{}statistic: 113.2 on 3 and 1170 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{13.g. Linear Regression II_5_1}.png}

\sphinxAtStartPar
In our new model, the intercept and slope among the non\sphinxhyphen{}smoking group are 19.64 ounces and 0.37 ounces respectively. The intercept and slope among the smoking group are 19.64\sphinxhyphen{}72.69=\sphinxhyphen{}53.05 ounces and  0.37+0.23=0.60 ounces respectively. The interaction term has \(p\)=0.0001, so there is evidence that the slopes are different.


\chapter{14. Linear Regression III}
\label{\detokenize{14.a. Linear Regression III:linear-regression-iii}}\label{\detokenize{14.a. Linear Regression III::doc}}
\sphinxAtStartPar
This is the final session on Linear Regression. In this session we focus on the assumptions underlying this model.



\sphinxAtStartPar
By the end of this session you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain the assumptions underlying multivariable linear regression

\item {} 
\sphinxAtStartPar
apply a range of graphical techniques to investigate the assumptions of the linear regression model;

\end{itemize}



\sphinxAtStartPar
\sphinxstylestrong{Acknowledgements:} Thank you to Jennifer Nicholas, Chris Frost and Ruth Keogh whose notes on linear regression and generalised linear models were particularly useful in the development of the current session.


\section{14.1 Assumptions}
\label{\detokenize{14.b. Linear Regression III:assumptions}}\label{\detokenize{14.b. Linear Regression III::doc}}
\sphinxAtStartPar
The linear regression model makes a number of assumptions. All inferences made from a model are contingent on these assumptions being correct. It is therefore important that we have statistical techniques (or \sphinxstylestrong{diagnostic tools}) to investigate these assumptions.

\sphinxAtStartPar
In practice, it is rare for all the assumptions of a statistical procedure to hold exactly. We may have evidence in the data, or prior knowledge about the data, that lead us to believe that the assumptions made by the model do not hold. This does not necessarily mean that the results from the model should be disregarded, since statistical procedures are \sphinxstylestrong{robust} to departures from assumptions in many settings. When conducting statistical analyses, it is a good idea to first try to establish to what extent assumptions hold and then consider whether the methods used can be adapted to improve the extent to which assumptions hold. If adaptations cannot be made, it is necessary to consider to what extent the results of an analysis can be trusted.

\sphinxAtStartPar
In this section we largely focus on diagnostic tools that can be used to identify assumption violations. Some pointers are given to possible adaptations and alternative techniques that can be used when assumptions are violated, however issues of robustness are not considered in great detail. It is worth noting that, broadly speaking, the central limit theorem implies that departures from assumptions are less important for large datasets than for small ones, and so assumption violations are less of a concern when working with big data.


\section{14.1.1 Assumptions of the linear regression model}
\label{\detokenize{14.b. Linear Regression III:assumptions-of-the-linear-regression-model}}
\sphinxAtStartPar
The assumptions made by the linear regression model are as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linearity:} There is a linear relationship between the dependent variable \(Y\) and each of the independent variables. Here we are contrasting a linear relationship with a non\sphinxhyphen{}linear relationship, not with no relationship. A model in which one of the regression coefficients is zero can satisfy the assumptions of linear regression.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normality:} The error terms follow a normal distribution.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Homoscedasticity:} The error variance is constant i.e. the scatter of points around the true regression line has the same variance, irrespective of the value of \(x_i\). The converse of this feature is termed \sphinxstylestrong{heteroscedasticity}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Independence:} The observations of \(y_i\) are independent.

\end{enumerate}

\sphinxAtStartPar
In this session we will focus on the first three assumptions. Violations of the independence assumption are often more apparant from the context of a study than from the data itself. For example, if we carry out a study in which the blood pressure of 100 people are each measured twice, and then treat the 200 measurements as independent in the statistical analysis it is clear that the assumption of independence is violated.

\sphinxAtStartPar
Notice that the normality and homoscedasticity assumptions concern the error terms, which can be thought of as the \sphinxstyleemphasis{true} residuals, defined in terms of deviations from the model defined by population parameters. Since these errors or true residuals can never be observed in practice, we have to use the observed residuals (obtained by replacing the population parameters with their estimates). In fact, observed residuals are neither independent nor do they have constant variance, but in most settings the departures from independence and homoscedasticity are very small. Consequently, we can proceed as if the observed residuals were the true residuals when investigating assumptions.


\section{14.2 Investigating assumptions using plots}
\label{\detokenize{14.c. Linear Regression III:investigating-assumptions-using-plots}}\label{\detokenize{14.c. Linear Regression III::doc}}
\sphinxAtStartPar
It is a good idea to explore your data using a number of simple plots. Here we will introduce the most useful plots for both simple and multivariable linear regression models.


\subsection{14.2.1 Scatter plots of the outcome against independent variables}
\label{\detokenize{14.c. Linear Regression III:scatter-plots-of-the-outcome-against-independent-variables}}
\sphinxAtStartPar
For simple linear regression models, a scatter plot of the outcome against the independent variable can usually make serious violations of assumptions apparent. Such plots are particularly good for identfiying non\sphinxhyphen{}linearity, heteroscedasticity and \sphinxstylestrong{outliers} (points which lie atypically far from the regression line).

\sphinxAtStartPar
Let our outcome and independent variable be denoted by \(Y\) and \(X\), respectively. The figure below depicts four different scenarios where various assumptions are violated. In Scenario A, there is a slight curvature in the scatter of points between \(Y\) and \(X\), suggesting a non\sphinxhyphen{}linear relationship which violates the linearity assumption. In Scenario B, the variance of \(Y\) is larger for larger values of \(X\), violating the homoscedasticity assumption. In Scenario C, the linearity and homoscedasticity assumptions appear to hold, but there is a possible outlier (circled in red). Scenario D depicts an ideal situation for simple linear regression, where there appears to be no violations.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=600\sphinxpxdimen]{{simple_plots}.png}
\end{figure}

\sphinxAtStartPar
For multivariable linear regression, the linearity assumption requires that the relationship between the outcome and each independent variable is linear \sphinxstyleemphasis{conditional on the other covariates in the model}. So, there is no requirement that the relationship between the outcome and each individual covariate is linear when other covariates are ignored. This means that assessment of the fit of a multivariable linear regression cannot be inferred from a series of scatter plots relating the outcome to each covariate. Such plots can be useful for detecting points with extreme values, but the residual plots considered next are more useful for multivariable models.


\subsection{14.2.2  Plots of residuals against fitted values or covariates}
\label{\detokenize{14.c. Linear Regression III:plots-of-residuals-against-fitted-values-or-covariates}}
\sphinxAtStartPar
Plots of the observed residuals against the fitted values are useful for investigating the assumptions of linearity and homoscedasticity. For linearity: if a non\sphinxhyphen{}linear relationship is present, then the residuals will not be equally distributed above and below zero across the range of fitted values. For homoscedasticity: if there is heterogeneity in the residuals, the variance of residuals will not be constant across the range of fitted values.

\sphinxAtStartPar
The figure below uses the same data from Scenarios A\sphinxhyphen{}D above, but displays the observed residuals against fitted values. We can see that linearity is violated in Scenario A, since the scatter points are not equally distributed above and below the line at \(\epsilon=0\). Furthermore, in Scenario B we can see that the variance of residuals increase with increasing \(\hat{y}\), indicating a violation of homoscedasticity.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=600\sphinxpxdimen]{{residual_plots}.png}
\end{figure}

\sphinxAtStartPar
It can also be useful to plot residuals against each covariate, as a futher check for a linear relationship between \(Y\) and each of the independent variables (conditional on the other covariates in the model). If there are only a small number of covariates in the model, then these plots can be done for all variables. However, if the model is very complex, it may be judged sufficient to only plot residuals against fitted values and residuals against the most important covariates.


\subsection{14.2.3  Normal plots of residuals}
\label{\detokenize{14.c. Linear Regression III:normal-plots-of-residuals}}
\sphinxAtStartPar
Normal plots (such as the \sphinxstylestrong{Q\sphinxhyphen{}Q plot}) provide the best means of visually detecting departures from normality. The normal Q\sphinxhyphen{}Q plot plots observed values against a standard normal distribution with the same number of points. If the data are perfectly normally distributed, the points on a Q\sphinxhyphen{}Q plot would lie on the line \(Y=X\). Deviations from this line indicate deviations from normality. Q\sphinxhyphen{}Q plots of residuals can be used to investigate the normality assumption.

\sphinxAtStartPar
As previously mentioned, the observed residuals do not have constant variance even when true residuals do. Therefore, some authors suggest using \sphinxstylestrong{standardised} residuals in the normal plots (since standardised residuals do have constant variance). On the other hand, some prefer to work with the observed residuals since these have the same units as the outcome. In practice, the differences between the two approaches are minor.

\sphinxAtStartPar
The figure below depicts normal Q\sphinxhyphen{}Q plots for the standardised residuals in Scenarios A\sphinxhyphen{}D. In such plots we might expect to see some deviation from the straight line in the extreme values of the residuals and so the variation in the tails are not of great concern. In Scenario A however, there is deviation away from the line \(Y=X\) towards the middle, indicating a violation of the normality assumption. Furthermore, the outlier in Scenario C may need further investigation (we discuss outliers further in a subsequent section).

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=600\sphinxpxdimen]{{qqplots}.png}
\end{figure}


\subsection{14.2.4  Plots based on Cook’s distance}
\label{\detokenize{14.c. Linear Regression III:plots-based-on-cook-s-distance}}
\sphinxAtStartPar
Cook’s distance is a measure of the \sphinxstylestrong{influence} of an observation. An influential observation is one that has a large impact on the model parameter estimates. It is worth checking the influence of observations, particularly potential outliers, to see if they are having a much larger impact on model fit than we would expect.

\sphinxAtStartPar
For a model with \(p\) parameters (with estimated residual variance \(\sigma^2\)), the Cook’s distance for the \(i^{th}\) observation (\(D_i\)) is obtained by refitting the model excluding this observation and obtaining new fitted values (\(\hat{y_{j(i)}}\)) for all \(n\) observations (including the omitted one). \(D_i\) is then defined as:
\begin{equation*}
\begin{split}D_i = \frac{\sum_{i=1}^n(\hat{y}_{j(i)}-\hat{y}_i)^2}{(p+1)\hat{\sigma}^2}\end{split}
\end{equation*}
\sphinxAtStartPar
The higher the value of \(D_i\), the more influential the observation.

\sphinxAtStartPar
It can be informative to display Cook’s distances graphically. The figure below plots the Cook’s distances for each observation in Scenarios A\sphinxhyphen{}D.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[height=600\sphinxpxdimen]{{cooks_distance}.png}
\end{figure}

\sphinxAtStartPar
In Scenario C the outlier identified in the previous plots and potentially problematic has a much higher Cook’s distance than the other observations, indicating that it is highly influential and worth further investigation.


\section{14.2 Statistical tests of assumptions}
\label{\detokenize{14.d. Linear Regression III:statistical-tests-of-assumptions}}\label{\detokenize{14.d. Linear Regression III::doc}}
\sphinxAtStartPar
It might be anticipated that the assumptions of the linear regression model can be investigated using formal hypothesis tests. Indeed there exist a number of statistical tests for normalilty including the Kolmorogorov\sphinxhyphen{}Smirnov test and the Shapiro\sphinxhyphen{}Wilk test. Further, there exist statistical tests for heteroscedasticity of rediduals.

\sphinxAtStartPar
However, these tests suffer from the drawback that they tend to only have statistical power to detect model violations when datasets are large and when datasets are large the central limit theorem means that the consequences of these violations of are less important than in small datasets. With large datasets, tests of normality and heteroscedasticity can often be statistically significant, but the impact of these violations may be practically unimportant. For these reasons, the tests are considered by many statisticians to be of limited practical use and so details of these procedures will not be given here.


\subsection{14.2.1 Examples using the birthweight data}
\label{\detokenize{14.d. Linear Regression III:examples-using-the-birthweight-data}}
\sphinxAtStartPar
We will use some of the graphical tools discussed above to assess the validity of assumptions in the multivariable model defined in the previous session (Model 4). Recall Model 4 was defined as:
\begin{equation*}
\begin{split}
\text{Model 4: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i + \epsilon_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
The outcome \(y_i\) denotes the birthweight (in oz) for the \(i^{th}\) baby. The covariates \(l_i\) and \(h_i\) denote the length of pregnancy (i.e. number of gestational days), and the height of the mother (in inches) for the \(i^{th}\) baby, respectively.

\sphinxAtStartPar
The code below fits Model 4 to the birthweight data, and then produces (1) a plot of residuals against fitted values (2) a Q\sphinxhyphen{}Q plot of the standardised residuals and (3) a plot of Cook’s distances by observation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}Load the data}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Fit Model 3 to the data}
\PYG{n}{model4}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{o}{+}\PYG{n}{Maternal.Height}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Set the graphical space so that two plots are shown side\PYGZhy{}by\PYGZhy{}side in one row}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Plot the residuals against the fitted values}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{model4}\PYG{o}{\PYGZdl{}}\PYG{n}{fitted.values}\PYG{p}{,} \PYG{n}{model4}\PYG{o}{\PYGZdl{}}\PYG{n}{residuals}\PYG{p}{,} \PYG{n}{main} \PYG{o}{=} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Residual plot\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Fitted values\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Residuals\PYGZdq{}}\PYG{p}{,} \PYG{n}{pch}\PYG{o}{=}\PYG{l+m}{19}\PYG{p}{)}
\PYG{n+nf}{abline}\PYG{p}{(}\PYG{n}{h}\PYG{o}{=}\PYG{l+m}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Obtain the standardised residuals}
\PYG{n}{Standardised.Residuals}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{rstandard}\PYG{p}{(}\PYG{n}{model4}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Normal Q\PYGZhy{}Q plot of the standardised residuals }
\PYG{n+nf}{qqnorm}\PYG{p}{(}\PYG{n}{Standardised.Residuals}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Normal Q\PYGZhy{}Q plot\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Expected normal value\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Standardised Residuals\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{qqline}\PYG{p}{(}\PYG{n}{Standardised.Residuals}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}Plot of Cook\PYGZsq{}s distance}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{model4}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{14.d. Linear Regression III_2_0}.png}

\sphinxAtStartPar
We make the following observations:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linearity:} The residuals are equally distributed above and below zero in the “Residual plot”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normality:} There do not appear to be any serious departures from normality, based on the “Normal Q\sphinxhyphen{}Q plot”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Homoscedasticity:} The variance of residuals are constant across the fitted values (based on the “Residual plot”)

\end{enumerate}

\sphinxAtStartPar
However, the Cook’s distance plot reveals that observation 239 is highly influential, compared to the remaining observations. Observations 405 and 820 also have a relatively high Cook’s distance. Sensitivity analyses may be required to assess model fit with and without these observations (this is discussed in Section 3.5.3).

\sphinxAtStartPar
Finally, we can assume that the independence assumption holds, since the birthweight of a baby from one mother is not expected to be associated with the birthweight of a baby from a different mother. Therefore, we can reasonably conclude that all the assumptions are met in this model (but there are some potentially problematic observations in terms of influence).

\sphinxAtStartPar
Next, we briefly introduce some of the statisical solutions available for when assumptions are not met.


\section{14.3 Dealing with violations of assumptions}
\label{\detokenize{14.e. Linear Regression III:dealing-with-violations-of-assumptions}}\label{\detokenize{14.e. Linear Regression III::doc}}
\sphinxAtStartPar
So far, we have discussed diagnostic tools that are useful for identifying possible violations of the assumptions of a linear model. Identification of potential violations of concern is only the first, and arguably the easiest, aspect of an exploration of the robustness of the results of fitting a model. Here, we briefly describe some approaches that can be used to deal with violations. When these approaches do not work, then more complex methods (beyond the scope of this lesson) may be needed to analyse the data.


\subsection{14.3.1 Checking the data}
\label{\detokenize{14.e. Linear Regression III:checking-the-data}}
\sphinxAtStartPar
Clearly, it is important that errors in data are eliminated as far as possible. In practice, ensuring that a large dataset is 100\% error free may be impossible. Observations with large standardised residuals can potentially arise through data entry or coding errors and so a useful first step is to check such values with the data provider or original source of data, if available.


\subsection{14.3.2 Transformations}
\label{\detokenize{14.e. Linear Regression III:transformations}}
\sphinxAtStartPar
Sometimes it can be useful to transform either the outcome variable and/or one or more of the covariates. The transformed variables are then used in the analysis in replacement of the original variables. There are a number of possible motivations for this:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Transformations can be used to convert a non\sphinxhyphen{}linear relationship into a linear one. For example:

\end{enumerate}
\begin{equation*}
\begin{split}
y_i = \alpha(x_i)^{\beta} ⇒log(y_i) = log (\alpha)+\beta log (x_i).
\end{split}
\end{equation*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Transformations can be used to improve the normality of residuals. For example, the Box\sphinxhyphen{}Cox transformation is a power transformation for this purpose.

\item {} 
\sphinxAtStartPar
Transformations can help stabilise the variance of residuals. For example, if \(\hat{\sigma}^2\) is proportional to \([E(Y)]^2\) then \(y^*=log(y)\) is a useful variance\sphinxhyphen{}stabilising transformation. Alternatively, if \(\hat{\sigma}^2\) is proportional to \([E(Y)]^3\) then \(y^*=1/\sqrt{y}\) can be used.

\end{enumerate}


\subsection{14.3.3 Sensitivity analyses}
\label{\detokenize{14.e. Linear Regression III:sensitivity-analyses}}
\sphinxAtStartPar
If we observe potentially problematic outliers, sensitivity analyses can be used to assess how problematic they are. This involves repeating the analysis after omitting the outlier (or group of outliers) and considering the extent to which the results are altered.

\sphinxAtStartPar
However, even if the outlier affects the results (and/or assumptions) it is not a good idea to simply drop the data point. If it is not a data error, then it is a legitimate observation that should be included and understanding the reasons why it is an outlier could be important. In most cases, it is preferable to report the results including all data points, but discuss the impact removing the outlier had on the results.


\section{14.5 Collinearity}
\label{\detokenize{14.f. Linear Regression III:collinearity}}\label{\detokenize{14.f. Linear Regression III::doc}}
\sphinxAtStartPar
A potential issue that can arise from including higher\sphinxhyphen{}order terms or interaction terms in a linear regression model is collinearity. Collinearity occurs when there is correlation between one or more of the independent covariates. If the degree of correlation between covariates is high enough, it can cause the following problems:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Estimated coefficients can swing in either direction: known important variables may have surprisingly small coefficient estimates and known less important variables may have surprisingly large coefficient estimates.

\item {} 
\sphinxAtStartPar
Increased variance of estimated coefficients, therefore reducing the statistical power of the model.

\end{enumerate}

\sphinxAtStartPar
Including higher\sphinxhyphen{}order terms or interaction terms can result in collinearity due to the inevitable correlation between variables, their powers and the interaction terms involving them. Having said that, collinearity is only a concern in particular situations.

\sphinxAtStartPar
Collinearity only affects the specific independent variables that are correlated. Therefore, if the aim of the analysis is to estimate the way \(X\) influences \(Y\) after adjusting for \(W\) and \(V\), and there is only correlation between \(W\) and \(V\), then collinearity is not a concern. Moreover, collinearity rarely affects the predicted outcomes, so if the aim of the analysis is to predict \(Y\) using data from \(X\), \(W\) and \(V\), then collinearity between any of the covariates is not a concern. Finally, the severity of the problems caused by collinearity increases with the degree of correlation. Therefore, if only moderate or weak correlation is present, then collinearity is not a concern.

\sphinxAtStartPar
If we are in a situation where collinearity is causing problems, then we could either remove some of the highly correlated variables, or transform one of them. Examples of transformations include:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Instead of using systolic and diastolic blood pressure as collinear predictor variables, use diastolic blood pressure and (systolic\sphinxhyphen{}diastolic blood pressure).

\item {} 
\sphinxAtStartPar
Instead of using height and weight as predictor variables, use height and body mass index (weight/height\(^2\)).

\item {} 
\sphinxAtStartPar
When fitting a quadratic regression model, use \(X\) and \((X-\bar{X})^2\), rather than \(X\) and \(X^2\) as covariates.

\end{enumerate}


\section{14.6 Optional Reading: Analysis of Variance}
\label{\detokenize{14.g. Linear Regression III:optional-reading-analysis-of-variance}}\label{\detokenize{14.g. Linear Regression III::doc}}

\subsection{14.6.1 Partitioning variance}
\label{\detokenize{14.g. Linear Regression III:partitioning-variance}}
\sphinxAtStartPar
The total variation in \(Y\) is equal to:
\begin{equation*}
\begin{split} 
SS_{TOT} = \sum_{i=1}^n (Y_i-\bar{Y})^2
\end{split}
\end{equation*}
\sphinxAtStartPar
This is often referred to as the sum of squares of the \(Y\)’s. This represents all of the variation in \(Y\) about its overall mean value. We can think about two components of this total variation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the predictable variation in \(Y\) (predicted by the variables included in the model) and

\item {} 
\sphinxAtStartPar
the unpredictable variation in \(Y\) (the remaining “noise”).

\end{itemize}

\sphinxAtStartPar
The predictable variation represents the variation of the predicted values \(\hat{Y}\) about the mean. We can measure this as:
\begin{equation*}
\begin{split}
\begin{align}
SS_{REG}= \sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
The unpredictable variation represents the variation of the observed values about their predicted values:
\begin{equation*}
\begin{split}
\begin{align}
SS_{RES} =  \sum_{i=1}^n(Y_i-\hat{Y_i})^2
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Note that in the equations above, SS stands for sums of squares, REG for regression and RES for residual.

\sphinxAtStartPar
A key result for ANOVA is that the total variation in \(Y\) can be partitioned into the predictable variation, explained by the regression model, and the unpredictable (residual) variation:
\begin{equation*}
\begin{split}
\begin{align}
SS_{TOT} &= SS_{REG}+SS_{RES} \\
\rightarrow \sum_{i=1}^n (Y_i-\bar{Y})^2 &= \sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 + \sum_{i=1}^n(Y_i-\hat{Y_i})^2
\end{align}
\end{split}
\end{equation*}

\subsubsection{14.6.1.1 The coefficient of determination}
\label{\detokenize{14.g. Linear Regression III:the-coefficient-of-determination}}
\sphinxAtStartPar
Using the sums of squares defined above, we can calculate the proportion of variance explained by the statistical model, known as the \sphinxstylestrong{coefficient of determination}.

\sphinxAtStartPar
The proportion of variation which is explained by a statistical model is denoted by \(R^2\) and is given by:
\begin{equation*}
\begin{split}
R^2 = \frac{SS_{REG}}{SS_{TOT}}.
\end{split}
\end{equation*}

\subsubsection{Example}
\label{\detokenize{14.g. Linear Regression III:example}}
\sphinxAtStartPar
The coefficient of determination is given in the \sphinxcode{\sphinxupquote{summary()}} output for a linear regression in R. In Model 1, \(R^2=0.1661\) (see output below). This means that Model 1 explains 16.6\% of the total variation in \(Y\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}The coefficient of determination}
\PYG{n}{data}\PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZsq{}}\PYG{l+s}{https://www.inferentialthinking.com/data/baby.csv\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model1}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}49.348 \PYGZhy{}11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)      \PYGZhy{}10.75414    8.53693   \PYGZhy{}1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1661,	Adjusted R\PYGZhy{}squared:  0.1654 
F\PYGZhy{}statistic: 233.4 on 1 and 1172 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
While \(R^2\) is sometimes used an overall measure of goodness\sphinxhyphen{}of\sphinxhyphen{}fit (or predictive performance), it isn’t used to formally compare models. This is because \(R^2\) will never decrease when new covariates are added to a model (provided that the number and identity of observations remains the same). Therefore, using \(R^2\) for model comparisons, we would always conclude that the more complex model is at least as good a fit as the simpler model, even if this is not true.

\sphinxAtStartPar
An adjusted \(R^2\) has been proposed to account for this issue. Above, for example, the \(R^2\) is 0.166 but the adjusted R\sphinxhyphen{}squared is a little smaller \(R^2 = 0.165\). However, we would not recommend using the \(R^2\) for formal model comparison.


\subsection{14.6.2 Comparing models}
\label{\detokenize{14.g. Linear Regression III:comparing-models}}
\sphinxAtStartPar
When fitting statistical models, we may wish to compare how well two models fit the data, to see which is most appropriate. Consider the following three models:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Model 1 (birthweight\textasciitilde{}length of pregnancy)

\item {} 
\sphinxAtStartPar
Model 2 (birthweight\textasciitilde{}mother’s smoking status)

\item {} 
\sphinxAtStartPar
Model 4 (birthweight\textasciitilde{}length of pregnancy+mothers height)

\end{itemize}

\sphinxAtStartPar
We might want to make the following comparisons between models:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Comparison 1: Model 1 vs Model 4

\item {} 
\sphinxAtStartPar
Comparison 2: Model 2 vs Model 4

\end{itemize}

\sphinxAtStartPar
In these examples, Comparison 1 is much simpler than Comparison 2, because the models in Comparison 1 are \sphinxstylestrong{nested}.

\sphinxAtStartPar
Statistical models are said to be \sphinxstylestrong{nested} when one model (the simpler model) contains a subset of the covariates in the other one (the complex model) and no other additional variables. In Comparison 2, the models are not nested because the simpler model (Model 2) contains mother’s smoking status as a variable, which is not included in Model 4.

\sphinxAtStartPar
Nested models can be compared using \sphinxstylestrong{Analysis of Variance (ANOVA)} (the comparison of non\sphinxhyphen{}nested models is much more complicated and is beyond the scope of this module).

\sphinxAtStartPar
The main idea of ANOVA is that: if the complex model better describes the data than the simpler model, then we would expect a reasonably large amount of the residual variation that is unexplained by the simpler model to be explained by the complex one. ANOVA provides a statistical framework that can formally test this.

\sphinxAtStartPar
We will first consider ANOVA in the context of simple linear regression, where the simpler model assumes no association between the outcome and the independent variable (the \sphinxstylestrong{null} model). We will then consider ANOVA in the context of multivariable linear regression and we end by learning how ANOVA can be used to test for differences between groups in a categorical variable.


\subsection{14.6.3 The ANOVA table}
\label{\detokenize{14.g. Linear Regression III:the-anova-table}}
\sphinxAtStartPar
\sphinxstylestrong{Sums of squares (SS):} The first step is to partition the total variation into the regression (predictable) and residual (unpredictable) components. Variation is measured by sums of squares (SS). So we partition the total sum of squares (\(SS_{TOT}\)) into  (\(SS_{REG}\) and \(SS_{RES}\))

\sphinxAtStartPar
\sphinxstylestrong{Degrees of freedom:} Each of these sum of squares have an associated degrees of freedom (d.f.). The d.f. for the total sum of squares is \((n-1)\), since the variance of \(Y\) is \(\sum_{i=1}^n (Y_i-\bar{Y})^2/(n-1)\). The d.f. for the regression sum of squares in the number of covariates in the regression model (when a simple linear regression model is used this is equal to 1). The residual d.f. is found by subtracting the regression d.f. from the total d.f.

\sphinxAtStartPar
\sphinxstylestrong{Mean squares (MS)} The sums of squares also have associated mean squares, which are obtained by dividing each sum of squares by its associated degrees of freedom (note that the residual mean square is then equal to \(\hat{\sigma}^2\)).

\sphinxAtStartPar
These statistics are typically summarised in an ANOVA table. The table for simple linear regression is shown below.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Source
&\sphinxstyletheadfamily 
\sphinxAtStartPar
d.f.
&\sphinxstyletheadfamily 
\sphinxAtStartPar
SS
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Mean Square
\\
\hline
\sphinxAtStartPar
Regression
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\(SS_{REG}\)
&
\sphinxAtStartPar
\(MS_{REG}=\frac{SS_{REG}}{1}\)
\\
\hline
\sphinxAtStartPar
Residual
&
\sphinxAtStartPar
n\sphinxhyphen{}2
&
\sphinxAtStartPar
\(SS_{RES}\)
&
\sphinxAtStartPar
\(MS_{RES}=\frac{SS_{RES}}{n-2}\)
\\
\hline
\sphinxAtStartPar
Total
&
\sphinxAtStartPar
n\sphinxhyphen{}1
&
\sphinxAtStartPar
\(SS_{TOT}\)
&
\sphinxAtStartPar
\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Notes
\begin{quote}

\sphinxAtStartPar
Very loosely speaking, degrees of freedom are “bits of information”. We start with \(n\) bits of information. Every time we estimate something we “use” a bit of information (and so lose a degree of freedom. Therefore, when we calculate the overall variation in \(Y\), we lose one of the \(n\) bits of information because we need to calculate the overall mean to obtain the sum of squares of \(Y\). Therefore, we have \(n-1\) bits of information overall.  In a simple linear regression model, we estimate two parameters, so we’re using two bits of information, but one of these is essentially the same bit we lost from calculating the overall mean, so we say that the regression model is using 1 degree of freedom. We started with n\sphinxhyphen{}1 df and the regression model used 1 of them so there are n\sphinxhyphen{}2 left for the remaining component, the residual SS.
\end{quote}


\subsubsection{14.6.3.1 Hypothesis testing using ANOVA}
\label{\detokenize{14.g. Linear Regression III:hypothesis-testing-using-anova}}
\sphinxAtStartPar
The values in the ANOVA table can be used to conduct formal hypothesis tests.

\sphinxAtStartPar
ANOVA is used to test the null hypothesis that the simpler of the two nested models better fits the data. In simple linear regression, the simpler model is the null model, in which case:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0:\) The null model is a better fit

\item {} 
\sphinxAtStartPar
\(H_1:\) The simple linear regression model is a better fit

\end{itemize}

\sphinxAtStartPar
To test the null hypothesis defined above, we use an \(F\) statistic, defined as:
\begin{equation*}
\begin{split}
F = \frac{MS_{REG}}{MS_{RES}}
\end{split}
\end{equation*}
\sphinxAtStartPar
This ratio measures how much more variation in \(Y\) is explained by the model than would be expected by chance. If the model does not fit the data well, then we would expect this ratio to be equal to 1. The larger the value of \(F\), the stronger the evidence that the complex model is a better fit. To obtain a \(p\)\sphinxhyphen{}value for a formal hypothesis test, \(F\) can be compared to the \(F_{1,(n-2)}\) distribution (where 1 and (n\sphinxhyphen{}2) are the relevant degrees of freedom for the mean squares).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} F\PYGZhy{}test using anova()}
\PYG{n+nf}{anova}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A anova: 2 × 5
\begin{tabular}{r|lllll}
  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\
  & <int> & <dbl> & <dbl> & <dbl> & <dbl>\\
\hline
	Gestational.Days &    1 &  65449.51 & 65449.5131 & 233.4293 & 3.395226e-48\\
	Residuals & 1172 & 328608.34 &   280.3825 &       NA &           NA\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
The \(F\)\sphinxhyphen{}statistic is equal to 233.4 with \(p\)\sphinxhyphen{}value \(<2.2\times10^{-16}\). With such a small \(p\)\sphinxhyphen{}value there is strong evidence against the null hypothesis. Therefore we conclude that the model which includes gesational days is a better fit.


\subsubsection{14.6.3.2 Connection between F tests and t\sphinxhyphen{}tests in simple linear regression}
\label{\detokenize{14.g. Linear Regression III:connection-between-f-tests-and-t-tests-in-simple-linear-regression}}
\sphinxAtStartPar
Above, we used a \(F\)\sphinxhyphen{}test to compare the model for birthweight (Y) including gestational days (L):
\begin{equation*}
\begin{split}
y_i = \beta_0  + \beta_1 l_i + \epsilon_i
\end{split}
\end{equation*}
\sphinxAtStartPar
with a model including just a constant
\begin{equation*}
\begin{split}
y_i = \alpha_0  + \epsilon_i
\end{split}
\end{equation*}
\sphinxAtStartPar
In other words, we have just used a \(F\)\sphinxhyphen{}test to test the null hypothesis \(H_0: \beta_1 = 0\). This is exactly the same hypothesis test we tested previously using a \(t\)\sphinxhyphen{}test.

\sphinxAtStartPar
In other words, the \(F\)\sphinxhyphen{}test for a simple linear regression model is the same as the \(t\)\sphinxhyphen{}test of the null hypothesis that the slope parameter is equal to 0. Below, we perform the \(t\)\sphinxhyphen{}test again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} F\PYGZhy{}test}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}49.348 \PYGZhy{}11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)      \PYGZhy{}10.75414    8.53693   \PYGZhy{}1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1661,	Adjusted R\PYGZhy{}squared:  0.1654 
F\PYGZhy{}statistic: 233.4 on 1 and 1172 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
We see that the t\sphinxhyphen{}statistic for the slope is \(t=15.28\), with p\sphinxhyphen{}value \(p<2e-16\). Previously, we had \(F=233.4=15.24^2=t^2\). The p\sphinxhyphen{}value for the \(F\)\sphinxhyphen{}test was identical to the \(t\)\sphinxhyphen{}test.

\sphinxAtStartPar
The two tests are equivalent, with \(F=t^2\), providing identical p\sphinxhyphen{}values. Consequently, it is not particularly common to use \(F\)\sphinxhyphen{}tests in the simple linear regression model, they are more useful for assessing more complex models with multiple covariates.


\subsection{14.6.4 ANOVA for multivariable linear regression}
\label{\detokenize{14.g. Linear Regression III:anova-for-multivariable-linear-regression}}
\sphinxAtStartPar
In the context of multivariable linear regression, ANOVA can be used to test whether a more complex model is a better fit than the null model (\sphinxstylestrong{the Global F test}), or whether a more complex model is a better fit than a simpler model that includes a subset of the covariates in the complex model (\sphinxstylestrong{the partial F test}). Each test requires slight modifications to the ANOVA table defined above and we will discuss these in turn.


\subsubsection{14.6.4.1 The Global F test}
\label{\detokenize{14.g. Linear Regression III:the-global-f-test}}
\sphinxAtStartPar
The general formulation of the ANOVA table (suitable for simple and multivariable linear regression models) is given below. \(p\) is the number of covariates in the model.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Source
&\sphinxstyletheadfamily 
\sphinxAtStartPar
d.f.
&\sphinxstyletheadfamily 
\sphinxAtStartPar
SS
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Mean Square
\\
\hline
\sphinxAtStartPar
Regression
&
\sphinxAtStartPar
\(p\)
&
\sphinxAtStartPar
\(SS_{REG}\)
&
\sphinxAtStartPar
\(MS_{REG}=\frac{SS_{REG}}{p}\)
\\
\hline
\sphinxAtStartPar
Residual
&
\sphinxAtStartPar
\(n-(p+1)\)
&
\sphinxAtStartPar
\(SS_{RES}\)
&
\sphinxAtStartPar
\(MS_{RES}=\frac{SS_{RES}}{n-p-1}\)
\\
\hline
\sphinxAtStartPar
Total
&
\sphinxAtStartPar
\(n-1\)
&
\sphinxAtStartPar
\(SS_{TOT}\)
&
\sphinxAtStartPar
\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Note that this is equivalent to the previous table (for simple linear regression) when \(p=1\).

\sphinxAtStartPar
The Global F test tests the null hypothesis (\(H_0\)) that the null model is a better fit than the more complex model against the alternative hypothesis (\(H_1\)) that the complex model is a better fit. Or, equivalently:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0:\) All slope parameters in the complex model are equal to 0.

\item {} 
\sphinxAtStartPar
\(H_1:\) At least one of the slope parameters in the complex model is not equal to 0.

\end{itemize}

\sphinxAtStartPar
The appropriate \(F\) statistic is the ratio
\begin{equation*}
\begin{split}
F = \frac{MS_{REG}}{MS_{RES}}
\end{split}
\end{equation*}
\sphinxAtStartPar
Under the null hypothesis, \(F\) follows an \(F_{p,(n-(p+1))}\) distribution.


\subsubsection{Example}
\label{\detokenize{14.g. Linear Regression III:id1}}
\sphinxAtStartPar
We can use \sphinxcode{\sphinxupquote{summary()}} to conduct a global \(F\)\sphinxhyphen{}test for Model 4, our model relating birthweight to both length of pregnancy and maternal height.

\sphinxAtStartPar
The null and alternative hypotheses, for the global \(F\)\sphinxhyphen{}test, are defined as:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0\): the regression coefficients for both gestational days and mother’s height are equal to 0.

\item {} 
\sphinxAtStartPar
\(H_1\): the regression coefficient for either gestational days or mother’s height (or both) is not equal to 0.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} ANOVA for Model 4 }
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days.Centered}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Gestational.Days}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height.Centered}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height}\PYG{o}{\PYGZhy{}}\PYG{n+nf}{mean}\PYG{p}{(}\PYG{n}{data}\PYG{o}{\PYGZdl{}}\PYG{n}{Maternal.Height}\PYG{p}{)}

\PYG{n}{model4}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n}{Gestational.Days.Centered}\PYG{o}{+}\PYG{n}{Maternal.Height.Centered}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} Gestational.Days.Centered + Maternal.Height.Centered, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}53.829 \PYGZhy{}10.589   0.246  10.254  54.403 

Coefficients:
                           Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)               119.46252    0.47980 248.983  \PYGZlt{} 2e\PYGZhy{}16 ***
Gestational.Days.Centered   0.45237    0.03006  15.051  \PYGZlt{} 2e\PYGZhy{}16 ***
Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e\PYGZhy{}11 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R\PYGZhy{}squared:  0.1969,	Adjusted R\PYGZhy{}squared:  0.1955 
F\PYGZhy{}statistic: 143.5 on 2 and 1171 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
The \(F\) statistic is 143.5 with a \(p\)\sphinxhyphen{}value \(<2.2 \times 10^{-16}\). Therefore, there is strong evidence against the null and we can conclude that at least one of the estimated regression coefficients is non\sphinxhyphen{}zero (i.e Model 4 is a better fit than the null model).


\subsubsection{14.6.4.2 The partial F test}
\label{\detokenize{14.g. Linear Regression III:the-partial-f-test}}
\sphinxAtStartPar
The global \(F\)\sphinxhyphen{}test is a joint test of the statistical signficance of all the slope parameters in a linear regression model. On the other hand, the partial \(F\)\sphinxhyphen{}test compares the fit of:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Model A (complex model, with  \(p\) predictors)

\item {} 
\sphinxAtStartPar
Model B (simpler nested model with \(p-k\) predictors).

\end{itemize}

\sphinxAtStartPar
The key to the partial \(F\)\sphinxhyphen{}test is the construction of an Analysis of Variance table that partitions the sum of squares explained by the complex model into that explained by the simple model and the extra sum of squares only explained by the complex model. Using the notation that \(SS_{REG_A}\) denotes the sum of squares explained by the complex model, whilst \(SS_{REG_B}\) denotes the sum of squares explained by the simpler model, the ANOVA table is as shown below.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Source
&\sphinxstyletheadfamily 
\sphinxAtStartPar
d.f.
&\sphinxstyletheadfamily 
\sphinxAtStartPar
SS
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Mean Square
\\
\hline
\sphinxAtStartPar
Explained by Model B
&
\sphinxAtStartPar
\(p-k\)
&
\sphinxAtStartPar
\(SS_{REG_B}\)
&
\sphinxAtStartPar
\(MS_{REG_B}=\frac{SS_{REG_B}}{p-k}\)
\\
\hline
\sphinxAtStartPar
Explained by Model A
&
\sphinxAtStartPar
\(p\)
&
\sphinxAtStartPar
\(SS_{REG_A}\)
&
\sphinxAtStartPar
\(MS_{REG_A}=\frac{SS_{REG_A}}{p}\)
\\
\hline
\sphinxAtStartPar
Extra explained by Model A
&
\sphinxAtStartPar
\(k\)
&
\sphinxAtStartPar
\(SS_{REG_A}-SS_{REG_B}\)
&
\sphinxAtStartPar
\(MS_{REG_X}=\frac{(SS_{REG_A}-SS_{REG_B})}{k}\)
\\
\hline
\sphinxAtStartPar
Residual from Model A
&
\sphinxAtStartPar
\(n-(p+1)\)
&
\sphinxAtStartPar
\(SS_{RES_A}\)
&
\sphinxAtStartPar
\(MS_{RES}=\frac{SS_{RES_A}}{n-(p+1)}\)
\\
\hline
\sphinxAtStartPar
Total
&
\sphinxAtStartPar
\((n-1)\)
&
\sphinxAtStartPar
\(SS_{TOT}\)
&
\sphinxAtStartPar
\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The partial \(F\)\sphinxhyphen{}test tests the following null hypothesis:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \) all of the slope parameters included in Model A but omitted from Model B are equal to zero.

\item {} 
\sphinxAtStartPar
\(H_1: \) at least one of the additional parameters in Model A is not equal to 0.

\end{itemize}

\sphinxAtStartPar
The appropriate test statistic (\(F\)) is the ratio of extra mean sum of squares in Model A to the mean residual sum of squares from Model A. Under the null hypothesis, this test statistic follows an \(F\)\sphinxhyphen{}distribution:
\begin{equation*}
\begin{split}
\text{Under } H_0: F = \frac{MS_{REG_X}}{MS_{RES}} \sim F_{k,(n-(p+1))}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstyleemphasis{Example}: We can use \sphinxcode{\sphinxupquote{anova()}} to conduct a partial F\sphinxhyphen{}test to compare Models 1 and 3:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0:\) Model 1 is the better fit

\item {} 
\sphinxAtStartPar
\(H_0:\) Model 3 is the better fit

\end{itemize}


\paragraph{Example}
\label{\detokenize{14.g. Linear Regression III:id2}}
\sphinxAtStartPar
We can use \sphinxcode{\sphinxupquote{anova()}} to conduct a partial \(F\)\sphinxhyphen{}test to compare Models 1 and 4:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Model 1 (birthweight\textasciitilde{}length of pregnancy)

\item {} 
\sphinxAtStartPar
Model 4 (birthweight\textasciitilde{}length of pregnancy+mothers height)

\end{itemize}

\sphinxAtStartPar
Model 1 is nested within Model 4. Model 4 is our complex model.

\sphinxAtStartPar
In this case, the two models only differ by one variable (mother’s height) and so the hypotheses being tested within the partial \(F\)\sphinxhyphen{}test could be written as:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \beta_2=0\), where \(\beta_2\) is the regression coefficient for mother’s height.

\item {} 
\sphinxAtStartPar
\(H_0: \beta_2 \neq 0\).

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{anova}\PYG{p}{(}\PYG{n}{model1}\PYG{p}{,} \PYG{n}{model4}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A anova: 2 × 6
\begin{tabular}{r|llllll}
  & Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\
  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\
\hline
	1 & 1172 & 328608.3 & NA &       NA &       NA &           NA\\
	2 & 1171 & 316482.2 &  1 & 12126.13 & 44.86728 & 3.266475e-11\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
The \(F\)\sphinxhyphen{}statistic is 44.87 with a \(p\)\sphinxhyphen{}value of \(3.23\times 10^{-11}\). This is strong evidence against the null hypothesis. Hence the data indicates that Model 4 (the more complex model) is the better fit.

\sphinxAtStartPar
When the two models being compared only differ by one variable, the partial F test is equivalent to the t test of the null hypothesis that the regression coefficient for that variable is equal to 0. Notice in our example that the results of the partial F test are the same as the t\sphinxhyphen{}test for \(\beta_2=0\), with \(F=t^2\).

\sphinxAtStartPar
For this reason, partial F tests are more useful in situations where we wish to compare models that differ by more than one variable. The approach is identical to that shown above.


\subsection{14.6.5 ANOVA for models with categorical independent variables}
\label{\detokenize{14.g. Linear Regression III:anova-for-models-with-categorical-independent-variables}}
\sphinxAtStartPar
Another useful application of ANOVA is to test for differences in means between categories of a categorical variable.

\sphinxAtStartPar
Suppose we are interested in the association between an outcome \(Y\) and a categorical variable \(X\) with \(K\) groups. We have already seen how to define a multivariable linear regression model using dummy variables for this situation. An alternative model, often termed the \sphinxstylestrong{ANOVA model}, is as follows:

\sphinxAtStartPar
Let \(y_{ki}\) be the value of the outcome for the \(i^{th}\) observation in the \(k^{th}\) group (\(i=1,...,n_k\) and \(k=1,...,K\)). The ANOVA model is then defined as:
\begin{equation*}
\begin{split}
y_{ki}=\mu_k + \epsilon_{ki} \text{, where } \epsilon_{ki} \overset{iid}{\sim} N(0,\sigma^2)
\end{split}
\end{equation*}
\sphinxAtStartPar
Here, \(\mu_k\) is the mean of the outcome in the \(k^{th}\) group. With this representation, the null and alternative hypothesis are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \mu_k= \mu\) (i.e. the means in all groups defined by the categorical variables are equal to a common value).

\item {} 
\sphinxAtStartPar
\(H_1: \mu_k \neq \mu\) (i.e. the group means are not all equal).

\end{itemize}


\subsubsection{14.6.5.1 Sum of squares for models with categorical variables}
\label{\detokenize{14.g. Linear Regression III:sum-of-squares-for-models-with-categorical-variables}}
\sphinxAtStartPar
For models with a single independent categorical variable the fitted values are simply the group means (\(\bar{y_k}\)). Under the null hypothesis that the group means are all equal, the fitted values are all equal to the overall mean (\(\bar{y}\)). This leads to new terminology for the residual sum of squares (\(SS_{RES}\)) and the sum of squares explained by the model (\(SS_{REG}\)):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(SS_{RES} = \sum_{k=1}^K \sum_{i=1}^{n_k} (y_{ki} - \bar{y}_k)^2\)

\item {} 
\sphinxAtStartPar
\(SS_{REG} = \sum_{k=1}^K \sum_{i=1}^{n_k} (\bar{y}_k - \bar{y})^2 = \sum_{k=1}^K n_k (\bar{y}_k - \bar{y})^2 \)

\end{itemize}

\sphinxAtStartPar
In this case, the residual sum of squares is often termed the \sphinxstylestrong{within group sum of squares \((SS_{Within})\)} and the regression sum of squares is often termed the \sphinxstylestrong{between group sum of squares \((SS_{Between})\)}.


\subsubsection{14.6.5.2 The ANOVA table}
\label{\detokenize{14.g. Linear Regression III:id3}}
\sphinxAtStartPar
When there are \(K\) groups, the degrees of freedom for the within groups sum of squares is \(n-K\) (because the model includes \(K\) parameters) and the degrees of freedom for the between groups sum of squares is \(K-1\) (because the null model contains a single parameter, the overall mean). Hence the ANOVA table is as follows:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Source
&\sphinxstyletheadfamily 
\sphinxAtStartPar
d.f.
&\sphinxstyletheadfamily 
\sphinxAtStartPar
SS
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Mean Square
\\
\hline
\sphinxAtStartPar
Between groups
&
\sphinxAtStartPar
\(K-1\)
&
\sphinxAtStartPar
\(SS_{Between}\)
&
\sphinxAtStartPar
\(MS_{Between}=\frac{SS_{Between}}{(K-1)}\)
\\
\hline
\sphinxAtStartPar
WIthin Groups
&
\sphinxAtStartPar
\(n-K\)
&
\sphinxAtStartPar
\(SS_{Within}\)
&
\sphinxAtStartPar
\(MS_{Within}=\frac{SS_{RES}}{n-K}\)
\\
\hline
\sphinxAtStartPar
Total
&
\sphinxAtStartPar
\(n-1\)
&
\sphinxAtStartPar
\(SS_{TOT}\)
&
\sphinxAtStartPar
\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{14.6.5.3 The F\sphinxhyphen{}test}
\label{\detokenize{14.g. Linear Regression III:the-f-test}}
\sphinxAtStartPar
To test the null hypothesis that the means in all groups are equal to a common value, the appropriate \(F\)\sphinxhyphen{}statistic is:
\begin{equation*}
\begin{split}
F = \frac{MS_{Between}}{MS_{Within}} \sim F_{(K-1), n-K} \text{ under } H_0.
\end{split}
\end{equation*}
\sphinxAtStartPar
If this test obtains a small \(p\)\sphinxhyphen{}value, then we have evidence that the means in the groups are not all the same. However, it does not tell us which of the group means differed from which other group means. For this reason, if we do find evidence of difference in means on an \(F\)\sphinxhyphen{}test, we may want to follow up with further analysis. Such further analysis may include pair\sphinxhyphen{}wise comparisons of means through analysis restricted to two groups.


\paragraph{Example}
\label{\detokenize{14.g. Linear Regression III:id4}}
\sphinxAtStartPar
We conduct an \(F\)\sphinxhyphen{}test to compare the average birthweights between babies whose mothers smoke and whose mothers don’t smoke using the birthweight data.

\sphinxAtStartPar
Let \(\mu_1\) and \(\mu_0\) denote the mean birthweight for babies whose mothers do smoke and don’t smoke, respectively. Then, the relevant hypotheses are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \mu_1= \mu_0\) (i.e. the birthweight of a baby does not depend on whether the mother smoked)

\item {} 
\sphinxAtStartPar
\(H_1: \mu_1\neq \mu_0\)

\end{itemize}

\sphinxAtStartPar
Recall that we previously defined Model 2 to related birthweight and mother’s smoking status:
\begin{equation*}
\begin{split} 
y_i = \alpha_0 + \alpha_1 s_i + \epsilon_i
\end{split}
\end{equation*}
\sphinxAtStartPar
Where \(Y\) denotes the birthweight and
\begin{equation*}
\begin{split} 
s_{i} =
\begin{cases}
    1 & \text{ if the mother smokes} \\
    0 & \text{ if the mother does not smoke}
\end{cases} 
\end{split}
\end{equation*}
\sphinxAtStartPar
We can rewrite this equation using the ANOVA model as follows:
\begin{equation*}
\begin{split}
\begin{align}
    y_{1i} &=\mu_1 + \epsilon_{1i}  \text{ if the mother smokes} \\
    y_{0i} &=\mu_0 + \epsilon_{0i}  \text{ if the mother does not smoke}
\end{align} \end{split}
\end{equation*}
\sphinxAtStartPar
Where \(y_{ki}\) is the mean birthweight in the \(k^{th}\) group (groups are defined by mother’s smoking status), \(\mu_1 = \beta_0 + \beta_1\) and \(\mu_0=\beta_0\) (in other words, our null hypothesis can be rewritten as: \(\beta_1=0\)).

\sphinxAtStartPar
We can use either \sphinxcode{\sphinxupquote{anova()}} or \sphinxcode{\sphinxupquote{summary()}} to conduct the test in R:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model2}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{lm}\PYG{p}{(}\PYG{n}{Birth.Weight}\PYG{o}{\PYGZti{}}\PYG{n+nf}{factor}\PYG{p}{(}\PYG{n}{Maternal.Smoker}\PYG{p}{)}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
\PYG{n+nf}{anova}\PYG{p}{(}\PYG{n}{model2}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{model2}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A anova: 2 × 5
\begin{tabular}{r|lllll}
  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\
  & <int> & <dbl> & <dbl> & <dbl> & <dbl>\\
\hline
	factor(Maternal.Smoker) &    1 &  24002.06 & 24002.0638 & 76.0167 & 9.461068e-18\\
	Residuals & 1172 & 370055.79 &   315.7473 &      NA &           NA\\
\end{tabular}\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
lm(formula = Birth.Weight \PYGZti{} factor(Maternal.Smoker), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
\PYGZhy{}68.085 \PYGZhy{}11.085   0.915  11.181  52.915 

Coefficients:
                            Estimate Std. Error t value Pr(\PYGZgt{}|t|)    
(Intercept)                 123.0853     0.6645 185.221   \PYGZlt{}2e\PYGZhy{}16 ***
factor(Maternal.Smoker)True  \PYGZhy{}9.2661     1.0628  \PYGZhy{}8.719   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 17.77 on 1172 degrees of freedom
Multiple R\PYGZhy{}squared:  0.06091,	Adjusted R\PYGZhy{}squared:  0.06011 
F\PYGZhy{}statistic: 76.02 on 1 and 1172 DF,  p\PYGZhy{}value: \PYGZlt{} 2.2e\PYGZhy{}16
\end{sphinxVerbatim}

\sphinxAtStartPar
In the ANOVA table, the \sphinxcode{\sphinxupquote{factor(Maternal.Smoker)}} row gives the between groups results and the \sphinxcode{\sphinxupquote{Residuals}} row gives the within group results.

\sphinxAtStartPar
The \(F\)\sphinxhyphen{}statistic is equal to 76.02 with a \(p\)\sphinxhyphen{}value equal to \(9.46\times10^{-18}\). This evidence suggests that there is a difference in the mean birthweight between the two groups defined by mother’s smoking status.


\section{14.7 Proofs}
\label{\detokenize{14.h. Linear Regression III:proofs}}\label{\detokenize{14.h. Linear Regression III::doc}}
\sphinxAtStartPar
This section contains two important proofs. These are not examinable.


\subsection{14.7.1 Proof for the ordinary least squares estimates in simple linear regression}
\label{\detokenize{14.h. Linear Regression III:proof-for-the-ordinary-least-squares-estimates-in-simple-linear-regression}}
\sphinxAtStartPar
Recall the ordinary least square (OLS) estimates of the intercept (\(\beta_0\)) and slope (\(\beta_1\)) in simple linear regression are:
\begin{equation*}
\begin{split}
\begin{align}
\hat{\beta_0} &= \bar{y} - \hat{\beta_0}\bar{x} \\
\hat{\beta_1} &= \frac{\sum_{i=1}^2 (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x_i})^2}
\end{align}
\end{split}
\end{equation*}
\sphinxAtStartPar
Proof:

\sphinxAtStartPar
To solve for the value of \(\beta_0\) that minimises \(SS_{RES}\), we differentiate \(SS_{RES}\) with respect to \(\hat{\beta}_0\) and set the derivative to zero:
\begin{equation*}
\begin{split} 
\frac{d(SS_{RES})}{d(\hat{\beta}_0)} = \sum_{i=1}^n -2(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)=0
\end{split}
\end{equation*}
\sphinxAtStartPar
Since \(\sum_{i=1}^n (y_i)=n\bar{y}\) and \(\sum_{i=1}^n(x_i)=n\bar{x}\), we can simplify to:
\begin{equation*}
\begin{split}
-n\bar{y}+n\hat{\beta}_0+n\hat{\beta}_1\bar{x}=0
\end{split}
\end{equation*}
\sphinxAtStartPar
Rearranging the above and divide by \(n\) to give:
\begin{equation*}
\begin{split}
\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}.
\end{split}
\end{equation*}
\sphinxAtStartPar
To solve for the value of \(\beta_1\) that minimises \(SS_{RES}\), we have to differentiate with respect to \(\hat{\beta}_1\). First, we substitute in our solution for \(\hat{\beta}_0\) as follows:
\begin{equation*}
\begin{split}
SS_{RES}=\sum_{i=1}^n(y_i-(\bar{y}-\hat{\beta}_1\bar{x})-\hat{\beta}_1x_i)^2=\sum_{i=1}^n ((y_i-\bar{y})-\hat{\beta}_1(x_i-\bar{x}))^2
\end{split}
\end{equation*}
\sphinxAtStartPar
Now differentiating the above with respect to \(\hat{\beta}_1\) and setting the differential to zero gives:
\begin{equation*}
\begin{split}
\frac{d(SS_{RES})}{d(\hat{\beta}_1)} = \sum_{i=1}^n -2(x_i-\bar{x})(y_i-\bar{y})+2\hat{\beta}_1(x_i-\bar{x})^2=0
\end{split}
\end{equation*}
\sphinxAtStartPar
Rearranging gives:
\begin{equation*}
\begin{split}
\hat{\beta}_1\sum_{i=1}^n (x_i-\bar{x})^2 = \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{split}
\end{equation*}

\subsection{14.7.2 Proof that the OLS estimates are also the maximum likelihood estimates}
\label{\detokenize{14.h. Linear Regression III:proof-that-the-ols-estimates-are-also-the-maximum-likelihood-estimates}}
\sphinxAtStartPar
If \(Y_i \overset{iid}{\sim} N(\mu, \sigma^2)\), the log likelihood function is:
\begin{equation*}
\begin{split}
l(\mu | y_1,...,y_n) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu)^2
\end{split}
\end{equation*}
\sphinxAtStartPar
So, for the simple linear regression model:
\begin{equation*}
\begin{split}
l(\beta_0, \beta_1 | y_1,...,y_n) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \beta_1x_i-\beta_0)^2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Therefore, for any fixed positive value for \(\sigma^2\), maximising the log likelihood function is equivalent to minimising \(SS_{RES}\) and so the OLS estimates are also maximum likelihood estimates of \(\beta_0\) and \(\beta_1\).


\chapter{15 Logistic Regression}
\label{\detokenize{15.a. Logistic Regression:logistic-regression}}\label{\detokenize{15.a. Logistic Regression::doc}}
\sphinxAtStartPar
In this session, we continue exploring regression modelling. We now extend the ideas encountered in the context of linear regression models and apply them to a setting where the outcome of interest is a binary variable.



\sphinxAtStartPar
By the end of this session, you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain the rationale and the structure behind the logistic regression

\item {} 
\sphinxAtStartPar
apply a logistic regression model to real data

\item {} 
\sphinxAtStartPar
interpret the results of a logistic regression

\item {} 
\sphinxAtStartPar
interpret output from key diagnosis tools used for logistic regression

\end{itemize}




\section{15.1 Regression modelling for binary outcomes}
\label{\detokenize{15.b. Logistic Regression:regression-modelling-for-binary-outcomes}}\label{\detokenize{15.b. Logistic Regression::doc}}
\sphinxAtStartPar
In many health applications, the outcome of interest is a binary outcome. Examples are 30\sphinxhyphen{}day mortality following surgery, 5\sphinxhyphen{}year survival following diagnosis of breast cancer, whether or not a particular side effect was experienced after taking a particular.

\sphinxAtStartPar
In this session, we explore a very commonly used technique in health data science: logistic regression. This can be seen as an extension of the linear regression model we have been exploring in the last few sessions. We will see that many aspects of linear regression modelling extend quite naturally to logistic regression modelling.


\section{15.1.2 Why can’t we just use Linear Regression?}
\label{\detokenize{15.b. Logistic Regression:why-cant-we-just-use-linear-regression}}
\sphinxAtStartPar
We developed our linear regression model for a continuous outcome. As we have seen, the linear regression model relies on a number of assumptions. Two of them, normality of residuals and homoscedasticity (constant residual variance) are particularly relevant to this discussion.

\sphinxAtStartPar
\sphinxstylestrong{Normality of errors:} The usual inference procedures (calculating confidence intervals and p\sphinxhyphen{}values) for linear regression assume that the errors follow a normal distribution. This assumption will be violated with binary outcome data.

\sphinxAtStartPar
\sphinxstylestrong{Homoscedasticity:} The error variance is constant. With binary data the variance is a function of the mean. Therefore, the variance would change as the mean changes. Therefore, this assumption is unlikely to hold.

\sphinxAtStartPar
An additional problem arises when we attempt to use linear regression to model binary outcome data, which relates to the fitted values.

\sphinxAtStartPar
\sphinxstylestrong{Fitted values can be impossible:} The mean of a binary outcome is the probability that it is a “success”, using the terminology of Bernoulli trials. This mean, because it is a probability, must lie between 0 and 1. If we tried to use linear regression to model our binary outcome we may find that some of the fitted values are below 0 or greater than 1. This problem arises largely because of the assumption that the mean is modelled by the linear predictor, an additive function of the covariates. This assumption is unlikely to be valid.

\sphinxAtStartPar
Logistic regression avoids these problems by relating the covariates (through the linear predictor) to a \sphinxstyleemphasis{function of} the mean, instead of the mean.


\section{15.2 Data used in our examples}
\label{\detokenize{15.c. Logistic Regression:data-used-in-our-examples}}\label{\detokenize{15.c. Logistic Regression::doc}}
\sphinxAtStartPar
We will use a dataset that is simulated to represent data from electronic health records for 200,000 patients. The outcome we will consider is whether or not a patient is diagnosed with dementia. In this example, there is an additional complexity because patients were followed up for different amounts of time. A longer follow\sphinxhyphen{}up will naturally lead to a higher probability of being diagnosed with dementia. In later modules, we will encounter survival analysis which allows the aspect of time to be accounted for. For now, we will ignore this aspect.

\sphinxAtStartPar
The code below reads in the dataset and displays the first few rows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we load the dataset and display its first lines}
\PYG{n}{dementia} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/Dementia/dementia2.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{head}\PYG{p}{(}\PYG{n}{dementia}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A data.frame: 6 × 30
\begin{tabular}{r|lllllllllllllllllllll}
  & id & prac & pr\_lcd & sex & age & bmi & bmi\_category & consultations & agegp & alcohol & ⋯ & mortality & date\_death & timetodementia & dementia & date\_dementia & end\_date & dob & rsample & vitd & lp\\
  & <int> & <int> & <chr> & <int> & <int> & <dbl> & <chr> & <int> & <int> & <chr> & ⋯ & <int> & <chr> & <dbl> & <int> & <chr> & <chr> & <chr> & <int> & <dbl> & <dbl>\\
\hline
	1 &  23189 & 142 & 08dec2009 & 1 & 53 & 20.4 & Normal (18.5-<25)      & 12 & 50 & 3-6 units/day & ⋯ & 0 &  & NA & 0 &  & 08dec2009 & 01nov1941 & 1 &       NA & -0.8153054\\
	2 &  92186 & 132 & 03feb2003 & 0 & 73 & 21.5 & Normal (18.5-<25)      &  4 & 70 & <2 units/day  & ⋯ & 0 &  & NA & 0 &  & 03feb2003 & 16jan1928 & 1 &       NA & -1.2268275\\
	3 & 187963 &  43 & 06jul2001 & 0 & 40 & 27.1 & Overweight (25-<30)    &  0 & 40 & <2 units/day  & ⋯ & 0 &  & NA & 0 &  & 06jul2001 & 18jun1961 & 1 &       NA & -0.6602434\\
	4 & 148379 & 215 & 08mar2012 & 1 & 40 & 20.9 & Normal (18.5-<25)      &  3 & 40 & <2 units/day  & ⋯ & 0 &  & NA & 0 &  & 08mar2012 & 10feb1952 & 1 & 23.22692 & -0.9507329\\
	5 &  44194 & 225 & 02feb2011 & 1 & 92 & 32.5 & Obese class I (30-<35) & 10 & 90 & Non drinker   & ⋯ & 0 &  & NA & 0 &  & 02feb2011 & 09dec1912 & 1 &       NA &  1.0403746\\
	6 & 169915 & 175 & 02nov2011 & 1 & 55 & 26.3 & Overweight (25-<30)    &  3 & 55 & 3-6 units/day & ⋯ & 0 &  & NA & 0 &  & 02nov2011 & 06oct1946 & 1 &       NA & -0.1080445\\
\end{tabular}\end{split}
\end{equation*}

\section{15.2.1 Exploratory analyses}
\label{\detokenize{15.c. Logistic Regression:exploratory-analyses}}
\sphinxAtStartPar
The variables we will use during this session are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
id: a variable that identifies a patient

\item {} 
\sphinxAtStartPar
sex: a factor variable that gives the sex of the patient (\(0\) for men, \(1\) for women)

\item {} 
\sphinxAtStartPar
age: age in years of the patient at study baseline

\item {} 
\sphinxAtStartPar
bmi: Body Mass Index of the patient at study baseline

\item {} 
\sphinxAtStartPar
dementia: an indicator variable that equals \(1\) if the patient is diagnosed with dementia during follow\sphinxhyphen{}up, \(0\) if not.

\end{itemize}

\sphinxAtStartPar
In this session the outcome of interest is dementia diagnosis, which we will treat as a binary variable. We are interested in modelling the relationship between dementia diagnosis and age, sex and BMI. Generally, we would expect older people to have a higher risk of being diagnosed with dementia. Females typically have higher risk. The relationship with BMI is less well understood.

\sphinxAtStartPar
The code below tabulates dementia and sex and draws box\sphinxhyphen{}plots of age and BMI, separately by dementia diagnosis status.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Tabulate dementia diagnosis versus sex (dementia = right\PYGZhy{}hand column)}
\PYG{p}{(}\PYG{n}{table}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{n+nf}{table}\PYG{p}{(}\PYG{n}{dementia}\PYG{o}{\PYGZdl{}}\PYG{n}{sex}\PYG{p}{,} \PYG{n}{dementia}\PYG{o}{\PYGZdl{}}\PYG{n}{dementia}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{prop.table}\PYG{p}{(}\PYG{n}{table}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Box plot of age by dementia diagnosis}
\PYG{n+nf}{par}\PYG{p}{(}\PYG{n}{mfrow}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+m}{1}\PYG{p}{,}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{boxplot}\PYG{p}{(}\PYG{n}{dementia}\PYG{o}{\PYGZdl{}}\PYG{n}{age} \PYG{o}{\PYGZti{}} \PYG{n}{dementia}\PYG{o}{\PYGZdl{}}\PYG{n}{dementia}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Age\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Dementia diagnosis\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Baseline age (years)\PYGZdq{}}\PYG{p}{)}
\PYG{n+nf}{boxplot}\PYG{p}{(}\PYG{n}{dementia}\PYG{o}{\PYGZdl{}}\PYG{n}{bmi} \PYG{o}{\PYGZti{}} \PYG{n}{dementia}\PYG{o}{\PYGZdl{}}\PYG{n}{dementia}\PYG{p}{,} \PYG{n}{main}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{BMI\PYGZdq{}}\PYG{p}{,} \PYG{n}{xlab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Dementia diagnosis\PYGZdq{}}\PYG{p}{,} \PYG{n}{ylab}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Baseline BMI\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   
         0      1
  0 107981   1707
  1  88132   2180
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   
             0          1
  0 0.98443768 0.01556232
  1 0.97586146 0.02413854
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{15.c. Logistic Regression_3_2}.png}

\sphinxAtStartPar
From the output above, we see that dementia is fairly rare in this study population, with 1.6\% of males receiving a dementia diagnosis during follow\sphinxhyphen{}up compared to a slightly higher 2.4\% among females.

\sphinxAtStartPar
The box\sphinxhyphen{}plots show that patients who received a dementia diagnosis during follow\sphinxhyphen{}up generally had a much higher age at baseline, as expected. The second box\sphinxhyphen{}plot perhaps hints at a slightly lower BMI among those diagnosed with dementia, but there is a less evident relationship than for age.


\section{15.3 The logistic regression model}
\label{\detokenize{15.d. Logistic Regression:the-logistic-regression-model}}\label{\detokenize{15.d. Logistic Regression::doc}}
\sphinxAtStartPar
Throughout this session we will assume that the outcome \(Y\) is binary. Further, we assume that \(Y\) takes a value of 0 (“failure”) or 1 (“success”). As we discussed earlier, the terminology of success and failure does not imply success is a good thing; in health applications “success” often refers to a bad outcome such as death.

\sphinxAtStartPar
We will initially consider the simple situation with a single independent variable of interest, \(X\). We assume that conditional on \(X\), the outcome \(Y\) follows a Bernoulli distribution:
\begin{equation*}
\begin{split}
Y | X=x \sim Bernoulli(\pi_x)
\end{split}
\end{equation*}
\sphinxAtStartPar
Then \(\pi_x\) is the conditional probability of sucess, given \(X=x\). It also represents the conditional expectation of the outcome, given \(X=x\).
\begin{equation*}
\begin{split}
\pi_x = E[Y | X=x] = P(Y=1 | X=x)
\end{split}
\end{equation*}
\sphinxAtStartPar
Typically, our research question involves relating this probability to the covariate(s).


\subsection{15.3.1 Components of the model}
\label{\detokenize{15.d. Logistic Regression:components-of-the-model}}

\subsubsection{The logit function}
\label{\detokenize{15.d. Logistic Regression:the-logit-function}}
\sphinxAtStartPar
As we have discussed, we do not wish to directly model \(\pi\), because fitted values can lie outside the possible range of values. Instead, we will first transform \(\pi\). In other words, we will model a function of \(\pi\). We want a one\sphinxhyphen{}to\sphinxhyphen{}one function (so we can back\sphinxhyphen{}transform to the original scale, if we wish) that maps a probability \(\pi\) to the whole real line.

\sphinxAtStartPar
The function that we use in logistic regression is called the \sphinxstylestrong{logit function}. Specifically,
\begin{equation*}
\begin{split}
logit(\pi) = log\left(\frac{\pi}{1-\pi}\right)
\end{split}
\end{equation*}
\sphinxAtStartPar
The probability \(\pi\) lies in the interval \([0,1]\) but the transformed value, \(logit(\pi)\) lies in the range \((-\infty, \infty)\).

\sphinxAtStartPar
It will also be useful to know how to back\sphinxhyphen{}transform. If \(logit(\pi) = L\) then
\begin{equation*}
\begin{split}
\pi = \frac{exp(L)}{1 + exp(L)}
\end{split}
\end{equation*}
\sphinxAtStartPar
This relationship will allow us to obtain fitted probabilities from our logistic regression model.


\paragraph{Odds}
\label{\detokenize{15.d. Logistic Regression:odds}}
\sphinxAtStartPar
Suppose we have a binary outcome, where the probability of success is \(\pi\), i.e. \(P(Y=1) = \pi\). Then the \sphinxstylestrong{odds} of success are given by
\begin{equation*}
\begin{split}
\frac{\pi}{1-\pi}
\end{split}
\end{equation*}
\sphinxAtStartPar
Therefore, \(logit(\pi)\) is the logarithm of the odds, or the log\sphinxhyphen{}odds. We will see below that using the logit function leads to the parameters of the regression model being interpreted in terms of odds and odds ratios.


\subsubsection{The linear predictor}
\label{\detokenize{15.d. Logistic Regression:the-linear-predictor}}
\sphinxAtStartPar
Just as for linear regression models, the linear predictor is an additive function of the independent variables. With a single covariate, it is simply:
\begin{equation*}
\begin{split}
\beta_0 + \beta_1 X
\end{split}
\end{equation*}

\subsection{15.3.2 The basic logistic regression model}
\label{\detokenize{15.d. Logistic Regression:the-basic-logistic-regression-model}}
\sphinxAtStartPar
The equation for a logistic regression model with, relating \(X\) to a binary outcome \(Y\) is:
\begin{equation*}
\begin{split}
logit(\pi_x) = \beta_0 + \beta_1 X
\end{split}
\end{equation*}
\sphinxAtStartPar
Note that, unlike linear regression, there is no explicit error term in the logistic regression model.


\subsubsection{Interpreting the parameters}
\label{\detokenize{15.d. Logistic Regression:interpreting-the-parameters}}
\sphinxAtStartPar
Suppose that our single covariate \(X\) is binary, taking values 1 (exposed, say) and 0 (unexposed). Our model is then:
\begin{equation*}
\begin{split}
logit(\pi_x) =
\begin{cases} \beta_0 &\text{when $X$=0 (unexposed group)} \\
\beta_0 + \beta_1 &\text{when $X$=1 (exposed group)}
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
In other words, we have:
\begin{equation*}
\begin{split}
\begin{align*}
\beta_0 &\qquad \text{is the log-odds of the outcome in the unexposed group} \\
\beta_0 + \beta_1 &\qquad \text{is the log-odds of the outcome in the exposed group}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Taking the exponential, we have
\begin{equation*}
\begin{split}
\begin{align*}
e^{\beta_0} &\qquad \text{is the odds of the outcome in the unexposed group} \\
e^{\beta_0 + \beta_1} &\qquad \text{is the odds of the outcome in the exposed group}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Now we have that \(e^{\beta_0 + \beta_1}  = e^{\beta_0} \times e^{\beta_1}\). Therefore, \(e^{\beta_1}\) also represents the multiplicative increase in the odds, going from the unexposed group to the exposed group. This multiplicative increase is known as the \sphinxstylestrong{odds ratio}.  Therefore,  we can also write:
\begin{equation*}
\begin{split}
\begin{align*}
e^{\beta_0} &\qquad \text{is the odds of the outcome in the unexposed group} \\
e^{\beta_1} &\qquad \text{is the odds ratio of the outcome, comparing the exposed group to the unexposed group}
\end{align*}
\end{split}
\end{equation*}

\subsubsection{General interpretation}
\label{\detokenize{15.d. Logistic Regression:general-interpretation}}
\sphinxAtStartPar
This leads us to the following interpretation of the model:
\begin{equation*}
\begin{split}
logit(\pi_x) = \beta_0 + \beta_1 X
\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
The intercept, \(\beta_0\) is the log\sphinxhyphen{}odds among those with \(X=0\). This is often called the \sphinxstylestrong{baseline log\sphinxhyphen{}odds}. Alternatively, the exponential \(e^{\beta_0}\) is the odds among those with \(X=0\).

\item {} 
\sphinxAtStartPar
The slope, \(\beta_1\), is the difference in the log\sphinxhyphen{}odds associated with a one\sphinxhyphen{}unit increase in \(X\). Equivalently, \(e^{\beta_1}\) is the odds ratio associated with a one\sphinxhyphen{}unit increase in \(X\).

\end{itemize}


\section{15.4 Estimating the parameters}
\label{\detokenize{15.e. Logistic Regression:estimating-the-parameters}}\label{\detokenize{15.e. Logistic Regression::doc}}
\sphinxAtStartPar
Having specified our model, we now want to use a sample of data to obtain estimates of the model parameters.


\subsection{15.4.1 Statistical model and observed data}
\label{\detokenize{15.e. Logistic Regression:statistical-model-and-observed-data}}
\sphinxAtStartPar
\sphinxstylestrong{Data:} Suppose we have a sample of \(n\) people. Person \(i\) has an observed \(X\) value of \(x_i\) and an observed outcome \(y_i\). Therefore, our sample of data consists of: \(\{ (x_i, y_i); i=1,2,...,n\}\).

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:} Our statistical model assumes that these observations are independent (between people) and are drawn from the distribution:
\begin{equation*}
\begin{split}
Y_i | X=x_i \sim Bernoulli(\pi_i)
\end{split}
\end{equation*}
\sphinxAtStartPar
where
\begin{equation*}
\begin{split}
\pi_i = P(Y_i=1 | X=x_i)
\end{split}
\end{equation*}
\sphinxAtStartPar
We further have a model relating the outcome to the independent variable:
\begin{equation*}
\begin{split}
logit(\pi_i) = \beta_0 + \beta_1 x_i \qquad \text{or, equivalently:} \qquad \pi_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}
\end{split}
\end{equation*}
\sphinxAtStartPar
We could put these aspects all together to write our statistical model concisely as:
\begin{equation*}
\begin{split}
Y_i | X=x_i \sim Bernoulli\left(  \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} \right)
\end{split}
\end{equation*}
\sphinxAtStartPar
Note
\begin{quote}

\sphinxAtStartPar
In the previous section, we used the notation \(\pi_x\) in order to emphasise that this probability is conditional on the value of \(X\). Now we are applying the distribution to a sample of people so we have changed to \(\pi_i\) to emphasise that the probability is conditional on whatever value \(X\) takes for person \(i\).
\end{quote}


\subsection{15.4.2 Maximum likelihood estimation}
\label{\detokenize{15.e. Logistic Regression:maximum-likelihood-estimation}}
\sphinxAtStartPar
We first need to derive the likelihood of the model. we assume that observations (people) are independent of each other, thus:
\begin{equation*}
\begin{split}
\begin{align*}
L(\beta_0, \beta_1) &= \prod_{i=1}^n Pr(Y_i = y_i|X_i=x_i) 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
We can write this as
\begin{equation*}
\begin{split}
\begin{align*}
L(\beta_0, \beta_1) &= \prod_{i=1}^n Pr(Y_i = 1 | X_i = x_i)^{y_i}Pr(Y_i = 0 | X_i = x_i)^{1-y_i}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Because when the observed outcome is 1, the second term above is 1 (recall \(x^0\)=1 for any \(x\)) so we just have \(Pr(Y_i = 1|X_i=x_i)\), which is equal to \(Pr(Y_i = y_i|X_i=x_i)\) when \(y_i = 1\). Conversely, when \(y_i = 0\), the first term becomes 1 and we are left with just the second term.

\sphinxAtStartPar
Now \(Pr(Y_i = 1 | X_i = x_i)\) is just the fraction within the Bernoulli distribution above, so we can substitute this in to get:
\begin{equation*}
\begin{split}
\begin{align*}
L(\beta_0, \beta_1) &= \prod_{i=1}^n \left(  \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} \right)^{y_i} \left( \frac{1}{1 + e^{\beta_0 + \beta_1 x_i}} \right)^{1-y_i} \\ & = \prod_{i=1}^n (e^{\beta_0 + \beta_1 x_i})^{y_i} \times \left(\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}} \right) 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Taking the log of the above likelihood, we derive the following log\sphinxhyphen{}likelihood
\begin{equation*}
\begin{split}
\begin{align*}
l(\beta_0, \beta_1) &= \sum_{i=1}^n \{ y_i log(e^{\beta_0 + \beta_1 x_i})  - log \left(1 + e^{\beta_0 + \beta_1 x_i} \right)  \} \\
 &= \sum_{i=1}^n \{ y_i (\beta_0 + \beta_1 x_i)  - log \left(1 + e^{\beta_0 + \beta_1 x_i} \right)  \}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
By maximizing this log\sphinxhyphen{}likelihood over the parameters \((\beta_0, \beta_1)\), we can obtain the maximum likelihood estimates of the parameters: \((\hat{\beta}_0, \hat{\beta}_1)\). There is no closed\sphinxhyphen{}form solution to this optimisation problem. Therefore, the maximisation over the parameters is done numerically.


\section{15.5 Examples}
\label{\detokenize{15.f. Logistic Regression:examples}}\label{\detokenize{15.f. Logistic Regression::doc}}

\subsection{15.5.1 Dementia and sex}
\label{\detokenize{15.f. Logistic Regression:dementia-and-sex}}
\sphinxAtStartPar
We now return to the dementia dataset and explore the relationship between sex and diagnosis of dementia during the study period. In this is example, our outcome \(Y\) is the binary variable of whether the patient was diagnosed with dementia during follow\sphinxhyphen{}up (1=yes, 0=no). Our single independent variable \(S\) is sex (0=male, 1=female). The logistic regression model we will fit is:
\begin{equation*}
\begin{split} 
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 s_i
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\pi_i=E(Y| S=s_i)\).

\sphinxAtStartPar
We will use the \sphinxcode{\sphinxupquote{glm()}} to perform simple linear regressions in R. Click \sphinxhref{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm}{here} for details of how this command works.

\sphinxAtStartPar
The following code can be used to perform this logistic regression in R. We need to specify the formula for the model, which is very similar to the syntax used in linear regression modelling. In addition, we now need to tell R that we are using the \sphinxcode{\sphinxupquote{logit}} function and that we are assuming that the data are assumed to follow a Bernoulli distribution (which, recall is a special case of the Binomial distribution).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dementia} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/Dementia/dementia2.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n}{dementia1} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{dementia} \PYG{o}{\PYGZti{}} \PYG{n}{sex}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{dementia}\PYG{p}{,} \PYG{n}{family} \PYG{o}{=} \PYG{n+nf}{binomial}\PYG{p}{(}\PYG{n}{link}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{logit\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
glm(formula = dementia \PYGZti{} sex, family = binomial(link = \PYGZdq{}logit\PYGZdq{}), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
\PYGZhy{}0.2211  \PYGZhy{}0.2211  \PYGZhy{}0.1771  \PYGZhy{}0.1771   2.8855  

Coefficients:
            Estimate Std. Error z value Pr(\PYGZgt{}|z|)    
(Intercept) \PYGZhy{}4.14722    0.02439 \PYGZhy{}170.01   \PYGZlt{}2e\PYGZhy{}16 ***
sex          0.44771    0.03264   13.72   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 38143  on 199998  degrees of freedom
AIC: 38147

Number of Fisher Scoring iterations: 7
\end{sphinxVerbatim}

\sphinxAtStartPar
We interpret the two estimated coefficients as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The estimated log\sphinxhyphen{}odds of dementia diagnosis among males (the “baseline” group, with \(S=0\)) is \sphinxhyphen{}4.147.

\item {} 
\sphinxAtStartPar
The estimated log odds ratio for females, compared with males, is 0.4477.

\end{itemize}

\sphinxAtStartPar
For a slightly more intuitive interpretation, we will take the exponential transformation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{exp}\PYG{p}{(}\PYG{n+nf}{coefficients}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}\begin{description*}
\item[(Intercept)] 0.0158083366516896
\item[sex] 1.5647202094567
\end{description*}\end{split}
\end{equation*}
\sphinxAtStartPar
Now we can equivalently, and perhaps more intuitively, interpret the coefficients as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The estimated odds of dementia diagnosis among males is 0.0158.

\item {} 
\sphinxAtStartPar
The estimated odds ratio for females, compared with males, is 1.576. In other words, the odds of dementia diagnosis among females is estimated to be 1.576 times higher than among males.

\end{itemize}


\subsection{15.5.2 Dementia and age}
\label{\detokenize{15.f. Logistic Regression:dementia-and-age}}
\sphinxAtStartPar
We now explore the relationship of dementia diagnosis and age, measured in years. In this is example, our outcome \(Y\) remains dementia diagnosis, as above, but our single independent variable \(A\) is age, measured in years. The logistic regression model we will fit is:
\begin{equation*}
\begin{split} 
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 a_i
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\pi_i=E(Y| A=a_i)\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dementia2} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{dementia} \PYG{o}{\PYGZti{}} \PYG{n}{age}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{dementia}\PYG{p}{,} \PYG{n}{family} \PYG{o}{=} \PYG{n+nf}{binomial}\PYG{p}{(}\PYG{n}{link}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{logit\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
glm(formula = dementia \PYGZti{} age, family = binomial(link = \PYGZdq{}logit\PYGZdq{}), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
\PYGZhy{}0.9935  \PYGZhy{}0.1989  \PYGZhy{}0.1140  \PYGZhy{}0.0721   3.5947  

Coefficients:
              Estimate Std. Error z value Pr(\PYGZgt{}|z|)    
(Intercept) \PYGZhy{}10.533958   0.103139 \PYGZhy{}102.13   \PYGZlt{}2e\PYGZhy{}16 ***
age           0.101865   0.001402   72.66   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 31876  on 199998  degrees of freedom
AIC: 31880

Number of Fisher Scoring iterations: 8
\end{sphinxVerbatim}

\sphinxAtStartPar
We interpret the two estimated coefficients as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The estimated log\sphinxhyphen{}odds of dementia diagnosis among people aged 0 is is \sphinxhyphen{}10.53. Of course, this is not a meaningful quantity. As for linear regression, we could center the age variable to provide an interpretable intercept.

\item {} 
\sphinxAtStartPar
The estimated log odds ratio for each increase of one year in age is 0.101.

\end{itemize}

\sphinxAtStartPar
For a slightly more intuitive interpretation, we will take the exponential transformation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{exp}\PYG{p}{(}\PYG{n+nf}{coefficients}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}\begin{description*}
\item[(Intercept)] 2.66170781376369e-05
\item[age] 1.10723429559233
\end{description*}\end{split}
\end{equation*}
\sphinxAtStartPar
Now we can interpret the two estimated coefficients as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The estimated odds of dementia diagnosis among people aged 0 is is 2.66.

\item {} 
\sphinxAtStartPar
The estimated odds ratio for each increase of one year in age is 1.107. In other words, the estimated odds of dementia diagnosis is multiplied by 1.11 (or, increased by 11\%) with each increase in year of age at study baseline.

\end{itemize}


\section{15.6 Inference}
\label{\detokenize{15.g. Logistic Regression:inference}}\label{\detokenize{15.g. Logistic Regression::doc}}
\sphinxAtStartPar
We have fitted the following logistic regression model:
\begin{equation*}
\begin{split}
logit(\pi_i) = \beta_0 + \beta_1 x_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
Having estimated the parameters of the logistic regression model using maximum likelihood estimation, we would like to obtain 95\% confidence intervals for the parameters and perform hypothesis testing. We will now explore options available to do those things.

\sphinxAtStartPar
A sketch of the relevant statistical theory is provided in the optional reading in the appendix to this session.


\subsection{15.6.1 Confidence intervals}
\label{\detokenize{15.g. Logistic Regression:confidence-intervals}}
\sphinxAtStartPar
A number of approximate confidence intervals can be obtained. Two commonly used confidence intervals are the Wald\sphinxhyphen{}type confidence intervals and profile confidence intervals.

\sphinxAtStartPar
\sphinxstylestrong{Wald\sphinxhyphen{}type confidence interval:} This confidence interval takes a familiar form. For slope parameter \(\beta_1\), an approximate 95\% confidence inteval is given by
\begin{equation*}
\begin{split}
\hat{\beta}_1 \pm 1.96 \hat{SE}(\hat{\beta}_1)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\hat{\beta}_1\) is the maximum likelihood estimate for \(\beta_1\) and \(\hat{SE}(\hat{\beta}_1)\) is its standard error.

\sphinxAtStartPar
\sphinxstylestrong{Profile likelihood confidence intervals}  These intervals are based on the log\sphinxhyphen{}likelihood\sphinxhyphen{}ratio. For each parameter of interest, a \sphinxstylestrong{profile} likelihood is constructed, which treats all other parameters as nuisances and removes them from the likelihood (by setting to their values which maximise the likelihood for each value of the parameter of interest). Then confidence intervals are constructed based on the profile likelihood. The Wald\sphinxhyphen{}type confidence intervals provide an approximation to this process. Profile likelihod confidence intervals are provided in R using the command \sphinxcode{\sphinxupquote{confint}}.


\subsection{15.6.2 Hypothesis tests}
\label{\detokenize{15.g. Logistic Regression:hypothesis-tests}}
\sphinxAtStartPar
Often, the  hypothesis we are interested in testing is that the independent variable \(X\) is \sphinxstyleemphasis{not associated with} the outcome. Therefore, the null and alternative hypotheses are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \beta_1 = 0\)

\item {} 
\sphinxAtStartPar
\(H_1: \beta_1 \neq 0\)

\end{itemize}

\sphinxAtStartPar
This is the null hypothesis tested, by default, in regression output provided in R.

\sphinxAtStartPar
There are three important type of tests available. These are all approximate tests and are asymptotically equivalent to one another. So in large samples, we would expect to see similar p\sphinxhyphen{}values from each test.

\sphinxAtStartPar
\sphinxstylestrong{Likelihood ratio test} This test is based directly on the approximate distribution of the log\sphinxhyphen{}likelihood\sphinxhyphen{}ratio.

\sphinxAtStartPar
\sphinxstylestrong{Wald test}  This test is based on a quadratic approximation to the log\sphinxhyphen{}likelihood\sphinxhyphen{}ratio. As such, it can be less accurate than the likelihood ratio test, particularly if the null value is a long way from the maximum likelihood estimate. However, in this case all tests are likely to provide small p\sphinxhyphen{}values and similar qualitative conclusions.

\sphinxAtStartPar
The Wald test is used to obtain the p\sphinxhyphen{}values automatically displayed in regression output for GLMs in R and many other software platforms. This is because Wald tests are computationally less intensive than likelihood ratio tests.

\sphinxAtStartPar
\sphinxstylestrong{Score test} These tests are based on a slightly different quadratic approximation to the log\sphinxhyphen{}likelihood\sphinxhyphen{}ratio. This type of test is much less used than the other types, so we do not pursue this further here. Early tests used in epidemiology tended to be score tests, since they are less computationally intensive than the other approaches.


\subsection{15.6.3 Example}
\label{\detokenize{15.g. Logistic Regression:example}}
\sphinxAtStartPar
We return to our model exploring the association between sex and diagnosis of dementia. We first perform a hypothesis test investigating the null hypothesis that sex is not associated with dementia diagnosis. Then we obtain 95\% confidence intervals for our two parameters of interest.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dementia} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/Dementia/dementia2.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n}{dementia1} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{dementia} \PYG{o}{\PYGZti{}} \PYG{n}{sex}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{dementia}\PYG{p}{,} \PYG{n}{family} \PYG{o}{=} \PYG{n+nf}{binomial}\PYG{p}{(}\PYG{n}{link}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{logit\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
glm(formula = dementia \PYGZti{} sex, family = binomial(link = \PYGZdq{}logit\PYGZdq{}), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
\PYGZhy{}0.2211  \PYGZhy{}0.2211  \PYGZhy{}0.1771  \PYGZhy{}0.1771   2.8855  

Coefficients:
            Estimate Std. Error z value Pr(\PYGZgt{}|z|)    
(Intercept) \PYGZhy{}4.14722    0.02439 \PYGZhy{}170.01   \PYGZlt{}2e\PYGZhy{}16 ***
sex          0.44771    0.03264   13.72   \PYGZlt{}2e\PYGZhy{}16 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 38143  on 199998  degrees of freedom
AIC: 38147

Number of Fisher Scoring iterations: 7
\end{sphinxVerbatim}

\sphinxAtStartPar
The p\sphinxhyphen{}value for sex is \(p<0.001\), providing strong evidence against the null hypothesis that sex is not associated with the odds of being diagnosed with dementia.

\sphinxAtStartPar
Now we will obtain the profile confidence intervals for the two estimated regression coefficients:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{confint}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Waiting for profiling to be done...
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 2 × 2 of type dbl
\begin{tabular}{r|ll}
  & 2.5 \% & 97.5 \%\\
\hline
	(Intercept) & -4.1954026 & -4.0997726\\
	sex &  0.3838153 &  0.5117587\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
In fact, these are more easily interpreted on the exponentiated scale, as below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{cbind}\PYG{p}{(}\PYG{n+nf}{exp}\PYG{p}{(}\PYG{n+nf}{coefficients}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{exp}\PYG{p}{(}\PYG{n+nf}{confint}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Waiting for profiling to be done...
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 2 × 3 of type dbl
\begin{tabular}{r|lll}
  &  & 2.5 \% & 97.5 \%\\
\hline
	(Intercept) & 0.01580834 & 0.01506468 & 0.01657644\\
	sex & 1.56472021 & 1.46787427 & 1.66822252\\
\end{tabular}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
The estimated odds in males is 0.0158 (95\% CI 0.01506, 0.01657). We are 95\% confident that thee odds of dementia diagnosis among males lies within this range.

\item {} 
\sphinxAtStartPar
The estimated odds ratio for females, compared with males, is 1.56 (95\% CI 1.47, 1.67). We estimate that the odds of dementia diagnosis is 1.56 times higher among females than among males. The data are consistent with this value being as low as 1.47 or as high as 1.67.

\end{itemize}

\sphinxAtStartPar
Below is the code to obtain Wald test confidence intervals. Comparing these with the (unexponentiated) confidence intervals above, we see these are very similar, as we would expect.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{confint.default}\PYG{p}{(}\PYG{n}{dementia1}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 2 × 2 of type dbl
\begin{tabular}{r|ll}
  & 2.5 \% & 97.5 \%\\
\hline
	(Intercept) & -4.1950299 & -4.0994058\\
	sex &  0.3837405 &  0.5116735\\
\end{tabular}\end{split}
\end{equation*}

\section{15.7 Multivariable logistic regression}
\label{\detokenize{15.h. Logistic Regression:multivariable-logistic-regression}}\label{\detokenize{15.h. Logistic Regression::doc}}
\sphinxAtStartPar
Suppose we wish to relate a binary outcome (\(Y\)) to \(p\) predictor variables \((X_1, X_2, ..., X_p)\). The appropriate multivariable logistic regression model is a straightforward extension of the simple logistic regression model:
\begin{equation*}
\begin{split}
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ,..., \beta_p x_{ip}
\end{split}
\end{equation*}
\sphinxAtStartPar
where, \(x_{ji}\) is the value of the jth predictor variable for the ith participant and \(\pi_i = P(Y_i = 1 | X_1=x_1, ..., X_p=x_p)\).

\sphinxAtStartPar
The parameters in the model are interpreted as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\) is the intercept. It is the estimated log\sphinxhyphen{}odds of \(Y\) when all the \(X_j\)’s are zero.

\item {} 
\sphinxAtStartPar
\(\beta_j\) is the expected change in the log\sphinxhyphen{}odds of \(Y\) for a 1 unit increase in \(X_j\) \sphinxstyleemphasis{with all the other covariates held constant}.

\end{itemize}

\sphinxAtStartPar
The \(\beta_j\)’s are the \sphinxstylestrong{regression coefficients} (otherwise known as \sphinxstylestrong{partial regression coefficients}). Each one measures the effect of one covariate controlled (or adjusted) for all of the others.

\sphinxAtStartPar
The maximum likelihood estimation process outlined earlier can be naturally extended to the multivariable model above.


\subsection{15.7.1 Example}
\label{\detokenize{15.h. Logistic Regression:example}}
\sphinxAtStartPar
We consider an example using the dementia dataset. This time, our interest lies in modeling the relationship between the odds of being diagnosed with dementia during study follow\sphinxhyphen{}up and to sex (\(S\)), age (\(A\)) and BMI (\(B\)) at study baseline.

\sphinxAtStartPar
Our multivariable logistic regression model is:
\begin{equation*}
\begin{split}
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 s_i + \beta_2 a_i + \beta_3 b_i
\end{split}
\end{equation*}
\sphinxAtStartPar
This model can be estimated in \sphinxcode{\sphinxupquote{R}}using the \sphinxcode{\sphinxupquote{glm}} function

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dementia} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/Dementia/dementia2.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n}{dementia2} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{dementia} \PYG{o}{\PYGZti{}} \PYG{n}{sex} \PYG{o}{+} \PYG{n}{age} \PYG{o}{+} \PYG{n}{bmi}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{dementia}\PYG{p}{,} \PYG{n}{family} \PYG{o}{=} \PYG{n+nf}{binomial}\PYG{p}{(}\PYG{n}{link}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{logit\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
glm(formula = dementia \PYGZti{} sex + age + bmi, family = binomial(link = \PYGZdq{}logit\PYGZdq{}), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
\PYGZhy{}1.1067  \PYGZhy{}0.1959  \PYGZhy{}0.1134  \PYGZhy{}0.0732   3.6917  

Coefficients:
             Estimate Std. Error z value Pr(\PYGZgt{}|z|)    
(Intercept) \PYGZhy{}9.783837   0.152138 \PYGZhy{}64.309  \PYGZlt{} 2e\PYGZhy{}16 ***
sex          0.306798   0.033773   9.084  \PYGZlt{} 2e\PYGZhy{}16 ***
age          0.098682   0.001413  69.826  \PYGZlt{} 2e\PYGZhy{}16 ***
bmi         \PYGZhy{}0.025619   0.003596  \PYGZhy{}7.124 1.05e\PYGZhy{}12 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 31732  on 199996  degrees of freedom
AIC: 31740

Number of Fisher Scoring iterations: 8
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{cbind}\PYG{p}{(}\PYG{n+nf}{exp}\PYG{p}{(}\PYG{n+nf}{coefficients}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n+nf}{exp}\PYG{p}{(}\PYG{n+nf}{confint}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Waiting for profiling to be done...
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}A matrix: 4 × 3 of type dbl
\begin{tabular}{r|lll}
  &  & 2.5 \% & 97.5 \%\\
\hline
	(Intercept) & 5.635516e-05 & 4.179134e-05 & 7.587428e-05\\
	sex & 1.359066e+00 & 1.272090e+00 & 1.452170e+00\\
	age & 1.103716e+00 & 1.100675e+00 & 1.106790e+00\\
	bmi & 9.747061e-01 & 9.678335e-01 & 9.815740e-01\\
\end{tabular}\end{split}
\end{equation*}
\sphinxAtStartPar
We can interpret the parameters as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{sex}: Females are estimated to have 1.36 times higher odds of being diagnosed with dementia than men \sphinxstyleemphasis{who have the same age and BMI at study baseline}. The data are consistent with the true odds ratio lying between 1.27 and 1.45. The p\sphinxhyphen{}value, \(p<0.001\), provides strong evidence against the null hypothesis of no association between sex and dementia \sphinxstyleemphasis{after adjusting for age and BMI}. 

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{age}: The odds of being diagnosed with dementia is estimated to increase 1.1\sphinxhyphen{}fold for each increase in year of age at study baseline. The data are consistent with the true odds ratio lying between 1.1006 and 1.107.  The p\sphinxhyphen{}value, \(p<0.001\), provides strong evidence against the null hypothesis of no association between age and dementia \sphinxstyleemphasis{after adjusting for sex and BMI}. 

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{bmi}: The odds of being diagnosed with dementia is estimated to reduce by 0.97 times for each increase in unit of BMI, suggesting an inverse association between BMI and odds of dementia diagnosis.  The p\sphinxhyphen{}value, \(p<0.001\), provides strong evidence against the null hypothesis of no association between BMI and dementia \sphinxstyleemphasis{after adjusting for sex and age}. 

\end{itemize}


\section{15.8 Interactions and higher\sphinxhyphen{}order terms}
\label{\detokenize{15.i. Logistic Regression:interactions-and-higher-order-terms}}\label{\detokenize{15.i. Logistic Regression::doc}}
\sphinxAtStartPar
We are often interested in exploring whether associations between an independent variable and our outcome differ depending on the value taken by another independent variable, i.e. we are interested in \sphinxstyleemphasis{interactions}.

\sphinxAtStartPar
Similarly, we may be modelling the relationship between the odds of the outcome and a continuous covariate, and wish to explore whether the relationshp is linear on the log\sphinxhyphen{}odds scale.

\sphinxAtStartPar
The good news is that the same techniques that we met in linear regression modelling can be applied here. We can add interactions, quadratic or higher order terms or splines to our logistic regression model.


\section{15.9 Model diagnostics}
\label{\detokenize{15.j. Logistic Regression:model-diagnostics}}\label{\detokenize{15.j. Logistic Regression::doc}}
\sphinxAtStartPar
This material is not examinable and is provided for your information.

\sphinxAtStartPar
Many model diagnostics are available for logistic regression models. We touch on a few very briefly here.


\subsection{15.9.1 Goodness\sphinxhyphen{}of\sphinxhyphen{}fit}
\label{\detokenize{15.j. Logistic Regression:goodness-of-fit}}

\subsubsection{Deviance}
\label{\detokenize{15.j. Logistic Regression:deviance}}
\sphinxAtStartPar
The deviance of a model \(M\) is a measure of the goodness\sphinxhyphen{}of\sphinxhyphen{}fit of the model. It is defined as
\begin{equation*}
\begin{split}
D = -2(l_M - l_S)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(l_M\)  is the log\sphinxhyphen{}likelihood of model \(M\) and \(l_S\) is the log\sphinxhyphen{}likelihood of the saturated model (one which uses the maximum possible number of parameters without redundancies; this is the model with the best possible fit).

\sphinxAtStartPar
In general, higher values of deviance indicate worse model fit to the data. Two deviance statistics are often produced in output following logistic regression:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Null deviance: the deviance computed for the null model, i.e. the minimal model containing only an intercept.

\item {} 
\sphinxAtStartPar
Residual deviance: the deviance computed for the model that has just been estimated.

\end{itemize}

\sphinxAtStartPar
Note
\begin{quote}

\sphinxAtStartPar
When computing deviances of different models for the same dataset, the log\sphinxhyphen{}likelihood of the saturated model \(l_S\) is constant. Therefore, statistical software (including the output from \sphinxcode{\sphinxupquote{glm}}) often provides the deviance in a simplified form: as \(-2 l_M\).
\end{quote}


\subsubsection{Akaike information criterion}
\label{\detokenize{15.j. Logistic Regression:akaike-information-criterion}}
\sphinxAtStartPar
The Akaike information criterion (AIC) quantifies model fit as a function of the likelihood and the number of parameters being estimated. It is defined as \(AIC = 2k - 2 l(\hat{\beta})\) where \(k\) is the number of parameter in the model and \(l(\hat{\beta})\) the log\sphinxhyphen{}likelihood of the model computed at the estimated parameter values \(\hat{\beta}\).

\sphinxAtStartPar
The AIC is mainly used as a way to compare different models. The best model, in the scale of the AIC, is the one with the lowest AIC. (Note that sometimes, contrarily to the \sphinxcode{\sphinxupquote{glm}} package, the AIC is computed as \(AIC = -2k + 2l(\hat{\beta})\) in which case the best model would be the one with the highest AIC value.)

\sphinxAtStartPar
The AIC is actually minus the sum of the deviance and twice the number of the parameters. By including the number of parameters, the AIC penalizes models that have too many parameters, thus avoiding the selection of overfitted models.


\subsubsection{McFadden pseudo\sphinxhyphen{}\protect\(R^2\protect\)}
\label{\detokenize{15.j. Logistic Regression:mcfadden-pseudo-r-2}}
\sphinxAtStartPar
For the linear regression model, the coefficient of determination (\(R^2\)) measures how much variability is explained by the model.

\sphinxAtStartPar
For the logistic regression model, several generalization of the \(R^2\) measure have been proposed. Here, we will focus on the McFadden’s pseudo\sphinxhyphen{}\(R^2\). The McFadden \(R^2\) is defined as follow:
\begin{equation*}
\begin{split}
R^2_{McFadden} = 1 - \frac{l_M}{l_0}
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(l_M\) is the log\sphinxhyphen{}likelihood of the estimated model and \(l_0\) is the log\sphinxhyphen{}likelihood of the null model (containing an intercept only).

\sphinxAtStartPar
The rationale behind this measure is that when the estimated model does not explain correctly the variability, its log\sphinxhyphen{}likelihood will be close to the null log\sphinxhyphen{}likelihood so that the ratio will be close to \(1\) and the McFadden’s pseudo\sphinxhyphen{}\(R^2\) close to \(0\). Conversely, when the model correctly explains the variability of the model, the likelihood will be close to \(1\) and therefore \(l_M\) will be close to \(0\) so that the McFadden’s pseudo\sphinxhyphen{}\(R^2\) will be close to \(1\). However, when applied to a classic linear regression model, the McFadden’s pseudo\sphinxhyphen{}\(R^2\) is not equivalent to the classic \(R_2\).


\subsubsection{The Hosmer\sphinxhyphen{}Lemeshow test}
\label{\detokenize{15.j. Logistic Regression:the-hosmer-lemeshow-test}}
\sphinxAtStartPar
The Hosmer\sphinxhyphen{}Lemeshow test is a classic approach to assess the goodness\sphinxhyphen{}of\sphinxhyphen{}fit of a logistic regression model. The rationale of this test is to divide the vector of predicted probabilites \(\hat{\pi} = (\hat{\pi}_i)\) with \(i=1,\dots,n\) into \(G\) groups, e.g. based on the quantiles, with \(n_g\) subjects. In each group, the mean of the predicted probabilites \(\bar{\pi}_g\) is compared to the proportion of observed success. Formally, for the group \(g=1,\dots,G\), we have that
\begin{itemize}
\item {} 
\sphinxAtStartPar
the observed values are
\begin{itemize}
\item {} 
\sphinxAtStartPar
for Y = 1: \(y_g\)

\item {} 
\sphinxAtStartPar
for Y = 0: \(n_g - y_g\)

\end{itemize}

\item {} 
\sphinxAtStartPar
the predicted values are
\begin{itemize}
\item {} 
\sphinxAtStartPar
for Y = 1: \(\bar{\pi}_gn_g\)

\item {} 
\sphinxAtStartPar
for Y = 0: \(n_g(1 - \bar{\pi}_g)\)

\end{itemize}

\end{itemize}

\sphinxAtStartPar
The Hosmer\sphinxhyphen{}Lemeshow test statistics is based on the chi\sphinxhyphen{}square statistics computed over all groups and all possible values for \(Y\)
\begin{equation*}
\begin{split}\sum_{g=1}^G\sum_{l=0}^1 \frac{(o_{gl} - e_{gl})^2}{e_{gl}} = \sum_{g=1}^G \frac{(n_g\bar{\pi}_g - y_g)^2}{n_g\bar{\pi}_g(1-\bar{\pi}_g)}\end{split}
\end{equation*}
\sphinxAtStartPar
and has been shown to follow asymptotically a \(\chi^2\) distribution with \(g-2\) degrees of freedom under the null hypothesis of a correctly specified model. However, we insist on the fact that this test if often criticized for several reasons. First it is known to have low power. Secondly, its results can be sensible to the choice of the number of groups \(G\) and this is even worst for small sample sizes.

\sphinxAtStartPar
The Hesmer\sphinxhyphen{}Lemeshow test statistics has not been implemented into the \sphinxcode{\sphinxupquote{glm}} package but is available on the \sphinxcode{\sphinxupquote{ResourceSelection}} package through the \sphinxcode{\sphinxupquote{hoslem.test}} function.


\section{15.12 Common pitfalls}
\label{\detokenize{15.k. Logistic Regression:common-pitfalls}}\label{\detokenize{15.k. Logistic Regression::doc}}

\subsection{15.12.1 Perfect separation}
\label{\detokenize{15.k. Logistic Regression:perfect-separation}}
\sphinxAtStartPar
Perfect separation happens when the outcome can be directly predicted from one of the predictor variables. For example, let say that we model an outcome \(Y\) using one explanatory standard gaussian variable \(X_1\) and that \(Y\) is such that \(Y=0\) whenever \(X_1\leq0\) and \(Y=1\) whenever \(X_1>0\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x1} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rnorm}\PYG{p}{(}\PYG{l+m}{1000}\PYG{p}{,} \PYG{l+m}{0}\PYG{p}{,} \PYG{l+m}{1}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{p}{(}\PYG{n}{x1} \PYG{o}{\PYGZlt{}=} \PYG{l+m}{0}\PYG{p}{)}\PYG{o}{*}\PYG{l+m}{1}

\PYG{n}{data\PYGZus{}sep} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{data.frame}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{n}{x1}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Let us try to estimate this logistic regression model

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model\PYGZus{}sep} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZti{}} \PYG{n}{x1}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{data\PYGZus{}sep}\PYG{p}{,} \PYG{n}{family} \PYG{o}{=} \PYG{n+nf}{binomial}\PYG{p}{(}\PYG{n}{link}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{logit\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning message:
“glm.fit: algorithm did not converge”
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning message:
“glm.fit: fitted probabilities numerically 0 or 1 occurred”
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{R}} detects the perfect separation and prompts an error that states that \sphinxcode{\sphinxupquote{fitted probabilities numerically 0 or 1 occurred}}. The reason of this error is that, due to the perfect separation, the maximum likelihood of the parameter \(\beta_1\) for the variable \(X_1\) cannot be estimated as its value is actually infinite. Options to consider when facing this issue include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
removing the problematic variable from the model

\item {} 
\sphinxAtStartPar
setting \(\beta_1\) at an arbitrary high value and estimate the model

\item {} 
\sphinxAtStartPar
changing the model or manipulating the data

\end{itemize}

\sphinxAtStartPar
Note that, in practice, perfect separation is not very likely to happen. However, \sphinxstyleemphasis{quasi\_perfect} separation is totally possible and needs to be tackled. For more details about how to handle separation, one can read the following articles:
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{Heinze, G., \& Schemper, M. (2002). A solution to the problem of separation in logistic regression. Statistics in Medicine}

\sphinxAtStartPar
\sphinxstyleemphasis{Firth, D. (1993). Bias Reduction of Maximum Likelihood Estimates. Biometrika}
\end{quote}


\subsection{15.12.2 Low events per variable}
\label{\detokenize{15.k. Logistic Regression:low-events-per-variable}}
\sphinxAtStartPar
A common issue when estimating logistic regression model is the problem of the ratio between the number of events and the number of predictive variables. This ratio is known as \sphinxstyleemphasis{Events Per Variable}. When this ratio is low, it can lead to biased estimation and models with poor predictive abilities.

\sphinxAtStartPar
In the biomedical literature, the so\sphinxhyphen{}called \sphinxstyleemphasis{ten events per variable rule} is commonly used. However, we emphasize here the absence of theoretical justification and even the lack of actual evidence that this rule gives good results. If you want more information about the issues raised by this commonly used rule, you can read the following article:
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{Smeden, M., de Groot, J.A., Moons, K.G. et al. (2016) No rationale for 1 variable per 10 events criterion for binary logistic regression analysis. BMC Med Res Methodol}.
\end{quote}


\subsection{15.12.3 Influential values}
\label{\detokenize{15.k. Logistic Regression:influential-values}}
\sphinxAtStartPar
Another aspect to take into account when estimating a logistic regression model is the presence of influential values among the observations which, as their names indicates, might have a huge impact on the estimation of the model. The Cook’s distance is a useful measure to assess how influential an observation is. It measures how much the outcome would be modifier by removing this observation from the data.

\sphinxAtStartPar
In, \sphinxcode{\sphinxupquote{R}}, the Cook’s distance can be easily plotted and directly plotted by specifying \sphinxcode{\sphinxupquote{which = 4}} as an argument to the \sphinxcode{\sphinxupquote{plot}} function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dementia} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{read.csv}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Practicals/Datasets/Dementia/dementia2.csv\PYGZdq{}}\PYG{p}{)}
\PYG{n}{dementia2} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{dementia} \PYG{o}{\PYGZti{}} \PYG{n}{sex} \PYG{o}{+} \PYG{n}{age} \PYG{o}{+} \PYG{n}{bmi}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{dementia}\PYG{p}{,} \PYG{n}{family} \PYG{o}{=} \PYG{n+nf}{binomial}\PYG{p}{(}\PYG{n}{link}\PYG{o}{=}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{logit\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{)}
\PYG{n+nf}{options}\PYG{p}{(}\PYG{n}{repr.plot.height}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{,} \PYG{n}{repr.plot.width}\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}
\PYG{n+nf}{plot}\PYG{p}{(}\PYG{n}{dementia2}\PYG{p}{,} \PYG{n}{which} \PYG{o}{=} \PYG{l+m}{4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
glm(formula = dementia \PYGZti{} sex + age + bmi, family = binomial(link = \PYGZdq{}logit\PYGZdq{}), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
\PYGZhy{}1.1067  \PYGZhy{}0.1959  \PYGZhy{}0.1134  \PYGZhy{}0.0732   3.6917  

Coefficients:
             Estimate Std. Error z value Pr(\PYGZgt{}|z|)    
(Intercept) \PYGZhy{}9.783837   0.152138 \PYGZhy{}64.309  \PYGZlt{} 2e\PYGZhy{}16 ***
sex          0.306798   0.033773   9.084  \PYGZlt{} 2e\PYGZhy{}16 ***
age          0.098682   0.001413  69.826  \PYGZlt{} 2e\PYGZhy{}16 ***
bmi         \PYGZhy{}0.025619   0.003596  \PYGZhy{}7.124 1.05e\PYGZhy{}12 ***
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 31732  on 199996  degrees of freedom
AIC: 31740

Number of Fisher Scoring iterations: 8
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{15.k. Logistic Regression_5_1}.png}

\sphinxAtStartPar
As you can see from the above example, some observations seems to have higher influence than the other. However, if we look at the y\sphinxhyphen{}axis scale, this difference is not huge.

\sphinxAtStartPar
If some observations appear to have a lot of influence on the estimated regression coefficients, it is important to assess the robustness of your conclusions to these observations. This is typically done using \sphinxstyleemphasis{sensitivity analysis}, i.e. performing the analysis including and excluding the problematic observations.

\sphinxAtStartPar
Note that we would not recommend excluding observations from an analysis entirely just because they are influential or outlying.


\section{15.13 Further resources}
\label{\detokenize{15.l. Logistic Regression:further-resources}}\label{\detokenize{15.l. Logistic Regression::doc}}
\sphinxAtStartPar
Note:  resources below are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. The extensions below (conditional logistic regression, multinomial regression, ordinal logistic regression and neural networks) are not examinable. These comments are provided for your interest.

\sphinxAtStartPar
The following book contains a much more detailed presentation of logistic regression. Chapter 7, 8 and 11 are particularly useful.
\begin{quote}

\sphinxAtStartPar
Agresti, A., Categorical Data Analysis, 3rd edition, 2012
\end{quote}

\sphinxAtStartPar
The following also provide more detail on the subjects we have covered:
\begin{quote}

\sphinxAtStartPar
Hosmer D. W., Lemeshow S. and Rodney X. S. Applied logistic regression. Wiley, 2013.
\end{quote}
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{Hosmer, D. W., Lemeshow, S. (1980) Goodness of fit tests for the multiple logistic regression model. Communications in Statistics – Theory and Methods}
\end{quote}


\subsection{15.13.1  Conditional logistic regression}
\label{\detokenize{15.l. Logistic Regression:conditional-logistic-regression}}
\sphinxAtStartPar
Conditional logistic regression is specifically designed for grouped data and in particular for pair\sphinxhyphen{}matched studies where each group is a pair of two matched subjects.

\sphinxAtStartPar
To analyse a pair\sphinxhyphen{}matched study, a naïve idea would be to include into a logistic regression model a parameter specific to each data stratum. However, for matched studies, it would mean having as many parameters as pairs. This raised two main issues. First, each of this stratum specific parameter would be estimated only from the information contained in a unique pair, i.e. very little information. Secondly, as the number of pairs increases, the number of parameters would also increase and maximum likelihood theory would fail to provide valid estimation.

\sphinxAtStartPar
The idea of conditional logistic regression is to remove the dependence upon the stratum specific parameters by conditioning the probability on sufficient exposure information. This way, because the stratum specific parameters vanish from the equation, there is no violation of the assumptions underlying maximum likelihood theory  and the other model parameters can be estimated consistently using classic techniques.

\sphinxAtStartPar
We note that in some cases (e.g. where the matching was on measurable characteristics, such as age and sex only), standard logistic regression adjusting for the matching variables is valid.


\subsection{15.13.2  Multinomial logistic regression}
\label{\detokenize{15.l. Logistic Regression:multinomial-logistic-regression}}
\sphinxAtStartPar
The multinomial logistic regression model is a generalization of the logistic regression model for outcomes that have more than \(2\) categories, \(Y\in\{1,\dots,J\}\) with \(J\geq 2\) a natural number. In this case, the conditional distribution of \(Y_i\) given the covariates \(X_i\) is the multinomial distribution. Among the \(J\) categories, a reference one is chosen, e.g. the first category, and for \(j=2,\dots,J\)
\begin{equation*}
\begin{split}\log\left( \frac{P(Y_i=j|X_i)}{P(Y_i=1|X_i)} \right) = \beta_{0,j} + \sum_{k=1}^p \beta_{k,j}X_{i,k}\end{split}
\end{equation*}
\sphinxAtStartPar
The model is estimated simultaneously for all values of \(j\). For a fixed \(j\), the interpretation of the parameters is similar to the logistic regression model.


\subsection{15.13.3 Ordinal logistic regression}
\label{\detokenize{15.l. Logistic Regression:ordinal-logistic-regression}}
\sphinxAtStartPar
The ordinal logistic regression is designed for outcomes that have more than \(2\) categories, \(Y\in\{1,\dots,J\}\) with \(J\geq 2\), and whose categories have an explicit ordering. In the ordinal logistic regression, the modelled quantity is \(\mathrm{logit}(P(Y_i\geq j|X_i))\) for \(j\geq 2\). Indeed, when \(j=1\), t
\(P(Y\geq 1)=1\) and does not need to be modelled. As the categories are ordered, a fundamental assumption made by ordinal logistic regression is that the effect of the covariates are homogenous between the different categories. Therefore, the model is written
\begin{equation*}
\begin{split}\mathrm{logit}(P(Y_i\geq j|X_i)) = \beta_{0j} + \sum_{k=1}^n \beta_{k}X_{i,k}\end{split}
\end{equation*}
\sphinxAtStartPar
where only the intercept term depends upon the category. However, it is important to carefully check for violations of this assumption.


\subsection{15.13.4 Neural networks}
\label{\detokenize{15.l. Logistic Regression:neural-networks}}
\sphinxAtStartPar
Artificial neural networks are a class of model widely used for data classification in machine learning. Artificial neural networks are at the heart of \sphinxstyleemphasis{deep learning} methods used to develop computer vision, speech recognition, audio recognition, etc. Actually, the basic logistic regression model happens to be a special case of artificial neural network. If you are interested in this subject and want to have more insight on the relation between these two models, you might want to read the following article:
\begin{quote}

\sphinxAtStartPar
Dreiseitl, S., \& Ohno\sphinxhyphen{}Machado, L. (2002). Logistic regression and artificial neural network classification models: a methodology review. Journal of Biomedical Informatics
\end{quote}


\section{15.14 Additional reading}
\label{\detokenize{15.m. Logistic Regression:additional-reading}}\label{\detokenize{15.m. Logistic Regression::doc}}
\sphinxAtStartPar
The following notes sketch out the statistical theory underlying confidence interval construction and hypothesis testing using the log\sphinxhyphen{}likelihood ratio. This material is not examinable.

\sphinxAtStartPar
These notes are intended to draw connections between the previous material surrounding maximum likelihood and the material concerning frequentist inference.


\subsection{15.14.1 Likelihood for simple logistic regression}
\label{\detokenize{15.m. Logistic Regression:likelihood-for-simple-logistic-regression}}
\sphinxAtStartPar
\sphinxstylestrong{Data:} We have a sample of \(n\) binary observations \(y_1, ..., y_n\). We will consider a very simple situation, with no covariates of interest.

\sphinxAtStartPar
\sphinxstylestrong{Statistical model:} We assume our sample arises from \(n\) independent variables \(Y_1, ..., Y_n\), with \(Y_i \sim Bernoulli(\pi)\). Our logistic regression model is:
\begin{equation*}
\begin{split}
logit(\pi) = \beta
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Likelihood:} Following the notes in the main text, we can obtain the likelihood function:
\begin{equation*}
\begin{split}
\begin{align*}
L(\beta)  = e^{k \beta}  \times \left(\frac{1}{1 + e^{\beta}} \right)^n
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(k = \sum_i y_i\).

\sphinxAtStartPar
\sphinxstylestrong{Log\sphinxhyphen{}likelihood:} Taking the log of the above likelihood, we derive the following log\sphinxhyphen{}likelihood
\begin{equation*}
\begin{split}
\begin{align*}
l(\beta) = k \beta   - n log \left(1 + e^{\beta} \right) 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Likelihood ratio:} This is the likelihood function divided through by the likelihood function evaluated at its maximum point (i.e. at the maximum likelihood estimator, \(\hat{\beta}\). Therefore, this is simply the likelihood scaled to have a maximum of 1:
\begin{equation*}
\begin{split}
\begin{align*}
LR = \frac{L(\beta)}{L(\hat{\beta})}  = \frac{e^{k \beta} \left/ \left(1 + e^{\beta} \right. \right)^{n}}{e^{k \hat{\beta}}  \left/ \left(1 + e^{\hat{\beta}} \right.  \right)^{n}}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Log likelihood ratio:} The log of the likelihood ratio is:
\begin{equation*}
\begin{split}
\begin{align*}
llr(\beta) = k \beta   - n log \left(1 + e^{\beta} \right)  - \left\{ k \hat{\beta}   - n log \left(1 + e^{\hat{\beta}} \right)  \right\}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
\sphinxstylestrong{Maximum likelihood estimate:} We take the derivative of the log\sphinxhyphen{}likelihood and evaluate it at zero to obtain the maximum likelihood estimator, \(\hat{\beta}\):
\begin{equation*}
\begin{split}
\begin{align*}
\frac{d l(\beta)}{d \beta} = \frac{d}{d \beta} \left\{ k \beta   - n log \left(1 + e^{\beta} \right)  \right\} = k - n \frac{e^{\beta}}{\left(1 + e^{\beta} \right)}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Setting this to zero gives
\begin{equation*}
\begin{split}
\begin{align*}
\hat{\beta} = log\left( \frac{\bar{y}}{1 - \bar{y}} \right) 
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\bar{y} = k/n\) is the sample proportion of successes.

\sphinxAtStartPar
The figure below shows the Likelihood function and the log\sphinxhyphen{}likelihood at the top and the likelihood ratio and log\sphinxhyphen{}likelihood ratio on the bottom. We see that all four have a maximum at the same value of \(\beta\). The two graphs on the log scale (on the right hand side) are flatter and more symmetric. The likelihood ratio is simply the likelihood but scaled so the maximum value is 1. The log\sphinxhyphen{}likelihood ratio is scaled so the maximum value is 0.

\sphinxAtStartPar
The code is suppressed to focus on the output but you can click to see the code.

\noindent\sphinxincludegraphics{{15.m. Logistic Regression_3_0}.png}


\subsection{15.14.2 Confidence intervals based on the likelihood}
\label{\detokenize{15.m. Logistic Regression:confidence-intervals-based-on-the-likelihood}}
\sphinxAtStartPar
Remember that the likelihood is a measure of how consistent the different values of the parameter are with the observed data. The most consistent value is at the maximum, i.e. the maximum likelihood estimator. We can also see that values with a much lower likelihood are much less consistent with the data.

\sphinxAtStartPar
This suggests the idea of obtaining a confidence interval by taking all values that have a likelihood within a certain range of the maximum.

\sphinxAtStartPar
In fact, when we have a single parameter of interest (which we will call \(\beta_0\)) then it turns out that for an independent sample (under a number of “regularity” conditions not stated here), we have the following asymptotic distribution:
\begin{equation*}
\begin{split}
-2 llr(\beta_0) = -2 (l(\beta_0) - l(\hat{\beta})) \sim \chi^2_1    \qquad \text{as} \ n \rightarrow \infty
\end{split}
\end{equation*}
\sphinxAtStartPar
A \(\chi^2_1\) distribution has 5\% of the distribution above the value 3.84. Therefore, this means that
\begin{equation*}
\begin{split}
P(-2 llr(\beta_0) \geq 3.84) = 0.95 \rightarrow P(llr(\beta_0) \geq  -1.92) = 0.95
\end{split}
\end{equation*}
\sphinxAtStartPar
leading to the 95\% confidence interval of all values of \(\beta\) that have a log\sphinxhyphen{}likelihood ratio at most 1.92 units lower than the maximum:
\begin{equation*}
\begin{split}
\{ \beta s.t. l(\beta) - l(\hat{\beta}) \geq -1.92\}
\end{split}
\end{equation*}
\sphinxAtStartPar
The plot below shows the line \sphinxhyphen{}1.92. Our 95\% confidence interval is formed by all values of \(\beta\) which have a log\sphinxhyphen{}likelihood falling above this line. This is approximately the iterval: (\sphinxhyphen{}1.34, 0.48). The MLE and confidnence limits are shown in orange dashed lines.

\noindent\sphinxincludegraphics{{15.m. Logistic Regression_5_0}.png}


\subsection{15.14.2 Quadratic approximation}
\label{\detokenize{15.m. Logistic Regression:quadratic-approximation}}
\sphinxAtStartPar
There is often no closed form solution to obtain the exact values at which the log\sphinxhyphen{}likelihood ratio takes value \sphinxhyphen{}1.92. An often simpler approach is to work instead with a quadratic approximation to the log\sphinxhyphen{}likelihood ratio, for which there is a simple closed\sphinxhyphen{}form solution.

\sphinxAtStartPar
We will now make a quadratic approximation to the log likelihood ratio. In the plot above, we see that this graph is not quite symmetric but looks fairly quadratic near the maximum.

\sphinxAtStartPar
To obtain our quadratic approximation, we will look for a function of the (quadratic) form:
\begin{equation*}
\begin{split}
f(\beta) = -\frac{1}{2} \left( \frac{\beta - M}{S} \right)^2
\end{split}
\end{equation*}
\sphinxAtStartPar
We want our quadratic approximation to
\begin{itemize}
\item {} 
\sphinxAtStartPar
have the same maximum

\item {} 
\sphinxAtStartPar
have the same curvature near the maximum

\end{itemize}

\sphinxAtStartPar
The first condition above means that we need \(f(\hat{\beta}) = 0\). This fixes \(M = \hat{\beta}\).

\sphinxAtStartPar
The second condition means that we need the second derivatives of \(f(\beta)\) and \(llr(\beta)\) to be equal at the MLE, \(\hat{\beta}\), since curvature is measured by the second derivative. In fact, we will consider making the curvature (second derivatives) of \(f(\beta)\) and \(l(\beta)\) to be equal at the MLE, since this is algebraically a little simpler. From the plots above we can see that the curvature of \(l(\beta)\) and \(llr(\beta)\) are identical.

\sphinxAtStartPar
Differentiating \(f(\beta)\) twice shows that \(f''(\beta) = -1/S^2\) for any value of \(\beta\). Thus we set
\begin{equation*}
\begin{split}
S^2 = -\frac{1}{l''(\hat{\beta})}
\end{split}
\end{equation*}
\sphinxAtStartPar
It also turns out that the resulting value for \(S\) is also the standard error of \(\hat{\beta}\), i.e. \(S = SE(\hat{\beta})\).

\sphinxAtStartPar
This gives us our required quadratic approximation to the log\sphinxhyphen{}likelihood ratio:
\begin{equation*}
\begin{split}
f(\beta) = -\frac{1}{2} \left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \qquad  \text{with SE obtained as: } SE^2 = -\frac{1}{l''(\hat{\beta})}
\end{split}
\end{equation*}

\subsubsection{Example}
\label{\detokenize{15.m. Logistic Regression:example}}
\sphinxAtStartPar
Returning to our example, we take the second derivative of the log\sphinxhyphen{}likelihood to obtain \(S\). First, we have already taken the first derivative to obtain our MLE:
\begin{equation*}
\begin{split}
\begin{align*}
\frac{d l(\beta)}{d \beta} = \frac{d}{d \beta} \left\{ k \beta   - n log \left(1 + e^{\beta} \right)  \right\} = k - n \frac{e^{\beta}}{\left(1 + e^{\beta} \right)}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Taking the derivative of this, we get:
\begin{equation*}
\begin{split}
\begin{align*}
\frac{d^2 l(\beta)}{d \beta^2} = \frac{d}{d \beta} \left\{ k - n \frac{e^{\beta}}{\left(1 + e^{\beta} \right)} \right\} = -\frac{n e^\beta}{\left(1 + e^{\beta} \right)^2}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Thus we set
\begin{equation*}
\begin{split}
S^2 = -\frac{1}{l''(\hat{\beta})} =  \frac{\left(1 + e^{\hat{\beta}} \right)^2}{n e^\hat{\beta}}
\end{split}
\end{equation*}
\sphinxAtStartPar
The figure below shows the log likelihood ratio and the quadratic approximation.

\noindent\sphinxincludegraphics{{15.m. Logistic Regression_7_0}.png}

\sphinxAtStartPar
The quadratic approximation is very good near to the maximum. The horizontal red line indicates the 95\% confidence interval obtained using the log likelihood ratio. The quadratic approximation starts to deviate from the log likelihood ratio at that point, but not by much. overall, this plot suggests that the quadratic approximation will provide us with a 95\% confidence interval very close to the one obtained directly from the log likelihood ratio.


\subsection{15.14.3 Quadratic approximation}
\label{\detokenize{15.m. Logistic Regression:id1}}
\sphinxAtStartPar
If our quadratic approximation is a good approximation to the log\sphinxhyphen{}likelihood ratio then we will have, approximately
\begin{equation*}
\begin{split}
-2 f(\beta) \sim \chi^2_1
\end{split}
\end{equation*}
\sphinxAtStartPar
Thus
\begin{equation*}
\begin{split}
-2 f(\beta) = -2 \times -\frac{1}{2} \left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \sim \chi^2_1
\end{split}
\end{equation*}
\sphinxAtStartPar
In other words,
\begin{equation*}
\begin{split}
\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \sim \chi^2_1
\end{split}
\end{equation*}
\sphinxAtStartPar
A \(\chi^2_1\) distribution has 5\% of the distribution above the value 3.84, so
\begin{equation*}
\begin{split}
P\left(\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \geq 3.84\right)  = 0.95
\end{split}
\end{equation*}
\sphinxAtStartPar
Noticing that \(\sqrt{3.84} = 1.96\), this gives the 95\% confidence interval:
\begin{equation*}
\begin{split}
\hat{\beta} \pm 1.96 \times SE(\hat{\beta})
\end{split}
\end{equation*}
\sphinxAtStartPar
This is sometimes called a \sphinxstylestrong{Wald\sphinxhyphen{}type} confidence interval.

\sphinxAtStartPar
The plot below graphs the interval constructed in this way (indicated by the purple dashed lines), along with the previous  confidence interval calculated from the log\sphinxhyphen{}likelihood ratio.

\noindent\sphinxincludegraphics{{15.m. Logistic Regression_10_0}.png}

\sphinxAtStartPar
In the plot above, the two confidence intervals are very similar.


\subsection{15.14.4 Hypothesis testing}
\label{\detokenize{15.m. Logistic Regression:hypothesis-testing}}
\sphinxAtStartPar
Suppose we wish to test the null hypothesis:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(H_0: \beta = \beta_0\)

\item {} 
\sphinxAtStartPar
\(H_1: \beta \neq \beta_0\)

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Likelihood ratio test}

\sphinxAtStartPar
Under the null hypothesis,
\begin{equation*}
\begin{split}
-2 llr(\beta_0) \sim \chi^2_1
\end{split}
\end{equation*}
\sphinxAtStartPar
Tests can be based on this distribution, giving a p\sphinxhyphen{}value.

\sphinxAtStartPar
\sphinxstylestrong{Wald test}

\sphinxAtStartPar
Using the quadratic approximation,
\begin{equation*}
\begin{split}
\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \sim \chi^2_1
\end{split}
\end{equation*}
\sphinxAtStartPar
Or, equivalently:
\begin{equation*}
\begin{split}
\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)  \sim N(0,1)
\end{split}
\end{equation*}
\sphinxAtStartPar
This is exactly the form of hypothesis tests we encountered in the session about hypothesis testing.


\subsection{15.14.5 Additional comments}
\label{\detokenize{15.m. Logistic Regression:additional-comments}}
\sphinxAtStartPar
Notes
\begin{quote}

\sphinxAtStartPar
We often construct confidence intervals on the log scale (e.g. the log odds ratios). Confidence intervals based on the log likelihood ratio are transformation invariant, but Wald\sphinxhyphen{}type intervals are not. Often, basing calculations on a log scale improves the approximations made above. 
We have focused on situations with a single unknown parameter. With more than one unknown parameter, things are a little more complex. The profile likelihood, which treats some parameters as “nuisance” parameters and \sphinxstyleemphasis{removes} them from the likelihood, using a process called profiling, is beyond the scope of these notes. The fundamental principles remain the same.
\end{quote}


\chapter{16. Generalised Linear Models (GLMs)}
\label{\detokenize{16.a. Generalised Linear Model (GLM):generalised-linear-models-glms}}\label{\detokenize{16.a. Generalised Linear Model (GLM)::doc}}


\sphinxAtStartPar
By the end of this session, you will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
explain what Generalised Linear Model is;

\item {} 
\sphinxAtStartPar
describe the role of a link function;

\item {} 
\sphinxAtStartPar
apply a GLM to Poisson distributed data and evaluate the findings;

\item {} 
\sphinxAtStartPar
demonstrate how to explore the goodness of fit.

\end{itemize}




\section{16.1 Introduction to Generalised Linear Models (GLMs)}
\label{\detokenize{16.b. Generalised Linear Model (GLM):introduction-to-generalised-linear-models-glms}}\label{\detokenize{16.b. Generalised Linear Model (GLM)::doc}}
\sphinxAtStartPar
The term Generalised Linear Model (GLM) refers to a large class of models popularised by McCullagh and Nelder in 1982. It should not be confused with the similarly named method, General Linear Model (which was covered in sessions 12 to 14).

\sphinxAtStartPar
GLMs can be seen as a extension to the familiar regression models you have already been introduced to in previous chapters. GLM allows for the outcome variable to have an error distribution other than the normal distribution. The name comes from the method which generalises linear regression by allowing the linear model to be related to the outcome variable via something called a link function. This means that GLMs can model outcomes with distributions in the exponential family with a link function which varies linearity with the predictors (covariates) rather than assuming the outcome itself must vary linearly.


\section{16.2 Generalised Linear Model Components}
\label{\detokenize{16.c. Generalised Linear Model (GLM):generalised-linear-model-components}}\label{\detokenize{16.c. Generalised Linear Model (GLM)::doc}}
\sphinxAtStartPar
A generalised linear model consists of three components:


\subsection{A random component}
\label{\detokenize{16.c. Generalised Linear Model (GLM):a-random-component}}
\sphinxAtStartPar
This refers to the probability distribution of the outcome variable \(Y_{i}\) (for the ith of n independently sampled observations).  It specifies the conditional distribution of the outcome given the values of the predictors (covariates) in the model. \(Y_{i}\) is generally formulated as distribution from the exponential family, however subsequent work has extended GLMs to multivariate exponential families, to certain non\sphinxhyphen{}exponential families and to also to situations where the distribution of \(Y_{i}\) is not completely specified. Within this chapter we will only explore the application to distributions from the exponential family (i.e. Normal, Gamma, Poisson, Bernoulli etc.)


\subsection{A systematic component  (the linear predictor)}
\label{\detokenize{16.c. Generalised Linear Model (GLM):a-systematic-component-the-linear-predictor}}
\sphinxAtStartPar
This is the linear function of the predictors (covariates) in the model
\begin{equation*}
\begin{split}
\eta_{i} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}
\end{split}
\end{equation*}
\sphinxAtStartPar
Just as in  linear and logistic regression models, the predictors (covariates) \(X_{ij}\) may be continuous and/or categorical.


\subsection{A link function}
\label{\detokenize{16.c. Generalised Linear Model (GLM):a-link-function}}
\sphinxAtStartPar
This function transforms the expectation of the predictors (covariates) to be linear with the outcome variable.

\sphinxAtStartPar
Suppose we let \(\mu_{i} = E[Y_{i}]\). Then the \sphinxstyleemphasis{link function} is a function \(g(.)\) with
\begin{equation*}
\begin{split}
g(\mu_{i} ) = \eta_{i} 
\end{split}
\end{equation*}
\sphinxAtStartPar
Or in other words,
\begin{equation*}
\begin{split}
g(\mu_{i} ) = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}
\end{split}
\end{equation*}
\sphinxAtStartPar
This can be inverted so
\begin{equation*}
\begin{split}
\mu_{i}  = g^{-1}(\eta_{i}) = g^{-1}(\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}).
\end{split}
\end{equation*}
\sphinxAtStartPar
The inverse link \(g^{-1}(.)\) is often called the \sphinxstyleemphasis{mean function} as it  gives the expected value of the outcome.


\subsection{16.2.1 An example \sphinxhyphen{} logistic regression}
\label{\detokenize{16.c. Generalised Linear Model (GLM):an-example-logistic-regression}}
\sphinxAtStartPar
Suppose we wish to fit a logistic regression model (which we will see is one particular type of GLM) for a binary outcome \(Y\) and a single covariate \(X\). Normally we write \(\pi_i\) for the expected outcome but here we will use \(\mu_i\) instead, so you can see how the model we had previously connects with the more general notation above. Thus, we let \(\mu_i = E[Y_i]\) (=\(\pi_i\) in previous sessions). We have:
\begin{equation*}
\begin{split}
Y_i \overset{iid}{\sim} Bernouilli(\mu_i), \qquad \text{for} \ i=1, 2, ..., n
\end{split}
\end{equation*}
\sphinxAtStartPar
The linear predictor is given by:
\begin{equation*}
\begin{split}
\eta_i = \beta_0 + \beta_1 X_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
The link function can be defined generically. In the equation below, \(z\) has no intrinsic meaning; it is just used here to enable us to define a function. The link function for logistic regression is the logit function:
\begin{equation*}
\begin{split}
g(z) = log \left\{ \frac{z}{1-z} \right\}
\end{split}
\end{equation*}
\sphinxAtStartPar
Setting this equal to \(\eta_i\), as per the definition above, we get:
\begin{equation*}
\begin{split}
g(\mu_i) = log \left\{ \frac{\mu_i}{1-\mu_i} \right\} = \beta_0 + \beta_1 X_i 
\end{split}
\end{equation*}
\sphinxAtStartPar
which is the logistic regression model we met previously.


\section{16.3 GLM Assumptions}
\label{\detokenize{16.d. Generalised Linear Model (GLM):glm-assumptions}}\label{\detokenize{16.d. Generalised Linear Model (GLM)::doc}}
\sphinxAtStartPar
To successful apply a GLM, a number of assumptions about the data must be met.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The data must be independently distributed.

\item {} 
\sphinxAtStartPar
The outcome variable \(Y_{i}\) does not have to be normally distributed but should typically form a distribution from the exponential family

\item {} 
\sphinxAtStartPar
GLM’s must assume a linear relationship between the transformed outcome in terms of the link function and the predictor (covariate) variables.

\item {} 
\sphinxAtStartPar
The homogeneity of variance does not need to be satisfied. Generally the model structure, and overdispersion (when the observed variance is larger than what the model assumes) can be present.

\item {} 
\sphinxAtStartPar
Errors need to be independent but not normally distributed.

\item {} 
\sphinxAtStartPar
GLM’s use maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large\sphinxhyphen{}sample approximations.

\end{enumerate}


\section{16.4 Link Functions}
\label{\detokenize{16.e. Generalised Linear Model (GLM):link-functions}}\label{\detokenize{16.e. Generalised Linear Model (GLM)::doc}}
\sphinxAtStartPar
The link function provides the relationship between the systematic component and the mean of the distribution. There are many commonly used link functions, the table below lists only three examples with their distributions and mean functions. Here we use matrix notation where \(\mu_{i}  = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}\)  is represented by \(\mathbf{X}\mathbf{\beta}\).


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Distribution
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Data
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Link Name
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Link function
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Mean function
\\
\hline
\sphinxAtStartPar
Normal
&
\sphinxAtStartPar
real: (\sphinxhyphen{}\(\infty\) , + \(\infty\))
&
\sphinxAtStartPar
Identity \(     \)
&
\sphinxAtStartPar
\(\mathbf{X}\mathbf{\beta} =\mu\)
&
\sphinxAtStartPar
\( \mu = \mathbf{X}\mathbf{\beta} \)  \(     \)
\\
\hline
\sphinxAtStartPar
Poisson
&
\sphinxAtStartPar
integer: 0,1,2,…
&
\sphinxAtStartPar
Log
&
\sphinxAtStartPar
\(\mathbf{X}\mathbf{\beta} =ln( \mu)\)
&
\sphinxAtStartPar
\( \mu = exp(\mathbf{X}\mathbf{\beta} )\)
\\
\hline
\sphinxAtStartPar
Binomial
&
\sphinxAtStartPar
integer: 0,1,2,…N
&
\sphinxAtStartPar
Logit
&
\sphinxAtStartPar
\(\mathbf{X}\mathbf{\beta}=ln(\frac{\mu}{n-\mu})\)
&
\sphinxAtStartPar
\( \mu =\frac{exp(\mathbf{X}\mathbf{\beta})}{1 + exp(\mathbf{X}\mathbf{\beta}} \)
\\
\hline
\sphinxAtStartPar
Gamma
&
\sphinxAtStartPar
real: (0, + \(\infty\))
&
\sphinxAtStartPar
Negative Inverse
&
\sphinxAtStartPar
\(\mathbf{X}\mathbf{\beta} = -\mu^{-1}\)
&
\sphinxAtStartPar
\( \mu = - (\mathbf{X}\mathbf{\beta} )^{-1}\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
It is important to note that both linear regression which is covered in sessions 12 to 14 and logistic regression in sessions 15 can be reproduced through a GLM.

\sphinxAtStartPar
Recall that a linear regression assumes data is  normal distributed so using the identity link function for a normal distribution within the GLM framework will give the same estimated regression coefficients. However, the inference (p\sphinxhyphen{}values and confidence intervals) is slightly better using ordinary least squares compared to than maximum likelihood estimation thus we prefer to fit linear regression models using OLS.

\sphinxAtStartPar
In logistic regression if you use the logit function for a binomial family (recalling that Bernoulli is a special type of binomial distribution)  you will be able to reproduce the same results as obtained through standard logistic regression modelling. For binary outcomes, the GLM has the extra flexibility compared to the logistic regression model. You can also use other link functions, for example the Probit, the Log\sphinxhyphen{}Log and the Complementary log\sphinxhyphen{}log functions. These will give similar results but adjust for slight differences from data collection situations to improve the transformation of the expectation of the outcome to the systematic component. In this module we only focus on the logit link, however if you wish to explore further, more information can be found here: \sphinxurl{https://aip.scitation.org/doi/pdf/10.1063/1.5139815}


\section{16.5 Programming GLM’s in R}
\label{\detokenize{16.f. Generalised Linear Model (GLM):programming-glm-s-in-r}}\label{\detokenize{16.f. Generalised Linear Model (GLM)::doc}}
\sphinxAtStartPar
To fit a GLM in R you will need to use the glm() function where we tell R what the distribution of the errors and linear predictor should be.

\sphinxAtStartPar
The function syntax is as followed:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
glm(formula, family = gaussian, data, weights, subset,
na.action, start = NULL, etastart, mustart, offset,
control = list(…), model = TRUE, method = \PYGZdq{}glm.fit\PYGZdq{},
x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, …)
\end{sphinxVerbatim}

\sphinxAtStartPar
The minimal inputted parameters required is the \sphinxstyleemphasis{formula} and the \sphinxstyleemphasis{family}. The \sphinxstyleemphasis{formula} tells R in a symbolic description what model is required to be fitted and the \sphinxstyleemphasis{family} describes what error distribution and link function are required.


\section{16.6 Introduction to Poisson Generalised Linear Modelling (Poisson Regression)}
\label{\detokenize{16.g. Generalised Linear Model (GLM):introduction-to-poisson-generalised-linear-modelling-poisson-regression}}\label{\detokenize{16.g. Generalised Linear Model (GLM)::doc}}
\sphinxAtStartPar
In the previous session we met an important type of GLM \sphinxhyphen{} logistic regression. Now we will now consider another important GLM \sphinxhyphen{} Poisson regression.


\subsection{16.6.1 Poisson Distribution Recap}
\label{\detokenize{16.g. Generalised Linear Model (GLM):poisson-distribution-recap}}
\sphinxAtStartPar
The Poisson distribution was first published by Siméon Denis Poisson in 1838. Poisson was a French mathematician, engineer, and physicist, his name is one of 72 engraved on the Eiffel Tower in Paris. The Poisson distribution is a skewed, discrete distribution restricted to non\sphinxhyphen{}negative numbers. The shape of the distribution is defined by the shape parameter \(\lambda\) which represents the average number of events in the given time interval. As \(\lambda\) increases the distribution looks more and more like the normal distribution. When \(\lambda\) is about 10 or greater, then a normal distribution is a good approximation.


\subsection{16.6.2 Why can’t we just use Ordinary Linear Regression?}
\label{\detokenize{16.g. Generalised Linear Model (GLM):why-can-t-we-just-use-ordinary-linear-regression}}
\sphinxAtStartPar
One of the main assumptions required for fitting an ordinary linear regression (OLR) is that the residual errors must follow a normal distribution. For this to be achieved with data from a skewed distribution, a transformation must be applied however with discrete data this can be very problematic (making the interpretation of the findings unfeasibly difficult) or impossible (for example, a high number of 0’s could prevent normality from being achieved). Another issue is that an OLR has the ability to create negative predicted values which would be theoretically impossible. For these reasons it is better to apply a method which actually reflects the natural distribution instead of trying to make the distribution reflect the method. This is why a Poisson regression is generally more suited to count data than OLR.


\subsection{16.6.3 Poisson Regression}
\label{\detokenize{16.g. Generalised Linear Model (GLM):poisson-regression}}
\sphinxAtStartPar
A GLM for Poisson distributed outcome is commonly known as Poisson regression but is sometimes referred to as a log\sphinxhyphen{}linear model.


\subsubsection{Random component}
\label{\detokenize{16.g. Generalised Linear Model (GLM):random-component}}
\sphinxAtStartPar
Supppose we have an outcome \(Y\) representing counts of events over a fixed time period \(T\). For simplicity, we will let \(T=1\), i.e. we have followed our individuals up for a single unit of time (e.g. one year).

\sphinxAtStartPar
Suppose we are happy to assume that \(Y\) follows a Poisson distribution. We wish to model the relationship between a vector of \(p\) covariates \(\mathbf{X}\) and the expectation of \(Y\) (and therefore also the variance of \(Y\), since the mean is equal to the variance for a Poisson variable).

\sphinxAtStartPar
We let \(\mu = E[Y | X]\) and assume:
\begin{equation*}
\begin{split} 
\mathbf{Y} \overset{iid}{\sim} Poisson(\mu)
\end{split}
\end{equation*}

\subsubsection{Systematic component (linear predictor)}
\label{\detokenize{16.g. Generalised Linear Model (GLM):systematic-component-linear-predictor}}
\sphinxAtStartPar
The linear predictor is a linear function of the covariates \(\mathbf{X}\):
\begin{equation*}
\begin{split}
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + .... + \beta_p X_p 
\end{split}
\end{equation*}
\sphinxAtStartPar
Using vector notation to simplify the maths, this is equivalent to:
\begin{equation*}
\begin{split}
\mathbf{X}^T\mathbf{\beta}
\end{split}
\end{equation*}
\sphinxAtStartPar
(Note that we assume the vector \(\mathbf{X}\) contains a constant, so \(\mathbf{X}^T = (1, X_1, X_2, ..., X_p)\).


\subsubsection{Link function}
\label{\detokenize{16.g. Generalised Linear Model (GLM):link-function}}
\sphinxAtStartPar
We want an equation that connects the expected outcome \(\mu\) (which must be positive) to the linear predictor \(\mathbf{X}^T\mathbf{\beta}\)  (which can be any real value, positive or negative). The link function is applied to the expected outcome, therefore in this case an appropriate link function must map the positive real values to all real values. An obvious candidate is the natural logarithm. This is the default (often called the \sphinxstyleemphasis{canonical}) link function for Poisson variables. Then, applying the link function to the expected value of the outcome, we have:
\begin{equation*}
\begin{split}
ln(\mu) = \mathbf{X}^T\mathbf{\beta}
\end{split}
\end{equation*}
\sphinxAtStartPar
This is the \sphinxstylestrong{Poisson regression} model.


\subsubsection{Regression coefficients}
\label{\detokenize{16.g. Generalised Linear Model (GLM):regression-coefficients}}
\sphinxAtStartPar
In the model above, \(\beta\) is a vector of regression coefficients. An element of \(\beta\) represents the expected change in the natural \(log\) of the mean per unit change of one explanatory variable in \(X\) (constraining the other elements to not change).

\sphinxAtStartPar
We can interpret \(\mu\) as the expected rate of the outcome (remember we assume the fixed time period being considered is \(T=1\)). Consider a simpler example with a single binary covariate \(X\). Then the model above becomes
\begin{equation*}
\begin{split}
ln(\mu) = \beta_0 + \beta_1 X
\end{split}
\end{equation*}
\sphinxAtStartPar
This says that
\begin{equation*}
\begin{split}
\begin{align*}
ln(\mu) &=
\begin{cases} 
\beta_0 &\mbox{if } X=0 \\
\beta_0 + \beta_1  &\mbox{if } X=1 \\
\end{cases}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
The expeted rates in the two groups \((X=0)\) and \((X=1)\) are then:
\begin{equation*}
\begin{split}
\begin{align*}
\mu &=
\begin{cases} 
e^{\beta_0} &\mbox{if } X=0 \\
e^{\beta_0 + \beta_1}  &\mbox{if } X=1 \\
\end{cases}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
So the rate ratio comparing the rate in the exposed \((X=1)\) with the rate in the unexposed \((X=0)\) is:
\begin{equation*}
\begin{split}
\begin{align*}
RR = \frac{e^{\beta_0 + \beta_1}}{e^{\beta_0}} = e^{\beta_1}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
Therefore, \(e^{\beta_1}\) can be interpreted as the (incidence) rate ratio comparing the exposed with the unexposed groups. Or \(\beta_1\) can be interpreted as the log rate ratio.

\sphinxAtStartPar
More generally, a regression coefficient can be intepreted as the log rate ratio associated with a unit change of that covariate. Its exponential is the analogous rate ratio.


\subsubsection{Eestimating the regression coefficients}
\label{\detokenize{16.g. Generalised Linear Model (GLM):eestimating-the-regression-coefficients}}
\sphinxAtStartPar
As for all GLMs, the regression coefficients, \(\mathbf{\beta}\), are estimated by maximum likelihood estimation.


\subsubsection{The mean and variance}
\label{\detokenize{16.g. Generalised Linear Model (GLM):the-mean-and-variance}}
\sphinxAtStartPar
To obtain an equation for the mean of the outcome, we need to apply the inverse link function:
\begin{equation*}
\begin{split}
E[\mathbf{Y}|\mathbf{X}] = \mu = g^{-1} (\mathbf{X}^T\mathbf{\beta})
\end{split}
\end{equation*}
\sphinxAtStartPar
Which here is equal to
\begin{equation*}
\begin{split}
E[\mathbf{Y}|\mathbf{X}]  = \mu = e^{\mathbf{X^T\beta}},
\end{split}
\end{equation*}
\sphinxAtStartPar
Similarly the variance of \(\mathbf{Y}\)  is written:
\begin{equation*}
\begin{split} 
Var[\mathbf{Y}|\mathbf{X}] = Var[\mu] =Var[ g^{-1} (\mathbf{X}^T\mathbf{\beta})].
\end{split}
\end{equation*}

\subsection{16.6.4 Offsets  {[}optional{]}}
\label{\detokenize{16.g. Generalised Linear Model (GLM):offsets-optional}}
\sphinxAtStartPar
In this short sub\sphinxhyphen{}section, we explore an extension of the Poisson regression model above which allows us to take into account the fact that the observations in our data may represent counts from different lengths of observation time. This is a common occurrence in practice. We handle this through something called an \sphinxstylestrong{offset term}.

\sphinxAtStartPar
Above, we simplified the model by assuming each individual was observed for the same period of time. But suppose that was not he case. Suppose, for example, we are counting the number of asthma attacks experienced by school\sphinxhyphen{}aged children over time. Some children are followed up for one year and others are followed up for up to five years. Naturally, we would expect those followed up for longer to experience more asthma attacks, on average.

\sphinxAtStartPar
Suppose individual \(i\) is followed up for \(T_i\) years and experiences \(Y_i\) asthma attacks. We have:
\begin{equation*}
\begin{split} 
\mathbf{Y_i} \overset{iid}{\sim} Poisson(\lambda_i T_i)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\lambda_i\) is the annual rate of asthma attacks for individual \(i\). The expected number of attacks for individual \(i\) is \(\mu_i = \lambda_i T_i\). We might wish to propose the following model for the rate (based on the Poisson regression model we met previously):
\begin{equation*}
\begin{split}
ln(\lambda_i) = \mathbf{X}_i^T\mathbf{\beta}
\end{split}
\end{equation*}
\sphinxAtStartPar
This implies the following model for the expected number of attacks:
\begin{equation*}
\begin{split}
ln(\mu_i) = ln(\lambda_i T_i) =  \mathbf{X}_i^T\mathbf{\beta} + ln(T_i)
\end{split}
\end{equation*}
\sphinxAtStartPar
This is very similar to our previous model. The only difference is that we now have a covariate (\(ln(T_i)\)) appearing in the model without a regression coefficient. In other words, the regression coefficient for \(ln(T_i)\) is constrained to be equal to 1.

\sphinxAtStartPar
This is exactly what an offset term is: a covariate in a regression model with its regression coefficient constrained to be equal to 1.


\section{16.7 Poisson Regression Example}
\label{\detokenize{16.h. Generalised Linear Model (GLM):poisson-regression-example}}\label{\detokenize{16.h. Generalised Linear Model (GLM)::doc}}

\subsection{16.7.1 Example data set}
\label{\detokenize{16.h. Generalised Linear Model (GLM):example-data-set}}
\sphinxAtStartPar
For the purpose of illustration, we will simulate some data and pretend it comes from a clinical trial. We generate 100 participants ( 𝑛 ) and three variables. The first is a count variable representing the number of hospital admissions (\sphinxstyleemphasis{counts}) a participant has had in a year and it is created from a Poisson distribution with  𝑙𝑎𝑚𝑏𝑑𝑎=2 . The second is a categorical variable (\sphinxstyleemphasis{country}) with 4 groups representing the country a participant lives in (England, Northern Ireland, Scotland, Wales) and the last is a binary variable (\sphinxstyleemphasis{treatment}) representing which treatment arm the participant was randomised to. Let’s start with simulating the data and looking at some descriptive statistics.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}\PYGZsh{} Simulate Data}
\PYG{n+nf}{set.seed}\PYG{p}{(}\PYG{l+m}{42}\PYG{p}{)}
\PYG{n}{n}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{100}
\PYG{n}{lambda}\PYG{o}{\PYGZlt{}\PYGZhy{}}\PYG{l+m}{6}
\PYG{n}{counts} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{rpois}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{lambda}\PYG{p}{)}
\PYG{n}{country} \PYG{o}{\PYGZlt{}\PYGZhy{}}  \PYG{n+nf}{factor}\PYG{p}{(}\PYG{n+nf}{sample}\PYG{p}{(}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{n+nb+bp}{T}\PYG{p}{)}\PYG{p}{,} \PYG{n}{levels}\PYG{o}{=}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{4}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{England\PYGZdq{}}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Northern Ireland\PYGZdq{}}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Scotland\PYGZdq{}}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Wales\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{treatment} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{factor}\PYG{p}{(}\PYG{n+nf}{gl}\PYG{p}{(}\PYG{l+m}{2}\PYG{p}{,}\PYG{n}{n}\PYG{o}{/}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{levels}\PYG{o}{=}\PYG{l+m}{1}\PYG{o}{:}\PYG{l+m}{2}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n+nf}{c}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Active Arm\PYGZdq{}}\PYG{p}{,} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{Placebo Arm\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{df} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{data.frame}\PYG{p}{(}\PYG{n}{treatment}\PYG{p}{,} \PYG{n}{country}\PYG{p}{,} \PYG{n}{counts}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Assume we wish to model \sphinxstyleemphasis{counts} using a GLM with \sphinxstyleemphasis{treatment} and \sphinxstyleemphasis{country} as predictors. We already know the admissions count variable follows a Poisson distribution as we have simulated the data directly from the distribution without adding noise, therefore we know a Poisson regression is suitable. To fit the model, we call the glm() function with the family set to “poisson” and use the summary command to look at the output.

\sphinxAtStartPar
Note: We have used the option \sphinxcode{\sphinxupquote{family=poisson}}. We could be more explicit and state the link function we want R to use by replacing this with \sphinxcode{\sphinxupquote{family=poisson(link=log)}}. Try re\sphinxhyphen{}running the command using \sphinxcode{\sphinxupquote{family=poisson(link=identity)}}. What is this doing? Is this a sensible/useful model?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{set.seed}\PYG{p}{(}\PYG{l+m}{42}\PYG{p}{)}
\PYG{n+nf}{summary}\PYG{p}{(}\PYG{n}{m1} \PYG{o}{\PYGZlt{}\PYGZhy{}} \PYG{n+nf}{glm}\PYG{p}{(}\PYG{n}{counts} \PYG{o}{\PYGZti{}} \PYG{n}{treatment} \PYG{o}{+} \PYG{n}{country}\PYG{p}{,} \PYG{n}{family}\PYG{o}{=}\PYG{n}{poisson}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{df}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Call:
glm(formula = counts \PYGZti{} treatment + country, family = poisson, 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
\PYGZhy{}3.2467  \PYGZhy{}0.5783   0.0477   0.6381   2.2260  

Coefficients:
                        Estimate Std. Error z value Pr(\PYGZgt{}|z|)    
(Intercept)              1.82733    0.08448  21.631  \PYGZlt{} 2e\PYGZhy{}16 ***
treatmentPlacebo Arm    \PYGZhy{}0.23187    0.08226  \PYGZhy{}2.819  0.00482 ** 
countryNorthern Ireland  0.17001    0.10822   1.571  0.11620    
countryScotland          0.14936    0.12396   1.205  0.22822    
countryWales             0.06669    0.11640   0.573  0.56670    
\PYGZhy{}\PYGZhy{}\PYGZhy{}
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 130.13  on 99  degrees of freedom
Residual deviance: 120.53  on 95  degrees of freedom
AIC: 483.69

Number of Fisher Scoring iterations: 5
\end{sphinxVerbatim}


\subsection{16.7.2 Example GLM output}
\label{\detokenize{16.h. Generalised Linear Model (GLM):example-glm-output}}
\sphinxAtStartPar
The first part of the output gives information on deviance residuals. We would expect to see the deviance residuals to be approximately normally distributed if the model is correctly specified. Here we can see the median is close to 0 (0.05) and there does not appear to be any skewness as Q1 (quartile 1 = \sphinxhyphen{}0.58) and Q3 (quartile 3 = 0.64) have a similar distance from the median and so are the minimum and maximum.

\sphinxAtStartPar
The second part of the output gives the Poisson regression coefficients for each variable with their standard errors, z values, p\sphinxhyphen{}values. We interpret Poisson regression coefficients as if there was a one unit change in the predictor variable (if a continuous variable otherwise change from the reference category to the category listed) the regression coefficient tells us the effect on the logs of the expected counts (admission counts in our example), given the other variables in the model are held constant. The coefficient for treatment is \sphinxhyphen{}0.23 which tells use the expected log admissions count for being randomised to the active arm compared to the placebo arm is \sphinxhyphen{}0.23. The expected log admissions count for the other countries compared to England are all positive.

\sphinxAtStartPar
We can also see the regression estimate when all the variables in the model are evaluated at zero (or categorical reference group) and this is called the constant and labelled “(Intercept)”. In our model this would represent the expected log admissions count for participants in the placebo arm who live in England.

\sphinxAtStartPar
The standard errors are given which are used to calculate the z\sphinxhyphen{}value which in turn is used to calculate the p value. The null hypothesis for each p value is that the corresponding regression coefficient is zero given the rest of the variables in the model. The z value here is just the ratio of the coefficient to the standard error for example treatment we can see the estimate/standard error equals the z value: \sphinxhyphen{}0.23187/0.08226=\sphinxhyphen{}2.819. The z value follows a normal distribution and is tested against a two\sphinxhyphen{}sided alternative hypothesis that the coefficient is not equal to zero. We can see for treatment the p value is 0.005 and if we set out alpha significant level at  𝛼=0.05  we would reject the null hypothesis and conclude the Poisson regression coefficient for treatment is statistically different from zero, given country is in the model.

\sphinxAtStartPar
Lastly, at the bottom of the output, we have information on the residual deviance which can be used to perform a goodness of fit test for the overall model.


\subsection{16.7.3 Poisson Regression Goodness of Fit Example}
\label{\detokenize{16.h. Generalised Linear Model (GLM):poisson-regression-goodness-of-fit-example}}
\sphinxAtStartPar
At the bottom of the output we see the null deviance and residual deviance from the model. The residual deviance is 120.53 on 95 degrees of freedom (df). There are 100 observations in our model and 5 estimates which gives us 95 df (100\sphinxhyphen{}1df for treatment\sphinxhyphen{} 3df for each country – 1df for the constant) . To calculate the p\sphinxhyphen{}value for the deviance goodness of fit test we simply calculate the probability to the right of the deviance value for the chi\sphinxhyphen{}squared distribution on 95 df

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nf}{pchisq}\PYG{p}{(}\PYG{n}{m1}\PYG{o}{\PYGZdl{}}\PYG{n}{deviance}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{m1}\PYG{o}{\PYGZdl{}}\PYG{n}{df.residual}\PYG{p}{,} \PYG{n}{lower.tail}\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{equation*}
\begin{split}0.0395607905595946\end{split}
\end{equation*}
\sphinxAtStartPar
The null hypothesis is that our model is correctly specified. Here we can see the p value is 0.0396 which is significant if we set our level of significant at 0.05. We therefore have strong evidence to reject the null hypothesis. This result is expected as when creating the simulated data we made no relationship between any of the variables in the model, so we would expect a poor fit.


\section{16.8 Common Problems in Poisson Regression}
\label{\detokenize{16.i. Generalised Linear Model (GLM):common-problems-in-poisson-regression}}\label{\detokenize{16.i. Generalised Linear Model (GLM)::doc}}

\subsection{16.8.1 Problems}
\label{\detokenize{16.i. Generalised Linear Model (GLM):problems}}
\sphinxAtStartPar
There are two frequent common problems when applying Poisson Regression to count data and both are caused by the deviations from the Poisson distribution assumptions. The first problem is overdispersion and the second is zero inflation.


\subsection{16.8.2 Overdispersion}
\label{\detokenize{16.i. Generalised Linear Model (GLM):overdispersion}}
\sphinxAtStartPar
Overdispersion happens with then the variance is no longer equal to the mean but larger which violates the Poisson distribution principle. There are two main ways to handle overdispersion, the first is through using a negative binomial distribution (not covered here) instead and the second is to implement something called a quasi\sphinxhyphen{}likelihood through a GLM also called a Quasi\sphinxhyphen{}Poisson regression.


\subsection{16.8.3 Quasi\sphinxhyphen{}Poisson regression}
\label{\detokenize{16.i. Generalised Linear Model (GLM):quasi-poisson-regression}}
\sphinxAtStartPar
A Quasi\sphinxhyphen{}Poisson regression is often fitted to handle over\sphinxhyphen{}dispersion, it uses the same mean regression function and variance function from Poisson regression but allows the dispersion parameter \(\phi\) to be unrestriced from 1. In Poisson regression \(\phi\) is assumed to be fixed at 1 to make the mean and variance equal, in Quasi\sphinxhyphen{}Poisson regression \(\phi\) is not fixed and is estimated from the data. Quasi\sphinxhyphen{} Poisson regression leads to the same coefficient estimates as the Poisson regression model but inference are adjusted for the over\sphinxhyphen{}dispersion through the standard errors. To run a Quasi\sphinxhyphen{}Poisson regression in R we just tell the glm() function that the family is “quasipoisson”


\subsection{15.3.5 Zero inflation}
\label{\detokenize{16.i. Generalised Linear Model (GLM):zero-inflation}}
\sphinxAtStartPar
Zero inflation happens when the distribution contains a large number of zero’s. For example, if you were to count how many occasions people drank alcohol in a month but included a large number of non\sphinxhyphen{}drinkers you will expect to have multiple counts of 0. A Zero\sphinxhyphen{}Inflated Poisson (ZIP) distribution can be thought of being generated by two processes, the first generates zeros and the second is generated by the Poisson distribution (which will contain zeros). The two processes look like this:

\sphinxAtStartPar
\(P[\mathbf{Y}=0] = \pi (1-\pi)e^{- \lambda }\),

\sphinxAtStartPar
\(P[\mathbf{Y}=k] = (1-\pi)\frac{\lambda^{k}e^{-\lambda}}{k!}\),

\sphinxAtStartPar
Where \(k\) is a non\sphinxhyphen{}negative integer value, \(\lambda\) is the expected Poisson count and \(\pi\) is the probability of extra zeros. The mean of a ZIP is \((1-\pi)\lambda\) and the variance is \(\lambda (1-\pi) (1+\pi \lambda)\).

\sphinxAtStartPar
Unfortunately the glm() function is incapable of running a ZIP regression to run, you will need to use the “pscl” package which fits a GLM with a binomial logit link to predict the excess zeros and a GLM with a Poisson log link to model the rest of the distribution.


\chapter{17. The role of regression in different types of investigation}
\label{\detokenize{17. Investigations round up:the-role-of-regression-in-different-types-of-investigation}}\label{\detokenize{17. Investigations round up::doc}}
\sphinxAtStartPar
At the start of this section of the notes, we considered different types of investigation that might be of interest within a health data science project. We grouped these investigations into three classes: descriptive, predictive and causal.

\sphinxAtStartPar
Having explored various types of regression modelling, we now revisit the idea of the underlying investigation and consider the role of the regression model in different types of investigation.

\sphinxAtStartPar
We often use the same statistical tools to address research questions for investigations of different types. Regression is a key tool for analyses in all the types of investigation. In this short session we illustrate how the same regression model could be used in prediction and causal investigations, but that the output from the regression should be used and interpreted differently.


\section{17.1  Simple example}
\label{\detokenize{17. Investigations round up:simple-example}}
\sphinxAtStartPar
We focus on a simple (fictitious) observational study involving three variables: two binary explanatory variables ‘maternal smoking status’ (\(X_{1}\) = 1: smoker, \(X_{1}\) = 0: non\sphinxhyphen{}smoker) and maternal socioeconomic status (\(X_{2}\) = 1: low, \(X_{2}\) = 0: high), and a continuous outcome ‘birth weight’ (measured in grams). The assumed relationships between the three variables are summarised in the causal diagram in the figure below.

\sphinxAtStartPar
For the purposes of a simple illustration, we suppose that these are the only three variables at play in this ‘system’. In reality of course there are many other maternal and other characteristics that affect a baby’s birthweight, such as genetics, maternal diet and alcohol consumption, mother’s access to prenatal care, and other features of the environment.

\sphinxAtStartPar
Consider a linear regression of \(Y\) on \(X_{1}\), \(X_{2}\), i.e.
\begin{equation*}
\begin{split}
Y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \epsilon
\end{split}
\end{equation*}


\sphinxAtStartPar
The estimated regression coefficients and corresponding 95\% confidence intervals for this fictitious example are:
\begin{equation*}
\begin{split}
\begin{align*}
\hat{\beta_{0}} &= 3227 \ \ (95\% \text{CI}:   \  1603, 4851) \\
\hat{\beta_{1}} &= −341 \ \  (95\% \text{CI}:  \  −513, −169) \\
\hat{\beta_{2}} &= −214 \ \  (95\%  \text{CI}:  \  −410, −18)
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
We now consider how the output from this regression could be used in different investigation types.


\section{17.2 Prediction}
\label{\detokenize{17. Investigations round up:prediction}}
\sphinxAtStartPar
If the aim is to predict birth weight based on the two characteristics of the mother, this model allows us to do this. We could obtain the expected value of \(Y\) given \(X_{1}\) and \(X_{2}\) (in this very simple example there are only 4 possible combinations).

\sphinxAtStartPar
In this prediction setting we do not, however, particularly care about the estimates of the regression coefficients. We should instead be concerned with the predictive performance of the model. This could be measured, for example, using \(R^2\), which measures the proportion of variability in the outcome that is explained by the statistical model.

\sphinxAtStartPar
There are many details about how to appropriately assess and quantify the predictive performance of a prediction model which we do not discuss here.


\section{17.3 Causality and explanation}
\label{\detokenize{17. Investigations round up:causality-and-explanation}}
\sphinxAtStartPar
Suppose instead that the aim is to assess the causal effect of maternal smoking (\(X_{1}\)) on birth weight (\(Y\)). In the simple setting shown in the causal diagram above, maternal socioeconomic status (\(X_{2}\)) is the only confounder of the association between \(X_{1}\) and \(Y\).

\sphinxAtStartPar
The regression model in equation (1) adjusts for \(X_{2}\) and hence the coefficient for \(X_{1}\) can be interpreted as the conditional causal effect of \(X_{1}\) on \(Y\).

\sphinxAtStartPar
We can make the interpretation that if all mothers in the study population had smoked, the mean birthweight would have been 341 grams lower than had all mothers in the study population not smoked. This is referred to as an ‘average causal effect’. The 95\% confidence interval can be used to provide information about how precisely we believe we have estimated this causal effect. In this case, the 95\% confidence interval excludes 0 and includes only negative numbers, running from \sphinxhyphen{}513 to \sphinxhyphen{}169.

\sphinxAtStartPar
Here, we have not given any interpretation of the estimate of \(\beta_{2}\) because it wasn’t relevant for our research question, even though it was important to adjust for \(X_{2}\) to adjust for confounding. In a more realistic setting, there will be many other variables that confound the association between \(X_{1}\) and \(Y\) and which would need to be accounted for to enable a causal interpretation of \(\beta_{1}\).


\section{17.4 The “Table 2 Fallacy”}
\label{\detokenize{17. Investigations round up:the-table-2-fallacy}}
\sphinxAtStartPar
After adjusting for maternal socioeconomic status, maternal smoking was associated with a lowering of 341 grams in mean birthweight. After adjusting for maternal smoking, low maternal socioeconomic status was associated with a lowering of 214 grams in mean birthweight. However, \(\beta_{1}\) and \(\beta_{2}\) in model (1) do not have the same type of interpretation. This  is due to the relationships between the three variables.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Interpreting \(\beta_2\)}: According to the causal diagram above, maternal smoking status is on the causal pathway from socioeconomic status to birth weight. Hence the parameter \(\beta_2\) represents the effect of socioeconomic status on birth weight that does not go through smoking status \sphinxhyphen{} this is a ‘direct effect’ rather than a ‘total effect’. 

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Interpreting \(\beta_1\)}: By contrast, \(\beta_{1}\) represents the total effect of smoking status on birth weight. We do not go into details about definitions of different types of effect. The aim here is simply to point out that the correct interpretation of the coefficients in the regression model in (1) depends on assumptions about the inter\sphinxhyphen{}relationships between the three variables, including how they are ordered in time.

\end{itemize}

\sphinxAtStartPar
In some (or perhaps many) epidemiological investigations that involve exploration of risk factors, estimates of regression coefficients from multivariable models such as that in (1) (and versions with many more explanatory variables) are presented alongside one another in a table, together with confidence intervals and p\sphinxhyphen{}values. They may then be interpreted as though all coefficients had the same meaning, ignoring possible inter\sphinxhyphen{}relationships between the variables and temporal ordering. As we have seen from the above example, this could be misleading. This problem has been referred to in the literature the ‘Table 2 fallacy’, because the estimates of regression coefficients are often presented in ‘Table 2’ in a paper (where ‘Table 1’ is usually a table of descriptive statistics). See Westreich and Greenland (2013) for a description of the Table 2 fallacy. Bandoli et al. (2018) provide an example in the context of preeclampsia and preterm birth.


\section{17.5 Other analysis approaches}
\label{\detokenize{17. Investigations round up:other-analysis-approaches}}
\sphinxAtStartPar
Regression modelling is a fundamental part of the statistician’s toolbox and is used in many investigations of different types.  We have used regression modelling to illustrate the connection between the analysis method and the underlying aim of the investigation.

\sphinxAtStartPar
However, regression models are not the only tool available. You will come across many other types of analysis method, such as clustering or neural networks, which can also be used in various types of investigation.


\chapter{Statistics and Health Data Science}
\label{\detokenize{18. Statistics for HDS round up:statistics-and-health-data-science}}\label{\detokenize{18. Statistics for HDS round up::doc}}
\sphinxAtStartPar
We end with some brief remarks about the application of statistics in health data science.


\section{Focus on the research question}
\label{\detokenize{18. Statistics for HDS round up:focus-on-the-research-question}}
\sphinxAtStartPar
The more complicated the statistical analysis becomes, the easier it is to get lost in the technical details. As a data scientist, it is always important to be able to take a step back and re\sphinxhyphen{}focus on the underlying research question.

\sphinxAtStartPar
Ask yourself:
\begin{itemize}
\item {} 
\sphinxAtStartPar
What is the research question?

\item {} 
\sphinxAtStartPar
What assumptions can I reasonably make, taking into account where and when the data were collected and how they were collected?

\item {} 
\sphinxAtStartPar
Does the proposed statistical analysis answer the research question?

\item {} 
\sphinxAtStartPar
How can I assess the robustness of the conclusions of my analysis to the key assumptions I have made?

\end{itemize}


\section{Know your data}
\label{\detokenize{18. Statistics for HDS round up:know-your-data}}
\sphinxAtStartPar
We cannot stress too much the importance of being familiar with your data. Where does it come from? How was it collected? How accurate are measurements? Do similar biases affect measurements from different units/places/times?

\sphinxAtStartPar
A hugely important step in any data science project is to look at your data. The most sophisticated analysis will produce invalid results if based on data that contains substantial errors or incorrectly assembled datasets.


\section{Continue to learn}
\label{\detokenize{18. Statistics for HDS round up:continue-to-learn}}
\sphinxAtStartPar
This module has introduced some key building blocks, concepts and statistical tools that will be very useful for data science projects. However, there are many more statistical techniques that we have not touched on. In your career as a health data scientist, you will continue to learn new methods and approaches.

\sphinxAtStartPar
We hope that this module has provided a solid foundation to build on!







\renewcommand{\indexname}{Index}
\printindex
\end{document}