
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Statistics for Health Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-2 bd-sidebar site-navigation show single-page" id="site-navigation">
    
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https//github.com//LSHTM-HDS//Content-2021"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https//github.com//LSHTM-HDS//Content-2021/issues/new?title=Issue%20on%20page%20%2F00. Welcome.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh//LSHTM-HDS/main?urlpath=tree/00. Welcome.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <p class="caption">
 <span class="caption-text">
  Preamble
 </span>
</p>
<ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-00. Acknowledgements">
   Acknowledgements
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-00. How_to_use">
   How to use this book
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-01. Introduction">
   1 Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basic probability
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-02. Probability.Intro">
   Probability and statistics
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-02.a. Probability.Discrete">
   2. Discrete Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-02.b. Probability.Discrete">
     2.1 Application of Bayes’ Theorem
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-02.c. Probability.Discrete">
     2.2 The binomial distribution
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-02.d. Probability.Discrete">
     2.3 The Poisson distribution
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-02.e. Probability.Discrete">
     2.4 Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-03.a. Continuous Probability Distributions">
   3. Continuous distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-03.b. Continuous Probability Distributions">
     3.1 Continuous random variables
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-03.c. Continuous Probability Distributions">
     3.2 Useful continuous distributions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-03.d. Continuous Probability Distributions">
     3.3 Uses of the standard Normal distribution
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-03.e. Continuous Probability Distributions">
     3.4 Are the data normally distributed?
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-03.f. Continuous Probability Distributions">
     3.5 Joint distributions and correlations
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical Inference
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-04. Inference.Intro">
   Statistical Inference
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-04.a. Population.and.samples">
   4. Populations and Samples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-04.b. Population.and.samples">
     4.1 Sampling from a population
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-04.c. Population.and.samples">
     4.2 Statistical models
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-04.d. Population.and.samples">
     4.3 Sampling distributions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-04.e. Population.and.samples">
     4.4 Obtaining the sampling distribution
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-04.f. Population.and.samples">
     4.5 Summary
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-04.g. Population.and.samples">
     Appendix: Additional Reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-05.a. Likelihood">
   5. Likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-05.b. Likelihood">
     5.1 Maximum likelihood estimation
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-05.c. Likelihood">
     5.2 The likelihood
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-05.d. Likelihood">
     5.3 Log likelihood
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-05.e. Likelihood">
     5.4 Finding the MLE
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-05.f. Likelihood">
     5.5 Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-06.a. Maximum Likelihood">
   6. Maximum Likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-06.b. Maximum Likelihood">
     6.1 Likelihood with independent observations
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-06.c. Maximum Likelihood">
     6.2 Properties of maximum likelihood estimators
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-06.d. Maximum Likelihood">
     6.3 Summary
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-06.e. Maximum Likelihood">
     Appendix: Additional Reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-07.a. Frequentist I">
   7. Frequentist I: Confidence Intervals
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.b. Frequentist I">
     7.1 Confidence intervals
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.c. Frequentist I">
     7.2 Confidence intervals for the mean
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.d. Frequentist I">
     7.3 Interpretation of confidence intervals
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.e. Frequentist I">
     7.4 Approximate confidence intervals for parameters estimated using large samples
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.f. Frequentist I">
     7.5 Confidence Intervals using resampling
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.g. Frequentist I">
     7.6 Summary: Use of confidence intervals
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-07.h. Frequentist I">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-08.a. Frequentist II">
   8. Frequentist II: Hypothesis tests
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-08.b. Frequentist II">
     8.1 Evidence against hypotheses
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-08.c. Frequentist II">
     8.2 The p-value
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-08.d. Frequentist II">
     8.3 Connection between p-values and confidence intervals
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-08.e. Frequentist II">
     8.4 Other (mis-)interpretations of p-values
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-08.f. Frequentist II">
     8.5 Calculating p-values
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-08.g. Frequentist II">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-09.a. Bayesian Statistics I">
   9. Bayesian Statistics I
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.b. Bayesian Statistics I">
     9.1 Introduction to Bayesian Inference
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.c. Bayesian Statistics I">
     9.2 Bayes Theorem (recap)
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.d. Bayesian Statistics I">
     9.3 The Bayesian paradigm in Health data science problems.
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.e. Bayesian Statistics I">
     9.4 Bayes thorem for discrete and continous data
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.f. Bayesian Statistics I">
     9.5 Bayesian inference on proportions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.g. Bayesian Statistics I">
     9.6 Summarising Posteriors
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.h. Bayesian Statistics I">
     9.7 Prior Predictions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-09.i. Bayesian Statistics I">
     9.8 Conjugacy
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-10.a. Bayesian Statistics II">
   10. Bayesian Statistics II: Normal data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-10.b. Bayesian Statistics II">
     10.1 Example: CD4 cell counts
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-10.c. Bayesian Statistics II">
     10.2 Calculating the posterior
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-10.d. Bayesian Statistics II">
     10.3 Credible Intervals
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-10.e. Bayesian Statistics II">
     10.4 Predictions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-10.f. Bayesian Statistics II">
     10.5 Multiparameter models
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-10.g. Bayesian Statistics II">
     Further Resources
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical modelling
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-11. Investigation.Intro">
   Investigations and the role of regression modelling
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-11.a. Types of Investigation">
   11. Types of Investigation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.b. Types of Investigation">
     11.1 Specifying research questions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.c. Types of Investigation">
     11.2 Different types of investigation
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.d. Types of Investigation">
     11.3 Properties of different types of investigation
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.e. Types of Investigation">
     11.4 An example: stroke in women
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.f. Types of Investigation">
     11.5 Role of explanatory variables in different types of investigation
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.g. Types of Investigation">
     11.6 Summary
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-11.h. Types of Investigation">
     References
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-12.a. Linear Regression I">
   12. Linear Regression I
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.b. Linear Regression I">
     12.1 Introduction
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.c. Linear Regression I">
     12.2 Data used in our examples
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#exploratory-analyses">
     12.2.1 Exploratory analyses
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#determining-the-dependent-and-independent-variables">
     12.2.2 Determining the dependent and independent variables
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.d. Linear Regression I">
     12.3 The simple linear regression model
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.e. Linear Regression I">
     12.4 Estimation of the population parameters
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.f. Linear Regression I">
     12.5 Example: continuous independent variable
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.g. Linear Regression I">
     12.6 Inference for the slope
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.h. Linear Regression I">
     12.7 Example: binary independent variable
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-12.i. Linear Regression I">
     12.8 Additional material
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-13.a. Linear Regression II">
   13. Linear Regression II
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-13.b. Linear Regression II">
     13.1 Categorical independent variables
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-13.c. Linear Regression II">
     13.2 Multivariable linear regression
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-13.d. Linear Regression II">
     13.3 Including multiple covariates
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-13.e. Linear Regression II">
     13.4 Centering
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-13.f. Linear Regression II">
     13.6  Including higher-order terms
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-13.g. Linear Regression II">
     13.7  Modelling interaction terms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-14.a. Linear Regression III">
   14. Linear Regression III
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.b. Linear Regression III">
     14.1 Assumptions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#assumptions-of-the-linear-regression-model">
     14.1.1 Assumptions of the linear regression model
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.c. Linear Regression III">
     14.2 Investigating assumptions using plots
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.d. Linear Regression III">
     14.2 Statistical tests of assumptions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.e. Linear Regression III">
     14.3 Dealing with violations of assumptions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.f. Linear Regression III">
     14.5 Collinearity
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.g. Linear Regression III">
     14.6 Optional Reading: Analysis of Variance
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-14.h. Linear Regression III">
     14.7 Proofs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-15.a. Logistic Regression">
   15 Logistic Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.b. Logistic Regression">
     15.1 Regression modelling for binary outcomes
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#why-cant-we-just-use-linear-regression">
     15.1.2 Why can’t we just use Linear Regression?
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.c. Logistic Regression">
     15.2 Data used in our examples
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#exploratory-analyses">
     15.2.1 Exploratory analyses
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.d. Logistic Regression">
     15.3 The logistic regression model
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.e. Logistic Regression">
     15.4 Estimating the parameters
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.f. Logistic Regression">
     15.5 Examples
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.g. Logistic Regression">
     15.6 Inference
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.h. Logistic Regression">
     15.7 Multivariable logistic regression
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.i. Logistic Regression">
     15.8 Interactions and higher-order terms
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.j. Logistic Regression">
     15.9 Model diagnostics
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.k. Logistic Regression">
     15.12 Common pitfalls
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.l. Logistic Regression">
     15.13 Further resources
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-15.m. Logistic Regression">
     15.14 Additional reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-16.a. Generalised Linear Model (GLM)">
   16. Generalised Linear Models (GLMs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.b. Generalised Linear Model (GLM)">
     16.1 Introduction to Generalised Linear Models (GLMs)
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.c. Generalised Linear Model (GLM)">
     16.2 Generalised Linear Model Components
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.d. Generalised Linear Model (GLM)">
     16.3 GLM Assumptions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.e. Generalised Linear Model (GLM)">
     16.4 Link Functions
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.f. Generalised Linear Model (GLM)">
     16.5 Programming GLM’s in R
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.g. Generalised Linear Model (GLM)">
     16.6 Introduction to Poisson Generalised Linear Modelling (Poisson Regression)
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.h. Generalised Linear Model (GLM)">
     16.7 Poisson Regression Example
    </a>
   </li>
   <li class="toctree-l2 nav-item toc-entry">
    <a class="reference internal nav-link" href="00. Welcome.html#document-16.i. Generalised Linear Model (GLM)">
     16.8 Common Problems in Poisson Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-17. Investigations round up">
   17. The role of regression in different types of investigation
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="00. Welcome.html#document-18. Statistics for HDS round up">
   Statistics and Health Data Science
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="welcome-to-statistics-for-health-data-science">
<h1>Welcome to Statistics for Health Data Science<a class="headerlink" href="#welcome-to-statistics-for-health-data-science" title="Permalink to this headline">¶</a></h1>
<p>These notes provide the core material for the MSc module, Statistics for Health Data Science.</p>
<p>This is a compulsory module for the programme MSc Health Data Science. The module provides an introduction to the key statistical concepts and methods for health data science. Topics covered include probability, initial data description and
exploration, frequentist and Bayesian approaches to statistical inference and  regression modelling. These topics provide the framework needed for subsequent modules. The module places a focus on learning through practical examples and incorporates directed learning, lectures, group discussion, and computer practical exercises.</p>
<div class="section" id="overall-aim-of-the-module">
<h2>1.1 Overall aim of the module<a class="headerlink" href="#overall-aim-of-the-module" title="Permalink to this headline">¶</a></h2>
<p>The overall module aims are to introduce:</p>
<ul class="simple">
<li><p>the motivation and critical thinking towards solving a question in health science through interrogation of data and drawing conclusions from evidence;</p></li>
<li><p>the principles of probability, regression modelling and statistical inference within frequentist and Bayesian frameworks.</p></li>
</ul>
</div>
<div class="section" id="module-intended-learning-outcomes">
<h2>1.2 Module Intended Learning Outcomes<a class="headerlink" href="#module-intended-learning-outcomes" title="Permalink to this headline">¶</a></h2>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>Upon successful completion of the module you will be able to:</p>
<ul class="simple">
<li><p>evaluate the application of different probability distributions to model health data (including Poisson, Binomial and Normal);</p></li>
<li><p>critically analyse frameworks for frequentist and Bayesian inference and evaluate their strengths, limitations and differences;</p></li>
<li><p>examine the concepts of sampling variability, estimators, bias, confidence intervals and credible intervals;</p></li>
<li><p>examine the theoretical basis of linear regression and generalized linear models;</p></li>
<li><p>assess the application of regression modelling to address specific health data science questions;</p></li>
<li><p>critically evaluate strengths and limitations of different statistical methods, including regression models, within a health data science project;</p></li>
<li><p>draw conclusions from the results of a data analysis and justify those conclusions, appropriately acknowledging uncertainty in the results.</p></li>
</ul>
</div>
</div>
<div class="section" id="module-content">
<h2>1.3 Module Content<a class="headerlink" href="#module-content" title="Permalink to this headline">¶</a></h2>
<p>The module is split into 16 taught sessions, each building statistical knowledge for health data science. The sessions are:</p>
<ol class="simple">
<li><p>Introduction</p></li>
<li><p>Probability and Discrete Probability Distributions</p></li>
<li><p>Continuous Probability Distribution</p></li>
<li><p>Populations and Sampling</p></li>
<li><p>Likelihood</p></li>
<li><p>Maximum Likelihood Estimation</p></li>
<li><p>Frequentist Inference I</p></li>
<li><p>Frequentist Inference II</p></li>
<li><p>Bayesian Statistics I</p></li>
<li><p>Bayesian Statistics II</p></li>
<li><p>Types of Investigation</p></li>
<li><p>Linear Regresion I</p></li>
<li><p>Linear Regresion II</p></li>
<li><p>Linear Regresion III</p></li>
<li><p>Logistic Regression</p></li>
<li><p>GLMs and Poisson Regression</p></li>
</ol>
<p>A final short section (17) connects the regression models to the session regarding types of investigation. This is optional reading and does not have an accompanying taught session.</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-00. Acknowledgements"></span><div class="section" id="acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this headline">¶</a></h2>
<p>Many people have contributed to this document over time, including a large number of previous and current members of the Department of Medical Statistics at the London School of Hygiene and Tropical Medicine. In particular, we would like to acknowledge contributions from Corentin Segalas, Elizabeth Williamson, Emily Granger, Emily Nightingale, Kathleen O’Reilly, Linda Sharples, Melanie Smuk, Mia Tackney, Nicholas Jewell and Ruth Keogh.</p>
<p>We thank Jennifer Nicholas, whose notes  were particularly useful in the development of the linear regression sessions.  We thank Katy Morgan for notes which helped inform the development of the section about inference for maximum likelihood models.</p>
<p>The notes for the Bayesian Inference sessions are heavily based on the Foundations course material created by Alex Lewin and Alexina Mason, which was previously developed by James Carpenter, Marcel Zwahlen and Beat Neuenschwander. Some sections are inspired also by notes from Michail Papathomas. We are grateful for their work and permission to re-use.</p>
<div class="section" id="version">
<h3>Version<a class="headerlink" href="#version" title="Permalink to this headline">¶</a></h3>
<p>This document was last updated: September 2021</p>
<p>Inevitably there will be some typos in these notes. Please do let us know any you spot (at: <a class="reference external" href="mailto:mscHDS&#37;&#52;&#48;lshtm&#46;ac&#46;uk">mailto:mscHDS<span>&#64;</span>lshtm<span>&#46;</span>ac<span>&#46;</span>uk</a>) and we will correct them.</p>
</div>
</div>
<span id="document-00. How_to_use"></span><div class="section" id="how-to-use-this-book">
<h2>How to use this book<a class="headerlink" href="#how-to-use-this-book" title="Permalink to this headline">¶</a></h2>
<p>This book contains the core content for the module. It is designed to be read in conjunction with the practical sessions and accompanying videos.</p>
<p>Each numbered session has an accompanying lecture and practical session. Each section of the book additionally has a short summary of the contents of that section to help you build an overview of the material. The final sections of the book do not have accompanying taught sessions but have some brief comments to help you pull all the material together.</p>
<div class="section" id="r-code">
<h3>R code<a class="headerlink" href="#r-code" title="Permalink to this headline">¶</a></h3>
<p>Analyses are illustrated using R code. Each page which contains R code should be self-contained. You should be able to copy and paste each chunk of R code (using the little icon to the top-right of the code cell) into R and run all the code in that page.</p>
<p>A number of packages will be required. These are loaded using the <code class="docutils literal notranslate"><span class="pre">library</span></code> function, at the appropriate points. If you are using your local version of R you may need to install these packages. Below, you will find code to install all packages used in these notes.</p>
<div class="cell tag_remove_output docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Graphics package</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggplot2&quot;</span><span class="p">)</span>

<span class="c1"># Will be used for fitting some generalised models:</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;VGAM&quot;</span><span class="p">)</span>

<span class="c1"># Contains some useful goodness-of-fit diagnostics:</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;pscl&quot;</span><span class="p">)</span>

<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;sandwich&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-01. Introduction"></span><div class="section" id="introduction">
<h2>1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Statistics for Health Data Science is the scientific approach behind investigating health. The organisers of this module have been specific in the wording. Of course, we will learn techniques to interogate data using statistics! But also, the focus on the approach is to think about a problem scientifically, for example to consider a research question, or a hypothesis. The scientific approach is important, and is described in more detail in this Introduction and the associated lecture. The scientific inquiry is applied to health data, which can take a number of forms, including ‘found data’. We think this is what makes data science for health unique; found data presents great advantages as there may be a lot of found data avaialble, but also challenges as the origin of the data and the potential for biases in the data can make analysis more challenging.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<ul class="simple">
<li><p>consider the concept of Statistics for Health Data Science and the bigger picture of scientific inquiry</p></li>
<li><p>understand the data by identifying broad issues of structure, type, provenance and design</p></li>
<li><p>describe variable types</p></li>
<li><p>understand the concept of selection bias</p></li>
<li><p>think about data summaries using exploratory data analysis and visualizations (simple examples)</p></li>
<li><p>consider what to measure and why in a scientific study</p></li>
<li><p>have a basic understanding of the difference between frequentist (Fisher) and Bayesian inference</p></li>
</ul>
</div><div class="section" id="what-you-will-learn">
<h3>1.1 What you will learn<a class="headerlink" href="#what-you-will-learn" title="Permalink to this headline">¶</a></h3>
<p>By engaging with module you will acquire skills that a data scientist will need to interogate data to answer a health related question. Much of the focus is on the statistical tools that are most often used, as shown above in the <em>Intended Learning Objectives</em>. The module is designed towards using statistics within a problem solving cycle (more in Section 1.3).</p>
<p>Consider this book to be a <em>practical guide</em> in using statistics. Every session provides some statistical theory and examples so you can see the theory in action. Especially in the earlier sessions, some of the examples can be done without using a computer. As the module progresses many of the calculations are carried out using R, and we will increasingly apply the concepts to real data. We provide the code for each example.</p>
<p>As you work through the sessions, your ability to use statistics in health data science should improve. We will begin with relatively simple questions that we want to answer. As the module progresses, the questions will be more relevant to a health, and the steps involved will require more statistical inference and scientific inquiry.</p>
</div>
<div class="section" id="module-layout">
<h3>Module layout<a class="headerlink" href="#module-layout" title="Permalink to this headline">¶</a></h3>
<p>The module is roughly divided into three sections: <strong>Basic Probability</strong>, <strong>Statistical inference</strong> and <strong>Statistical modelling</strong>. Each sections build upon the previous. Probability is perhaps an obvious underpinning of statistics; it gives us the <em>building blocks</em> for both statistical inference and modelling. In probability (sessions 2 and 3) we cover discrete and continuous distributions, and make use of the <em>maths refresher</em> in several sessions. Fundamentally, when dealing with data we often need to make assumptions about what <em>distribution</em> the data is drawn from, and knowing the properties of these distributions then enables us to carry out statistical inference. When we move to statistical inference (sessions 4 to 10) this gives us the understanding of how statistical theory can be used to make statements during our investigations. An important consideration is thinking about the statistical theory that enables us to invesitgate our data, but then use this knowledge to then make statements about the wider population (or target population). With this knowledge we then move to Bayesian statistics, where we apply our knowledge to specific health questions and make use of prior knowledge. We then move onto applications that are more likely to be encountered in health applications; and consider the process of investigation and statistical modelling (sessions 11 to 17). These sessions are linked together because as well as being able to run a model and generate results, it is very important to articulate why it was done, ie. identify the purpose of investigation. In the sessions on regression modelling several classes of model will be described and illustrated in detail.</p>
<p>At the end of the module there will be a revision session, and an assessment (more detail is provided on Moodle).</p>
</div>
<div class="section" id="what-you-won-t-learn">
<h3>1.2 What you won’t learn<a class="headerlink" href="#what-you-won-t-learn" title="Permalink to this headline">¶</a></h3>
<p>We provide important aspects of statistical theory in order for you to understand the reasoning behind the approaches, but there are aspects of statistical theory that are outside of the module scope. In this case, we may provide further reading. Other modules within the LSHTM may cover this in more detail, such as <em>Foundations of Medical Statistics</em>.</p>
<p>This module provides the basics that further modules in the MSc may require, such as <em>Data Challege</em> and <em>Machine Learning</em>. As the name might imply, the statistical techniques used in machine learning are covered in the other module.</p>
<p>The programming associated with this module is carried out in R. The statistical analysis can be carried out in other software, such as Python. In some of the later sessions, we will provide the equivalent Python code, but we do not expect you to use this (and you will not be assessed on this).</p>
</div>
<div class="section" id="the-data-science-project">
<h3>1.3 The Data Science Project<a class="headerlink" href="#the-data-science-project" title="Permalink to this headline">¶</a></h3>
<p>Once a health related question is posed, this could be the start of a Data Science Project. Typically, we try to frame the question around a scientific hypothesis, identify some data that can be used to address this question/hypothesis, carry out some anlaysis and draw a conclusion. In David Spiegelhalter’s Book <em>The Art of Statistics</em> this process is referred to as the PPDAC cycle (fig. 1.1). If doing data science is new to you, this might be considered a linear process. But, in many circumstances the problem solving process is a cycle, where a problem may be solved using a iterative process. The iterative process doesn’t mean that the first attempt was <em>wrong</em>, but instead this iterative way of thinking enables the data scientist to think critically about each stage of the cycle and identify strengths, weaknesses and opportunities for improvement.</p>
<p>Note that there are many ways to describe the cycle of a Data Science Project, and more examples are given in the lecture by Prof. Nick Jewell. Some might chime with you more (or less) than the PPDAC presented here.</p>
<p><img alt="" src="_images/01_intro_PPDAC_adapt.png" /></p>
<p><b> Fig. 1.1 The PPDAC cycle (From <em>The Art of Statistics</em> by David Spiegelhalter) </b></p>
<div class="section" id="identify-the-problem-and-generate-a-hypothesis">
<h4>1.3.1 Identify the problem and generate a hypothesis<a class="headerlink" href="#identify-the-problem-and-generate-a-hypothesis" title="Permalink to this headline">¶</a></h4>
<p>Typically Health Data Science projects start with a question. The question may be framed around one of three investigation types: <strong>description</strong>, <strong>prediction</strong> and <strong>causality and explanation</strong>. The specific of this is explored in more detail in Session 11 (Types of Investigation).</p>
<p>When developing a Data Science Project there is a need to create a question that is answerable within the timeframe available, and sufficiently precise. It is also preferable to frame a question around a hypothesis that can be a testable prediction, and this is where statistics can be used, because a lot of statistics are framed using a hypothesis (this is especialy true of frequentist statistics). However, it is not always necessary to have a hypothesis, for example if the question is exploratory.</p>
</div>
<div class="section" id="develop-a-plan-consider-the-data-design">
<h4>1.3.2 Develop a plan, consider the data design<a class="headerlink" href="#develop-a-plan-consider-the-data-design" title="Permalink to this headline">¶</a></h4>
<p>The plan to answer the question/hypothesis will involve some data. For data science it is likely that the data has not been collected specifically for the purposes of answering the question. Examples may include surveillance data for infectious diseases (eg. self-reported cases of influenza-like illness to a public website) or internet searches for “sore throat”. In this case, it is important to recognise specific attributes of the data:</p>
<ul class="simple">
<li><p>Where did the data come from? What is its provenance?</p></li>
<li><p>How and why was the data collected?</p></li>
<li><p>What kind of individuals provided data, and why were they selected?</p></li>
</ul>
<p>These are important questions because they relate to the principles of <em>statistical inference</em>, which is covered in sessions 4-10. Central to using data to draw a conclusion is that your <em>sample</em> data is representive of the <em>population</em>. Consequently, we can carry out an analysis on the data and make statements about the wider population. This is covered in more detail in session 4 (<em>Populations and Samples</em>).</p>
<p>At this stage it is important to identify the “outcome” variable and the “explanatory variables” present in the dataset, and whether we know already that some explanatory variables are associated with the outcome variable. It is also a good idea to idenitfy what type of data each variable corresponds to: continuous, ordinal, categorical.</p>
<p>The design of the dataset is also important, as this helps us understand the structure of the data, and a framework for analyses on the data. Commonly encountered designs are (note that these are also covered in the <em>Epidemiology for Health Data Science</em> module):</p>
<ul class="simple">
<li><p>A cross-sectional design</p></li>
<li><p>A cohort design</p></li>
<li><p>An outcome-based design</p></li>
<li><p>A longitudinal design</p></li>
</ul>
<p>At this stage, you may start to consider the appropriate analysis to make considering the data. As the module (and others, for example the <em>Data Challenge</em> module) develops you will identify the analysis steps that can be undertaken according to the question.</p>
</div>
<div class="section" id="the-data">
<h4>1.3.3 The data<a class="headerlink" href="#the-data" title="Permalink to this headline">¶</a></h4>
<p>There are several aspects of the data that need to be considered, and some of which are covered in the module <em>Health Data Management</em>, such as entering the data, managing the data, and cleaning the data.</p>
<p>Here we will focus on aspects which might affect the analysis and conclusions that you make later in the PPDAC cycle.</p>
<p>The first is the presence of potential data filtering, ie. is there any reason to suspect that data are missing or censored, in reference to to wider population? If so, this could result in potential bias. The most commonly encountered bias is <em>selection bias</em>, where extrapolation to the wider population may be challenging. Additionally, <em>collider bias</em> may result in inappropriate conclusions being made on the effect of explanaotry variables on the outcome.</p>
<p>The second consideration is confounding, where there may be a common cause for both a explanatory variable and the outcome. The result is that an assoication between the explanatory variable and the outcome may be identified, but the relationship is not causal.</p>
</div>
<div class="section" id="data-analysis">
<h4>1.3.4 Data analysis<a class="headerlink" href="#data-analysis" title="Permalink to this headline">¶</a></h4>
<p>Exploratory data anlaysis, and especially <strong>plotting your data</strong> is a really important part of the Data Science Project. As you progress through the module, this will become more and more familiar. Plotting your data is important to <em>sense check</em> the data and identify any errors, outliers or omissions (this is especially important with found data). Further to this, many statistical anlaysis benefit from plotting the results, for example by plotting the residuals of a linear model against the outcome to check that the model is correctly specified. Often, suitable plots may carry with them <em>parameter estimates</em> from the data, for example the mean number of influenza-like illnesses reported per week when the data are available daily.</p>
<p>It is at this stage that you do the analysis. This is where the concepts covered in this <em>statistics module</em> become useful. What we want to emphasize here is that this is done while considering all the others factors within the PPDAC cycle.</p>
</div>
<div class="section" id="conclusions">
<h4>1.3.5 Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h4>
<p>So you’ve gotten this far! An ideal conclusion has brought all the other aspects together, and at most stages some form of statistical inference is considered. The conclusion then needs to consider the statistical result <em>in the context of the other considerations</em>, such as wanting to make inference about the population from the sample of data.</p>
<p>For example, let’s say the influenza-like illness data from the internet reported 40 cases per 100,000 of the population from November to January. Reporting symptoms might be skewed towards people who regularly use the internet, which might exclude elderly individuals. Consequently, this mean estimate may be an under-estimate of the population incidence due to selection bias.</p>
</div>
</div>
<div class="section" id="why-we-teach-both-frequentist-and-bayesian-statistics">
<h3>1.6 Why we teach both frequentist and Bayesian statistics<a class="headerlink" href="#why-we-teach-both-frequentist-and-bayesian-statistics" title="Permalink to this headline">¶</a></h3>
<p>A majority of the module covers statistical inference from the <em>frequentist</em> perspective. Much of frequentist statistical inference was developed by Ronald Fisher, who has been described as the founder of modern statistics, and much of his focus was on experimental design in agriculture. A simple explanation of the philosophy behind frequentist statistics is that a <em>fact</em> is either true or not true, and data can be used to assess which of these outcomes can be accepted. In contrast, Bayesian statistics suggests that a probability can be assigned to whether the fact is true. The field of Bayesian statistics is named after Reverend Thomas Bayes, who developed the theory almost 200 years before Fisher was alive (and owing to improvements in computation the theory is now much more accessible and has overtaken frequentist approaches in some scientific fields). In addition, frequentist statistics makes use of the data available, and there is little (or any) ability to incorporate additional knowledge. Within a Bayesian framework, inclusion of prior knowledge is inherent, and this prior knowledge can be combined with data.</p>
<p>Some argue that the philosophies are diametrically opposed to each other, and statisticians should choose a side. This is a strong view (and perhaps not the majority view?), but first it is important to understand the principles behind each approach. We have opted to teach both because of this reason, and leave it up to you to consider the advantages and disadvantages of each approach. Ultimately, both have data central to the approach in making statistical inference, so it is likely that both should be considered as perspectives in a Data Science Project.</p>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-02. Probability.Intro"></span><div class="section" id="probability-and-statistics">
<h2>Probability and statistics<a class="headerlink" href="#probability-and-statistics" title="Permalink to this headline">¶</a></h2>
<p>We assume that students are already familiar with the basic probability concepts covered in the pre-course <a class="reference external" href="https://lshtm-hds.github.io/Math-Refresher">Refresher</a>.</p>
<div class="section" id="how-does-probability-relate-to-statistics">
<h3>How does probability relate to statistics?<a class="headerlink" href="#how-does-probability-relate-to-statistics" title="Permalink to this headline">¶</a></h3>
<p>Probability theory describes the chance of an event occurring while statistics concerns the collection, organization, analysis and interpretation of data. However, probability theory and statistics are intrinsically linked.</p>
<p>A typical <strong>probability</strong> problem is as follows. We are planning to run a small clinical study, which involves giving 8 patients a particular drug. We are told that the probability that a single patient experiences a side effect from a particular drug is 0.23. From this information, we can calculate the probability of various complex events occurring. For example, we might want to know the probability that more than 6 of the 8 patients will experience a side effect. Or we might wish to know the probability that none of the 8 patients experience a side effect. Here, we are assuming that a characteristic (parameter) of the population is known. Specifically, we are assuming that we know the true probability of a single patient experiencing a side effect.</p>
<div class="figure align-default" id="probability-vs-inference">
<a class="reference internal image-reference" href="_images/Probability_vs_inference.png"><img alt="_images/Probability_vs_inference.png" src="_images/Probability_vs_inference.png" style="height: 300px;" /></a>
</div>
<p>This is not how real life works! Typically, in health data science studies, we have observed some data which we believe can be modelled using a particular distribution (such as the binomial distribution that we will soon meet), but the parameters of that distribution are unknown. For the small clinical study, for example, in real life we would run the study and observe how many of the 8 patients did in fact experience a side effect. But the probability of a patient experiencing a side effect would be unknown. The study aim would be to use the observed data to make statements - <strong>inferences</strong> - about this unknown probability. So in some senses, the problem is the opposite way round.</p>
<p>It turns out that the process of statistical inference relies very heavily on probability calculations. Suppose we conduct our small clinical study and, for example, observe that in fact 2 of the 8 patients experience the side effect. Loosely speaking, the process of inference involves the following steps. We use probability theory to calculate the probability that 2 people within our sample of 8 experience the side effect, for every possible value of the unknown probability of experiencing a side effect. We then use these probabilities to make statements about plausible values of the unknown probability. As we will see, we can take various approaches to this inference, in particular using the frequentist or Bayesian frameworks.</p>
<p>Therefore, the next two lectures, concerning probability theory, comprise building blocks that you will need when you subsequently meet ideas about likelihood, inference and regression modelling.</p>
</div>
</div>
<span id="document-02.a. Probability.Discrete"></span><div class="section" id="discrete-distributions">
<h2>2. Discrete Distributions<a class="headerlink" href="#discrete-distributions" title="Permalink to this headline">¶</a></h2>
<p>This session is the first of two sessions covering useful elements and applications of basic probability. In this first session, we focus on variables which have a <strong>discrete</strong> distribution. In the next session, we extend these ideas to variables which have a <strong>continuous</strong> distribution.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<ul class="simple">
<li><p>apply Bayes’ Theorem to obtain useful properties of screening tests</p></li>
<li><p>derive the binomial and Poisson probability distribution functions</p></li>
<li><p>apply the binomial and Poisson distributions to health settings</p></li>
<li><p>evaluate the appropriateness of the binomial and Poisson distributions to health settings</p></li>
</ul>
</div><p>The first part of this session explores Bayes’ Theorem. This is a crucial probability theorem, which underlies the Bayesian approach to inference. Another important use is its application to quantify how well a screening test or prognostic classification tool is performing. In this session, we focus on this latter application but you will return to Bayes’ Theorem in the later sessions about Bayesian statistics.</p>
<p>The second and third parts of this lecture explore the binomial distribution and the Poisson distribution. For these sessions, we will assume we know characteristics of the population (e.g. the true prevalence of a disease, or incidence rate of a disease) and will explore how to calculate probabilities of various outcomes occurring. In subsequent sessions we will see how these sorts of calculations are used within the important area of statistical inference.</p>
<div class="toctree-wrapper compound">
<span id="document-02.b. Probability.Discrete"></span><div class="section" id="application-of-bayes-theorem">
<h3>2.1 Application of Bayes’ Theorem<a class="headerlink" href="#application-of-bayes-theorem" title="Permalink to this headline">¶</a></h3>
<p>Bayes’ Theorem has important and powerful applications in medical statistics. One important link, which you will return to later, is its connection to Bayesian statistics. In this session, we focus on another common application of Bayes’ theorem, in the area of assessing the accuracy of screening tests and prognostic scores.</p>
<div class="section" id="screening-tests-and-prognostic-scores">
<h4>2.1.1 Screening tests and prognostic scores<a class="headerlink" href="#screening-tests-and-prognostic-scores" title="Permalink to this headline">¶</a></h4>
<p>Screening tests are tests that attempt to identify people with a particular condition or disease of interest. Babies are often screened for cystic fibrosis at birth, for example. Sometimes, screening tests attempt to identify high risk people rather than those who already have the condition of interest. Cervical screening, for example, which is offered to all women and people with a cervix aged 25 to 64 in the UK by the National Health Service, is a test to help prevent cancer. It doesn’t look for existing cancer, but instead looks for certain viruses which can increase the subsequent risk of cancer. Similarly, prognostic tests or prognostic scores are used to identify a high risk group.</p>
<p>Screening tests or prognostic scores can be based on one genetic marker, as in our example below, or many. Or they might incorporate information from other sources (e.g. biomarkers, family history of the disease). These processes typically result in a binary classification of “positive” or “negative”.</p>
</div>
<div class="section" id="bayes-theorem">
<h4>2.1.2 Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h4>
<p>Suppose we have an event <span class="math notranslate nohighlight">\(A\)</span> and a set of events <span class="math notranslate nohighlight">\(B_1, B_2, ..., B_n\)</span> that partition the sample space. Suppose that we have information about the conditional probability of <span class="math notranslate nohighlight">\(A\)</span> conditional on event <span class="math notranslate nohighlight">\(B_j\)</span>, i.e. we know <span class="math notranslate nohighlight">\(P(A | B_j)\)</span>, for each <span class="math notranslate nohighlight">\(j\)</span>. However, what we actually want to know about is <span class="math notranslate nohighlight">\(P(B_j | A)\)</span>.</p>
<p>Bayes’ Theorem provides a way of reversing the conditioning.</p>
<div class="alert alert-success">
<p><b> Bayes’ Theorem:</b></p>
<div class="math notranslate nohighlight">
\[
P(B_{j}|A) = \frac{P(A|B_{j}) P(B_{j})}{P(A)} = \frac{P(A|B_{j}) P(B_{j})}{\sum^{n}_{k=1} P(A|B_{k}) P(B_{k})}.
\]</div>
</div>
</div>
<div class="section" id="example-genetic-marker-in-childhood-cancer">
<h4>2.1.3 Example: Genetic marker in childhood cancer<a class="headerlink" href="#example-genetic-marker-in-childhood-cancer" title="Permalink to this headline">¶</a></h4>
<p>In a population, 10% of people develop a particular childhood cancer.  Of those who develop the cancer (<span class="math notranslate nohighlight">\(C\)</span>), 1 in 4 carry a genetic marker, <span class="math notranslate nohighlight">\(M\)</span>, whereas of those who don’t develop the cancer, 1 in 10 carry <span class="math notranslate nohighlight">\(M\)</span>.  A newly born infant is tested for the genetic marker and is found to carry it. What is the probability that this infant will develop cancer?</p>
<p>The first couple of sentences tell us that <span class="math notranslate nohighlight">\(P(C) = 0.1\)</span>, <span class="math notranslate nohighlight">\(P(M|C) = 0.25\)</span>  and <span class="math notranslate nohighlight">\(P(M | \bar{C})=0.1.\)</span> Our interest lies in <span class="math notranslate nohighlight">\(P(C|M)\)</span>. So we wish to reverse the conditioning. We can obtain this by applying Bayes’ Theorem:</p>
<div class="math notranslate nohighlight">
\[
P(C|M) = \frac{P(M|C)P(C)}{P(M|C)P(C) + P(M|\bar{C})P(\bar{C})}
\]</div>
<p>Substituting in the values above gives</p>
<div class="math notranslate nohighlight">
\[
P(C|M) = \frac{0.25\times 0.1}{0.25\times 0.1 + 0.1\times 0.9}=0.22
\]</div>
<p><span class="math notranslate nohighlight">\(P(C|M)\)</span> is called the <strong>positive predictive value</strong> (PPV) of the test.  It is the probability, given a positive test result, that the individual actually will develop the disease.  i.e. in this case there is a 22% chance that the infant will develop the disease if they tested positive.</p>
</div>
<div class="section" id="the-confusion-matrix">
<h4>2.1.4 The confusion matrix<a class="headerlink" href="#the-confusion-matrix" title="Permalink to this headline">¶</a></h4>
<p>More generally, suppose we have a procedure that results in a binary classification (a binary prediction). This might be a screening test, which could be based on one or more genetic markers or biomarkers. It could be based on the output from a  prognostic risk score or a label derived from an algorithm. Whatever the procedure, suppose we end up with a binary classification: “Positive” or “Negative”. In the health context, this terminology (positive/negative) might represent pairs such as: “Diseased” and “Undiseased”; “Dead” and “Alive” or “Hospitalised” and “Not hospitalised”. We can contrast the binary classification with the (binary) true status. In the general discussion below, we will also use the terms positive and negative to denote the two possible true statuses.</p>
<p>The following table is often called a <strong>confusion matrix</strong>, or sometimes error matrix. The name confusion matrix stems from the fact that the matrix allows you to see whether the classification is <em>confusing</em> two classes. The values <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span> and <span class="math notranslate nohighlight">\(D\)</span> are the numbers in each category. The name comes from the fact that the table allows you to see if the classification procedure is “confusing” two categories.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>classification</p></th>
<th class="head"><p>Truth:</p></th>
<th class="text-align:center head"><p>Positive</p></th>
<th class="text-align:center head"><p>Negative</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Positive</p></td>
<td><p></p></td>
<td class="text-align:center"><p>A</p></td>
<td class="text-align:center"><p>B</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Negative</p></td>
<td><p></p></td>
<td class="text-align:center"><p>C</p></td>
<td class="text-align:center"><p>D</p></td>
</tr>
</tbody>
</table>
<p>Two groups of people were correctly classified:</p>
<ul class="simple">
<li><p><em>True Positives</em>. The <span class="math notranslate nohighlight">\(A\)</span> individuals are people who are, in truth, positive (for the disease or outcome of interest) and were classified as positive. So they are often called true positives.</p></li>
<li><p><em>True Negatives</em>. The <span class="math notranslate nohighlight">\(D\)</span> individuals are people who are, in truth, negative and and were classified as negative.</p></li>
</ul>
<p>Two groups of people were incorrectly classified:</p>
<ul class="simple">
<li><p><em>False Positives</em>. The <span class="math notranslate nohighlight">\(B\)</span> individuals are people who are, in truth, negative but were incorrectly classified as positive. These are sometimes called Type I errors.</p></li>
<li><p><em>False Negatives</em>. The <span class="math notranslate nohighlight">\(C\)</span> individuals are people who are, in truth, positive but were incorrectly classified as negative. These are sometimes called Type II errors.</p></li>
</ul>
<p>Now let us imagine the same table but with joint probabilities rather than numbers from a sample.  So, for instance, <span class="math notranslate nohighlight">\(p_A\)</span> is the joint probability of being classified as positive <em>and</em> being truly positive for the outcome.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Prediction</p></th>
<th class="head"><p>Truth:</p></th>
<th class="text-align:center head"><p>Positive</p></th>
<th class="text-align:center head"><p>Negative</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Positive</p></td>
<td><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(p_A\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(p_B\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Negative</p></td>
<td><p></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(p_C\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(p_D\)</span></p></td>
</tr>
</tbody>
</table>
<p>We can obtain estimates of various useful quantities from this matrix. We will use the following notation: <span class="math notranslate nohighlight">\(O\)</span> represents being, in truth, positive for the outcome of interest, and <span class="math notranslate nohighlight">\(\bar{O}\)</span> represents being truly negative. <span class="math notranslate nohighlight">\(P\)</span> represents being classified as positive and <span class="math notranslate nohighlight">\(\bar{P}\)</span> being classified as negative.</p>
<p>The tabs below show various useful quantities.</p>
<div class="tabbed-set docutils">
<input checked="checked" id="d679129a-0728-4d9e-8e45-547e5bfc29ae" name="ce820be7-105e-41d4-a654-b57a470d3321" type="radio">
</input><label class="tabbed-label" for="d679129a-0728-4d9e-8e45-547e5bfc29ae">
Outcome prevalence</label><div class="tabbed-content docutils">
<p>The prevalence of the outcome is:</p>
<div class="math notranslate nohighlight">
\[
P(O) = \frac{p_A+p_C}{p_A+p_B+p_C+p_D}
\]</div>
<p>Prevalence is another word for risk or proportion. It tells us the fraction of the population of interest who have the outcome.</p>
</div>
<input id="914d5074-6627-49a0-990b-6c0703748c88" name="ce820be7-105e-41d4-a654-b57a470d3321" type="radio">
</input><label class="tabbed-label" for="914d5074-6627-49a0-990b-6c0703748c88">
Sensitivity</label><div class="tabbed-content docutils">
<p>This is a property of the test. The sensitivity of the test remains the same, irrespective of how common or rare the outcome is.</p>
<p>The sensitivity is:</p>
<div class="math notranslate nohighlight">
\[
P(P|O)=\frac{p_A}{p_A+p_C}
\]</div>
<p>The terminology comes from the setting of clinical tests, i.e. how sensitive this test is to the presence of the disease. It is also often called the recall in the fields of machine learning and computer science.</p>
<p>This is also sometimes referred to as the true positive rate.</p>
</div>
<input id="30df210b-533f-4876-b925-2bbc96b17a76" name="ce820be7-105e-41d4-a654-b57a470d3321" type="radio">
</input><label class="tabbed-label" for="30df210b-533f-4876-b925-2bbc96b17a76">
Specificity</label><div class="tabbed-content docutils">
<p>This is a property of the test. The specificity of the test remains the same, irrespective of how common or rare the outcome is.</p>
<p>The specificity is:</p>
<div class="math notranslate nohighlight">
\[
P(\bar{P}|\bar{O})=\frac{p_D}{p_B+p_D}
\]</div>
<p>As for the sensitivity, this terminology comes from the setting of clinical tests, i.e. how specific  this test is to the presence of this disease (versus other diseases). So a test which is only positive for this specific disease is very specific. A test which picks up the presence of this disease, and other similar diseases, is not very specific.</p>
<p>This is sometimes called the selectivity or true negative rate.</p>
</div>
<input id="6a7a0507-35d6-4fe0-a00c-1a909ff69f8a" name="ce820be7-105e-41d4-a654-b57a470d3321" type="radio">
</input><label class="tabbed-label" for="6a7a0507-35d6-4fe0-a00c-1a909ff69f8a">
PPV</label><div class="tabbed-content docutils">
<p>PPV stands for Positive Predictive Value, which is shorthand for the predictive value of a positive classification. This is very common terminology in clinical settings.</p>
<p>This quantity is often of most interest to the person having the test. It answers the question: “what is the probability that I have (or will have) the outcome, given that I have just received a positive classification?”</p>
<p>The PPV is:</p>
<div class="math notranslate nohighlight">
\[
P(O|P) = \frac{p_A}{p_A+p_B}
\]</div>
<p>In machine learning, it is typically called the precision.</p>
</div>
<input id="7f19534f-7eff-4bae-9883-94007c9ffa54" name="ce820be7-105e-41d4-a654-b57a470d3321" type="radio">
</input><label class="tabbed-label" for="7f19534f-7eff-4bae-9883-94007c9ffa54">
Accuracy</label><div class="tabbed-content docutils">
<p>The accuracy is:</p>
<div class="math notranslate nohighlight">
\[
P(\mbox{correct classification}) = \frac{p_A + p_D}{p_A+p_B+p_C+p_D}
\]</div>
<p>This quantity is less used in medical settings but is commonly used elsewhere.</p>
</div>
</div>
</div>
</div>
<span id="document-02.c. Probability.Discrete"></span><div class="section" id="the-binomial-distribution">
<h3>2.2 The binomial distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this headline">¶</a></h3>
<p>The binomial distribution is used to model the number of successes out of a fixed number of trials.</p>
<p>In the following calculations, we will assume that we know the true probability of success within each trial. In practice, of course, this probability is often the unknown quantity that we are trying to estimate. Later sessions will revisit this example, under the more realistic scenario where this probability is unknown and we are using the sample of data to <em>make inferences</em> about the probability. The calculations in the current session will form important building blocks for those later sessions.</p>
<blockquote>
<div><p>Note on terminology:  <br> <br> - Do not confuse the word “trial” here with the idea of a clinical trial or randomised controlled trial. In our discussion of the binomial distribution, we simply mean a Bernoulli trial, which is a statistical experiment which results in a binary outcome. So the trial in question could be whether or not a baby is a male; whether or not someone is alive in 30 days time; whether or not someone experiences a side effect.  <br> <br> - Similarly, the word “success” can be confusing. We use the word success to denote having the event of interest. It does not imply that this is a good event. In fact, the event we are interested in, in health applications, is often a bad one. It might be diagnosis of cancer or death, in which case a success would refer to someone having cancer or dying. Conversely, if our study was looking at treatments for improving pregnancy rates, our event, and thus the definition of success, might be a couple becoming pregnant. So the word success, in this context, does not necessarily refer to a good event (although sometimes it does!).</p>
</div></blockquote>
<div class="section" id="example-of-a-binomial-distribution">
<h4>2.2.1 Example of a binomial distribution<a class="headerlink" href="#example-of-a-binomial-distribution" title="Permalink to this headline">¶</a></h4>
<p>A small study of 8 participants is being run. All 8 participants will be given an experimental drug. The aim of this study is to obtain data about how many people will experience a side-effect of the drug.</p>
<p>From previous data, the clinical researcher running the trial believes that the probability of the side-effect is 0.23.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be the number of people in the study (i.e. among the 8 participants) who experience a side-effect. Suppose we are happy to assume that <span class="math notranslate nohighlight">\(X\)</span> follows a binomial distribution. Then, using the formula for the probability distribution function that we derive below, we can calculate the probability that <span class="math notranslate nohighlight">\(P(X=x)\)</span> for all possible values <span class="math notranslate nohighlight">\(x=0,1,...,8\)</span>.</p>
<p>The code below (in R) does that calculation and displays a bar chart of the probability distribution function.</p>
<div class="cell tag_thebe-kernel:ir docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain the probability distribution function (for values x=0,1,...,8)</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">8</span><span class="p">)</span>
<span class="kc">pi</span> <span class="o">&lt;-</span> <span class="m">0.23</span>
<span class="n">px</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="kc">pi</span><span class="p">)</span>

<span class="c1"># Create bar chart of PDF</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">barplot</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="n">px</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02.c. Probability.Discrete_3_0.png" src="_images/02.c. Probability.Discrete_3_0.png" />
</div>
</div>
</div>
<div class="section" id="deriving-the-binomial-distribution">
<h4>Deriving the binomial distribution<a class="headerlink" href="#deriving-the-binomial-distribution" title="Permalink to this headline">¶</a></h4>
<p>Suppose we are conducting research on quadruplets (sets of four siblings born within the same pregnancy). In this session we will consider the number of boys among a set of quadruplets.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be the number of boys within a particular set of quadruplets. The sample space for <span class="math notranslate nohighlight">\(X\)</span> (the set of possible values <span class="math notranslate nohighlight">\(X\)</span> could take) is: <span class="math notranslate nohighlight">\(\{0, 1, 2, 3, 4 \}\)</span>. We will now derive the full probability distribution function for <span class="math notranslate nohighlight">\(X\)</span>.  In our calculations, we will assume that the proportion of males at birth is 0.51 and that the gender of each birth is an independent event.</p>
<p>Consider one set of quadruplets. We will start by calculating the probability of no boys i.e. the probability of four girls. By applying the multiplication rule (using the assumption of independence between sex of the children) we obtain:</p>
<div class="math notranslate nohighlight">
\[
P(X=0) = P(\mbox{four girls}) = P(GGGG) = 0.49^4,
\]</div>
<p>where <span class="math notranslate nohighlight">\(GGGG\)</span> is shorthand for the event that the first child is a girl, <em>and</em> the second is a girl, <em>and</em> the third is a girl, <em>and</em> the fourth is a girl,</p>
<p>Consider now the probability of one boy and three girls.  This may occur in one of four ways:
BGGG, GBGG, GGBG and GGGB, each of which has probability <span class="math notranslate nohighlight">\(0.49^3\times 0.51\)</span>.  Thus</p>
<div class="math notranslate nohighlight">
\[
P(X=1) = P(\mbox{one boy}) = 4 \times 0.49^3 \times 0.51.
\]</div>
<p>A family of 2 boys and 2 girls will arise in one of the following 6 ways: BBGG, BGBG, BGGB, GBBG, GBGB, GGBB each with a probability <span class="math notranslate nohighlight">\(0.49^2 \times 0.51^2\)</span> and a total probability of</p>
<div class="math notranslate nohighlight">
\[
P(X=2) =  P(\mbox{two boys}) = 6 \times 0.49^2 \times 0.51^2.
\]</div>
<p>With similar reasoning we have that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(X=3) =  P(\mbox{three boys}) = 4 \times 0.49 \times 0.51^3 \\
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
P(X=4) =  P(\mbox{four boys}) = 0.51^4.
\]</div>
<p>We now let <span class="math notranslate nohighlight">\(X\)</span> be the random variables which records the number of boys in a randomly selected family of size four. This random variable takes four possible values: 0, 1, 2, 3 or 4. Its probability distribution is given by the following table:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>x</p></th>
<th class="head"><p>P(X=<span class="math notranslate nohighlight">\(x\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>0</p></td>
<td><p>0.49<span class="math notranslate nohighlight">\(^4\)</span> = 0.0576</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>1</p></td>
<td><p>4 <span class="math notranslate nohighlight">\(\times\)</span> 0.49<span class="math notranslate nohighlight">\(^3\)</span> <span class="math notranslate nohighlight">\(\times\)</span> 0.51 = 0.2400</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>2</p></td>
<td><p>6 <span class="math notranslate nohighlight">\(\times\)</span> 0.49<span class="math notranslate nohighlight">\(^2\)</span> <span class="math notranslate nohighlight">\(\times\)</span> 0.51<span class="math notranslate nohighlight">\(^2\)</span> = 0.3747</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>3</p></td>
<td><p>4 <span class="math notranslate nohighlight">\(\times\)</span> 0.49 <span class="math notranslate nohighlight">\(\times\)</span> 0.51<span class="math notranslate nohighlight">\(^3\)</span> = 0.2600</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>4</p></td>
<td><p>0.51<span class="math notranslate nohighlight">\(^4\)</span>=0.0677</p></td>
</tr>
</tbody>
</table>
<p>More generally, consider a sequence of <span class="math notranslate nohighlight">\(n\)</span> independent observations/trials (in the example above it was four). Each observation results in a binary outcome, e.g. each trial is a success or a failure. In fact, a Binomial sequence is the sum of <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli trials (i.e. <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli variables). Let <span class="math notranslate nohighlight">\(\pi\)</span> denote the probability of an individual success (or the defined binary feature, e.g. boy vs. girl).</p>
<p>How do we obtain the probability distribution for the random variable <span class="math notranslate nohighlight">\(X\)</span> which records the number of successes in a sequence of <span class="math notranslate nohighlight">\(n\)</span> trials? The possible values for the random variable are <span class="math notranslate nohighlight">\(0,1,..,n-1,n\)</span>. We saw from the previous example that the probability of <span class="math notranslate nohighlight">\(x\)</span> successes and <span class="math notranslate nohighlight">\(n-x\)</span> failures is</p>
<div class="math notranslate nohighlight">
\[
P(X=x) = \pi^{x} (1-\pi)^{n-x} \times \mbox{number of ways of obtaining } x \mbox{ successes}.
\]</div>
<p>The multiplying factor on the right above is the binomial coefficient, i.e. the number of combinations of <span class="math notranslate nohighlight">\(x\)</span> objects chosen from <span class="math notranslate nohighlight">\(n\)</span>.  The number of ways <span class="math notranslate nohighlight">\(x\)</span> successes can be obtained from <span class="math notranslate nohighlight">\(n\)</span> observations is equal to <span class="math notranslate nohighlight">\(^n C_x\)</span> as we are not interested in the order of the successes, only the number of combinations in which such a number of successes could have occurred, and a “success” can be considered the same as “choosing” an object: we are “choosing” <span class="math notranslate nohighlight">\(x\)</span> successes and <span class="math notranslate nohighlight">\(n-x\)</span> failures out of a “bag” of <span class="math notranslate nohighlight">\(n\)</span> successes and failures.</p>
<p>So we have that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(X=x) = \begin{pmatrix}n \\ x \end{pmatrix} \pi^{x} (1-\pi)^{n-x}
\end{split}\]</div>
</div>
<div class="section" id="general-form-of-the-binomial-distribution">
<h4>General form of the binomial distribution<a class="headerlink" href="#general-form-of-the-binomial-distribution" title="Permalink to this headline">¶</a></h4>
<p>Suppose we have a sequence of <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli trials (i.e. <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli variables). Let <span class="math notranslate nohighlight">\(\pi\)</span> denote the probability of an individual “success”.  To write that <span class="math notranslate nohighlight">\(X\)</span> follows a binomial distribution with these features, we write <span class="math notranslate nohighlight">\(X\sim binomial(n,\pi)\)</span>, (where <span class="math notranslate nohighlight">\(\sim\)</span> means “follows”).</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(X=x) = \begin{pmatrix}n \\ x \end{pmatrix} \pi^{x} (1-\pi)^{n-x}, \mbox{ for } x=0,1,2,..,n.
\end{split}\]</div>
<p><em>Expectation and variance</em></p>
<p>The expected value of a binomial variable is <span class="math notranslate nohighlight">\(E(X) = n\pi\)</span></p>
<p>The variance of a binomial variable is <span class="math notranslate nohighlight">\(Var(X) = n\pi (1-\pi)\)</span></p>
</div>
<div class="section" id="applications-of-the-binomial-distribution">
<h4>Applications of the Binomial distribution<a class="headerlink" href="#applications-of-the-binomial-distribution" title="Permalink to this headline">¶</a></h4>
<p><em>Assumptions</em></p>
<p>In order for a variable to follow a binomial distribution, some “structural” things need to be true.</p>
<ol class="simple">
<li><p>There must be a fixed number of Bernoulli trials</p></li>
<li><p>Each trial must result in a binary outcome (success or failure)</p></li>
<li><p>The outcome we are interested in must be defined as the total number of successes.</p></li>
</ol>
<p>There are also two key <em>statistical assumptions</em>, implied by our derivation above:</p>
<ol class="simple">
<li><p>The Bernoulli trials must be <em>independent</em> of one another</p></li>
<li><p>The probability of success must be the same across Bernoulli trials</p></li>
</ol>
<p><em>Applications</em></p>
<p>Suppose we are interested in a particular disease within a large population of <span class="math notranslate nohighlight">\(N\)</span> individuals. If, in the population, <span class="math notranslate nohighlight">\(M\)</span> is the number of individuals with the disease of interest, then the probability of “success” (i.e. an individual having the disease) is <span class="math notranslate nohighlight">\(\pi = M/N\)</span>.</p>
<p>Suppose we take a random sample of <span class="math notranslate nohighlight">\(n\)</span> individuals from the large population. We will use <span class="math notranslate nohighlight">\(X\)</span> to be the random variable for the number of “successes” out of the <span class="math notranslate nohighlight">\(n\)</span> individuals. Then we might be happy to assume that <span class="math notranslate nohighlight">\(X\)</span> follows a Binomial distribution.</p>
<blockquote>
<div><p>Notes <br> <br>  - In order for the probability <span class="math notranslate nohighlight">\(\pi\)</span> to remain constant, if we took another sample of <span class="math notranslate nohighlight">\(n\)</span> we would have to “replace” the original <span class="math notranslate nohighlight">\(n\)</span> individuals, so there would be some small possibility of picking the same person twice. In practice, people are not sampled twice. But populations are usually so large that we can ignore this. <br> <br> - We also need to assume that individual outcomes (here, having the disease or not) are independent. There are many ways in which this could be violated. People within the same family have shared genetics, shared environments, etc. all of which might lead to outcomes that are more similar between family members than between individuals from different families.</p>
</div></blockquote>
</div>
</div>
<span id="document-02.d. Probability.Discrete"></span><div class="section" id="the-poisson-distribution">
<h3>2.3 The Poisson distribution<a class="headerlink" href="#the-poisson-distribution" title="Permalink to this headline">¶</a></h3>
<p>The Poisson distribution is used to model the <em>number of events</em> occurring in a fixed time interval.</p>
<p>Similarly to our approach with the Binomial distribution, in the following calculations we will assume that we know the true rate at which events occur. In practice, of course, this rate is often the unknown quantity that we are trying to estimate. Later sessions will revisit this example, under the more realistic scenario where this rate is unknown and we are using the sample of data to <em>make inferences</em> about the rate. The calculations in the current session will form important building blocks for those later sessions.</p>
<div class="section" id="example-of-the-poisson-distribution">
<h4>2.3.1 Example of the Poisson distribution<a class="headerlink" href="#example-of-the-poisson-distribution" title="Permalink to this headline">¶</a></h4>
<p>A clinical research is interested in modelling the number of asthma attacks that people with asthma experience in one year. Based on a large sample the researcher has estimated that the average number of attacks in a year is 2.5.</p>
<p>If we let <span class="math notranslate nohighlight">\(X\)</span> be the variable for the number of attacks a randomly selected person with asthma will experience in a year and we are happy to assume that <span class="math notranslate nohighlight">\(X\)</span> follows a Poisson distribution, then we can calculate <span class="math notranslate nohighlight">\(P(X=x)\)</span> for any given value of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>The code below (in R) does this calculation and plots the probability distribution function of the number of asthma attacks in a year.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain the probability distribution function (for values x=0,1,...,10)</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">10</span><span class="p">)</span>
<span class="n">lambda</span> <span class="o">&lt;-</span> <span class="m">2.5</span>
<span class="n">px</span> <span class="o">&lt;-</span> <span class="nf">dpois</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>

<span class="c1"># Create bar chart of PDF</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">barplot</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="n">px</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/02.d. Probability.Discrete_2_0.png" src="_images/02.d. Probability.Discrete_2_0.png" />
</div>
</div>
</div>
<div class="section" id="deriving-the-poisson-distribution">
<h4>2.3.2 Deriving the Poisson distribution<a class="headerlink" href="#deriving-the-poisson-distribution" title="Permalink to this headline">¶</a></h4>
<p>To give a heuristic derivation of the probability distribution function of the Poisson, we divide the total time <span class="math notranslate nohighlight">\(T\)</span> into a very large number of small intervals (see Figure below). As the number of intervals we divide <span class="math notranslate nohighlight">\(T\)</span> into increases, at most one event will occur in each interval, and so <span class="math notranslate nohighlight">\(X\)</span> will equal the number of intervals in which an event occurs. Since the occurrence of events in each interval are assumed independent of each other, <span class="math notranslate nohighlight">\(X \sim Bin(n,\pi)\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of intervals and <span class="math notranslate nohighlight">\(\pi\)</span> is the probability of an event occurring in any given interval.</p>
<div class="figure align-default" id="poisson">
<a class="reference internal image-reference" href="_images/poisson.png"><img alt="_images/poisson.png" src="_images/poisson.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Derivation of Poisson distribution by dividing time into small intervals</span><a class="headerlink" href="#poisson" title="Permalink to this image">¶</a></p>
</div>
<p>With a rate of <span class="math notranslate nohighlight">\(\lambda\)</span> events per unit of time, we expect <span class="math notranslate nohighlight">\(\mu=\lambda T\)</span> events in the whole period, and therefore we expect <span class="math notranslate nohighlight">\(\lambda T / n = \mu/n\)</span> events in each interval. Thus <span class="math notranslate nohighlight">\(\pi=\mu/n\)</span>. Therefore, using the probability distribution function for the binomial we have that</p>
<div class="math notranslate nohighlight">
\[
P(X=x) = {n \choose x} \pi^{x} (1-\pi)^{n-x} = {n \choose x} \left(\frac{\mu}{n}\right)^{x} \left(1-\frac{\mu}{n}\right)^{n-x}
\]</div>
<p>Then we have that</p>
<div class="math notranslate nohighlight">
\[
P(X=x) = {n \choose x} \left(\frac{\mu}{n}\right)^{x} \left(1-\frac{\mu}{n}\right)^{n-x} 
\]</div>
<div class="math notranslate nohighlight">
\[
= \frac{n!}{x! (n-x)!} \left(\frac{\mu}{n}\right)^{x} \left(1-\frac{\mu}{n}\right)^{n-x}
\]</div>
<div class="math notranslate nohighlight">
\[
= \frac{n!}{n^{x} (n-x)!}  \frac{\mu^{x}}{x!} \left(1-\frac{\mu}{n}\right)^{n-x}
\]</div>
<p>Now to simplify the first term, we note that:</p>
<div class="math notranslate nohighlight">
\[
\frac{n!}{n^{x} (n-x)!} = \frac{n(n-1)...(n-x+1)}{n^{x}} \rightarrow 1 \mbox{ as } n \rightarrow \infty,
\]</div>
<p>and to simplify the third term, we note that:</p>
<div class="math notranslate nohighlight">
\[
\left(1-\frac{\mu}{n}\right)^{n-x} \rightarrow \left(1-\frac{\mu}{n}\right)^{n} \rightarrow e^{-\mu}
\]</div>
<p>Replacing the first and third terms by these limits gives</p>
<div class="math notranslate nohighlight">
\[
P(X=x) \rightarrow \frac{\mu^{x}}{x!} e^{-\mu} \mbox{ as } n \rightarrow \infty.
\]</div>
</div>
<div class="section" id="general-form-of-the-poisson-distribution">
<h4>2.3.3 General form of the Poisson distribution<a class="headerlink" href="#general-form-of-the-poisson-distribution" title="Permalink to this headline">¶</a></h4>
<p>We can now define a Poisson distribution for the number of events occurring in a fixed interval <span class="math notranslate nohighlight">\(T\)</span> at a constant rate <span class="math notranslate nohighlight">\(\lambda\)</span> with parameter <span class="math notranslate nohighlight">\(\mu=\lambda T\)</span>, which we write as</p>
<div class="math notranslate nohighlight">
\[
X \sim \mbox{Poisson}(\mu=\lambda T)
\]</div>
<p>as the distribution which has probability distribution function</p>
<div class="math notranslate nohighlight">
\[
P(X=x) = \frac{\mu^{x}}{x!} e^{-\mu}, \ \mbox{ for } x=0,1,2,...
\]</div>
<p><em>Expectation and variance</em></p>
<p>The derivation of the expectation and variance of a Poisson random variable <span class="math notranslate nohighlight">\(X\)</span> with parameter <span class="math notranslate nohighlight">\(\mu\)</span> will be set as a practical question.</p>
</div>
<div class="section" id="applications-of-the-poisson-distribution">
<h4>2.3.4 Applications of the Poisson distribution<a class="headerlink" href="#applications-of-the-poisson-distribution" title="Permalink to this headline">¶</a></h4>
<p><em>Assumptions</em></p>
<p>The Poisson distribution is used to model the <em>number of events</em> occurring in a fixed time interval <span class="math notranslate nohighlight">\(T\)</span> when:</p>
<ul class="simple">
<li><p>events occur randomly in time,</p></li>
<li><p>they occur at a constant rate <span class="math notranslate nohighlight">\(\lambda\)</span> per unit time,</p></li>
<li><p>they occur independently of each other.</p></li>
</ul>
<p><em>Applications</em></p>
<p>A random variable <span class="math notranslate nohighlight">\(X\)</span> which follows a Poisson distribution can  take any non-negative integer value. Examples where the Poisson distribution might be appropriate include:</p>
<ul class="simple">
<li><p>Emissions from a radioactive source,</p></li>
<li><p>The number of deaths in a large cohort of people over a year,</p></li>
<li><p>The number of accidental deaths occurring in a city over a year.</p></li>
</ul>
</div>
<div class="section" id="approximating-the-binomial-by-a-poisson">
<h4>2.3.5 Approximating the binomial by a Poisson<a class="headerlink" href="#approximating-the-binomial-by-a-poisson" title="Permalink to this headline">¶</a></h4>
<p>When <span class="math notranslate nohighlight">\(n\)</span> is large relative to <span class="math notranslate nohighlight">\(\pi\)</span>, the binomial distribution can be approximated by a Poisson with a
mean <span class="math notranslate nohighlight">\(n\pi\)</span>. That this approximation is reasonable follows directly from our earlier heuristic derivation
of how a Poisson distribution arises as an approximation to a binomial distribution when the number
of trials tends to infinity.</p>
<p>There are many such approximations. Nowadays, we may not need to use them because we have enormous computing power at our disposal. In earlier times, in contrast, calculations could take a long time so any simplification that could be reasonably applied could provide meaningful extra calculation speed.</p>
</div>
</div>
<span id="document-02.e. Probability.Discrete"></span><div class="section" id="summary">
<h3>2.4 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>In this session, we have met a number of useful applications of discrete probability theory.</p>
<ul class="simple">
<li><p>Bayes’ Theorem can be used to quantify properties of screening and prognostic tests and forms the basis of Bayesian statistics. We will return to Bayes’ Theorem when we explore Bayesian inference.</p></li>
<li><p>The binomial distribution is used to model the number of successes out of a fixed number of trials. We will revisit the binomial distribution in the sessions about likelihood. Logistic regression, a very commonly used regression model, is based on an underlying Bernoulli distribution (remember that the binomial distribution can be obtained by summing multiple identical independent Bernoulli distributions). As such, our work with the binomial distribution is closely connected to the later logistic regression sessions.</p></li>
<li><p>The Poisson distribution is used to model the number of events occurring in a fixed time interval. It forms the basis for Poisson regression, which you will meet later in this module.</p></li>
</ul>
</div>
</div>
</div>
<span id="document-03.a. Continuous Probability Distributions"></span><div class="section" id="continuous-distributions">
<h2>3. Continuous distributions<a class="headerlink" href="#continuous-distributions" title="Permalink to this headline">¶</a></h2>
<p>This session is the second of two sessions covering basic probability. In this session, we extend the ideas from discrete distributions, from the previous session, to variables which have a <strong>continuous</strong> distribution.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>explain the concept of a continuous random variable</p></li>
<li><p>define several continuous probability distributions, and relationships between parameters, expectations and variance</p></li>
<li><p>understand the relationship between normally distributed data and standard scores</p></li>
<li><p>evaluate the appropriateness of assuming normality in data and other options</p></li>
<li><p>understand properties of joint distributions, such as the multivariate normal distribution</p></li>
</ul>
</div><p>The five sub-sections describe properties of continuous random variables, explore a number of useful continuous distributions, consider direct applications for the standard Normal (Gaussian) distribution, consider how to assess whether a variable follows a normal distribution and, finally, describe joint distributions and correlations.</p>
<div class="toctree-wrapper compound">
<span id="document-03.b. Continuous Probability Distributions"></span><div class="section" id="continuous-random-variables">
<h3>3.1 Continuous random variables<a class="headerlink" href="#continuous-random-variables" title="Permalink to this headline">¶</a></h3>
<p>We have previously seen several discrete probability distributions (including the binomial and the Poisson). We now extend random variables to those that are continuous. A continuous random variable is one that can take a value in continuous space; this may vary from <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(+\infty\)</span> (like the normal distribution) or have limits set on the lower (eg. the log-normal) or upper bound (eg. the uniform).</p>
<div class="section" id="the-probability-density-function">
<h4>3.1.1. The probability density function<a class="headerlink" href="#the-probability-density-function" title="Permalink to this headline">¶</a></h4>
<p>Previously we characterised the distribution of a variable by assigning a probability to each specific value. However, because athere are infinitely many values that could be taken by a continuous variable, paradoxically, the probability of a continuous random variable taking any specific value is zero. Therefore, we cannot use a probability distribution function to characterise the distribution of a continuous variable.</p>
<p>Instead, we turn to something called a <strong>probability density function</strong>. Instead of attaching a probability to each value the variable could take, the probability density function tells us the probability that a continuous variable lies within each possible interval (range of values). Specifically, the area under the curve (of the probability density function) between two limits tells us the probability that the continuous variable takes a value between those two limits.</p>
<p>Generally, a random variable <span class="math notranslate nohighlight">\(X\)</span> has density <span class="math notranslate nohighlight">\(f_X\)</span> where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x) \geq 0\)</span> for all of <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} f(x) \hspace{0.2cm} dx = 1.00\)</span></p></li>
</ul>
<p>which states that the “sum” of all probabilities of <span class="math notranslate nohighlight">\(f(x)\)</span> from the minimum to the maximum is equal to 1.</p>
<p>We can obtain various useful probabilities from this density function. We can calculate the probability that the variable takes a value within a given interval, the probability that it is below or above a given value. For example:</p>
<div class="math notranslate nohighlight">
\[Pr(X&gt;b) = \int_{b}^{max} f(X) \hspace{0.2cm} dx \]</div>
<p>Further information about continuous probability distributions are given in the <a class="reference external" href="https://lshtm-hds.github.io/Math-Refresher">Refresher</a>.</p>
</div>
</div>
<span id="document-03.c. Continuous Probability Distributions"></span><div class="section" id="useful-continuous-distributions">
<h3>3.2 Useful continuous distributions<a class="headerlink" href="#useful-continuous-distributions" title="Permalink to this headline">¶</a></h3>
<p>Below are several useful probability distributions for data science in health. Some of the information below is a repeat of the <strong><a class="reference external" href="https://statsfizz.github.io/Maths_Refresher/pr6_distributions.html">Maths refresher</a></strong>, but we include some practical applications of each distribution.</p>
<div class="section" id="the-normal-distribution">
<h4>3.2.1 The normal distribution<a class="headerlink" href="#the-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>The normal distribution is defined with the following probability density function:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi}\sigma}exp \Big[-\Big(\frac{1}{2}\Big)\Big(\frac{x-\mu}{\sigma}\Big)^2\Big]
\]</div>
<p>for values <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\((-\infty, +\infty)\)</span>. If we have a random variable <span class="math notranslate nohighlight">\(X\)</span> that is normally distributed we can specify this using <span class="math notranslate nohighlight">\(X {\sim} N(\mu, \sigma^2)\)</span>. The expected value is given by <span class="math notranslate nohighlight">\(E[X]=\mu\)</span> and the variance is given by <span class="math notranslate nohighlight">\(Var[X] = \sigma^2\)</span>.</p>
<p>A <strong>standard normal</strong> distribution has a mean of 0 and a variance of 1. A standard normal random variable is usually represented by <span class="math notranslate nohighlight">\(Z {\sim} N(0,1)\)</span> and is sometimes called the <em>Z-score</em>.</p>
<p>So much of statistics relies on the normal distribution, so it is an important distribution to be familiar with. We will see that the normal distribution has an important role to play in statistical inference. It is also sometimes a good distribution for directly modelling continuous variables, for example blood pressure.</p>
</div>
<div class="section" id="the-log-normal-distribution">
<h4>3.2.2 The log-normal distribution<a class="headerlink" href="#the-log-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>The log-normal distribution is essentialy a transformed version of the normal distribution, and has its own probability density function;</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{x \sigma \sqrt{2\pi}}exp\Big(-\frac{(ln(x)-\mu)^2}{2\sigma^2}\Big)
\]</div>
<p>for values <span class="math notranslate nohighlight">\(x\)</span> in  <span class="math notranslate nohighlight">\([0,+\infty)\)</span>. If a random variable <span class="math notranslate nohighlight">\(X\)</span> is log-normally distributed, <span class="math notranslate nohighlight">\(Y=ln(X)\)</span> has a normal distribution, and if <span class="math notranslate nohighlight">\(Y\)</span> is a normal distribution then <span class="math notranslate nohighlight">\(X=exp(Y)\)</span> has a log-normal distribution. These simple transformations mean that calculations using transformed data is the standard approach. The parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> refer to the mean and standard deviation on the <em>normal scale</em>. Consequently, the median of a log-normally distributed sample is <span class="math notranslate nohighlight">\(exp(\mu)\)</span>.</p>
<p>Many biological datasets are log-normally distributed, for example most measurements (height, weight, speed) will be above 0, and will often be right-skewed. A good approach to take with these sorts of data is to log the data, and work on the <em>log scale</em>. Any inference should be converted back to the <em>natural scale</em>. Sometimes measurements are sufficiently greater than 0 that they become more centered. In this case, it may not be necessary to assume that are log-normal, and assuming normality may be acceptible.</p>
</div>
<div class="section" id="the-chi-2-distribution">
<h4>3.2.3 The <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution<a class="headerlink" href="#the-chi-2-distribution" title="Permalink to this headline">¶</a></h4>
<p>The <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution is here because we will use the properties of this distribution later in hypothesis testing. Its origins come from a random sample of the <em>standard normal</em>, where the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution is the distribution of the sum of squared standard normals. The degrees of freedom come from the number of standard normal random variables being summed. It is not necessary to know the parameters or estimates of the <span class="math notranslate nohighlight">\(\chi^2\)</span> parameters. A variable which follows the chi-squared distribution can only take positive values (i.e. greater than zero).</p>
</div>
<div class="section" id="the-t-distribution">
<h4>3.2.4 The t-distribution<a class="headerlink" href="#the-t-distribution" title="Permalink to this headline">¶</a></h4>
<p>Student’s t-distribution arises as the ratio of the sample mean to its standard error. The t-distribution has a complex density function which we shall not state here.</p>
<p>For now we note that the t-distribution has an additional parameter of sorts, known as the degrees of freedom (d.f.). The density function is similar to that of the standard normal, but the t-distribution has heavier tails. If <span class="math notranslate nohighlight">\(X\)</span> follows a t-distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom, we write</p>
<div class="math notranslate nohighlight">
\[X \sim t_\nu\]</div>
<p>The expectation and variance of a variable <span class="math notranslate nohighlight">\(X\)</span> which follows a t-distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom are given by:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[X] = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Var[X] = \frac{\nu}{\nu-2}\)</span> if <span class="math notranslate nohighlight">\(\nu&gt;2\)</span>; <span class="math notranslate nohighlight">\(\infty\)</span> for <span class="math notranslate nohighlight">\(1&lt;\nu&lt;2\)</span>; undefined otherwise</p></li>
</ul>
<p>As the number of degrees of freedom increases the t-distribution gets closer and closer to the standard normal distribution.</p>
</div>
<div class="section" id="the-f-distribution">
<h4>3.2.5 The F distribution<a class="headerlink" href="#the-f-distribution" title="Permalink to this headline">¶</a></h4>
<p>The F distribution doesn’t have a simple mathematical formula, but is used extensively to compare equality of variances of two normal populations (<em>think anova</em>), and is used in linear regression.</p>
<p>For two normal populations with variances <span class="math notranslate nohighlight">\(\sigma_1^2\)</span> and <span class="math notranslate nohighlight">\(\sigma_2^2\)</span>, the two random samples of size <span class="math notranslate nohighlight">\(n_1\)</span> and <span class="math notranslate nohighlight">\(n_2\)</span> with corresponding sample variance(s) <span class="math notranslate nohighlight">\(s_1^2\)</span> and <span class="math notranslate nohighlight">\(s_2^2\)</span> has the variable</p>
<div class="math notranslate nohighlight">
\[F = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}\]</div>
<p>with <span class="math notranslate nohighlight">\(n_1-1\)</span> and <span class="math notranslate nohighlight">\(n_2-1\)</span> degrees of freedom.</p>
</div>
<div class="section" id="the-exponential-distribution">
<h4>3.2.6 The exponential distribution<a class="headerlink" href="#the-exponential-distribution" title="Permalink to this headline">¶</a></h4>
<p>The exponential distribution is defined with the probability density function:</p>
<div class="math notranslate nohighlight">
\[f(x)=\lambda e^{-\lambda x}\]</div>
<p>with parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, which is usually described as the rate. The limits of the distribution are <span class="math notranslate nohighlight">\([0,\infty)\)</span>, which means values of <span class="math notranslate nohighlight">\(x\)</span> are always greater than 0 (and not including it).</p>
<p>The expected value is given by <span class="math notranslate nohighlight">\(E[X]=\frac{1}{\lambda}\)</span> and variance <span class="math notranslate nohighlight">\(Var[X]=\frac{1}{\lambda^2}\)</span>.</p>
<p>The exponential distribution is really useful in statistics because its distribution nicely describes <em>the time to which something occurs</em>, if the event happens at a roughly constant rate in time. Health related examples include injuries, births and deaths (although in reality not all occur at a constant rate). The exponential distribution is important in methods such as <em>survival analysis</em>.</p>
</div>
<div class="section" id="the-uniform-distribution">
<h4>3.2.7 The uniform distribution<a class="headerlink" href="#the-uniform-distribution" title="Permalink to this headline">¶</a></h4>
<p>The uniform distribution is in some ways the simplest to conceptualise. A random variable that is uniformly distributed can have any value between the parameters <span class="math notranslate nohighlight">\(a\)</span> (min) and <span class="math notranslate nohighlight">\(b\)</span> (max) with equal probability;</p>
<div class="math notranslate nohighlight">
\[f(x)= \frac{1}{b-a}\]</div>
<p>Outside of these limits, the probability density is 0. The expected value is <span class="math notranslate nohighlight">\(E[X] = \frac{(a+b)}{2}\)</span> and variance <span class="math notranslate nohighlight">\(Var[X] = \frac{(b-a)^2}{12}\)</span>.</p>
<p>The uniform distribution is very commonly used when randomly allocating outcomes. An example in statistical modelling includes stochastic infectious disease modelling; here several different events (transmission, death) may have a corresponding probability and one event needs to be selected from the two options. A uniform distribution (where the maximum is the total probability of all events) is used to select</p>
</div>
</div>
<span id="document-03.d. Continuous Probability Distributions"></span><div class="section" id="uses-of-the-standard-normal-distribution">
<h3>3.3 Uses of the standard Normal distribution<a class="headerlink" href="#uses-of-the-standard-normal-distribution" title="Permalink to this headline">¶</a></h3>
<p>Suppose we wanted to answer the following question:</p>
<blockquote>
<div><p>What is the probability of having a ‘healthy’ weight?</p>
</div></blockquote>
<p>A healthy weight is often is often measured using the Body Mass Index (BMI - although see <a class="reference external" href="https://www.health.harvard.edu/blog/how-useful-is-the-body-mass-index-bmi-201603309339">here</a> and <a class="reference external" href="https://www.bbc.co.uk/news/health-43895508">here</a> for a discussion on why this may be too simplistic a measure). An individual’s BMI can be calculated using their height and weight, using the formula BMI <span class="math notranslate nohighlight">\(= \frac{mass(kg)}{height(m)^2}\)</span>. Then people can be classified as:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Classification</p></th>
<th class="text-align:left head"><p>BMI</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Underweight</p></td>
<td class="text-align:left"><p>&lt;18.5</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Normal</p></td>
<td class="text-align:left"><p>18.5-24.9</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Overweight</p></td>
<td class="text-align:left"><p>25-29.9</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Obese</p></td>
<td class="text-align:left"><p>30-39.9</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Extremely obese</p></td>
<td class="text-align:left"><p>&gt;40</p></td>
</tr>
</tbody>
</table>
<p>To address our question, we will use data taken from a study undertaken among a group of 76 cleaners, that investigated whether telling the cleaners they had an active lifestyle influenced their BMI. We will assume that values of BMI approximately follow a normal distribution. We do not know the true values of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> so we will replace these with the sample mean and standard deviation. This gives us values of <span class="math notranslate nohighlight">\(\mu=\)</span>26.5 and <span class="math notranslate nohighlight">\(\sigma^2=\)</span> 18.1, as demonstrated in the snippet of code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># BMI dataset</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/BMI/MindsetMatters.csv&quot;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">dat</span><span class="p">)</span>
<span class="c1">#remove observations with no BMI data</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="n">dat</span><span class="p">[</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">),]</span>
<span class="c1">#estimate mu and sigma</span>
<span class="n">mu</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;value of mu is &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
<span class="n">sig</span> <span class="o">&lt;-</span> <span class="nf">sd</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;value of sigma is &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 6 × 14</caption>
<thead>
	<tr><th></th><th scope=col>Cond</th><th scope=col>Age</th><th scope=col>Wt</th><th scope=col>Wt2</th><th scope=col>BMI</th><th scope=col>BMI2</th><th scope=col>Fat</th><th scope=col>Fat2</th><th scope=col>WHR</th><th scope=col>WHR2</th><th scope=col>Syst</th><th scope=col>Syst2</th><th scope=col>Diast</th><th scope=col>Diast2</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>0</td><td>43</td><td>137</td><td>137.4</td><td>25.1</td><td>25.1</td><td>31.9</td><td>32.8</td><td>0.79</td><td>0.79</td><td>124</td><td>118</td><td>70</td><td>73</td></tr>
	<tr><th scope=row>2</th><td>0</td><td>42</td><td>150</td><td>147.0</td><td>29.3</td><td>28.7</td><td>35.5</td><td>  NA</td><td>0.81</td><td>0.81</td><td>119</td><td>112</td><td>80</td><td>68</td></tr>
	<tr><th scope=row>3</th><td>0</td><td>41</td><td>124</td><td>124.8</td><td>26.9</td><td>27.0</td><td>35.1</td><td>  NA</td><td>0.84</td><td>0.84</td><td>108</td><td>107</td><td>59</td><td>65</td></tr>
	<tr><th scope=row>4</th><td>0</td><td>40</td><td>173</td><td>171.4</td><td>32.8</td><td>32.4</td><td>41.9</td><td>42.4</td><td>1.00</td><td>1.00</td><td>116</td><td>126</td><td>71</td><td>79</td></tr>
	<tr><th scope=row>5</th><td>0</td><td>33</td><td>163</td><td>160.2</td><td>37.9</td><td>37.2</td><td>41.7</td><td>  NA</td><td>0.86</td><td>0.84</td><td>113</td><td>114</td><td>73</td><td>78</td></tr>
	<tr><th scope=row>6</th><td>0</td><td>24</td><td> 90</td><td> 91.8</td><td>16.5</td><td>16.8</td><td>  NA</td><td>  NA</td><td>0.73</td><td>0.73</td><td> NA</td><td> NA</td><td>78</td><td>76</td></tr>
</tbody>
</table>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;value of mu is 26.46&quot;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;value of sigma is 4.25&quot;
</pre></div>
</div>
</div>
</div>
<p>So what is the probability a randomly selected person in this sample has a normal BMI?</p>
<div class="section" id="approach-1-manual-calculation">
<h4>Approach 1: Manual calculation<a class="headerlink" href="#approach-1-manual-calculation" title="Permalink to this headline">¶</a></h4>
<p>One option is to make use of pre-calculated probabilities of the standard normal distribution. If we write <span class="math notranslate nohighlight">\(X\)</span> to represent the value of a person’s BMI, then we are assuming that</p>
<div class="math notranslate nohighlight">
\[
X \sim N(\mu=26.5, \sigma^2=18.1)
\]</div>
<p>To make use of the pre-calculated probabilities for the standard normal distribution, we must first transform our normally distributed variable to have a standard normal distribution. We know that the transformed variable <span class="math notranslate nohighlight">\(Z\)</span> (the <em>Z score</em>) has a standard normal distribution, where</p>
<div class="math notranslate nohighlight">
\[
Z = \frac{X - \mu}{\sigma}
\]</div>
<p>Given values for <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> we can go from the <em>X scale</em> to the <em>Z scale</em> and <em>vice versa</em>. The important point about describing a distribution on the Z scale is that this opens the ability to calculate specific probabilities. So back to answering the question…</p>
<p>From the table above we can see that a normal weight is classified as a BMI between 18.5 and 24.9, and we want to know what the probability is that a randomly selected person falls between these limits. We write this as;</p>
<div class="math notranslate nohighlight">
\[
P(18.5 &lt; X &lt; 24.9) 
\]</div>
<p>On the Z-scale, this is equivalent to saying that:</p>
<div class="math notranslate nohighlight">
\[
P(-1.87 &lt; Z &lt; -0.37) = P(Z &lt; -0.37) - P(Z &lt; -1.87)
\]</div>
<p>Tables exist containing a range of pre-calculated probabilities that a variable following a standard normal distribution takes a value of less than <span class="math notranslate nohighlight">\(k\)</span>, for a range of possible values of <span class="math notranslate nohighlight">\(k\)</span>. These are often called <em>z-tables</em> (found <a class="reference external" href="http://www.z-table.com/">online</a> or at the back of most stats books). From these tables, we can look up the corresponding probability for each z-score, giving:</p>
<div class="math notranslate nohighlight">
\[
0.3557 - 0.0307 = 0.325
\]</div>
</div>
<div class="section" id="approach-2-using-r-to-do-the-same-calculation">
<h4>Approach 2: Using R to do the same calculation<a class="headerlink" href="#approach-2-using-r-to-do-the-same-calculation" title="Permalink to this headline">¶</a></h4>
<p>Using this approach, R is ultimately using the same pre-calculated probability tables. However, it is considerably quicker and easier to ask R to look up the values rather than finding them in tables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># a) if we were to use Z tables within R (to illustrate the point)</span>

<span class="n">z_min</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="m">18.5</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="n">sig</span>
<span class="n">z_max</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="m">24.9</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="n">sig</span>

<span class="c1"># note when using pnorm we don&#39;t need to specify mu and sigma as the </span>
<span class="c1"># function assumes mu=0 and sigma=1 unless specified.</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;z_max is &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">z_max</span><span class="p">,</span><span class="m">2</span><span class="p">),</span><span class="s">&quot; and z_min is &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Probability of having a healthy BMI is (z-score) &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">pnorm</span><span class="p">(</span><span class="n">z_max</span><span class="p">)</span><span class="o">-</span><span class="nf">pnorm</span><span class="p">(</span><span class="n">z_min</span><span class="p">),</span><span class="m">3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;z_max is -0.37 and z_min is -1.87&quot;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Probability of having a healthy BMI is (z-score) 0.326&quot;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="approach-3-using-r-to-do-the-calculation-on-the-untransformed-scale">
<h4>Approach 3: Using R to do the  calculation on the untransformed scale<a class="headerlink" href="#approach-3-using-r-to-do-the-calculation-on-the-untransformed-scale" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># b) if we were to directly estimate</span>
 
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Probability of having a healthy BMI is (direct) &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">pnorm</span><span class="p">(</span><span class="m">24.9</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sig</span><span class="p">)</span><span class="o">-</span><span class="nf">pnorm</span><span class="p">(</span><span class="m">18.5</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sig</span><span class="p">),</span><span class="m">3</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Probability of having a healthy BMI is (direct) 0.326&quot;
</pre></div>
</div>
</div>
</div>
<p>Calculating directly gives the same result as using a z-score in R, and this returns the same information as using z-tables.</p>
<p>In answer to our question, we estimate that the probability of having a ‘healthy’ weight is 32.6%. We can compare this to the observed proportion of our sample of data with a ‘healthy’ BMI.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># c) provide a sanity check against the data</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="nf">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">BMI</span><span class="p">))</span> <span class="o">+</span> <span class="nf">geom_histogram</span><span class="p">(</span><span class="n">bins</span> <span class="o">=</span> <span class="m">30</span><span class="p">,</span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;steelblue&quot;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;grey80&quot;</span><span class="p">)</span> <span class="o">+</span>
    <span class="nf">geom_vline</span><span class="p">(</span><span class="n">xintercept</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">18.5</span><span class="p">,</span><span class="m">24.9</span><span class="p">))</span>
<span class="c1">#hist(dat$BMI,col=&quot;steelblue&quot;)</span>
<span class="c1">#abline(v=c(18.5,24.9),lty=2)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;Within the data a healthy BMI is seen &quot;</span><span class="p">,</span>
             <span class="nf">round</span><span class="p">(</span><span class="m">100</span><span class="o">*</span><span class="p">((</span><span class="nf">sum</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="o">&lt;</span><span class="m">24.9</span><span class="p">)</span><span class="o">-</span><span class="nf">sum</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="o">&lt;</span><span class="m">18.5</span><span class="p">))</span><span class="o">/</span><span class="nf">length</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">)),</span><span class="m">1</span><span class="p">),</span><span class="s">&quot;%&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Within the data a healthy BMI is seen 35.1%&quot;
</pre></div>
</div>
<img alt="_images/03.d. Continuous Probability Distributions_8_1.png" src="_images/03.d. Continuous Probability Distributions_8_1.png" />
</div>
</div>
<p>So we can see that the sample estimate (35.1%) is roughly similar to the population estimate of 32.6%.</p>
</div>
</div>
<span id="document-03.e. Continuous Probability Distributions"></span><div class="section" id="are-the-data-normally-distributed">
<h3>3.4 Are the data normally distributed?<a class="headerlink" href="#are-the-data-normally-distributed" title="Permalink to this headline">¶</a></h3>
<div class="section" id="data-and-their-relationship-with-statistical-distributions">
<h4>3.4.1 Data and their relationship with statistical distributions<a class="headerlink" href="#data-and-their-relationship-with-statistical-distributions" title="Permalink to this headline">¶</a></h4>
<p>We often have data on a particular characteristic and want to make general statements about it, such as: the probability of it being greater than or less than something, provide a range in which “most” observations will lie, what is the central value (e.g. mean/median), etc.</p>
<blockquote>
<div><p>However,
<br><br> - we rarely <em>know</em> the true distribution that a variable follows
<br> - a distribution will not quite fit the data but will form a sufficiently good approximation to address the questions above with sufficient accuracy.</p>
</div></blockquote>
<p>So we often want to find a distribution which fits our data well enough. How do we make a decision? Some of this comes with experience, but there are some useful steps to go through when confronted with data (this is covered in more detail in the lecture). Think back to the <strong>PPDAC</strong> cycle in the first session;</p>
<ul class="simple">
<li><p>plot your data. What does the data look like? Consider the lower and upper bounds, the most common number, and evidence of symmetry</p></li>
<li><p>summarise your data. Report the minimum, maximum, mean and mode. This should aid with thinking about the criteria of specific distributions</p></li>
<li><p>depending upon the application and what the data looks like, you may want to consider using the empirical distribution function rather than assumption a specific form. However, this gives you fewer options for inference</p></li>
</ul>
</div>
<div class="section" id="id1">
<h4>3.4.2 Are the data normally distributed?<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Many analyses and tests of data start with the assumption that the data are normally distributed. A simple example would be using a t-test to check whether the mean of 2 groups are different, more complex examples would include linear regression analysis. If the outcome being analysed is a qualitive outcome, or successes and failures, it should be obvious that the data aren’t normally distributed. But what if the data are continuous or count values, and they look like they are centered, but have some skewness? Is it safe to proceed as if they are normal?</p>
<p>The first step, as always, is to plot the data to see what they look like. A histogram, as above, or density plot is a good step forward. Additionally, a <em>quantile-quantile</em> plot calculates the correlation between a sample and the equivalent normal distribution with the same mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. If a variable follows a normal distribution, the quantile-quantile plot will follow the diagonal line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># BMI dataset</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/BMI/MindsetMatters.csv&quot;</span><span class="p">)</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="n">dat</span><span class="p">[</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">),]</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">BMI</span><span class="p">))</span> <span class="o">+</span> <span class="nf">stat_qq</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/03.e. Continuous Probability Distributions_2_0.png" src="_images/03.e. Continuous Probability Distributions_2_0.png" />
</div>
</div>
<p>From the figure you can see that the theoretical quantiles follow the diagonals reasonably well, and especially at the extremes do not move away much from the diagonal. A plot like this would be enough to show that the data approximately follows normally distribution. Looking at plots such as this to assess normality is a <em>judgement</em> which you will build up during this module.</p>
<p>To formally test for normality we can use the Shapiro-Wilk test, described briefly below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">shapiro.test</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>	Shapiro-Wilk normality test

data:  dat$BMI
W = 0.9692, p-value = 0.06756
</pre></div>
</div>
</div>
</div>
<p>Although we haven’t yet covered hypothesis testing (see <em>Session 8</em>), this is testing the null hypothesis that the data follow a normal distribution. In this case, the test returns a p-value of 0.067. This p-value suggests some, but not strong, evidence against normality of the data.</p>
</div>
<div class="section" id="approaches-to-non-normally-distributed-data">
<h4>3.4.3 Approaches to non-normally distributed data<a class="headerlink" href="#approaches-to-non-normally-distributed-data" title="Permalink to this headline">¶</a></h4>
<p>A really useful approach to dealing with non-normally distributed data is transformations. The most often used approach is to apply a log-transformation, either on the <em>natural</em> (<span class="math notranslate nohighlight">\(Y = log_e(X)\)</span>) or <em>log10</em> (<span class="math notranslate nohighlight">\(Y = log_{10}(X)\)</span>) scale. The transformed data may behave more like normally distributed data.</p>
<p>An example is given below for weights of 1174 babies. First we will look at the distribution of (untransformed) birth weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="c1"># mother-baby dataset</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/MotherBaby/baby.csv&quot;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">dat</span><span class="p">)</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="n">dat</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">,]</span> <span class="c1"># we will use just the first 100 observations</span>

<span class="c1"># plot the data on maternal age</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">Maternal.Age</span><span class="p">))</span> <span class="o">+</span> <span class="nf">geom_histogram</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">,</span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;steelblue&quot;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;grey80&quot;</span><span class="p">)</span>
<span class="c1"># plot a quantile plot of this log-normally distributed data</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">Maternal.Age</span><span class="p">))</span> <span class="o">+</span> <span class="nf">stat_qq</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 6 × 6</caption>
<thead>
	<tr><th></th><th scope=col>Birth.Weight</th><th scope=col>Gestational.Days</th><th scope=col>Maternal.Age</th><th scope=col>Maternal.Height</th><th scope=col>Maternal.Pregnancy.Weight</th><th scope=col>Maternal.Smoker</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>120</td><td>284</td><td>27</td><td>62</td><td>100</td><td>False</td></tr>
	<tr><th scope=row>2</th><td>113</td><td>282</td><td>33</td><td>64</td><td>135</td><td>False</td></tr>
	<tr><th scope=row>3</th><td>128</td><td>279</td><td>28</td><td>64</td><td>115</td><td>True </td></tr>
	<tr><th scope=row>4</th><td>108</td><td>282</td><td>23</td><td>67</td><td>125</td><td>True </td></tr>
	<tr><th scope=row>5</th><td>136</td><td>286</td><td>25</td><td>62</td><td> 93</td><td>False</td></tr>
	<tr><th scope=row>6</th><td>138</td><td>244</td><td>33</td><td>62</td><td>178</td><td>False</td></tr>
</tbody>
</table>
</div><img alt="_images/03.e. Continuous Probability Distributions_7_1.png" src="_images/03.e. Continuous Probability Distributions_7_1.png" />
<img alt="_images/03.e. Continuous Probability Distributions_7_2.png" src="_images/03.e. Continuous Probability Distributions_7_2.png" />
</div>
</div>
<p>We can see clearly that maternal age is right skewed. This is a classic log-normal distribution. The quantile plot is not straight along the diagonal but forms an <em>s-shape</em>. This confirms that the data does not conform to a normally distribution.</p>
<p>A sensible next step would be to log-transform the data using the natural logarithm. The distribution of the transformed birth weights is shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="c1"># plot the data on maternal age</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">Maternal.Age</span><span class="p">))</span> <span class="o">+</span> 
    <span class="nf">geom_histogram</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="m">30</span><span class="p">,</span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;steelblue&quot;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;grey80&quot;</span><span class="p">)</span> <span class="o">+</span>
    <span class="nf">scale_x_continuous</span><span class="p">(</span><span class="n">trans</span> <span class="o">=</span> <span class="s">&quot;log&quot;</span><span class="p">)</span>

<span class="c1"># but note that any analysis should be carried out on the transformed variable</span>
<span class="n">y</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">age_log</span><span class="o">=</span><span class="nf">log</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">Maternal.Age</span><span class="p">))</span>
<span class="c1"># and here we should check whether this is normally distributed using a qqplot</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">age_log</span><span class="p">))</span> <span class="o">+</span> <span class="nf">stat_qq</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/03.e. Continuous Probability Distributions_9_0.png" src="_images/03.e. Continuous Probability Distributions_9_0.png" />
<img alt="_images/03.e. Continuous Probability Distributions_9_1.png" src="_images/03.e. Continuous Probability Distributions_9_1.png" />
</div>
</div>
<p>The log-transformed data now looks more symmetrical in the histogram. And the quantile plot is much less <em>s-shaped</em>. While it’s not perfectly straight, it’s probably <em>good enough</em> for further analysis which relies on the assumption of normality.</p>
</div>
</div>
<span id="document-03.f. Continuous Probability Distributions"></span><div class="section" id="joint-distributions-and-correlations">
<h3>3.5 Joint distributions and correlations<a class="headerlink" href="#joint-distributions-and-correlations" title="Permalink to this headline">¶</a></h3>
<p>We are often interested not in the distribution of a single variable but in the relationship between two or more variables. This requires us to understand the concepts of <strong>joint distributions</strong> and <strong>correlation</strong>.</p>
<p>Returning to the BMI dataset, a high BMI is indicative of being overweight and this is likely to mean that an individual may have a high percentage of body fat. Typically, those individuals with high BMI may also be at risk of health conditions such as heart disease, which may be indicated by high blood pressure.</p>
<p>If we wish to address questions relating to two or more variables, we need to understand their joint distribution.</p>
<div class="section" id="joint-distributions">
<h4>3.5.1. Joint distributions<a class="headerlink" href="#joint-distributions" title="Permalink to this headline">¶</a></h4>
<p>If we have two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, the cumulative joint distribution function (CDF) is,</p>
<div class="math notranslate nohighlight">
\[F(x,y) = P(X \leq x,Y \leq y)\]</div>
<p>regardless of whether <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are continuous or discrete. For continuous random variables the joint density function will be <span class="math notranslate nohighlight">\(f(x,y)\)</span> and will be non-negative and</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y)\: dy\: dx = 1.\]</div>
</div>
<div class="section" id="marginal-distributions">
<h4>3.5.2 Marginal distributions<a class="headerlink" href="#marginal-distributions" title="Permalink to this headline">¶</a></h4>
<p>We might sometimes want to think about the marginal density of, say, <span class="math notranslate nohighlight">\(X\)</span>. This means we want to know the probability of <span class="math notranslate nohighlight">\(X\)</span> irrespective of <span class="math notranslate nohighlight">\(Y\)</span>, and consequently we will need to integrate over all possible values of <span class="math notranslate nohighlight">\(Y\)</span>. The marginal cdf of <span class="math notranslate nohighlight">\(X\)</span>, or <span class="math notranslate nohighlight">\(F_X\)</span> is</p>
<div class="math notranslate nohighlight">
\[F_X (x) = P(X \leq x)\]</div>
<div class="math notranslate nohighlight">
\[ = \lim_{y \rightarrow \infty} F(x,y)\]</div>
<div class="math notranslate nohighlight">
\[ = \int_{-\infty}^{x} \int_{-\infty}^{\infty} f(u,y)\: dy\: du\]</div>
<p>From this, it follows that the density function of <span class="math notranslate nohighlight">\(X\)</span> alone, known as the <strong>marginal density</strong> of <span class="math notranslate nohighlight">\(X\)</span>, is</p>
<div class="math notranslate nohighlight">
\[f_x (x) = F_{X}'(x) = \int_{-\infty}^{\infty} f(x,y)\: dy\]</div>
<p>Note that this is different to assuming that <span class="math notranslate nohighlight">\(X\)</span> is independent of <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>So what does this mean in practical terms? Returning to the BMI data we can report that the average BMI (<span class="math notranslate nohighlight">\(\mu_X\)</span>) is 26.46 and the average body fat percentage (<span class="math notranslate nohighlight">\(\mu_Y\)</span>) is 35.31. If BMI and body fat were independent variables knowing BMI would tell us nothing about body fat and <em>vice versa</em>. But plotting the data (and some common sense) tells us that this is not the case; if we know one we can say quite a lot about the other. We could explore the correlation between the data (more about this later), but we can also describe these variables together using a joint distribution. By defining them using a joint distribution we are saying nothing about <em>cause and effect</em>, just that they are dependent variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="c1"># BMI dataset</span>

<span class="n">dat</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/BMI/MindsetMatters.csv&quot;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">dat</span><span class="p">)</span>
<span class="c1">#remove observations with no BMI data</span>
<span class="n">dat</span> <span class="o">&lt;-</span> <span class="n">dat</span><span class="p">[</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">),]</span>
<span class="c1"># scatter plot of BMI and body fat</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">BMI</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">Fat</span><span class="p">))</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span>

<span class="c1"># report the mean of each variable</span>
<span class="c1"># note that some values of Y are missing...we need to add na.rm otherwise the estimate will be NA</span>
<span class="n">mux</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">BMI</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;value of mu_x is &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">mux</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
<span class="n">muy</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">Fat</span><span class="p">,</span><span class="n">na.rm</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;value of mu_y is &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="n">muy</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 6 × 14</caption>
<thead>
	<tr><th></th><th scope=col>Cond</th><th scope=col>Age</th><th scope=col>Wt</th><th scope=col>Wt2</th><th scope=col>BMI</th><th scope=col>BMI2</th><th scope=col>Fat</th><th scope=col>Fat2</th><th scope=col>WHR</th><th scope=col>WHR2</th><th scope=col>Syst</th><th scope=col>Syst2</th><th scope=col>Diast</th><th scope=col>Diast2</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>0</td><td>43</td><td>137</td><td>137.4</td><td>25.1</td><td>25.1</td><td>31.9</td><td>32.8</td><td>0.79</td><td>0.79</td><td>124</td><td>118</td><td>70</td><td>73</td></tr>
	<tr><th scope=row>2</th><td>0</td><td>42</td><td>150</td><td>147.0</td><td>29.3</td><td>28.7</td><td>35.5</td><td>  NA</td><td>0.81</td><td>0.81</td><td>119</td><td>112</td><td>80</td><td>68</td></tr>
	<tr><th scope=row>3</th><td>0</td><td>41</td><td>124</td><td>124.8</td><td>26.9</td><td>27.0</td><td>35.1</td><td>  NA</td><td>0.84</td><td>0.84</td><td>108</td><td>107</td><td>59</td><td>65</td></tr>
	<tr><th scope=row>4</th><td>0</td><td>40</td><td>173</td><td>171.4</td><td>32.8</td><td>32.4</td><td>41.9</td><td>42.4</td><td>1.00</td><td>1.00</td><td>116</td><td>126</td><td>71</td><td>79</td></tr>
	<tr><th scope=row>5</th><td>0</td><td>33</td><td>163</td><td>160.2</td><td>37.9</td><td>37.2</td><td>41.7</td><td>  NA</td><td>0.86</td><td>0.84</td><td>113</td><td>114</td><td>73</td><td>78</td></tr>
	<tr><th scope=row>6</th><td>0</td><td>24</td><td> 90</td><td> 91.8</td><td>16.5</td><td>16.8</td><td>  NA</td><td>  NA</td><td>0.73</td><td>0.73</td><td> NA</td><td> NA</td><td>78</td><td>76</td></tr>
</tbody>
</table>
</div><div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="n">Error</span> <span class="ow">in</span> <span class="n">ggplot</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">BMI</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">Fat</span><span class="p">)):</span> <span class="n">could</span> <span class="ow">not</span> <span class="n">find</span> <span class="n">function</span> <span class="s2">&quot;ggplot&quot;</span>
<span class="ne">Traceback</span>:
</pre></div>
</div>
</div>
</div>
<p>So this joint distribution has a joint cdf, <span class="math notranslate nohighlight">\(F(x,y)\)</span> and a continuous piecewise density function <span class="math notranslate nohighlight">\(f(x,y)\)</span>. The joint mean is defined as <span class="math notranslate nohighlight">\(\mu_x,\mu_y\)</span> What about the variance? Here we need to consider the variance and covaraince between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># correlation between variables</span>
<span class="n">dat2</span> <span class="o">&lt;-</span> <span class="n">dat</span><span class="p">[</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">dat</span><span class="o">$</span><span class="n">Fat</span><span class="p">),]</span>
<span class="nf">round</span><span class="p">(</span><span class="nf">cov</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">cbind</span><span class="p">(</span><span class="n">dat2</span><span class="o">$</span><span class="n">BMI</span><span class="p">,</span><span class="n">dat2</span><span class="o">$</span><span class="n">Fat</span><span class="p">)),</span><span class="m">3</span><span class="p">)</span>
<span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;variance of BMI = &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">var</span><span class="p">(</span><span class="n">dat2</span><span class="o">$</span><span class="n">BMI</span><span class="p">),</span><span class="m">3</span><span class="p">))</span>
<span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;variance of fat = &quot;</span><span class="p">,</span><span class="nf">round</span><span class="p">(</span><span class="nf">var</span><span class="p">(</span><span class="n">dat2</span><span class="o">$</span><span class="n">Fat</span><span class="p">),</span><span class="m">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 2 × 2 of type dbl</caption>
<tbody>
	<tr><td>15.850</td><td>20.696</td></tr>
	<tr><td>20.696</td><td>36.282</td></tr>
</tbody>
</table>
</div><div class="output text_html">'variance of BMI = 15.85'</div><div class="output text_html">'variance of fat = 36.282'</div></div>
</div>
<p>The <em>covariance matrix</em> is returned. The diagnoals return the variance of each parameter, and the off-diagnoals the covariance, indicating a positive correlation.</p>
</div>
<div class="section" id="correlation">
<h4>3.5.3  Correlation<a class="headerlink" href="#correlation" title="Permalink to this headline">¶</a></h4>
<p>Correlation and covariance are closely related.  Pearson’s correlation coefficient is defined as:</p>
<div class="math notranslate nohighlight">
\[ 
\rho(X,Y) = Corr(X,Y) = \frac{Cov(X,Y)}{SD(X)SD(Y)}
\]</div>
<p>So this helps us define BMI from body fat and <em>vice versa</em>. Examples of when this might be useful include;</p>
<ul class="simple">
<li><p>Inputing missing data</p></li>
<li><p>Summarising many variables with one metric (more about this in the Machine learning module)</p></li>
<li><p>Efficient sampling of distributions, which is used in Monte Carlo Markov Chain (MCMC) estimation</p></li>
</ul>
</div>
<div class="section" id="connections-to-regression-modelling">
<h4>3.5.4 Connections to regression modelling<a class="headerlink" href="#connections-to-regression-modelling" title="Permalink to this headline">¶</a></h4>
<p>Later sessions exploring regression modelling will provide a powerful and flexible approach to exploring and quantifying <em>dependencies</em> between variables.</p>
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-04. Inference.Intro"></span><div class="section" id="statistical-inference">
<h2>Statistical Inference<a class="headerlink" href="#statistical-inference" title="Permalink to this headline">¶</a></h2>
<p>This section of the notes concerns a really important part of statistics: <em>statistical inference</em>.</p>
<p>Statistical analysis is often separated into two types: descriptive and inferential. Descriptive statistics attempt to describe the data at hand (the sample). Inferential statistics go further - they attempt to use the data at hand to make statements about a wider population.</p>
<p>There is more than one framework for statistical inference. The traditional and most widely used is the frequentist or “classical” approach. An important alternative, the Bayesian approach, is increasingly influential.</p>
<div class="section" id="overview-of-the-statistical-inference-sessions">
<h3>Overview of the statistical inference sessions<a class="headerlink" href="#overview-of-the-statistical-inference-sessions" title="Permalink to this headline">¶</a></h3>
<p>This section of the notes comprises 7 sessions:</p>
<ul class="simple">
<li><p>Population and samples</p></li>
<li><p>Likelihood (x2)</p></li>
<li><p>Frequentist inference (x2)</p></li>
<li><p>Bayesian inference (x2)</p></li>
</ul>
<p>The first session introduces the concept of <strong>statistical inference</strong>, defining populations, samples and estimators. The second half of the session introduces the idea of <strong>sampling distributions</strong>, a fundamental building block for frequentist inference. The sampling distribution gives us information about how different our estimate of the unknown population quantity of interest might have been, had we selected a different sample. In other words, the sampling distribution describes how our estimate behaves under <strong>repeated sampling</strong>. One particular feature of the sampling distribution, the <strong>standard error</strong>, gives us information about the amount we might expect our estimate to change if we took a different sample (i.e. it describes the variability of the estimate between different samples).</p>
<p>The idea of sampling distributions is crucial to frequentist inference, but while it gives us important information about how our estimate behaves under repeated sampling, it does not provide a recipe for choosing an estimator. <strong>Maximum likelihood estimation</strong> (MLE), the subject of the following two sessions, does exactly this. Given a statistical model for the data, MLE provides a method for choosing an estimator with desirable statistical properties.</p>
<p>Having explored MLE to obtain our estimator, we return to the idea of sampling distributions in the following two frequentist inference sessions. We see how the idea of sampling distributions allows us to create <strong>confidence intervals</strong>, which are ranges of values of the population quantity which we believe are consistent with the observed data. A complementary frequentist inference tool, hypothesis testing, allows us to assess the evidence against a <strong>null hypothesis</strong>, which proposes a specific value (or range of values) for the unknown population parameter.</p>
<p>Thus far, our attention has been largely on the frequentist paradigm. The last two sessions focus instead on an important alternative approach, <strong>Bayesian inference</strong>. In this paradigm we do not base our inference on the idea of repeated sampling. Instead, we use the likelihood to update prior information (in the form of a probability distribution) about the unknown parameter, to provide a <strong>posterior distribution</strong> for the unknown parameter. The posterior can be summarised by obtaining its mean, or a <strong>credible interval</strong> (interval within which  the unknown parameter falls with a particular probability).</p>
</div>
</div>
<span id="document-04.a. Population.and.samples"></span><div class="section" id="populations-and-samples">
<h2>4. Populations and Samples<a class="headerlink" href="#populations-and-samples" title="Permalink to this headline">¶</a></h2>
<p>In this session we will begin thinking about statistical inference. Loosely, this describes the process of using a sample of data to make statements about a wider population. There is more than one framework for statistical inference. The traditional and most widely used approach is termed the “classical” or frequentist, and this is the one pursued in this and the next few sessions. An important alternative, the Bayesian approach, is increasingly influential. You will meet Bayesian inference later in this module.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>describe the process of frequentist statistical inference</p></li>
<li><p>define the terms population, sample, estimator and estimate</p></li>
<li><p>explain what a sampling distribution is and how it relates to the idea of repeated sampling</p></li>
<li><p>understand that sample estimates will vary as defined by the standard error</p></li>
<li><p>appreciate that the uncertainty in estimates can be described using central limit theorem and resampling (bootstrapping)</p></li>
</ul>
</div><p>The five sub-sections in this session explore the concept of sampling from a population and define parameters and estimators. They formally define the concept of a statistical model. Sampling distributions are described, and the standard error defined, along with key ways of obtaining the approximate sampling distribution.</p>
<div class="toctree-wrapper compound">
<span id="document-04.b. Population.and.samples"></span><div class="section" id="sampling-from-a-population">
<h3>4.1 Sampling from a population<a class="headerlink" href="#sampling-from-a-population" title="Permalink to this headline">¶</a></h3>
<p>Much of statistical inference is concerned with making statements about properties of populations, based on properties of samples from the populations. A helpful mental picture of the population and the sample to have in mind is as follows. Imagine that the <strong>population</strong> is a very large number of “objects” contained in a large urn, from which we can randomly sample a relatively small number of the “objects” at a time to provide our <strong>sample</strong>.</p>
<p>The objects in our population are often called <strong>sampling units</strong>. For many health research questions these sampling units are  individual patients. If we were collecting information about different hospitals in order to make comparison between different providers, then the sampling unit might be hospitals.</p>
<p>Making statements about a population using information contained in a sample of data relies critically on the process of how the sample was drawn from the population (i.e. the sampling process). A common example of a sample process is random sampling. Under random sampling, each object in the population has the same chance of appearing in the selected sample. Inference procedures tend to be most straightforward in this setting.</p>
<p>In many cases, sampling is not random. For example, many populations have intrinsic structure that might facilitate sampling. If our population is people in rural Gambia, for example, then the easiest way to sample individual people might be to choose 5 villages and then go and survey the people in those villages. While it is not necessarily difficult to modify the process of statistical inference for such situations, statistically invalid conclusions can be reached if such modifications are not undertaken.</p>
<div class="section" id="parameters-and-estimators">
<h4>4.1.1 Parameters and estimators<a class="headerlink" href="#parameters-and-estimators" title="Permalink to this headline">¶</a></h4>
<p>In statistical inference, the aim is to make statements about certain features of the population, using the information contained in the sample data. Typically, we quantify the features of interest in terms of unknown population quantities (some examples might be a population mean, standard deviation, proportion, or risk ratio) and attempt to <strong>estimate</strong> these population quantities. We call these unknown population quantities population <strong>parameters</strong>. Parameters are typically denoted using Greek letters. Often, certain letters tend to be used for certain types of quantities. For instance, <span class="math notranslate nohighlight">\(\mu\)</span> will often denote a population mean and <span class="math notranslate nohighlight">\(\pi\)</span> will often denote a population proportion. When we are talking about a general “parameter of interest”, we often use the letter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>A <strong>statistic</strong>, is any quantity that can be calculated from the known measurements on the sample data. It can be any function or combination of random variables that does not depend on unknown parameters for its calculation. As for population parameters, certain letters tend to be used for certain sample statistics. For instance, <span class="math notranslate nohighlight">\(\bar{x}\)</span> (“x bar”) will often denote a sample mean and <span class="math notranslate nohighlight">\(p\)</span> a sample proportion.</p>
<p>We often want to estimate a population parameter from the sample. We do this by using sample statistics to estimate population parameters. For example, the obvious statistic to use to estimate a population mean is the sample mean.  When a sample statistic is used for the purpose of estimating a population parameter it is known as an <strong>estimator</strong>.  So an estimator is a statistic that is  designed to be a “guess” at a particular parameter of a population.  When we use a sample statistic to esitmate a population parameter we use a “hat” to denote the estimator, e.g. <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is an estimator for the population quantity <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is an estimator for the population quantity <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Once we have drawn our sample of data and calculated the value of the estimator in that sample, we refer to this as the <strong>estimate</strong>. In other words, the term estimate is used for the value obtained by substituting sample data values into the formula for the estimator.</p>
<p>The basic structure of frequentist inference can be represented diagrammatically as follows:</p>
<div class="figure align-default" id="freq-inference">
<a class="reference internal image-reference" href="_images/Inference.png"><img alt="_images/Inference.png" src="_images/Inference.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Statistical inference</span><a class="headerlink" href="#freq-inference" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="example">
<h4>4.1.2 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>To explore these issues further, we will consider a study question investigated by a recent MSc student at LSHTM as part of their summer project. The student explored the question of whether people who engage with victims of violence themselves suffer from emotional distress. This question was assessed using a sample of 53 violence researchers in Uganda. Subsequently, these violence researchers took part in a randomised trial, but we will focus on the initial description of the sample. The full article can be found here <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455179/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455179/</a></p>
<p>For the purposes of illustration, in this session we will take a smaller sample of 10 violence researchers. Among our 10 sampled violence researchers, the sample mean age and the sample proportion suffering from emotional distress are</p>
<ul class="simple">
<li><p>Sample mean age <span class="math notranslate nohighlight">\(\bar{x}= 29.75\)</span>; sample standard deviation of age <span class="math notranslate nohighlight">\(SD = 4.49\)</span></p></li>
<li><p>Proportion suffering from emotional distress <span class="math notranslate nohighlight">\(p = 26\%\)</span> (14 out of 53)</p></li>
</ul>
<p>Let <span class="math notranslate nohighlight">\(X_1, ..., X_{10}\)</span> be random variables representing the ages of 53 sampled researchers. In other words, <span class="math notranslate nohighlight">\(X_1, ..., X_{10}\)</span> represent the random process by which the eventual 10 values of age are obtained. We call the realisation of these random variables (i.e. the observed data) <span class="math notranslate nohighlight">\(x_1, ..., x_{10}\)</span>.</p>
<p>We will initially focus on the population mean age (<span class="math notranslate nohighlight">\(\mu\)</span>) and its estimator. The obvious estimator for the population mean age is the sample mean age. In terms of the general random variables <span class="math notranslate nohighlight">\(X_1, ..., X_{10}\)</span>, we can write this estimator as,</p>
<div class="math notranslate nohighlight">
\[
\bar{X} = \frac{1}{10} \sum_{i=1}^{10} X_i
\]</div>
<p>This is a random variable representing the mean of the random variables <span class="math notranslate nohighlight">\(X_1\)</span>,…,<span class="math notranslate nohighlight">\(X_{10}\)</span>. The sample statistic estimate is,</p>
<div class="math notranslate nohighlight">
\[
\bar{x} = \frac{1}{10} \sum_{i=1}^n x_i = \mbox{sample mean age} = 29.75
\]</div>
<p>The estimate is the sample mean age, which is the realisation of <span class="math notranslate nohighlight">\(\bar{X}\)</span> in the observed data. Since we are now viewing this as an estimate of <span class="math notranslate nohighlight">\(\mu\)</span>, we can also write <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{x}\)</span>.</p>
<blockquote>
<div><p><strong>Discussion question</strong>
<br><br> - From above, our “best guess” at (our estimate of) the population mean age is 29.75.
<br> - Is this estimate a “good” estimate of the population mean? Do we think it is close to being correct?
<br> - If we sampled a different 10 researchers would we be likely to see a similar sample mean age? Or could we see a very different sample mean? Is it possible that, just by chance, this is a particularly old (or young) group of researchers?</p>
</div></blockquote>
</div>
</div>
<span id="document-04.c. Population.and.samples"></span><div class="section" id="statistical-models">
<h3>4.2 Statistical models<a class="headerlink" href="#statistical-models" title="Permalink to this headline">¶</a></h3>
<p>To extract information about population quantities from sample statistics we need a precise and formal description of the whole sampling process from population to sample. This description is called the <strong>statistical model</strong>. Relevant features of the population are represented by parameters, such as the mean, variance, or correlation. The structure of the population, together with the sampling process, allows a model to be formulated that describes the statistical behaviour of the sample.</p>
<p>The crucial importance of the statistical model is that, given a certain value of the population parameter (in the simple case where there is only one parameter of interest), it allows us to calculate the probability of drawing a sample with the properties we observe: this will allow us to quantify the compatibility between the observed data and possible values of the population parameter.</p>
<div class="section" id="example-a-statistical-model">
<h4>4.2.1 Example: a statistical model<a class="headerlink" href="#example-a-statistical-model" title="Permalink to this headline">¶</a></h4>
<p>We will now write down a formal statistical model for the (sub-sample from the) emotional distress study. Remember that <span class="math notranslate nohighlight">\(X_1, ...,X_{10}\)</span> are random variables representing the ages of 10 sampled researchers and <span class="math notranslate nohighlight">\(x_1, ..., x_{10}\)</span> are the realised values of these random variables (i.e. the observed ages).</p>
<p>We will assume that each random variable is drawn from the same population distribution, and that the observations are independent of each other. We use the term <strong>independent and identically distributed</strong> as a succinct way of describing these assumptions, often abbreviated as <strong>iid</strong>.</p>
<p>Finally, we will assume that ages of violence researchers in the wider population follows a normal distribution with population mean <span class="math notranslate nohighlight">\(\mu\)</span> and population variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>This model can be compactly written as follows</p>
<div class="math notranslate nohighlight">
\[ 
X_i \overset{\small{iid}}{\sim} N(\mu, \sigma^2), \qquad i=1,2,...,10
\]</div>
</div>
</div>
<span id="document-04.d. Population.and.samples"></span><div class="section" id="sampling-distributions">
<h3>4.3 Sampling distributions<a class="headerlink" href="#sampling-distributions" title="Permalink to this headline">¶</a></h3>
<p>In order to use our estimate (the value of the estimator in our sample of data) to make any sort of statement about the true but unknown value of the population parameter, we need to consider questions such as:</p>
<blockquote>
<div><p>How precise do we believe our estimate is?
<br><br> Are we fairly certain that the true parameter is close to the estimate, or do we believe the estimate may well be far from the true value?</p>
</div></blockquote>
<p>The following thought experiment might help to develop these ideas. Suppose our population is a large bucket full of identical marbles. We want to know the population mean weight of a marble (our population parameter of interest). To estimate this population mean, we can simply sample a single marble from the bucket. So our estimator is the weight of the single sampled marble. Now suppose we took two samples: we sample a single marble, weigh it, put it back in the bucket, sample another marble and weight that one. In this case, our estimate (the weight of the sampled marble) would be exactly the same as the estimate from the first sample. No matter how many different samples we took, the sample estimate would be identical. In this case, because all possible samples would give us an identical estimate of the mean, we can confidently say what the population mean is using a single sample of one marble.</p>
<p>Now consider a bucket full of different marbles. In this case, randomly sampling a single marble and using the weight of that marble as an estimate of the population mean weight could give us a weight far too large (if we just happened to sample one of the very large marbles) or far too small (if we happened to pick a very small marble). However, if we were to pick 100 marbles and take the sample mean of those 100 marbles as our estimator, we would expect our estimate to be closer to the population mean. If we were to resample another 100 marbles we would expect the sample mean weight to be fairly close to the mean weight of the previous 100 marbles. Conversely, if we took two samples containing one marble each, we might expect those two weights to be quite different from one-another.</p>
<p>This thought experiment makes it clear that in order to use our single sample of data to make statements about a wider population, we need to think about what would happen if we repeated our sampling: if we re-did our study many times, each time calculating the sample estimate, what values would those different sample estimates take? In fact, this is exactly what the <strong>sampling distribution</strong> is. It is the distribution of the <strong>estimator</strong> (the statistic we have chosen to use to estimate the population parameter of interest) under repeated sampling.</p>
<div class="section" id="simulated-data-sampling-distribution-of-a-mean">
<h4>4.3.2 Simulated data: sampling distribution of a mean<a class="headerlink" href="#simulated-data-sampling-distribution-of-a-mean" title="Permalink to this headline">¶</a></h4>
<p>We will return again to the emotional distress study. In reality, we do not know the true population mean and standard deviation. However, for the purposes of illustration, for the rest of the session we will imagine that we do know these values. Suppose that, in truth, the population mean age (<span class="math notranslate nohighlight">\(\mu\)</span>) is 30 and the population standard deviation (which will will call <span class="math notranslate nohighlight">\(\sigma\)</span>) is 4.8. Further, suppose that age follows a normal distribution in the population.</p>
<p>Under this scenario, the following code draws many (10,000) different samples from this population, with each sample containing the ages of 10 people. Note the line <code class="docutils literal notranslate"><span class="pre">set.seed(1042)</span></code> is coded to keep the same pseudo random number starting point.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Population parameters</span>
<span class="n">mu</span> <span class="o">&lt;-</span> <span class="m">30</span>
<span class="n">sd</span> <span class="o">&lt;-</span> <span class="m">4.8</span>
<span class="n">n_in_each_study</span> <span class="o">&lt;-</span> <span class="m">10</span>

<span class="c1"># Draw samples and ages for sampled individuals, for 100 different studies</span>
<span class="c1"># in this example we&#39;re going to have a list which generates study_measurements_age repeatedly</span>
<span class="n">different_studies</span> <span class="o">&lt;-</span> <span class="m">10000</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">1042</span><span class="p">)</span>
<span class="n">study_measurements_age</span> <span class="o">&lt;-</span> <span class="nf">list</span><span class="p">()</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">different_studies</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">study_measurements_age</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span> <span class="o">&lt;-</span> <span class="nf">round</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n_in_each_study</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="p">),</span><span class="m">3</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Print the sample data for two of the studies</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Ages of the 10 participants selected in study 1:&quot;</span><span class="p">)</span>
<span class="n">study_measurements_age</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span>

<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Ages of the 10 participants selected in study 5:&quot;</span><span class="p">)</span>
<span class="n">study_measurements_age</span><span class="p">[[</span><span class="m">5</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Ages of the 10 participants selected in study 1:&quot;
</pre></div>
</div>
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>17.897</li><li>30.27</li><li>26.896</li><li>35.448</li><li>28.514</li><li>33.891</li><li>42.021</li><li>25.994</li><li>31.061</li><li>28.756</li></ol>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Ages of the 10 participants selected in study 5:&quot;
</pre></div>
</div>
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>28.502</li><li>33.725</li><li>35.155</li><li>26.544</li><li>31.147</li><li>31.732</li><li>39.582</li><li>31.223</li><li>25.802</li><li>16.168</li></ol>
</div></div>
</div>
<p>Now we will calculate the sample mean for each sample of 10 people and plot them on a histogram. In the graphs below, we see a graph of sample means from 10,000 different studies (i.e. 10,000 different samples). This only gives us an approximation to the true sampling distribution, because it is based on a finite number of samples (10,000 samples). However, this is a large number so it will give us a fairly good approxiation to the sampling distribution of the sample mean.</p>
<p>For this estimator and this population, we can see that the sampling distribution follows a normal distribution. Note that the sampling distribution is centred around the true population value of 30. We also see that almost all sample means lie within 4 or so years of the mean either way (i.e. most sample means are between 26 and 34).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>
<span class="n">sample.means</span>   <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">study_measurements_age</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>

<span class="c1"># Draw graphs using base R</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">sample.means</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">10000</span><span class="p">],</span> <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span>
     <span class="n">breaks</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">25.5</span><span class="p">,</span> <span class="m">26.5</span><span class="p">,</span> <span class="m">27.5</span><span class="p">,</span> <span class="m">28.5</span><span class="p">,</span> <span class="m">29.5</span><span class="p">,</span> <span class="m">30.5</span><span class="p">,</span> <span class="m">31.5</span><span class="p">,</span> <span class="m">32.5</span><span class="p">,</span> <span class="m">33.5</span><span class="p">,</span> <span class="m">34.5</span><span class="p">,</span> <span class="m">100</span><span class="p">),</span> 
     <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">26</span><span class="p">,</span> <span class="m">35</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">0.3</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green4&quot;</span><span class="p">,</span>
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Sample mean age (years)&quot;</span><span class="p">,</span> 
     <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution of the sample mean age \n (from samples of 10)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/04.d. Population.and.samples_4_0.png" src="_images/04.d. Population.and.samples_4_0.png" />
</div>
</div>
</div>
<div class="section" id="the-standard-error-of-an-estimate">
<h4>4.3.3 The standard error of an estimate<a class="headerlink" href="#the-standard-error-of-an-estimate" title="Permalink to this headline">¶</a></h4>
<p>When we are talking about the sampling distribution (i.e. the distribution of an <em>estimator</em>), we call the standard deviation the <strong>standard error</strong>. The standard error refers to the variability we might expect in estimates of the parameter, because we are inferring the estimates from a sample. When we have two different estimators for the population parameter of interest, we would typically choose the one with the lower standard error.</p>
<div class="alert alert-block alert-info">
<b> The standard error</b> 
<p>If an independently distributed random variable <span class="math notranslate nohighlight">\(X\)</span> has population mean (<span class="math notranslate nohighlight">\(\mu\)</span>) and population variance (<span class="math notranslate nohighlight">\(\sigma^2\)</span>), the sampling distribution of sample means (of samples of size <span class="math notranslate nohighlight">\(n\)</span>) has population mean <span class="math notranslate nohighlight">\(\mu\)</span> and population variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>. This irrespective of the population distribution; it does not need to be <em>normal</em>. In other words the standard error is <span class="math notranslate nohighlight">\(\sigma_{\bar{X}} = \sigma/\sqrt{n}\)</span>.</p>
</div>
</div>
</div>
<span id="document-04.e. Population.and.samples"></span><div class="section" id="obtaining-the-sampling-distribution">
<h3>4.4 Obtaining the sampling distribution<a class="headerlink" href="#obtaining-the-sampling-distribution" title="Permalink to this headline">¶</a></h3>
<p>The sampling distribution is hypothetical: in reality, we are not going to repeat our study many times to see how much our estimates differ from sample to sample.</p>
<p>In many cases we can describe the sampling distribution of our estimator well enough to do statistical inference, i.e. well enough to make useful statements about the population parameter. There are three main approaches to obtaining the (approximate) sampling distribution of an estimator:</p>
<ol class="simple">
<li><p><strong>Algebraic calculation</strong>. Sometimes we can algebraically obtain the distribution of the estimator from our statistical model. An example is given below for the sampling distribution for the sample mean age in the emotional distress example.</p></li>
<li><p><strong>The Central Limit Theorem</strong>. If we have an estimator which can be written as the sum of independent random variables, then for large samples, the estimator will have an approximately normal distribution. This is described in more detail shortly.</p></li>
<li><p><strong>Resampling</strong>. In many situations, we can use a resampling principle to obtain an approximate sampling distribution.</p></li>
</ol>
<p>Returning to the question posed at the start of the previous section, those questions can be answered using the sampling distribution:</p>
<blockquote>
<div><p><br> How precise do we believe our estimate is?
<br><br> Are we fairly certain that the true parameter is close to the estimate, or do we believe the estimate may well be far from the true value?</p>
</div></blockquote>
<p>The first question asks whether the sampling distribution is centred at the right value (i.e. the population parameter being estimated). If it is, we say the estimator is <strong>unbiased</strong>. In the epidemiology module and earlier in this module, you have already come across how the <em>sample</em> can be biased. Here, we are referring to whether the estimator is biased, which is sometimes referred to as <em>statistical bias</em>. Most estimators are unbiased, and this can be shown using statistical theory. For a small number of estimators it can be shown that they are in fact biased, and sometimes a correction can be applied to account for this. An example of exploring whether estimators are biased is given in the Appendix.</p>
<p>The second question will be examined when looking at the standard error and forms the basis for constructing 95% confidence intervals.</p>
<div class="section" id="algebraic-calculation">
<h4>4.4.1 Algebraic calculation<a class="headerlink" href="#algebraic-calculation" title="Permalink to this headline">¶</a></h4>
<p>We will illustrate the idea of obtaining a sampling distribution via algebraic calculation by revisiting the sub-sample from the emotional distress study.</p>
<p>For the moment, we will assume that in truth, ages follow a normal distribution with population mean <span class="math notranslate nohighlight">\(\mu=30\)</span> and population standard deviation <span class="math notranslate nohighlight">\(\sigma=4.8\)</span>. Of course, in real life we would not have this information.</p>
<p>We now imagine that the population value of <span class="math notranslate nohighlight">\(\mu\)</span> is unknown to the investigators undertaking the study; indeed, making inferences about <span class="math notranslate nohighlight">\(\mu\)</span> is the aim of the study. We further imagine the rather unrealistic (but simplifying) situation that the investigators know the true value of the population standard deviation, <span class="math notranslate nohighlight">\(\sigma=4.8\)</span>.</p>
<p>Our model for the emotional distress study states that:</p>
<div class="math notranslate nohighlight">
\[ 
X_i \overset{\small{iid}}{\sim} N(\mu, 4.8^2), \qquad i=1,2,...,10
\]</div>
<p>Under this statistical model, we want to know the distribution of our estimator for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} = \frac{1}{10} \sum_{i=1}^n X_i
\]</div>
<p>In this case, it’s quite easy to derive the sampling distribution algebraically. We use the following fact:</p>
<blockquote>
<div><p>The mean of independent normally distributed variables also follows a normal distribution</p>
</div></blockquote>
<p>It is then  easy to calculate the expectation and variance of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> using techniques covered in the maths refresher. So we know that the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} \sim N\left(\mu, 1.52^2 \right) 
\]</div>
<p>where the variance of this normal distribution was obtained as</p>
<div class="math notranslate nohighlight">
\[
Var(\hat{\mu}) = \frac{1}{10^2} \times 10 \times Var(X_i) = \frac{4.8^2}{10}.
\]</div>
<p>This gives us a lot of useful information about how the sampling distribution in relation to the unknown parameter <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<ul class="simple">
<li><p>It follows a normal distribution (has a symmetric bell-shape)</p></li>
<li><p>It is centred around the true (unknown) population value</p></li>
<li><p>The standard error of the sample mean (the standard deviation of the estimator) is  <span class="math notranslate nohighlight">\(1.52\)</span>.</p></li>
</ul>
<p>In many situations, this sort of algebraic calculation is possible. If not, we often rely on the central limit theorem to obtain the approximate sampling distribution in large samples.</p>
</div>
<div class="section" id="the-central-limit-theorem">
<h4>4.4.2 The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this headline">¶</a></h4>
<p>The Central Limit Theorem (CLT) is a key concept in statistics and in estimation. When we use the mean from a sample to estimate a parameter, we already acknowledge that there will be some error around this estimate, as described above. The CLT takes this further;</p>
<div class="alert alert-block alert-info">
<b> The Central Limit Theorem</b> 
<p>If a random variable <span class="math notranslate nohighlight">\(X\)</span> has population mean <span class="math notranslate nohighlight">\(\mu\)</span> and population variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span>, based on <span class="math notranslate nohighlight">\(n\)</span> observations, is <em>approximately</em> normally distributed with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>, for sufficiently large <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
<p>So even for situations where <span class="math notranslate nohighlight">\(X\)</span> follows a distribution that is not even close to being normal (e.g. <span class="math notranslate nohighlight">\(X\)</span> might be Poisson, or Binomial, or some wacky distribution), for sufficiently large samples the <em>mean</em> will follow a normal distribution. An example where <span class="math notranslate nohighlight">\(X\)</span> is a binary variable is given in the Appendix to this session.</p>
<p>This theorem is hugely powerful. We will see that this allows us to conduct large-sample inference fairly easily on any type of data.</p>
</div>
<div class="section" id="resampling">
<h4>4.4.3 Resampling<a class="headerlink" href="#resampling" title="Permalink to this headline">¶</a></h4>
<p>An alternative approach, which is computationally intensive but very flexible, is to use a resampling approach.</p>
<p>For a population of interest, we want to estimate a parameter <span class="math notranslate nohighlight">\(\theta\)</span> using a sample <span class="math notranslate nohighlight">\(S\)</span> of <span class="math notranslate nohighlight">\(n\)</span> individuals (for our example <span class="math notranslate nohighlight">\(n=10\)</span>) from the population. We have an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> from our sample <span class="math notranslate nohighlight">\(S\)</span>. We want to know about the sampling distribution of the estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>.</p>
<p>We discussed above the idea that we could obtain the sampling distribution by repeatedly sampling from the population and calculating our estimate in each sample. Then a histogram of those many estimates would give us (approximately) the sampling distribution. In practice, it is logistically impossible to repeat the study a large number of times. However, we can mimic this process by using resampling.</p>
<p>The basic idea is to pretend that the observed data are the population and repeatedly sample from the data to learn about the relationship between <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> and the estimates obtained from the re-sampled data, which we will call <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span>.</p>
<p>Suppose we sample with replacement from the sample <span class="math notranslate nohighlight">\(S\)</span> to obtain “sub-samples” also of size <span class="math notranslate nohighlight">\(n\)</span>. These “sub-samples” are called <strong>bootstrap samples</strong>.</p>
<p>For example, suppose we have a sample <span class="math notranslate nohighlight">\(S\)</span> of size 10 (<span class="math notranslate nohighlight">\(n=10\)</span>):</p>
<div class="math notranslate nohighlight">
\[
S = \{ x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_{10} \}
\]</div>
<p>And suppose our estimate is the sample mean,</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = \frac{(x_1 + x_2 +  x_3 +  x_4 +  x_5 + x_6 + x_7 + x_8 + x_9 + x_{10})}{10}
\]</div>
<p>Then a bootstrap sample might be:</p>
<div class="math notranslate nohighlight">
\[
S^*_1 = \{x_{10}, x_3, x_2, x_8, x_6, x_2, x_4, x_1, x_8, x_1 \}
\]</div>
<p>Another bootstrap sample could be:</p>
<div class="math notranslate nohighlight">
\[
S^*_2 = \{ x_5, x_9, x_4, x_7, x_{10}, x_9, x_3, x_4, x_6, x_2 \}
\]</div>
<p>In each bootstrap sample, we obtain a new estimate (the sample mean in the bootstrap sample):</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta}^*_1 = \frac{(x_{10} + x_3 + x_2 + x_8 + x_6 + x_2 + x_4 + x_1 + x_8 + x_1)}{10}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta}^*_2 = \frac{(x_5 + x_9 + x_4 + x_7 + x_{10} + x_9 + x_3 + x_4 + x_6 + x_2)}{10}
\]</div>
<p>We do this a very large number of times to obtain lots of estimates from different bootstrap samples. Then we can draw a histogram of these many bootstrap estimates to see the shape and dispersion of the distribution.</p>
<p>The <strong>bootstrap principle</strong> says that the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> given <span class="math notranslate nohighlight">\(\theta\)</span> (i.e. the sampling distribution) is approximated by the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span> given <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>.  For example, if we find that our values of <span class="math notranslate nohighlight">\(\hat{\theta}^*\)</span> are approximately normally distributed and centred around <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> then the bootstrap principle tells us that <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> follows a normal distribution centred around <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="section" id="example-resampling">
<h5>4.4.3.1 Example: resampling<a class="headerlink" href="#example-resampling" title="Permalink to this headline">¶</a></h5>
<p>We illustrate the idea of resampling using the sub-sample of the emotional distress study. Suppose our data - the 10 sampled ages - are the set: <span class="math notranslate nohighlight">\(\{ 28.1, 27.5, 25, 29.9, 29.7, 29.9, 39.9, 33.6, 21.3, 30.8 \}\)</span>. Our estimate of the population age is the sample mean age, which is: <span class="math notranslate nohighlight">\(\hat{\mu} = 29.57\)</span>.</p>
<p>To obtain an approximation to the sampling distribution for the sample mean age, using a resampling approach, we first take a large number of bootstrap samples from the data. The code below does this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Our sample of data (ages for 10 sampled researchers)</span>
<span class="n">ages</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">28.1</span><span class="p">,</span><span class="m">27.5</span><span class="p">,</span><span class="m">25</span><span class="p">,</span><span class="m">29.9</span><span class="p">,</span><span class="m">29.7</span><span class="p">,</span><span class="m">29.9</span><span class="p">,</span><span class="m">39.9</span><span class="p">,</span><span class="m">33.6</span><span class="p">,</span><span class="m">21.3</span><span class="p">,</span><span class="m">30.8</span><span class="p">)</span>

<span class="c1"># Randomly select 10,000 bootstrap samples (each of size 10)</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">532</span><span class="p">)</span> 
<span class="n">bootstrap_samples</span> <span class="o">&lt;-</span> <span class="nf">lapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">10000</span><span class="p">,</span> <span class="nf">function</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="nf">sample</span><span class="p">(</span><span class="n">ages</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">T</span><span class="p">))</span>

<span class="c1"># List some of the bootstrap samples</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;First bootstrap sample:&quot;</span><span class="p">)</span>
<span class="n">bootstrap_samples</span><span class="p">[</span><span class="m">1</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Third bootstrap sample:&quot;</span><span class="p">)</span>
<span class="n">bootstrap_samples</span><span class="p">[</span><span class="m">3</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;500th bootstrap sample:&quot;</span><span class="p">)</span>
<span class="n">bootstrap_samples</span><span class="p">[</span><span class="m">500</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;First bootstrap sample:&quot;
</pre></div>
</div>
<div class="output text_html"><ol>
	<li><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>27.5</li><li>29.9</li><li>28.1</li><li>27.5</li><li>29.9</li><li>21.3</li><li>27.5</li><li>28.1</li><li>29.9</li><li>30.8</li></ol>
</li>
</ol>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Third bootstrap sample:&quot;
</pre></div>
</div>
<div class="output text_html"><ol>
	<li><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>30.8</li><li>28.1</li><li>21.3</li><li>29.9</li><li>39.9</li><li>21.3</li><li>27.5</li><li>29.9</li><li>29.9</li><li>29.7</li></ol>
</li>
</ol>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;500th bootstrap sample:&quot;
</pre></div>
</div>
<div class="output text_html"><ol>
	<li><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>21.3</li><li>29.9</li><li>39.9</li><li>29.9</li><li>29.9</li><li>27.5</li><li>29.9</li><li>29.9</li><li>29.9</li><li>21.3</li></ol>
</li>
</ol>
</div></div>
</div>
<p>The next step is to calculate the estimate (the sample mean, in our case) in each bootstrap sample. These estimates are called <span class="math notranslate nohighlight">\(\hat{\mu}^*_1, \hat{\mu}^*_2, .., \hat{\mu}^*_{10,000}\)</span>. Then we can plot the histogram of all the estimates across the bootstrap samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the sample mean in each of the bootstrap samples</span>
<span class="n">r.mean</span> <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">bootstrap_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>

<span class="c1"># Draw a histogram with a red vertical line indicating the original sample mean age</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">r.mean</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Distribution of sample means \n across the bootstrap samples&quot;</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Bootstrap sample means&quot;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;cornflowerblue&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="nf">mean</span><span class="p">(</span><span class="n">ages</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/04.e. Population.and.samples_7_0.png" src="_images/04.e. Population.and.samples_7_0.png" />
</div>
</div>
<p>We see a number of features from the graph above;</p>
<blockquote>
<div><p><br>- The histogram follows an approximately normal distribution (has a symmetric bell-shape)
<br>- It is centred around the sample mean age (from the original sample, <span class="math notranslate nohighlight">\(\hat{\mu} = 29.57\)</span>)
<br>- The code below tells us that the standard deviation of this distribution is 1.51.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">var</span><span class="p">(</span><span class="n">r.mean</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">1.50868637530863</div></div>
</div>
<p>So we have seen that the bootstrap approximation of the distribution of <span class="math notranslate nohighlight">\(\hat{\mu}^*\)</span> given <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is a normal distribution centred around <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> with standard deviation of 1.51. The bootstrap principle tells us that the distribution of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> given <span class="math notranslate nohighlight">\(\mu\)</span> is approximately the same. In other words, approximately:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} \sim N(\mu, 1.51^2)
\]</div>
<p>Remember, that we obtained the true distribution algebraically above and found that</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} \sim N(\mu, 1.52^2)
\]</div>
<p>So the resampling (bootstrap) approach has given us a very good approximation to the true sampling distribution. The code below redraws the histogram above, with the approximate (bootstrap) sampling distribution and the algebraically-calculated one.</p>
<p>We see that the bootstrap sampling distribution (shown in red) is simply a shift of the normal distribution which best follows the histogram (shown in orange), and that the bootstrap and true (algebraic, shown in blue) sampling distributions are very similar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Histogram of estimates (sample means) in bootstrap samples</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">r.mean</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Distribution of sample means \n across the bootstrap samples&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Bootstrap sample means&quot;</span><span class="p">)</span>

<span class="c1"># Add the normal distribution which most closely follows the histogram</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">35</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">),</span> <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">35</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">),</span> <span class="nf">mean</span><span class="p">(</span><span class="n">ages</span><span class="p">),</span> <span class="m">1.52</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="c1"># Add the bootstrap approximation to the sampling distribution: normal distribution with mean mu=30 SD=1.51</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">35</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">),</span> <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">35</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">),</span> <span class="m">30</span><span class="p">,</span> <span class="m">1.51</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="c1"># Add the algebraic sampling distribution: normal distribution with mean mu=30 SD=1.52</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">35</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">),</span> <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">35</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">5</span><span class="p">),</span> <span class="m">30</span><span class="p">,</span> <span class="m">1.52</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="c1"># Add a vertical line at original sample mean</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="nf">mean</span><span class="p">(</span><span class="n">ages</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/04.e. Population.and.samples_11_0.png" src="_images/04.e. Population.and.samples_11_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="what-do-we-use-a-sampling-distribution-for">
<h4>4.4.5 What do we use a sampling distribution for?<a class="headerlink" href="#what-do-we-use-a-sampling-distribution-for" title="Permalink to this headline">¶</a></h4>
<p>In practice, we have a single sample of data and a single estimate from that sample of the population parameter of interest.  When we present our single estimate of a population quantity, we need to be able to say something about how precise it is. Is it likely to be close to the true value? Can we provide a range of values within which we believe the true value lies?</p>
<p>We can only answer these questions, within the framework of frequentist statistical inference, by thinking about what estimates we might have got had we chosen a different sample. This leads us to the sampling distribution - the distribution of the estimator across samples.</p>
<p>In subsequent sessions we will see how the sampling distribution allows us to</p>
<ul class="simple">
<li><p>construct confidence intervals for population parameters (intervals within which we believe the true value is likely to lie)</p></li>
<li><p>conduct hypothesis tests for population parameters</p></li>
</ul>
</div>
</div>
<span id="document-04.f. Population.and.samples"></span><div class="section" id="summary">
<h3>4.5 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In this session we have introduced the concept of populations and samples, and how we use statistical inference to make statements about the unknown population parameters.</p></li>
<li><p>These unknown popultion parameters are estimated by sample statistics. Variability in the sample and any associated statistics are to be expected.</p></li>
<li><p>The variability in the sample statistics can be quantified by estimating the standard error.</p></li>
</ul>
</div>
<span id="document-04.g. Population.and.samples"></span><div class="section" id="appendix-additional-reading">
<h3>Appendix: Additional Reading<a class="headerlink" href="#appendix-additional-reading" title="Permalink to this headline">¶</a></h3>
<p>This appendix section contains additional information which will deepen your understanding. However, it is not examinable and is completely optional reading.</p>
<div class="section" id="a1-more-on-populations">
<h4>A1 More on populations<a class="headerlink" href="#a1-more-on-populations" title="Permalink to this headline">¶</a></h4>
<p>There are additional issues related to the definition of the population, that should be considered.</p>
<ul class="simple">
<li><p>Is the population well defined?</p></li>
</ul>
<p>Loosely speaking, we think about the population as being the wider group (often of people or patients) who we can generalise the results to. For some research questions the population of interest is well defined. For instance, suppose we undertake a study where we are attempting to estimate the proportion of adults (18 years and above) in the UK with hypertension in 2020. The population is well defined. Conversely, suppose we undertake a study to estimate the effect of a blood-pressure-lowering treatment among a sample of 50 patients in the UK in 2020. In this case, the population of interest can be difficult to pin down. Who can we generalize our results to? Is the population restricted in time and space? Can we generalise to patients in other countries? Can we generalise to future patients?</p>
<ul class="simple">
<li><p>Is the sample representative of the population?</p></li>
</ul>
<p>Clearly a sample can be chosen in may ways, and the way in which we are able to make inferences about the population depends critically on the way in which the sample is selected: it is hard to over-emphasize the importance and relevance of the sampling process to the meaning and validity of the subsequent inferences. In this module, we will assume that sampling units (in this case, people) are randomly sampled from the population.</p>
<ul class="simple">
<li><p>Is the population finite, or (effectively or potentially) infinite?</p></li>
</ul>
<p>For example, a study of a new treatment for a disease may wish to generalise to all potential patients.</p>
<ul class="simple">
<li><p>Have we sampled all the population?</p></li>
</ul>
<p>For example, a study of leukemia in the years following a leak from a nuclear power station may sample all subjects developing leukemia within the relevant time period in the vicinity of the power station. In such an example it is not clear how to define a wider population from which the sample can be considered to have been drawn. In these and other cases one approach is to consider a notional or counterfactual population, which can only have a conceptual existence.</p>
<p>In general the issues can be complex and will not be considered further here.</p>
</div>
<div class="section" id="a2-bias-of-estimators">
<h4>A2 Bias of estimators<a class="headerlink" href="#a2-bias-of-estimators" title="Permalink to this headline">¶</a></h4>
<p>Using statistical theory it is possible to show that the sample mean, <span class="math notranslate nohighlight">\(\bar{X}\)</span>, is an unbiased estimator of the population mean, <span class="math notranslate nohighlight">\(\mu\)</span>. One of the simplest examples is when our random variables follow the <em>Bernoulli</em> distribution.</p>
<p><strong>Example</strong> Let <span class="math notranslate nohighlight">\(X_1, X_2,.., X_n\)</span> be Bernoulli trials with success parameter <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Our estimate of <span class="math notranslate nohighlight">\(p\)</span> is the sample mean,</p>
<div class="math notranslate nohighlight">
\[
\hat{p} = \bar{X} = \frac{X_1 + X_2 + ... + X_n}{n}
\]</div>
<p>We will now show that the expected value of this estimator is equal to the population mean, <span class="math notranslate nohighlight">\(p\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E(\bar{X}) &amp; = E\left[\frac{X_1 + X_2 + ... + X_n}{n}\right] \\
&amp; = \frac{1}{n} E[X_1 + X_2 + ... + X_n] \qquad  \mbox{(we can take constants out of expectations)} \\
&amp;= \frac{1}{n} (E(X_1) + E(X_2) + ... + E(X_n)) \\
&amp;= \frac{1}{n} (p + p + ... + p) = p \\
\end{align*}
\end{split}\]</div>
<p>This simple use of algebra illustrates that <span class="math notranslate nohighlight">\(\bar{X}\)</span> is an unbiased estimator for <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p><strong>Exercise:</strong> You can use similar logic to demonstrate that the sample mean is an unbiased estimator for the population mean when the random variables <span class="math notranslate nohighlight">\(X\)</span> follow a normal distribution with known variance.</p>
</div>
<div class="section" id="a3-clt-for-binary-data">
<h4>A3 CLT for binary data<a class="headerlink" href="#a3-clt-for-binary-data" title="Permalink to this headline">¶</a></h4>
<p>We will return to the emotional distress study again, using the sub-sample of 10 people, but this time measure a binary characteristic for each person - the presence of emotional distress.</p>
<p>We suppose that, in the population, the true proportion is 28%. Under this assumption, we can simulate (draw) different samples,  each containing 10 people. If we do this a very large number of times, say 10,000, then the distribution of the different sample proportions we obtain will give us a very good picture of the true sampling distribution of the proportion. (Of course, remember that in practice we cannot do this because we won’t know the true population proportion).</p>
<p>The code below obtains the approximate sampling distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Population parameters</span>
<span class="kc">pi</span> <span class="o">&lt;-</span> <span class="m">0.28</span>
<span class="n">n_in_study</span> <span class="o">&lt;-</span> <span class="m">10</span>

<span class="c1"># Simulate data from multiple studies</span>
<span class="n">different_studies</span> <span class="o">&lt;-</span> <span class="m">10000</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">1042</span><span class="p">)</span>
<span class="n">study_measurements_ed</span> <span class="o">&lt;-</span> <span class="nf">list</span><span class="p">()</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">different_studies</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">study_measurements_ed</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span> <span class="o">&lt;-</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">n_in_study</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="kc">pi</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Calculate the proportion in each study</span>
<span class="n">sample.props</span>   <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">study_measurements_ed</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>

<span class="c1"># Draw graphs</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">sample.props</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">10000</span><span class="p">],</span> 
     <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> <span class="n">breaks</span><span class="o">=</span><span class="nf">seq</span><span class="p">(</span><span class="m">-0.05</span><span class="p">,</span> <span class="m">0.95</span><span class="p">,</span> <span class="m">0.05</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;cornflowerblue&quot;</span><span class="p">,</span>
     <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">6</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Sample proportion with emotional distress&quot;</span><span class="p">,</span> 
     <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution of the sample \n proportion (samples of n=10)&quot;</span><span class="p">)</span>  <span class="c1"># the &quot;\n&quot; makes a newline</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/04.g. Population.and.samples_5_0.png" src="_images/04.g. Population.and.samples_5_0.png" />
</div>
</div>
<p>The graph above shows us a reasonably accurate picture of the sampling distribution. Unlike the sample mean, the sampling distribution of the sample proportion is not quite symmetric. It is also not continuous - the sample statistic can only take 10 different values, with a sample size of <span class="math notranslate nohighlight">\(n=10\)</span>.</p>
<p>Below, the code shows that the mean of the sample means is approximately 0.28. (The discrepancy is just random error due to the fact that our “sampling distribution” does not come from an infinite number of samples. If we simulated a sufficiently large number of samples, this number would become closer to the true value of 0.28.)</p>
<p>The final line of code below lists the (approximate) probability density function, which gives us the whole sampling distribution for the sample proportion in this example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">### Summarise the approximate sampling distribution</span>

<span class="c1"># The mean value of the different sample means</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">sample.props</span><span class="p">)</span>

<span class="c1"># The whole sampling distribution (i.e. the PDF)</span>
<span class="nf">table</span><span class="p">(</span><span class="n">sample.props</span><span class="p">)</span><span class="o">/</span><span class="n">different_studies</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.27911</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sample.props
     0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9 
0.0365 0.1439 0.2630 0.2592 0.1793 0.0876 0.0257 0.0043 0.0004 0.0001 
</pre></div>
</div>
</div>
</div>
<p>Now we will explore what happens to the sampling distribution as we take larger samples. So instead of 10 people per sample, suppose we had 100 or 10,000 people in each sample. The central limit theorem tells us we expect the distribution to become more normal as we increase the sample size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x10n</span> <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="m">1000</span><span class="p">),</span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="m">0.28</span><span class="p">))</span> 
<span class="n">x10mean</span> <span class="o">&lt;-</span> <span class="nf">colMeans</span><span class="p">(</span><span class="n">x10n</span><span class="p">)</span>
                            
<span class="n">x50n</span> <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">50</span><span class="p">,</span><span class="m">1000</span><span class="p">),</span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="m">0.28</span><span class="p">))</span> 
<span class="n">x50mean</span> <span class="o">&lt;-</span> <span class="nf">colMeans</span><span class="p">(</span><span class="n">x50n</span><span class="p">)</span>

<span class="n">x100n</span> <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">1000</span><span class="p">),</span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="m">0.28</span><span class="p">))</span> 
<span class="n">x100mean</span> <span class="o">&lt;-</span> <span class="nf">colMeans</span><span class="p">(</span><span class="n">x100n</span><span class="p">)</span>

<span class="n">x1000n</span> <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span><span class="m">1000</span><span class="p">),</span><span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="m">0.28</span><span class="p">))</span> 
<span class="n">x1000mean</span> <span class="o">&lt;-</span> <span class="nf">colMeans</span><span class="p">(</span><span class="n">x1000n</span><span class="p">)</span>
                                
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">x10mean</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;cornflowerblue&quot;</span><span class="p">,</span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">),</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution of the sample \n proportion (samples of n=10)&quot;</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">x50mean</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;cornflowerblue&quot;</span><span class="p">,</span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">),</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution of the sample \n proportion (samples of n=50)&quot;</span><span class="p">)</span>        
<span class="nf">hist</span><span class="p">(</span><span class="n">x100mean</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;cornflowerblue&quot;</span><span class="p">,</span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">),</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution of the sample \n proportion (samples of n=100)&quot;</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">x1000mean</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;cornflowerblue&quot;</span><span class="p">,</span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">),</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution of the sample \n proportion (samples of n=1,000)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/04.g. Population.and.samples_9_0.png" src="_images/04.g. Population.and.samples_9_0.png" />
</div>
</div>
<p>As predicted by the CLT, even though the original data are binary (each person has emotional distress or does not), the sample proportion (which is the mean of the binary outcomes) has a distribution which becomes approximately normal with sufficiently large samples.</p>
</div>
</div>
</div>
</div>
<span id="document-05.a. Likelihood"></span><div class="section" id="likelihood">
<h2>5. Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this headline">¶</a></h2>
<p>In statistical inference, our task is to make statements about the underlying parameter(s) of our proposed model given the observed data. In particular, we typically wish to obtain the best estimate of the unknown parameters.  We also wish to know how well we have estimated the unknown parameter(s). The concept of likelihood provides the best single framework for this task. We will see that the likelihood function, often simply called the likelihood, plays a fundamental role in both frequentist and Bayesian inference.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>explain the concepts of likelihood and maximum likelihood estimation</p></li>
<li><p>derive a likelihood in a simple situation</p></li>
<li><p>explain the connection between maximising the likelihood and maximising the log-likelihood</p></li>
<li><p>describe and apply the process of obtaining a maximum likelihood estimator</p></li>
</ul>
</div><p>The next sub-sections introduce the idea of maximum likelihood estimation, define the likelihood and log-likelihood functions and illustrate the process of obtaining the maximum likelihood estimator.</p>
<div class="toctree-wrapper compound">
<span id="document-05.b. Likelihood"></span><div class="section" id="maximum-likelihood-estimation">
<h3>5.1 Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h3>
<p>Suppose we are interested in the probability that a single patient will experience a particular side effect from a particular drug. We decide to run a small clinical study including 8 patients. The observed data consist of the number, of those 8 patients, who experience a side effect. Suppose that we conduct the study and observe that 2 patients experience a side effect. We wish to use these observed data to make statements - inferences - about the unknown probability of experiencing a side effect from that drug.</p>
<p><strong>Statistical model:</strong> We begin by defining a model for the data. Here, we define <span class="math notranslate nohighlight">\(X\)</span> as the random variable representing the total number of the 8 patients who experience a side effect. Our model is that</p>
<div class="math notranslate nohighlight">
\[
X \sim binomial(8, \pi)
\]</div>
<p>which - we remember from the probability sessions - involves the assumptions that each Bernoulli event (whether or not each individual patient experiences a side effect) is independent and has the same probability of occurring.</p>
<p>This model involves the unknown parameter <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>Data:</strong> We have observed a realisation from this model, <span class="math notranslate nohighlight">\(X=2\)</span>. These are often called our observed data.</p>
<p>Under our proposed statistical model, the probability that 2 out of 8 patients experience a side effect is:</p>
<div class="math notranslate nohighlight">
\[
P(X=2) = {8 \choose 2} \pi^2 (1-\pi)^6
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\pi\)</span> is unknown, it is natural to consider how the probability of observing these data varies with different values of <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\pi\)</span></p></th>
<th class="text-align:center head"><p>P(<span class="math notranslate nohighlight">\(X\)</span>=<span class="math notranslate nohighlight">\(2\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>0.25</p></td>
<td class="text-align:center"><p>0.311</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>0.5</p></td>
<td class="text-align:center"><p>0.109</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>0.75</p></td>
<td class="text-align:center"><p>0.004</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Suppose that, in truth, the unknown probability of a patient experiencing a side effect from this drug was 0.75. The probability of then observing 2 from 8 patients experiencing a side effect is 0.004. This is a very low probability, so this would be an unusual or perhaps unexpected event, although not strictly impossible.</p>
<p>Suppose that, conversely, the unknown probability of a patient experiencing a side effect from this drug was actually 0.25. Then the probability of observing 2 from 8 patients experiencing a side effect would be 0.31 (<span class="math notranslate nohighlight">\(31\%\)</span>). If this were the case, there would be nothing unusual or unexpected about our observed data.</p>
<p>We do not know which value of <span class="math notranslate nohighlight">\(\pi\)</span> is the true value. But a sensible strategy to obtain a ‘best guess’, or estimate, of <span class="math notranslate nohighlight">\(\pi\)</span>, might be to pick the value which maximises the probability of observing the data that we observed. We will see below that this probability is in fact the likelihood, leading to the concept of maximising the likelihood or maximum likelihood. This is a term that you will encounter frequently in statistics.</p>
<p>Following these ideas, we can extend the table above by considering a finer range of possible values for <span class="math notranslate nohighlight">\(\pi\)</span> between 0 and 1, and plot the probability of observing <span class="math notranslate nohighlight">\(X=2\)</span>, assuming that that value of <span class="math notranslate nohighlight">\(\pi\)</span> were true. This gives the graph below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a range of values for pi</span>
<span class="kc">pi</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.05</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood for each value, given n=8 and x=2</span>
<span class="n">L_pi</span> <span class="o">&lt;-</span> <span class="nf">choose</span><span class="p">(</span><span class="m">8</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="kc">pi</span><span class="o">^</span><span class="m">2</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="kc">pi</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">8-2</span><span class="p">)</span>

<span class="c1"># Plot the output</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">pi</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">L_pi</span><span class="p">)</span>

<span class="c1"># Add a line to indicate the value which yields the highest likelihood</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.b. Likelihood_1_0.png" src="_images/05.b. Likelihood_1_0.png" />
</div>
</div>
<p>We see that <span class="math notranslate nohighlight">\(\pi=0.25\)</span> is the value that leads to the highest probability of observing the data that we did indeed observe (i.e <span class="math notranslate nohighlight">\(X=2\)</span>) so we choose this value as our best guess for <span class="math notranslate nohighlight">\(\pi\)</span>. We will see that this value is called the maximum likelihood estimator. We write <span class="math notranslate nohighlight">\(\hat{\pi} = 0.25\)</span>, where we have added a hat to indicate that this is being viewed as an estimate of an unknown parameter.</p>
<p>The likelihood when <span class="math notranslate nohighlight">\(\pi = 0\)</span> is exactly zero, as is the likelihood when <span class="math notranslate nohighlight">\(\pi = 1\)</span>. This makes sense because these two probabilities would make the observed data impossible - they imply that patients would either <em>never</em> or <em>always</em> experience side effects. Informally, we could say that these values are <em>inconsistent</em> with the data.</p>
<p>Note that, our estimate of the probability of a patient experiencing a side effect is intuitively a sensible one: it is the sample proportion, <span class="math notranslate nohighlight">\(\frac{2}{8}\)</span>.</p>
<p>We will see later on that estimators obtained in this way (by maximising a likelihood) have very nice statistical properties.</p>
</div>
<span id="document-05.c. Likelihood"></span><div class="section" id="the-likelihood">
<h3>5.2 The likelihood<a class="headerlink" href="#the-likelihood" title="Permalink to this headline">¶</a></h3>
<p>The function that we maximised above to find our estimate for the unknown parameter <span class="math notranslate nohighlight">\(\pi\)</span> took the same algebraic appearance as the probability distribution function, evaluated at the value of the observed data. We will see below that this function is called the likelihood. The likelihood looks like a probability distribution function. It has a probabilistic interpretation for any particular value of <span class="math notranslate nohighlight">\(\pi\)</span>: it’s the probability of seeing the observed data assuming that is the true value of <span class="math notranslate nohighlight">\(\pi\)</span>. However, in contrast to the probability distribution function, which is a function of <span class="math notranslate nohighlight">\(x\)</span> and sums to 1 over all possible values of <span class="math notranslate nohighlight">\(x\)</span>, the likelihood function is a function of <span class="math notranslate nohighlight">\(\pi\)</span>. So, for example, this does not sum to 1 over all possible values of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>A general definition of the likelihood is as follows.</p>
<p>For a probability model with parameter <span class="math notranslate nohighlight">\(\theta\)</span>, the likelihood of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> given the observed data <span class="math notranslate nohighlight">\(x\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
L(\theta | x) = P(x | \theta)
\]</div>
<p>On the right hand side of this equation:</p>
<ul class="simple">
<li><p>This is either a probability distribution function or a density function</p></li>
<li><p>If our distribution is discrete, as above, this is: <span class="math notranslate nohighlight">\(P(x | \theta) = P(X=x)\)</span></p></li>
<li><p>If our distribution is continuous, this becomes: <span class="math notranslate nohighlight">\(P(x | \theta) = f(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(x | \theta)\)</span> is a probability statement. It is the probability of seeing the observed data, under the assumed model, assuming that the true parameter value is equal to <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
<p>And on the left hand side of this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L(\theta | x)\)</span> is the likelihood function, often just called the likelihood.</p></li>
</ul>
<p>In an informal sense the likelihood conveys the <em>consistency</em> of different values of the parameter with the observed data.</p>
<p>We often just write the likelihood as <span class="math notranslate nohighlight">\(L(\theta)\)</span>. The additional notation (writing “<span class="math notranslate nohighlight">\(| x\)</span>”) is merely to remind ourselves that the likelihood function involves the observed data, but it is not a function of these: <span class="math notranslate nohighlight">\(x\)</span> is treated as a fixed quantity in the likelihood.</p>
<div class="section" id="example-the-binomial-model">
<h4>5.2.1 Example: the Binomial model<a class="headerlink" href="#example-the-binomial-model" title="Permalink to this headline">¶</a></h4>
<p>Consider a diabetes clinic at which patients present following initial diagnosis. The first line of intervention for diabetes is lifestyle change, and the clinician wants to determine what proportion of patients will respond to this intervention. She decides to conduct a study by following up twenty patients who present to the clinic in one day.</p>
<p><strong>Statistical model:</strong> We assume that a binomial model is appropriate for the number of patients who will respond to lifestyle changes out of the twenty patients in total.</p>
<div class="math notranslate nohighlight">
\[ X \sim binomial(20, \pi) \]</div>
<p><strong>Data:</strong> Out of the twenty patients sampled, she found that twelve of them had responded well after six weeks of recommended lifestyle changes. Our observed data are <span class="math notranslate nohighlight">\(x = 12\)</span>.</p>
<p><strong>Probability distribution function:</strong> As we described before, the likelihood of <span class="math notranslate nohighlight">\(\pi\)</span> given these data is the probability of observing the data for different values for <span class="math notranslate nohighlight">\(\pi\)</span>. Remember the probability distribution function for a binomial distribution of size 20 is</p>
<div class="math notranslate nohighlight">
\[ 
P(X = x|\pi) = {20 \choose x} \pi^{x} (1-\pi)^{20 - x} 
\]</div>
<p>for a given value of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>Likelihood:</strong> The likelihood has this same form but is viewed as a function of <span class="math notranslate nohighlight">\(\pi\)</span>, rather than a function of <span class="math notranslate nohighlight">\(x\)</span>. For our observed data of 12 out of 20 patients,</p>
<div class="math notranslate nohighlight">
\[ 
L(\pi | x = 12) = {20 \choose 12} \pi^{20} (1-\pi)^{20 - 12} 
\]</div>
<p>As before, we can identify the value of <span class="math notranslate nohighlight">\(\pi\)</span> which gives the maximum likelihood by plotting the likelihood for a range of values of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>

<span class="c1"># Define a range of values for pi</span>
<span class="kc">pi</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood for each value, this time given n=20 and x=12</span>
<span class="n">L_pi</span> <span class="o">&lt;-</span> <span class="nf">choose</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="m">12</span><span class="p">)</span><span class="o">*</span><span class="kc">pi</span><span class="o">^</span><span class="m">12</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="kc">pi</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">20-12</span><span class="p">)</span>

<span class="c1"># Plot the output</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">pi</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">L_pi</span><span class="p">)</span>

<span class="c1"># Find the value of pi for which L_pi is highest</span>
<span class="n">pi_max</span> <span class="o">&lt;-</span> <span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)]</span>

<span class="c1"># Add a line to the plot at pi_max</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="n">pi_max</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>

<span class="c1"># Add a title specifying the value of pi_max</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span> <span class="n">pi_max</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.c. Likelihood_2_0.png" src="_images/05.c. Likelihood_2_0.png" />
</div>
</div>
<p>The value which maximises this function is 0.6, the observed sample proportion; we’ll call this value <span class="math notranslate nohighlight">\(\hat{\pi}\)</span> to indicate that it is an estimate of <span class="math notranslate nohighlight">\(\pi\)</span>. Notice that the likelihood for values of <span class="math notranslate nohighlight">\(\pi\)</span> smaller than 0.3 or greater than 0.9 is very small - much smaller than that of values around 0.6 - suggesting that these values are inconsistent with the observed data.</p>
</div>
<div class="section" id="example-the-exponential-model">
<h4>5.2.2 Example: the Exponential model<a class="headerlink" href="#example-the-exponential-model" title="Permalink to this headline">¶</a></h4>
<p>Suppose we wish to estimate how long patients usually wait in reception before their GP appointment. At one practice, a patient walks through the door and the receptionist records the time until they get called through.</p>
<p><strong>Statistical model:</strong> The waiting time in minutes, <span class="math notranslate nohighlight">\(Y\)</span>, is a continuous random variable which must be non-negative. It is common to use an exponential distribution to model waiting times, so we will assume it’s a reasonable choice for this example.</p>
<div class="math notranslate nohighlight">
\[ 
Y \sim Exp(\lambda) 
\]</div>
<p><em>Remember that the mean of this distribution is equal to one over the rate parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, i.e. <span class="math notranslate nohighlight">\(E(Y) = \frac{1}{\lambda}\)</span>.</em></p>
<p><strong>Data:</strong> The receptionist observes that the patient waits for eight minutes and forty-five seconds, so <span class="math notranslate nohighlight">\(y = 8.75\)</span>.</p>
<p><strong>Probability density function:</strong> The PDF for an exponential distribution is</p>
<div class="math notranslate nohighlight">
\[
f_Y(y|\lambda) = \lambda e^{-y\lambda}
\]</div>
<p><strong>Likelihood:</strong> We write down the likelihood for <span class="math notranslate nohighlight">\(\lambda\)</span> based on the exponential PDF above.</p>
<div class="math notranslate nohighlight">
\[ 
L(\lambda | y = 8.75) = \lambda e^{-8.75\lambda}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>

<span class="c1"># Define a range of values for lambda, equating to mean waiting times from 1 to 100 minutes</span>
<span class="n">lam</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0.01</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood for each value, given y=8.75</span>
<span class="n">L_lam</span> <span class="o">&lt;-</span> <span class="n">lam</span><span class="o">*</span><span class="nf">exp</span><span class="p">(</span><span class="m">-8.75</span><span class="o">*</span><span class="n">lam</span><span class="p">)</span>

<span class="c1"># Find the value of lambda for which L_lam is highest</span>
<span class="n">lam_max</span> <span class="o">&lt;-</span> <span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_lam</span><span class="p">)]</span>

<span class="c1"># Plot the likelihood and indicate the maximum value</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">lam</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">L_lam</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="n">lam_max</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">lam_max</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.c. Likelihood_5_0.png" src="_images/05.c. Likelihood_5_0.png" />
</div>
</div>
<p>If we evaluate over a fine enough range of values for <span class="math notranslate nohighlight">\(\lambda\)</span>, we find that the value which maximises this exponential likelihood is equal to <span class="math notranslate nohighlight">\(\frac{1}{8.75}\)</span>, i.e. one over the observed waiting time. This defines an exponential distribution with mean equal to the observed waiting time.</p>
<p>As with the binomial example, the estimate obtained by maximising the likelihood is intuitively sensible based on the data we’ve observed.</p>
</div>
</div>
<span id="document-05.d. Likelihood"></span><div class="section" id="log-likelihood">
<h3>5.3 Log likelihood<a class="headerlink" href="#log-likelihood" title="Permalink to this headline">¶</a></h3>
<p>We have discussed the idea that finding the maximum value of a likelihood gives us sensible estimates for the unknown parameters. For the examples above it is relatively clear from calculating a few values of the likelihood where the maximum lies, but this will not always be the case.</p>
<p>A theoretical result which will come in handy is that a value which maximises the likelihood also maximises the log-transform of the likelihood, or the <em>log-likelihood</em>. This is because the log is a <em>concave</em> function, so when we use it to transform the likelihood, any maximum or minimum stays in the same place on the x-axis. We will denote the log-likelihood by <span class="math notranslate nohighlight">\(l(\theta) = \log(L(\theta))\)</span>.</p>
<p>This result is evident when plotting the transformation of the likelihoods for the binomial and exponential distributions.</p>
<p>For the binomial example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">12</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1"># likelihood L(pi)</span>
<span class="kc">pi</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span>
<span class="n">L_pi</span> <span class="o">&lt;-</span> <span class="nf">choose</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="m">12</span><span class="p">)</span><span class="o">*</span><span class="kc">pi</span><span class="o">^</span><span class="m">12</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="kc">pi</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">20-12</span><span class="p">)</span>
<span class="n">pi_max</span> <span class="o">&lt;-</span> <span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)]</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">pi</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">L_pi</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="n">pi_max</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span> <span class="n">pi_max</span><span class="p">))</span>

<span class="c1"># log-likelihood l(pi)</span>
<span class="n">l_pi</span> <span class="o">&lt;-</span> <span class="nf">log</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="kc">pi</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">l_pi</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_pi</span><span class="p">)],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span> <span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_pi</span><span class="p">)]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.d. Likelihood_1_0.png" src="_images/05.d. Likelihood_1_0.png" />
</div>
</div>
<p>For the exponential example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">12</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1"># likelihood L(beta)</span>
<span class="n">lam</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0.01</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span>
<span class="n">L_lam</span> <span class="o">&lt;-</span> <span class="n">lam</span><span class="o">*</span><span class="nf">exp</span><span class="p">(</span><span class="m">-8.75</span><span class="o">*</span><span class="n">lam</span><span class="p">)</span>
<span class="n">lam_max</span> <span class="o">&lt;-</span> <span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_lam</span><span class="p">)]</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">lam</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">L_lam</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="n">lam_max</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">lam_max</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>

<span class="c1"># log-likelihood l(beta)</span>
<span class="n">l_lam</span> <span class="o">&lt;-</span> <span class="nf">log</span><span class="p">(</span><span class="n">L_lam</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">lam</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">l_lam</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span> <span class="o">=</span> <span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_lam</span><span class="p">)],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_lam</span><span class="p">)],</span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05.d. Likelihood_3_0.png" src="_images/05.d. Likelihood_3_0.png" />
</div>
</div>
<div class="section" id="why-use-the-log-likelihood">
<h4>5.3.1 Why use the log likelihood?<a class="headerlink" href="#why-use-the-log-likelihood" title="Permalink to this headline">¶</a></h4>
<p>Log-transformed likelihoods are generally “better-behaved” and easier to work with than the original form. Remember the rules of logs for products and powers - we’ll see in the next section how these make computation with the log-likelihood very convenient.</p>
</div>
</div>
<span id="document-05.e. Likelihood"></span><div class="section" id="finding-the-mle">
<h3>5.4 Finding the MLE<a class="headerlink" href="#finding-the-mle" title="Permalink to this headline">¶</a></h3>
<p>So far we have obtained the maximum likelihood estimate (MLE) by plotting the likelihood for different parameter values and looking for the value which yields the maximum. Of course, the estimate obtained in this way depends on how many parameter values we evaluate.</p>
<p>A more formal way is to determine the location of that maximal point algebraically, from the likelihood function itself. In this way, we can directly obtain the general form for the MLE in terms of the data.</p>
<p>This is where the log-likelihood comes into its own; we know that a value which maximises the log-likelihood also maximises the likelihood, and the impact of logs on products and powers make the algebra much simpler.</p>
<p>We find the maximum likelihood estimator of a parameter from the log-likelihood function through the following steps:</p>
<div class="alert alert-success">
<p><b> Method for finding MLEs:</b></p>
<ol class="simple">
<li><p>Obtain the derivative of the log-likelihood: <span class="math notranslate nohighlight">\(\frac{d l(\theta \mid {x})}{d \theta}\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\frac{d l(\theta \mid {x})}{d \theta}=0\)</span> and solve for <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>Verify that it is a maximum by showing that the second derivative <span class="math notranslate nohighlight">\(\frac{d ^2 l(\theta \mid  {x})}{d \theta ^2 }\)</span> is negative when the MLE is substituted for <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</div>
<div class="section" id="binomial-model">
<h4>5.4.1 Binomial model<a class="headerlink" href="#binomial-model" title="Permalink to this headline">¶</a></h4>
<p>We will derive the MLE for the binomial example described earlier. In general, the likelihood given observed data of <span class="math notranslate nohighlight">\(x\)</span> responders out of <span class="math notranslate nohighlight">\(n\)</span> patients is</p>
<div class="math notranslate nohighlight">
\[ 
L(\pi|x) = {n \choose x} \pi^{x} (1-\pi)^{n - x}
\]</div>
<p>and so the log-likelihood is</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{align*}
l(\pi|x) &amp; = \log\left({n \choose x} \pi^{x} (1-\pi)^{n - x}\right) \\ 
&amp; = \log {n \choose x} + x \log \pi + (n-x)\log (1-\pi)
\end{align*}
\end{split}\]</div>
<p>We can now obtain the maximum likelihood estimate from this function.</p>
<p><strong>Step 1</strong>: Differentiate the log-likelihood with respect to our parameter <span class="math notranslate nohighlight">\(\pi\)</span></p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{d l \left( \pi \mid x \right)}{d \pi} =  \frac{x}{\pi}  -\frac{(n-x)}{(1-\pi)}
\end{equation}
\]</div>
<p><strong>Step 2</strong>: We set the derivative equal to zero and solve for <span class="math notranslate nohighlight">\(\pi\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
0 &amp;=  \frac{x}{\hat{\pi}}  -\frac{(n-x)}{(1-\hat{\pi})} \\
\frac{(n-x)}{(1-\hat{\pi})} &amp;=  \frac{x}{\hat{\pi}}   \\
(n-x)\hat{\pi} &amp;=  x(1-\hat{\pi}) \\
n\hat{\pi}-x\hat{\pi} &amp;=  x-x\hat{\pi} \\
\hat{\pi} &amp;=  \frac{x}{n} \\
\end{align*}
\end{split}\]</div>
<p>Having solved the equation, we get that the maximum likelihood estimator for <span class="math notranslate nohighlight">\(\pi\)</span> is <span class="math notranslate nohighlight">\(\hat{\pi} =  \frac{x}{n}\)</span> (note that we put a hat to indicate that it is an estimator).</p>
<p>There is one thing left for us to check: we have found that <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span> is the point where the derivative of the log-likelihood is zero, but that could mean that it is a maximum or a minimum of the log-likelihood function. To verify that this is indeed a maximum, we need to compute the second derivative of the log-likelihood and check that it takes a negative value when <span class="math notranslate nohighlight">\({\pi} =  \frac{x}{n}\)</span>.</p>
<p><strong>Step 3</strong>: Find the second derivative:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{d l^2 \left( \pi \mid x \right)}{d \pi ^2} =  -\frac{x}{\pi^2}  -\frac{(n-x)}{(1-\pi)^2}
\end{equation}
\]</div>
<p>This second derivative must be negative if we plug in <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span> for <span class="math notranslate nohighlight">\({\pi}\)</span>. Both fractions on the right hand side have a squared number in the denominator which is positive, so we only have to think about the numerators. We have that <span class="math notranslate nohighlight">\(-x \leq 0 \)</span> since <span class="math notranslate nohighlight">\(x \geq 0\)</span> and also <span class="math notranslate nohighlight">\(-(n-x) \leq 0 \)</span> since <span class="math notranslate nohighlight">\(n \geq x \geq 0\)</span>. Therefore, the value <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span> is indeed a maximum.</p>
<p>Thus the MLE for <span class="math notranslate nohighlight">\(\pi\)</span> is <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span>.</p>
</div>
<div class="section" id="exponential-model">
<h4>5.4.2 Exponential model<a class="headerlink" href="#exponential-model" title="Permalink to this headline">¶</a></h4>
<p>The likelihood function for the exponential example is</p>
<div class="math notranslate nohighlight">
\[
L(\lambda | y) = \lambda e^{-\lambda y}
\]</div>
<p>Therefore we have</p>
<div class="math notranslate nohighlight">
\[ 
l(\lambda | y) = \log \left( \lambda e^{-\lambda y} \right) = \log \lambda - y \lambda 
\]</div>
<p><strong>Step 1</strong>: Differentiate the log-likelihood with respect to our parameter <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{d l \left( \lambda \mid y \right)}{d \lambda} = \frac{1}{\lambda} - y
\]</div>
<p><strong>Step 2</strong>: Set the derivative to zero and solve for <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{align*}
\frac{1}{\lambda} - y &amp;= 0 \\
\hat{\lambda} &amp; = \frac{1}{y} 
\end{align*}
\end{split}\]</div>
<p><strong>Step 3</strong>: Verify that this is a maximum rather than a minimum by considering the second derivative</p>
<div class="math notranslate nohighlight">
\[ 
\frac{d l^2 \left(\lambda \mid y \right)}{d \lambda ^2} = -\frac{1}{\lambda^2}
\]</div>
<p>This is negative for any value of <span class="math notranslate nohighlight">\(\lambda\)</span>. So the MLE for <span class="math notranslate nohighlight">\(\lambda\)</span> is <span class="math notranslate nohighlight">\(\hat{\lambda} = \frac{1}{y}\)</span>, one over the observed waiting time.</p>
</div>
</div>
<span id="document-05.f. Likelihood"></span><div class="section" id="summary">
<h3>5.5 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>Likelihood is a fundamental concept in statistical inference. In this session we have introduced the definition of likelihood and demonstrated how it can be used to estimate an unknown parameter, through maximum likelihood estimation. For two examples, we have seen that estimates obtained by MLE are intuitively sensible for the parameter of interest and have derived them algebraically via the log-likelihood.</p>
<p>In the next session, we will find out about the specific mathematical properties which make the MLE a “good” estimator, and extend to the situation where our data consist of more than one observation.</p>
</div>
</div>
</div>
<span id="document-06.a. Maximum Likelihood"></span><div class="section" id="maximum-likelihood">
<h2>6. Maximum Likelihood<a class="headerlink" href="#maximum-likelihood" title="Permalink to this headline">¶</a></h2>
<p>In inferential statistics, the problem we are often faced is this: we have collected some data, and we have a statistical model for how this data was generated. However, we do not know what the values of the parameters of this model are. We need to find a way to estimate these parameters. In the previous session, we were introduced to the likelihood function, which measures how consistent different values of the parameter are with the data that we have observed. Extending this concept, we used calculus to obtain the maximum likelihood estimator for the parameter.</p>
<p>So far, we have only looked at examples where our data consists of one observation - surely, this is not a sufficient sample size!</p>
<p>We will now consider the more realistic scenario, where we have a random sample of observations from a particular distribution - in this case, we say that the sample is independently and identically distributed (i.i.d).</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>Derive the likelihood and log-likelihood functions given an i.i.d. sample</p></li>
<li><p>Derive maximum likelihood estimator from single and multi-parameter distributions given an i.i.d. sample</p></li>
<li><p>Describe the main properties of MLEs</p></li>
</ul>
</div><p>The following subsections define the likelihood function for <span class="math notranslate nohighlight">\(n\)</span> i.i.d observations and describe the process of obtaining the maximum likelihood estimator in this setting. The session ends with a demonstration of some important properties of maximum likelihood estimators.</p>
<div class="toctree-wrapper compound">
<span id="document-06.b. Maximum Likelihood"></span><div class="section" id="likelihood-with-independent-observations">
<h3>6.1 Likelihood with independent observations<a class="headerlink" href="#likelihood-with-independent-observations" title="Permalink to this headline">¶</a></h3>
<p>Suppose that the observed data consiste of a sample of <span class="math notranslate nohighlight">\(n\)</span> observations. If these observations are independent, then the joint likelihood function from these <span class="math notranslate nohighlight">\(n\)</span> observations has a very convenient form; it is the product of the likelihood from each observation.</p>
<p>Suppose that the random variables <span class="math notranslate nohighlight">\(X_1,..., X_n\)</span> are i.i.d., and that our observed data are <span class="math notranslate nohighlight">\(\mathbf{x} = \left\{ x_1, x_2, ..., x_n \right\}\)</span>. Then the likelihood function is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L \left( \theta \mid \mathbf{x} \right) &amp;=  L\left( \theta \mid x_1 \right) L\left( \theta \mid x_2 \right) ...  L \left( \theta \mid x_n \right) \\
 &amp;= \prod_{i=1}^n  L\left( \theta \mid x_i \right).
\end{align*}
\end{split}\]</div>
<p>Recall that we often prefer to work with the log-likelihood function, as it simplifies the algebra when it comes to finding the MLE. The log-likelihood function for <span class="math notranslate nohighlight">\(n\)</span> independent observations is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
l \left( \theta \mid \mathbf{x} \right) 
 &amp;= log \prod_{i=1}^n L\left( \theta \mid x_i \right) \\
  &amp;= \sum_{i=1}^n log L\left( \theta \mid x_i \right) \\
   &amp;= \sum_{i=1}^n l\left( \theta \mid x_i \right) 
\end{align*}
\end{split}\]</div>
<p>Finding the MLE involves the same three steps as we saw in the previous session, but the log-likelihood function is now a joint function for the <span class="math notranslate nohighlight">\(n\)</span> observations:</p>
<div class="alert alert-success">
<p><b> Method for finding MLEs:</b></p>
<ol class="simple">
<li><p>Obtain the derivative of the log-likelihood: <span class="math notranslate nohighlight">\(\frac{d l(\theta \mid \mathbf{x})}{d \theta}\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\frac{d l(\theta \mid \mathbf{x})}{d \theta}=0\)</span> and solve for <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>Verify that it is a maximum by showing that the second derivative <span class="math notranslate nohighlight">\(\frac{d ^2 l(\theta \mid  \mathbf{x})}{d \theta ^2 }\)</span> is negative when the MLE is substituted for <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</div>
<div class="section" id="example-exponential-distribution">
<h4>6.1.1 Example: Exponential distribution<a class="headerlink" href="#example-exponential-distribution" title="Permalink to this headline">¶</a></h4>
<p>Recall the example from the previous session, investigating the time that patients wait until their GP appointment in a particular practice. The receptionist records the time that elapses between when a patient walks through the door, and when they are called through for their appointment for a random sample of 8 people. These times (in minutes) are: 8.75, 10.20, 15.29, 7.89, 7.04, 12.04, 19.04, 17.50.</p>
<p>As a reminder, we can model the waiting time until a specific event using the exponential distribution with parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, which has a probability density function given by:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}  
f _X\left(x \mid \lambda \right)=\lambda e^{-x\lambda} , x &gt; 0, \lambda &gt; 0  
\end{equation}
\]</div>
<blockquote>
<div><p>Recall that the mean of this distribution is equal to one over the rate parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, i.e. <span class="math notranslate nohighlight">\(E(X) = \frac{1}{\lambda}\)</span>.</p>
</div></blockquote>
<p>We have that the log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\log L\left( \lambda \mid \mathbf{x} \right) &amp;= \sum_{i=1}^n \log  L(\lambda \mid x_i) \\ 
&amp;= \sum_{i=1}^n \log \left( \lambda e^{-x_i \lambda } \right) \\
&amp;= \sum_{i=1}^n \log \lambda -x_i\lambda \\  
&amp;= n \log \lambda -\lambda \sum_{i=1}^n x_i 
\end{align}
\end{split}\]</div>
<p>We can make a plot of this log-likelihood, using the data from our example with eight observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>

<span class="c1">#six independent observations for waiting times </span>
<span class="n">obs</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">8.75</span><span class="p">,</span> <span class="m">10.20</span><span class="p">,</span> <span class="m">15.29</span><span class="p">,</span> <span class="m">7.89</span><span class="p">,</span> <span class="m">7.04</span><span class="p">,</span> <span class="m">12.04</span><span class="p">,</span> <span class="m">19.04</span><span class="p">,</span> <span class="m">17.50</span><span class="p">)</span>
<span class="n">n</span> <span class="o">&lt;-</span> <span class="nf">length</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

<span class="c1">#possible values for the parameter lambda</span>
<span class="n">lambda</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>

<span class="c1">#plot the log-likelihood</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">lambda</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="n">lambda</span><span class="p">)</span> <span class="o">-</span> <span class="n">lambda</span><span class="o">*</span><span class="nf">sum</span><span class="p">(</span><span class="n">obs</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span> <span class="nf">expression</span><span class="p">(</span><span class="n">lambda</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-100</span><span class="p">,</span><span class="m">0</span><span class="p">),</span>
     <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Log-likelihood&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06.b. Maximum Likelihood_2_0.png" src="_images/06.b. Maximum Likelihood_2_0.png" />
</div>
</div>
<p>Graphically, we observe that the maximum is between 0 and 0.25. We will use the three steps, as before, to derive the MLE algebraically:</p>
<p><strong>Step1</strong>: Taking the derivative of the log-likelihood with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{d log L\left( \lambda \mid x_1 ,..., x_n \right) }{d \lambda} = \frac{n}{\lambda}- \sum_{i=1}^n x_i 
\end{equation}
\]</div>
<p><strong>Step2:</strong> Set the derivative equal to zero and solve for <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
0 &amp;= \frac{n}{\lambda}- \sum_{i=1}^n x_i \\
\hat{\lambda} &amp;= \frac{n }{\sum_{i=1}^n x_i} = \frac{1}{\bar{x}}
\end{align*}
\end{split}\]</div>
<p>The MLE is <span class="math notranslate nohighlight">\(\hat{\lambda}= \frac{1}{\bar{x}}\)</span>. And to check that this provides a maximum, we go on to the next step:</p>
<p><strong>Step3:</strong> Find the second derivative:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{d l^2 \left( \lambda \mid \boldsymbol{x} \right)}{d \lambda ^2} 
= - \frac{n}{\lambda^2}
\end{equation}
\]</div>
<p>When <span class="math notranslate nohighlight">\({\lambda}=\frac{1}{\bar{x}}\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
 \frac{d l^2 \left( \lambda \mid \boldsymbol{x} \right)}{d \lambda ^2}  
 &amp;=-n \bar{x}^2
 \end{align}
\]</div>
<p>which is negative. This verifies that we found the maximum likelihood estimate.</p>
<p>Going back to our example of eight patients waiting for their GP appointment, the maximum likelihood estimate <span class="math notranslate nohighlight">\(\lambda\)</span> is given by one over the average of the eight waiting times:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="m">1</span><span class="o">/</span><span class="nf">mean</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.0818414322250639</div></div>
</div>
<p>We have that <span class="math notranslate nohighlight">\(\hat{\lambda}=0.0818\)</span> minutes.</p>
</div>
<div class="section" id="example-normal-distribution">
<h4>6.1.2 Example: Normal distribution<a class="headerlink" href="#example-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>We will now consider the normal distribution. Remember that the normal distribution has two parameters, <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We will first obtain the MLE for <span class="math notranslate nohighlight">\(\mu\)</span> (treating <span class="math notranslate nohighlight">\(\sigma^2\)</span> as a constant), and in the practical, we will obtain the MLE for <span class="math notranslate nohighlight">\(\sigma^2\)</span> (treating <span class="math notranslate nohighlight">\(\mu\)</span> as a constant).</p>
<p>Recall that normal distribution has probability density function given by*:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}  
f_X \left( x \mid \mu, \sigma^2 \right)= (2 \pi \sigma^2)^{-\frac{1}{2}} \exp \left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}
\end{equation}
\]</div>
<p>(* note that the notation here is slightly different to section 3. Here we are more prescriptive; on the left hand side the notation says that the random variable <span class="math notranslate nohighlight">\(X\)</span> is sampled from parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>, where the distribution is defined on the right hand side. Both versions of notation are acceptible. Another notation style is to use a semi-colon instead, ie. <span class="math notranslate nohighlight">\(f_X( x ; \mu, \sigma^2)\)</span>).</p>
<p>We have that the log-likelihood given an i.i.d. sample of size <span class="math notranslate nohighlight">\(n\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
l \left(\mu, \sigma^2 \mid  \boldsymbol{x}  \right) &amp;=  \sum_{i=1}^n \log \left\{ (2 \pi \sigma^2)^{-\frac{1}{2}} \exp \left\{-\frac{(x_i-\mu)^2}{2\sigma^2} \right\} \right\} \\
&amp;= \sum_{i=1}^n \left\{ \log (2 \pi \sigma^2)^{-\frac{1}{2}}+ \log \exp  \left\{-\frac{(x_i-\mu)^2}{2\sigma^2} \right\}  \right\} \\
&amp;= \sum_{i=1}^n \left\{ -\frac{1}{2} \log (2 \pi \sigma^2) - \frac{(x_i-\mu)^2}{2\sigma^2}  \right\} \\
&amp;=  {-\frac{n}{2}}\log (2 \pi \sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2 
\end{align}
\end{split}\]</div>
<p>We will first find the MLE for the parameter <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><strong>Step1</strong>: Take the derivative of the log-likelihood with respect to  <span class="math notranslate nohighlight">\(\mu\)</span>. Note that this requires use of the chain rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}  
\frac{d l \left(\mu, \sigma^2 \mid  \mathbf{x}  \right) }{d \mu}
&amp;=  -\frac{2}{2\sigma^2}(-1) \sum_{i=1}^n (x_i-\mu) \\
&amp;=  \frac{ \sum_{i=1}^n (x_i-\mu)}{\sigma^2} \\
&amp;=  \frac{ \sum_{i=1}^n x_i-n\mu}{\sigma^2}
\end{align}
\end{split}\]</div>
<p><strong>Step2:</strong> Setting the derivative equal to zero and solving for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}  
0 &amp;=  \frac{ \sum_{i=1}^n x_i-n\mu}{\sigma^2} \\
\end{align}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>, we have that:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}  
0 = \sum_{i=1}^n x_i-n\mu 
\end{equation}
\]</div>
<div class="math notranslate nohighlight">
\[
\begin{equation}  
\hat{\mu} =\frac{ \sum_{i=1}^n x_i}{n} = \bar{x}
\end{equation}
\]</div>
<p>We have that the MLE for <span class="math notranslate nohighlight">\(\mu\)</span> is the sample mean, <span class="math notranslate nohighlight">\(\bar{x}\)</span>.</p>
<p><strong>Step3:</strong> Find the second derivative:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}  
\frac{d^2 l \left(\mu, \sigma^2 \mid  \mathbf{x}  \right) }{d \mu^2 }
&amp;=  -\frac{n}{\sigma^2},
\end{align}
\]</div>
<p>since both <span class="math notranslate nohighlight">\(n&gt;0\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 &gt;0\)</span>, we have that the second derivative is negative, verifying that we have found the maximum.</p>
<p>In the practical, we will find the MLE for <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
</div>
</div>
<span id="document-06.c. Maximum Likelihood"></span><div class="section" id="properties-of-maximum-likelihood-estimators">
<h3>6.2 Properties of maximum likelihood estimators<a class="headerlink" href="#properties-of-maximum-likelihood-estimators" title="Permalink to this headline">¶</a></h3>
<p>Maximum likelihood estimators can be shown to have some very useful properties. In particular, there are some very important asymptotic properties (properties that we observe as the sample size of our data gets very very large).</p>
<p>To explore these properties, have a look at the simulation below. We generate a sample of size 8 from the exponential distribution where <span class="math notranslate nohighlight">\(\lambda=12.22\)</span>. The MLE is calculated from this the observed mean of the sample. We repeat this 100 times, and we plot a histogram of the 100 MLEs that we obtain.</p>
<p>Change the sample size, <span class="math notranslate nohighlight">\(n\)</span>, to larger numbers and see what you notice about the histogram.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">8</span>  <span class="c1">#  make this sample size bigger, and see what happens to the histogram! </span>

<span class="c1"># MLEs will be stored in this vector</span>
<span class="n">mle</span> <span class="o">&lt;-</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">100</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">){</span>
  <span class="c1"># Generate a sample of size n from an exponential distribution with lambda=0.0818</span>
  <span class="n">sample</span> <span class="o">&lt;-</span> <span class="nf">rexp</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="m">0.0818</span><span class="p">)</span>
  <span class="c1"># Calculate the MLE (the reciprocal mean of the sample) and store it </span>
  <span class="n">mle</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;-</span> <span class="m">1</span><span class="o">/</span><span class="nf">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Plot a histogram of the 100 MLEs </span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">mle</span><span class="p">,</span> <span class="n">breaks</span><span class="o">=</span><span class="m">20</span><span class="p">,</span> 
     <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">0.3</span><span class="p">),</span> 
     <span class="n">main</span><span class="o">=</span><span class="s">&quot;Histogram of MLE&quot;</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;MLE&quot;</span><span class="p">)</span>
<span class="c1"># Add red line to indicate true lambda </span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">12.22</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06.c. Maximum Likelihood_1_0.png" src="_images/06.c. Maximum Likelihood_1_0.png" />
</div>
</div>
<p>You may notice that, as <span class="math notranslate nohighlight">\(n\)</span> becomes large, the distribution of the MLE becomes more and more concentrated around the true value, and the histrogram appears to look more bell-shaped.</p>
<p>Suppose we denote the parameter of interest as <span class="math notranslate nohighlight">\(\theta\)</span> and its MLE as <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. The tabs below show some important properties of MLEs.</p>
<div class="tabbed-set docutils">
<input checked="checked" id="ddb9a899-d013-4e63-87b5-10246a6cc6e6" name="81e92a3d-d388-41f6-8719-4bbe8aa1c1b4" type="radio">
</input><label class="tabbed-label" for="ddb9a899-d013-4e63-87b5-10246a6cc6e6">
Bias</label><div class="tabbed-content docutils">
<div class="alert alert-success">
<p>The MLE is <strong>asymptotically unbiased</strong>, i.e. on average we obtain the correct answer as samples become large.</p>
<p><span class="math notranslate nohighlight">\(\mathbb{E}(\hat{\theta}) \rightarrow \theta\)</span> as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>.</p>
</div>
</div>
<input id="ae7f4157-0805-40c3-889a-058b701944c3" name="81e92a3d-d388-41f6-8719-4bbe8aa1c1b4" type="radio">
</input><label class="tabbed-label" for="ae7f4157-0805-40c3-889a-058b701944c3">
Consistency</label><div class="tabbed-content docutils">
<div class="alert alert-success">
<p>The MLE is <strong>consistent</strong>, i.e. the MLE converges towards the correct answer as samples become large.</p>
<p><span class="math notranslate nohighlight">\(\hat{\theta} \rightarrow \theta\)</span> in probability as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>.</p>
</div>
</div>
<input id="7051f3cc-77ce-47fe-a7cb-dc1478308e8c" name="81e92a3d-d388-41f6-8719-4bbe8aa1c1b4" type="radio">
</input><label class="tabbed-label" for="7051f3cc-77ce-47fe-a7cb-dc1478308e8c">
Normality</label><div class="tabbed-content docutils">
<div class="alert alert-success">
<p>The MLE is <strong>asymptotically normal</strong>.</p>
<p><span class="math notranslate nohighlight">\(\hat{\theta} \sim N(\theta ,Var(\hat{\theta} ))\)</span> as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>.</p>
<p>The approximate normal distribution of the MLE means that confidence intervals and hypothesis tests for the parameters can be constructed easily.</p>
</div>
</div>
<input id="1a549384-b113-4437-85d2-96ce89a776f0" name="81e92a3d-d388-41f6-8719-4bbe8aa1c1b4" type="radio">
</input><label class="tabbed-label" for="1a549384-b113-4437-85d2-96ce89a776f0">
Efficiency</label><div class="tabbed-content docutils">
<div class="alert alert-success">
<p>The MLE is <strong>asymptotically efficient</strong>.</p>
<p><span class="math notranslate nohighlight">\(Var(\hat{\theta})\)</span> is the smallest variance amongst all unbiased estimators as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>.</p>
<p>This means that, for example, confidence intervals constructed around the MLE will be the narrowest amongst confidence intervals of estimators that are linear and unbiased.</p>
</div>
</div>
<input id="77ec6dc2-54cb-4a3b-b357-d3fc85a7ebee" name="81e92a3d-d388-41f6-8719-4bbe8aa1c1b4" type="radio">
</input><label class="tabbed-label" for="77ec6dc2-54cb-4a3b-b357-d3fc85a7ebee">
Transformation invariance</label><div class="tabbed-content docutils">
<div class="alert alert-success">
<p>The MLE is <strong>transformation invariant</strong>.</p>
<p>If <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the MLE for <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(g(\hat{\theta})\)</span> is the MLE of <span class="math notranslate nohighlight">\(g(\theta)\)</span> for any function <span class="math notranslate nohighlight">\(g\)</span>.</p>
</div>
</div>
</div>
<p>You might question to what extent these asymptotic properties are useful in practical examples where the sample size is relatively small.</p>
<p>Further, in the cases that we have covered so far, it is fairly straightforward to compute the likelihood function and to find the value that maximizes it, but in many situations, this will be a complex task that requires numerical approaches.</p>
<p>In the subsequent sessions on Bayesian Statistics, we will see a different paradigm for making inference which can address some of these issues.</p>
</div>
<span id="document-06.d. Maximum Likelihood"></span><div class="section" id="summary">
<h3>6.3 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>We now know how to obtain the likelihood and log-likelihood functions when you have an i.i.d. sample of observations. We can then obtain the maximum likelihood estimators of the parameters of the distribution. The MLE is an important tool as it has a number of important asymptotic properties, as we demonstrated using a simulation in R. Finally, we introduced the idea of a log-likelihood ratio, which is a way of comparing estimates of a parameter with the maximum likelihood estimate.</p>
<p>You may be wondering how you might measure the precision of your estimator. We will return to this question in our session about confidence intervals.</p>
<p>Note that the maximum likelihood estimator, and confidence intervals, are tools from the “frequentist” or “classical” approach to statistics. In later sessions, you will meet the Bayesian approach to statistics, where the Likelihood will also play an important role.</p>
</div>
<span id="document-06.e. Maximum Likelihood"></span><div class="section" id="appendix-additional-reading">
<h3>Appendix: Additional Reading<a class="headerlink" href="#appendix-additional-reading" title="Permalink to this headline">¶</a></h3>
<p>This appendix section contains additional information which will deepen your understanding. However, it is not examinable and is completely optional reading.</p>
<div class="section" id="a1-log-likelihood-ratios">
<h4>A1: Log-likelihood ratios<a class="headerlink" href="#a1-log-likelihood-ratios" title="Permalink to this headline">¶</a></h4>
<p>So far we have used the MLE to find an estimate of a parameter. Typically, the estimate is computed from a sample, so if we were to <em>sample again</em> we would expect the estimate to vary a little. But what about others values; what steps are involved to compare other estimates to the MLE? How much would the sample estimates vary? The <strong>log-likelihood ratio (LLR)</strong> is a useful approach. The LLR gives a measure of consistency of a value of <span class="math notranslate nohighlight">\(\theta\)</span> relative to the most likely value.</p>
<p>The LLR is defined as,</p>
<div class="math notranslate nohighlight">
\[ log\frac{L(\theta)}{L(\hat\theta)} \]</div>
<p>where <span class="math notranslate nohighlight">\(L(\theta)\)</span> is the likelihood evaluated at any value, and <span class="math notranslate nohighlight">\(L(\hat\theta)\)</span> is the likelihood evaluated at the MLE.</p>
<p>Alternatively, and especially when evaluating in software, the following is used;</p>
<div class="math notranslate nohighlight">
\[ LLR(\theta) = l(\theta)-l(\hat\theta) \]</div>
<p>Let’s explore the LLR and its properties with an small example.</p>
<p>For a simple coin-flipping example, from a trial of 10 coin-flips, 4 were heads (<span class="math notranslate nohighlight">\(X=4\)</span>) and the remainder were tails. From this experiment, we know that the MLE (ie. <span class="math notranslate nohighlight">\(\hat\theta\)</span>) is 0.4, but we also want to use the LLR to compare other estimates of <span class="math notranslate nohighlight">\(\theta\)</span>. Looking at the LLR graphically we note that the LLR is a negative value, the further away from zero the less consistent the parameter value with to the MLE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
 <span class="n">x</span> <span class="o">&lt;-</span> <span class="m">4</span><span class="p">;</span> <span class="n">n</span><span class="o">&lt;-</span><span class="m">10</span>
 <span class="n">theta_vals</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">0.01</span><span class="p">)</span>
 <span class="n">LLR</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">x</span><span class="o">/</span><span class="n">n</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="nf">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR</span><span class="p">,</span><span class="n">type</span><span class="o">=</span><span class="s">&#39;l&#39;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">)</span>
<span class="c1"># add additional things</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="nf">length</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">)),</span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06.e. Maximum Likelihood_1_0.png" src="_images/06.e. Maximum Likelihood_1_0.png" />
</div>
</div>
<p>In the above experiment, for a <em>fair</em> coin it would not be unusual to observe 4 heads from 10 trials. The MLE is 0.4 but we <em>know</em> for a fair coin that the true parameter <span class="math notranslate nohighlight">\(\pi\)</span> will be 0.5. The MLE is a <em>sample</em> of the distribution for <span class="math notranslate nohighlight">\(\pi\)</span>. So let’s zoom in on the figure previously generated;</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
 <span class="n">x</span> <span class="o">&lt;-</span> <span class="m">4</span><span class="p">;</span> <span class="n">n</span><span class="o">&lt;-</span><span class="m">10</span>
 <span class="n">theta_vals</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">0.01</span><span class="p">)</span>
 <span class="n">LLR</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">x</span><span class="o">/</span><span class="n">n</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="nf">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR</span><span class="p">,</span><span class="n">type</span><span class="o">=</span><span class="s">&#39;l&#39;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-5</span><span class="p">,</span><span class="m">0.01</span><span class="p">))</span>
<span class="c1"># add additional things</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="nf">length</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">)),</span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">rep</span><span class="p">(</span><span class="m">0.5</span><span class="p">,</span><span class="m">2</span><span class="p">),</span><span class="n">y</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="m">0.01</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span><span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06.e. Maximum Likelihood_3_0.png" src="_images/06.e. Maximum Likelihood_3_0.png" />
</div>
</div>
<p>We can see in the figure (red line) that when <span class="math notranslate nohighlight">\(\theta=0.5\)</span> the LLR is very close to 0. Values of <span class="math notranslate nohighlight">\(\theta\)</span> further away from the MLE than 0.5 will have a even lower LLR. So we can make qualitative statements using the LLR in relation to the MLE.</p>
<p>Let’s increase the sample size and observe what happens to the LLR.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
 <span class="n">ns</span><span class="o">&lt;-</span><span class="nf">c</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="m">20</span><span class="p">,</span><span class="m">40</span><span class="p">,</span><span class="m">80</span><span class="p">,</span><span class="m">160</span><span class="p">)</span>
<span class="c1"># assume we have an &#39;unfair coin&#39; with heads being more likely than tails</span>
<span class="c1"># rather than taking a random sample, assume a sample consistent with the null hypothesis - this is so the MLE remains 0.5</span>
 <span class="n">x1</span><span class="o">&lt;-</span><span class="nf">round</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="o">*</span><span class="m">0.4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span>
 <span class="n">x2</span><span class="o">&lt;-</span><span class="nf">round</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="m">2</span><span class="p">]</span><span class="o">*</span><span class="m">0.4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span>
 <span class="n">x3</span><span class="o">&lt;-</span><span class="nf">round</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="m">3</span><span class="p">]</span><span class="o">*</span><span class="m">0.4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span>
 <span class="n">x4</span><span class="o">&lt;-</span><span class="nf">round</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="m">4</span><span class="p">]</span><span class="o">*</span><span class="m">0.4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span> 
 <span class="n">x5</span><span class="o">&lt;-</span><span class="nf">round</span><span class="p">(</span><span class="n">ns</span><span class="p">[</span><span class="m">5</span><span class="p">]</span><span class="o">*</span><span class="m">0.4</span><span class="p">,</span><span class="m">0</span><span class="p">)</span> 
 <span class="n">theta_vals</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">0.01</span><span class="p">)</span>
 <span class="n">LLR01</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="n">x1</span><span class="o">/</span><span class="n">ns</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="n">LLR02</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="n">x2</span><span class="o">/</span><span class="n">ns</span><span class="p">[</span><span class="m">2</span><span class="p">],</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="n">LLR03</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">3</span><span class="p">],</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">3</span><span class="p">],</span><span class="n">x3</span><span class="o">/</span><span class="n">ns</span><span class="p">[</span><span class="m">3</span><span class="p">],</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="n">LLR04</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x4</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">4</span><span class="p">],</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x4</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">4</span><span class="p">],</span><span class="n">x4</span><span class="o">/</span><span class="n">ns</span><span class="p">[</span><span class="m">4</span><span class="p">],</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="n">LLR05</span> <span class="o">&lt;-</span> <span class="nf">dbinom</span><span class="p">(</span><span class="n">x5</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">5</span><span class="p">],</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span><span class="o">-</span><span class="nf">dbinom</span><span class="p">(</span><span class="n">x5</span><span class="p">,</span><span class="n">ns</span><span class="p">[</span><span class="m">5</span><span class="p">],</span><span class="n">x5</span><span class="o">/</span><span class="n">ns</span><span class="p">[</span><span class="m">5</span><span class="p">],</span><span class="n">log</span><span class="o">=</span><span class="bp">T</span><span class="p">)</span>
 <span class="nf">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR01</span><span class="p">,</span><span class="n">type</span><span class="o">=</span><span class="s">&#39;l&#39;</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-5</span><span class="p">,</span><span class="m">0.01</span><span class="p">))</span>
<span class="c1"># compare to large sample sizes</span>
 <span class="nf">lines</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR02</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
 <span class="nf">lines</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR03</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">1.5</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR04</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">LLR05</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">,</span><span class="n">lwd</span><span class="o">=</span><span class="m">2.5</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">theta_vals</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="nf">length</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">)),</span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">rep</span><span class="p">(</span><span class="m">0.5</span><span class="p">,</span><span class="m">2</span><span class="p">),</span><span class="n">y</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="m">0.01</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span><span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06.e. Maximum Likelihood_5_0.png" src="_images/06.e. Maximum Likelihood_5_0.png" />
</div>
</div>
<p>At smaller sample sizes the LLR is slightly left skewed when the sample mean is 0.4. As the sample size increases you can see that the LLR becomes more symmetrical about the sample mean, and that the slope of the LLR at values away from the sample mean is steeper. We can start to see the relationship between sample size and the precision of the sample mean. Qualitatively, if we wanted to test whether the coin was fair, it is clear that a larger sample would enable us to have more confidence in our assessment.</p>
<p>Returning to the fact that the data are a sample from a population distribution, we can explore what happens when multiple samples of the same size are drawn. MLE is a sample of the true parameter we can perform the above experiment multiple times and identify the parameters space where the LLR will be zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
 <span class="n">x</span> <span class="o">&lt;-</span> <span class="m">4</span><span class="p">;</span> <span class="n">n</span><span class="o">&lt;-</span><span class="m">10</span>
 <span class="n">sampl</span> <span class="o">&lt;-</span> <span class="nf">rbinom</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="m">1000</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="n">prob</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span>
<span class="n">mles</span> <span class="o">&lt;-</span> <span class="n">sampl</span><span class="o">/</span><span class="n">n</span> 
 <span class="nf">hist</span><span class="p">(</span><span class="n">mles</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="n">breaks</span><span class="o">=</span><span class="m">30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/06.e. Maximum Likelihood_8_0.png" src="_images/06.e. Maximum Likelihood_8_0.png" />
</div>
</div>
<p>From the histogram above you can see that repeating the experiment (different samples) will return different values of the MLE and corresponding LLR. The LLR ratio can be used to assess how consistent different values of the parameter are with the MLE.</p>
<p>The principles behind the LLR also relate to construction of confidence intervals, an issue which we will return to when we meet logistic regression and other models estimated using maximum likelihood estimation.</p>
</div>
</div>
</div>
</div>
<span id="document-07.a. Frequentist I"></span><div class="section" id="frequentist-i-confidence-intervals">
<h2>7. Frequentist I: Confidence Intervals<a class="headerlink" href="#frequentist-i-confidence-intervals" title="Permalink to this headline">¶</a></h2>
<p>In previous sessions we considered the concept of estimating population parameters using information from a sample from the population. When we present an estimate of a population quantity, it is important to also provide a measure of how precise that estimate is. Do we believe it is close to the true value? Can we provide a range of values within which we believe the true value lies?</p>
<p>This is the purpose of a confidence interval, often abbreviated by CI. Loosely speaking, a confidence interval provides a range of values for the population parameter which our observed data are consistent with.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>correctly interpret a 95% confidence interval interval</p></li>
<li><p>describe properties of a 95% confidence interval over repeated sampling</p></li>
<li><p>calculate a 95% confidence interval for the mean</p></li>
<li><p>use resampling (bootstrapping) approaches to obtain percentile confidence intervals</p></li>
</ul>
</div><div class="toctree-wrapper compound">
<span id="document-07.b. Frequentist I"></span><div class="section" id="confidence-intervals">
<h3>7.1 Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>To explore the concept of confidence intervals, we will return to the example of emotional distress among violence researchers.</p>
<p>We will again consider the smaller subsample of 10 researchers and focus on estimating the population mean age, <span class="math notranslate nohighlight">\(\mu\)</span>. Among our 10 sampled violence researchers, the sample mean age and the sample proportion suffering from emotional distress are:</p>
<blockquote>
<div><p>Sample mean age <span class="math notranslate nohighlight">\(\bar{x}= 29.57\)</span>; sample standard deviation of age <span class="math notranslate nohighlight">\(SD = 4.95\)</span></p>
</div></blockquote>
<p><strong>Statistical model:</strong> As before, we will let <span class="math notranslate nohighlight">\(X_1, ...,X_{10}\)</span> be  random variables representing the ages of 10 sampled researchers . For simplicity, we will assume that we know the true value of the population standard deviation, <span class="math notranslate nohighlight">\(\sigma = 4.8\)</span>. We assume the following model</p>
<div class="math notranslate nohighlight">
\[ 
X_i \overset{\small{iid}}{\sim} N(\mu, 4.8^2), \qquad i=1,2,...,10
\]</div>
<p><strong>Data:</strong> The realised values of the random variables are <span class="math notranslate nohighlight">\(x_1, ..., x_{10}\)</span> (i.e. the observed ages).</p>
<p><strong>Estimator and estimate:</strong> The best estimator of the population mean age is the sample mean age.</p>
<p>From our sample of data, the estimate is <span class="math notranslate nohighlight">\(\hat{\mu} = 29.57\)</span>. But how good an estimate is this? In order to answer that question, we will construct a 95% confidence interval around the estimate.</p>
<p><strong>Sampling distribution of the estimator:</strong> Recall that the sampling distribution of the sample mean is the distribution we would see if we repeatedly sampled 10 researchers a very large number of times, each time calculating the sample mean age, and drew a histogram of the sample means. We obtained the sampling distribution algebraically:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} \sim N(\mu, 1.52^2)
\]</div>
<p>Recall that when we are talking about the sampling distribution (i.e. the distribution of an <em>estimator</em>), we call the standard deviation the <strong>standard error</strong>. So the sample mean age follows a normal distribution, under repeated sampling, centred around the population mean <span class="math notranslate nohighlight">\(\mu\)</span> with standard error given by <span class="math notranslate nohighlight">\(SE(\hat{\mu}) = 1.52\)</span>.</p>
<p>We do not quite have sufficient information to plot the sampling distribution, because we still do not know where the central value <span class="math notranslate nohighlight">\(\mu\)</span> is. However, otherwise we can draw the exact shape. The graph below draws the sampling distribution around an unknown population mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>[The code used to generate the graph is suppressed, since it is not our focus here, but if you wish to see it you can click the button to the right.]</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Labels for the graph</span>
<span class="n">lab1</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="m">3</span><span class="p">)</span>
<span class="n">lab2</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="m">2</span><span class="p">)</span>
<span class="n">lab3</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="m">1</span><span class="p">)</span>
<span class="n">lab4</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
<span class="n">lab5</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="m">1</span><span class="p">)</span>
<span class="n">lab6</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="m">2</span><span class="p">)</span>
<span class="n">lab7</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="m">3</span><span class="p">)</span>

<span class="c1"># Plot a normal distribution centred around a value &quot;mu&quot; with an unspecified dispersion</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4.5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-4</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">05</span><span class="p">),</span> <span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">,</span>  <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Density&quot;</span><span class="p">,</span> 
     <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-4</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">05</span><span class="p">),</span> <span class="m">0</span><span class="p">,</span> <span class="m">1.52</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">)</span>
<span class="nf">axis</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nf">seq</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="m">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">lab1</span><span class="p">,</span> <span class="n">lab2</span><span class="p">,</span> <span class="n">lab3</span><span class="p">,</span> <span class="n">lab4</span><span class="p">,</span> <span class="n">lab5</span><span class="p">,</span> <span class="n">lab6</span><span class="p">,</span> <span class="n">lab7</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07.b. Frequentist I_1_0.png" src="_images/07.b. Frequentist I_1_0.png" />
</div>
</div>
<div class="section" id="confidence-interval-for-the-mean">
<h4>7.1.1 Confidence interval for the mean<a class="headerlink" href="#confidence-interval-for-the-mean" title="Permalink to this headline">¶</a></h4>
<p>We now use a general fact about normal distributions:</p>
<blockquote>
<div><p>For a normal distribution,  95% of the observations lie within 1.96 standard deviations of the mean.</p>
</div></blockquote>
<p>For the sampling distribution above, the “observations” are the different sample means we would see under (hypothetical) repeated sampling. Recall that when the we talk about a distribution of an estimator, we call the standard deviation the standard error. Thus the standard deviation of these observations (sample means) is the standard error of the mean, which takes a value of 1.52 here.</p>
<p>Therefore, 95% of the sample means lie within <span class="math notranslate nohighlight">\(1.52 \times 1.96 = 2.98\)</span> of the population mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>Imagine taking each (hypothetical) sample mean and “stretching out” a distance of 2.98 either way to give a range of values around that sample mean.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4.5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-6</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">05</span><span class="p">),</span> <span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">,</span>  <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Density&quot;</span><span class="p">,</span> 
     <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-6</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">05</span><span class="p">),</span> <span class="m">0</span><span class="p">,</span> <span class="m">1.52</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-2.98</span><span class="p">,</span> <span class="m">2.98</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="nf">axis</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nf">seq</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="m">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">lab1</span><span class="p">,</span> <span class="n">lab2</span><span class="p">,</span> <span class="n">lab3</span><span class="p">,</span> <span class="n">lab4</span><span class="p">,</span> <span class="n">lab5</span><span class="p">,</span> <span class="n">lab6</span><span class="p">,</span> <span class="n">lab7</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-6.17</span><span class="p">,</span> <span class="m">-0.22</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">02</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0.005</span><span class="p">,</span> <span class="m">298</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-3.56</span><span class="p">,</span> <span class="m">2.38</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">02</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0.13</span><span class="p">,</span> <span class="m">298</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-0.98</span><span class="p">,</span> <span class="m">4.98</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">02</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0.03</span><span class="p">,</span> <span class="m">299</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-3.2</span><span class="p">,</span> <span class="m">-0.6</span><span class="p">,</span> <span class="m">2</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.005</span><span class="p">,</span> <span class="m">0.13</span><span class="p">,</span> <span class="m">0.03</span><span class="p">),</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;orange&quot;</span><span class="p">)</span>
<span class="nf">text</span><span class="p">(</span><span class="m">4.5</span><span class="p">,</span> <span class="m">0.07</span><span class="p">,</span> <span class="s">&quot;2.5% of means&quot;</span><span class="p">)</span>
<span class="nf">text</span><span class="p">(</span><span class="m">-4.5</span><span class="p">,</span> <span class="m">0.07</span><span class="p">,</span> <span class="s">&quot;2.5% of means&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07.b. Frequentist I_3_0.png" src="_images/07.b. Frequentist I_3_0.png" />
</div>
</div>
<p>What proportion of such intervals would we expect to contain the true value <span class="math notranslate nohighlight">\(\mu\)</span>? Have a think about it and then click the button to the right.</p>
<div class="toggle docutils container">
<p>From the plot above we can see that if we created intervals for each sample mean by stretching a distance of 1.96 standard errors in either direction then most of such intervals would cross the true value <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<ul class="simple">
<li><p>For the 2.5% of sample means that lie to the right of the right-hand dashed green line (which is at 1.96 standard errors above the mean), these intervals will miss the true value <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>For the 2.5% of sample means that lie to the left of the left-hand dashed green line (which is at 1.96 standard errors above the mean), these intervals will miss the true value <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Thus, 95% of the intervals constructed in such a way will include the true value <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
</ul>
</div>
<p>These intervals are called <strong>95% confidence intervals</strong>.</p>
</div>
</div>
<span id="document-07.c. Frequentist I"></span><div class="section" id="confidence-intervals-for-the-mean">
<h3>7.2 Confidence intervals for the mean<a class="headerlink" href="#confidence-intervals-for-the-mean" title="Permalink to this headline">¶</a></h3>
<div class="section" id="example">
<h4>7.2.1 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>In the sample of 10 researchers, the estimate of the population mean age is <span class="math notranslate nohighlight">\(\hat{\mu} = 29.75\)</span>, the sample mean age. The standard error of the mean is</p>
<div class="math notranslate nohighlight">
\[
SE(\hat{\mu}) = \frac{\sigma}{\sqrt{n}} = \frac{4.8}{\sqrt{10}} = 1.52
\]</div>
<p>We have seen that the 95% confidence interval for the mean is calculated as</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} \pm 1.96 \times SE(\hat{\mu})
\]</div>
<p>Substituting in the sample mean and the standard error gives</p>
<div class="math notranslate nohighlight">
\[
29.57 \pm 1.96 \times 1.52 
\]</div>
<p>This gives the 95% confidence interval for the population mean age: <span class="math notranslate nohighlight">\((26.6, 32.5)\)</span>.</p>
<p>The code below reads in the data, prints the sample mean age and then calculates the 95% confidence interval for the population mean age.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Our sample of data (ages for 10 sampled researchers)</span>
<span class="n">ages</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">28.1</span><span class="p">,</span><span class="m">27.5</span><span class="p">,</span><span class="m">25</span><span class="p">,</span><span class="m">29.9</span><span class="p">,</span><span class="m">29.7</span><span class="p">,</span><span class="m">29.9</span><span class="p">,</span><span class="m">39.9</span><span class="p">,</span><span class="m">33.6</span><span class="p">,</span><span class="m">21.3</span><span class="p">,</span><span class="m">30.8</span><span class="p">)</span>

<span class="c1"># Sample mean (estimate of the population mean)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span>

<span class="c1"># Display the lower and upper limits of the confidence interval</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span> <span class="o">-</span> <span class="m">1.96</span><span class="o">*</span><span class="m">1.52</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">ages</span><span class="p">)</span> <span class="o">+</span> <span class="m">1.96</span><span class="o">*</span><span class="m">1.52</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">29.57</div><div class="output text_html">26.5908</div><div class="output text_html">32.5492</div></div>
</div>
</div>
<div class="section" id="confidence-interval-for-a-mean">
<h4>7.2.2  95% confidence interval for a mean<a class="headerlink" href="#confidence-interval-for-a-mean" title="Permalink to this headline">¶</a></h4>
<p>For random variables <span class="math notranslate nohighlight">\(𝑋_1,...,𝑋_n\)</span>, with <span class="math notranslate nohighlight">\(𝑋_i \overset{\small{iid}}{\sim} N (\mu, \sigma^2)\)</span> for <span class="math notranslate nohighlight">\(i=1,...,n\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> is a known value, a 95% confidence interval for <span class="math notranslate nohighlight">\(\mu\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} \pm 1.96 \ SE(\hat{\mu})
\]</div>
<p>where the standard error of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
SE(\hat{\mu}) = \frac{\sigma}{\sqrt{n}}
\]</div>
<p>The calculation of this confidence interval relies on the assumptions that</p>
<ul class="simple">
<li><p>the original random variables follow a normal distribution</p></li>
<li><p>the value of  <span class="math notranslate nohighlight">\(\sigma\)</span>  is known</p></li>
</ul>
<p>However, if these assumptions are not true, we can still obtain valid confidence intervals:</p>
<ul class="simple">
<li><p>If the original random variables do not follow a normal distribution but the sample size is large, then the Central Limit Theorem tells us that the sampling distribution of the mean is approximately normal. So this formula for the confidence interval is still valid.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\sigma\)</span> is unknown (which is typically the case), there is a modified confidence interval based on the t-distribution which provides a correct interval. Essentially, we replace the number 1.96 above by a slightly larger number to compensate for the estimation of the standard deviation. For large sample sizes (<span class="math notranslate nohighlight">\(n&gt;30\)</span> or so), the substitution of the estimated standard deviation makes little difference. More detail is provided later in this session.</p></li>
</ul>
</div>
</div>
<span id="document-07.d. Frequentist I"></span><div class="section" id="interpretation-of-confidence-intervals">
<h3>7.3 Interpretation of confidence intervals<a class="headerlink" href="#interpretation-of-confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>For our emotional distress sub-sample, our estimated mean age is <span class="math notranslate nohighlight">\(\hat{\mu}= 29.75\)</span>, with a 95% confidence interval of <span class="math notranslate nohighlight">\((26.6, 32.5)\)</span>. Having calculated this confidence interval for our unknown population age, <span class="math notranslate nohighlight">\(\mu\)</span>, how do we interpret it?</p>
<div class="section" id="operational-definition">
<h4>7.3.1 Operational definition<a class="headerlink" href="#operational-definition" title="Permalink to this headline">¶</a></h4>
<p>For the 95% confidence interval calculated above <span class="math notranslate nohighlight">\((26.6, 32.5)\)</span>, it is tempting to say that the probability that the population mean age, <span class="math notranslate nohighlight">\(\mu\)</span>, is between 26.6 and 32.5 is 95%. However, this is incorrect, because this  implies that <span class="math notranslate nohighlight">\(\mu\)</span> has a probability distribution, rather than being a fixed unknown number. Either the true value of <span class="math notranslate nohighlight">\(\mu\)</span> lies within the interval 26.6 to 32.5, or it does not.</p>
<p>Strictly, the interpretation of a confidence interval has to be with respect to the process of repeated sampling: if we repeated the study an infinite number of times, 95% of the 95% confidence intervals calculated would include the true population mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>This operational definition is long-winded and can be confusing. In practice,  we often use looser interpretations to aid communication of results, as described below.</p>
</div>
<div class="section" id="looser-interpretation-practical">
<h4>7.3.2 Looser interpretation (practical)<a class="headerlink" href="#looser-interpretation-practical" title="Permalink to this headline">¶</a></h4>
<p>In practice,  we often loosely interpret a 95% confidence interval by saying</p>
<ul class="simple">
<li><p>that we are 95% confident that the true population mean lies within the 95% confidence interval calculated.</p></li>
<li><p>that our data are consistent with values of the population mean within the 95% confidence interval calculated.</p></li>
</ul>
<p>It is important, however, to bear the strict operational definition of the confidence interval in mind when we use these types of interpretations.</p>
</div>
<div class="section" id="confidence-intervals-under-repeated-sampling">
<h4>7.3.3 Confidence intervals under repeated sampling<a class="headerlink" href="#confidence-intervals-under-repeated-sampling" title="Permalink to this headline">¶</a></h4>
<p>In order to see how confidence intervals behave under repeated sampling, we will now randomly draw 100 different samples of 10 people. Within each sample, we calculate the sample mean and the 95% confidence interval (assuming the population value <span class="math notranslate nohighlight">\(\sigma\)</span> is known).  The graph below shows the 95% confidence intervals from the 100 samples. [Click to view the code that generates the graph].</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Population parameters</span>
<span class="n">mu</span> <span class="o">&lt;-</span> <span class="m">30</span>
<span class="n">sd</span> <span class="o">&lt;-</span> <span class="m">4.8</span>
<span class="n">n_in_study</span> <span class="o">&lt;-</span> <span class="m">10</span>

<span class="c1"># Take 100 samples of 10 people, measure ages in each set of 10 people</span>
<span class="n">different_studies</span> <span class="o">&lt;-</span> <span class="m">100</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">1042</span><span class="p">)</span>
<span class="n">study_measurements_age</span> <span class="o">&lt;-</span> <span class="nf">list</span><span class="p">()</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">different_studies</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">study_measurements_age</span><span class="p">[[</span><span class="n">i</span><span class="p">]]</span> <span class="o">&lt;-</span>  <span class="nf">rnorm</span><span class="p">(</span><span class="n">n_in_study</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Calculate sample means and 95% confidence intervals</span>
<span class="n">sample.means</span>   <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">study_measurements_age</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="n">sample.cl</span>   <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">study_measurements_age</span><span class="p">,</span> <span class="nf">function </span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="m">1.96</span><span class="o">*</span><span class="m">1.52</span><span class="p">)</span>
<span class="n">sample.cu</span>   <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">study_measurements_age</span><span class="p">,</span> <span class="nf">function </span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="m">1.96</span><span class="o">*</span><span class="m">1.52</span><span class="p">)</span>

<span class="c1"># Extract means and CIs for samples which miss the true value</span>
<span class="n">out</span> <span class="o">&lt;-</span> <span class="p">((</span><span class="n">sample.cl</span><span class="o">&gt;</span><span class="m">30</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">sample.cu</span><span class="o">&lt;</span><span class="m">30</span><span class="p">))</span>
<span class="n">out.means</span> <span class="o">&lt;-</span> <span class="n">sample.means</span><span class="p">[</span><span class="n">out</span><span class="o">==</span><span class="m">1</span><span class="p">]</span>
<span class="n">out.cl</span> <span class="o">&lt;-</span> <span class="n">sample.cl</span><span class="p">[</span><span class="n">out</span><span class="o">==</span><span class="m">1</span><span class="p">]</span>
<span class="n">out.cu</span> <span class="o">&lt;-</span> <span class="n">sample.cu</span><span class="p">[</span><span class="n">out</span><span class="o">==</span><span class="m">1</span><span class="p">]</span>
<span class="n">out.samples</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">100</span><span class="p">,</span><span class="m">1</span><span class="p">)[</span><span class="n">out</span><span class="o">==</span><span class="m">1</span><span class="p">]</span>

<span class="c1"># Graph all the sample means and CIs</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">sample.means</span><span class="p">,</span> <span class="nf">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">100</span><span class="p">,</span><span class="m">1</span><span class="p">),</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;95% confidence intervals for the mean age&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Mean age (years)&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Sample&quot;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">22</span><span class="p">,</span> <span class="m">38</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">100</span><span class="p">),</span> <span class="n">yaxt</span><span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">30</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">)</span> <span class="p">{</span>
  <span class="nf">eval</span><span class="p">(</span><span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">sample.cl</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sample.cu</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="nf">c</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">)))</span>
<span class="p">}</span>

<span class="c1"># Highlight the CIs that miss the true value in red</span>
<span class="nf">points</span><span class="p">(</span><span class="n">out.means</span><span class="p">,</span> <span class="n">out.samples</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">out.samples</span><span class="p">))</span> <span class="p">{</span>
  <span class="nf">eval</span><span class="p">(</span><span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">out.cl</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">out.cu</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="nf">c</span><span class="p">(</span><span class="n">out.samples</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">out.samples</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07.d. Frequentist I_4_0.png" src="_images/07.d. Frequentist I_4_0.png" />
</div>
</div>
<p>In the figure above, what do you notice? Approximately what proportion of intervals include the true value of <span class="math notranslate nohighlight">\(\mu\)</span>? Have a think about this and then click to see some comments about the graph.</p>
<div class="toggle docutils container">
<p>We see that:</p>
<ul class="simple">
<li><p>the point estimates tend to cluster around the true value of <span class="math notranslate nohighlight">\(\mu\)</span>, and fall symmetrically either side</p></li>
<li><p>93 out of the 100 confidence intervals include the true value</p></li>
<li><p>3 out of 100 of the intervals to lie entirely above the true value</p></li>
<li><p>4 out of 100 of the intervals to lie entirely below the true value</p></li>
</ul>
<p>If we were to do simulate a much larger number of confidence intervals, we would see that:</p>
<ul class="simple">
<li><p>95% of the confidence intervals would include the true value</p></li>
<li><p>2.5% of the intervals would lie entirely above the true value</p></li>
<li><p>2.5% of the intervals would lie entirely below the true value</p></li>
</ul>
</div>
</div>
</div>
<span id="document-07.e. Frequentist I"></span><div class="section" id="approximate-confidence-intervals-for-parameters-estimated-using-large-samples">
<h3>7.4 Approximate confidence intervals for parameters estimated using large samples<a class="headerlink" href="#approximate-confidence-intervals-for-parameters-estimated-using-large-samples" title="Permalink to this headline">¶</a></h3>
<p>You will encounter many different confidence intervals during your studies. Many of these rely on an asymptotic normal distribution, as described below.</p>
<p>There are many different confidence intervals and many approaches to calculating confidence intervals. We do not aim to give you a comprehensive list here. Below, we describe a few commonly used confidence intervals to give you a flavour. <strong>Please note:</strong> We do not expect you to memorise these formulae.</p>
<div class="section" id="normal-based-confidence-intervals">
<h4>7.4.1 Normal-based confidence intervals<a class="headerlink" href="#normal-based-confidence-intervals" title="Permalink to this headline">¶</a></h4>
<p>The Central Limit Theorem tells us that the mean of independent identically distributed random variables, with finite expectation and variance, tends to a normal distribution as the sample size tends to infinity.</p>
<p>In fact, the Central Limit Theorem means that most typically encountered parameter estimators tends to normal as the sample sizes tend to infinity. So we can follow a very similar approach to the one above to construct confidence intervals for any parameter estimators that follow an approximate normal distribution when sample sizes are large, giving a confidence interval of the form</p>
<div class="math notranslate nohighlight">
\[
\mbox{Estimate} \pm 1.96 \times SE(\mbox{Estimator})
\]</div>
</div>
<div class="section" id="proportions-and-rates">
<h4>7.4.2 Proportions and rates<a class="headerlink" href="#proportions-and-rates" title="Permalink to this headline">¶</a></h4>
<p>First, we need some notation.</p>
<div class="sphinx-bs container pb-4 docutils">
<div class="row docutils">
<div class="d-flex col-4 docutils">
<div class="card w-100 border-2 docutils">
<div class="card-header docutils">
<p class="card-text">Proportion</p>
</div>
<div class="card-body docutils">
<p class="card-text">We are estimating a population proportion from a single observation from a binomial distribution.</p>
<p class="card-text">Our observed data consist of one observation from <span class="math notranslate nohighlight">\(X \sim binomial(n, \pi)\)</span>, with the realised (observed) value being <span class="math notranslate nohighlight">\(X=k\)</span>.</p>
</div>
</div>
</div>
<div class="d-flex col-4 docutils">
<div class="card w-100 border-2 docutils">
<div class="card-header docutils">
<p class="card-text">Rate</p>
</div>
<div class="card-body docutils">
<p class="card-text">We are estimating a population rate (per person-year), from the total number of events out of <span class="math notranslate nohighlight">\(P\)</span> person-years of observation.</p>
<p class="card-text">Our observed data consist of one observation from <span class="math notranslate nohighlight">\(X \sim Poisson(\lambda P)\)</span>. The realised (observed) value is <span class="math notranslate nohighlight">\(X=d\)</span>.</p>
</div>
</div>
</div>
<div class="d-flex col-4 docutils">
<div class="card w-100 border-2 docutils">
<div class="card-header docutils">
<p class="card-text">Logarithm of rate</p>
</div>
<div class="card-body docutils">
<p class="card-text">For the rate, we may wish to perform our calculations on the log scale. These confidence intervals are approximate; the approximation can work better following a transformation (e.g. the log). This is one example of that approach.</p>
<p class="card-text">To do this, we need to define the log-rate, <span class="math notranslate nohighlight">\(\nu = log(\lambda)\)</span></p>
</div>
</div>
</div>
</div>
</div>
<p>Using this notation, we can write down the estimate of the parameter of interest, it’s standard error and an approximate 95% confidence interval. These are shown in the table below.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:center head"><p>Estimate of parameter</p></th>
<th class="text-align:center head"><p>Standard Error</p></th>
<th class="text-align:center head"><p>Approximate 95% Confidence Interval</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Proportion</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\pi} = \frac{k}{n}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\sqrt{\frac{\pi (1-\pi)}{n}}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\pi} \pm 1.96 \times \sqrt{\frac{\hat{\pi} (1-\hat{\pi})}{n}}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Rate</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\lambda} = \frac{d}{P}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\frac{\lambda}{\sqrt{d}}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\lambda} \pm 1.96 \times \frac{\hat{\lambda}}{\sqrt{d}}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Log Rate</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\nu} = log\left(\frac{d}{P}\right)\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\sqrt{\frac{1}{\lambda P}}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\nu} \pm 1.96 \times \sqrt{\frac{1}{e^{\hat{\nu}} P}}\)</span></p></td>
</tr>
</tbody>
</table>
<p>The three tabs below provide examples of using the formulae above to obtain approximate 95% confidence intervals for proportions and rates.</p>
<div class="tabbed-set docutils">
<input checked="checked" id="3d38ff71-c9c1-4baf-828d-2e01ea1d55b9" name="16976c41-a68c-4bf9-8eaf-7a779abb14b6" type="radio">
</input><label class="tabbed-label" for="3d38ff71-c9c1-4baf-828d-2e01ea1d55b9">
Proportion</label><div class="tabbed-content docutils">
<ul class="simple">
<li><p>We want to estimate the population proportion of patients who experience a side effect from a particular drug.</p></li>
<li><p>In a clinical study of <span class="math notranslate nohighlight">\(80\)</span> patients given the drug, <span class="math notranslate nohighlight">\(X=20\)</span> experience a side effect.</p></li>
<li><p>Our estimate of the population proportion experiencing a side effect is <span class="math notranslate nohighlight">\(\hat{\pi} = 0.25\)</span></p></li>
<li><p>Our 95% confidence interval for this proportion is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0.25 \pm 1.96 \times \sqrt{\frac{0.25 (1-0.25)}{80}}
\]</div>
<ul class="simple">
<li><p>This gives a range of 0.155 to 0.349.</p></li>
<li><p>So our estimate of the proportion of patients who experience a side effect is: 0.25 (95% CI 0.155 to 0.349). Our best guess is that 25% of patients experience a side-effect from this drug. We are 95% confident that the true proportion lies between 15.5% and 34.9%.</p></li>
</ul>
</div>
<input id="9232f464-6a80-4c76-91fc-37e343754ce0" name="16976c41-a68c-4bf9-8eaf-7a779abb14b6" type="radio">
</input><label class="tabbed-label" for="9232f464-6a80-4c76-91fc-37e343754ce0">
Rate</label><div class="tabbed-content docutils">
<ul class="simple">
<li><p>We want to estimate the rate of panic attacks among adults with a mild anxiety disorder.</p></li>
<li><p>Suppose we observe <span class="math notranslate nohighlight">\(80\)</span> patients with the disorder for 1 year, so <span class="math notranslate nohighlight">\(P=80\)</span>. In total, these patients experience <span class="math notranslate nohighlight">\(d=2\)</span> panic attacks during the year.</p></li>
<li><p>Our estimate of the annual rate of panic attacks per person is <span class="math notranslate nohighlight">\(\hat{\lambda} = 2/80 = 0.025\)</span>.</p></li>
<li><p>Our 95% confidence interval for the rate is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0.025 \pm 1.96 \times \frac{0.025}{\sqrt{2}}
\]</div>
<ul class="simple">
<li><p>This gives a range of -0.0096 to 0.0596. This illustrates an important point - approximate confidence intervals sometimes contain impossible parameter values (the rate <span class="math notranslate nohighlight">\(\lambda\)</span> cannot be negative). To resolve this problem, we will re-do the calculations on the log scale.</p></li>
</ul>
</div>
<input id="46b208c8-1ce2-407f-a0cb-4deb213eb7c3" name="16976c41-a68c-4bf9-8eaf-7a779abb14b6" type="radio">
</input><label class="tabbed-label" for="46b208c8-1ce2-407f-a0cb-4deb213eb7c3">
Logarithm of the rate</label><div class="tabbed-content docutils">
<ul class="simple">
<li><p>The logarithm of the observed rate is <span class="math notranslate nohighlight">\(\nu = log(0.025) = -3.689\)</span>. So this is our estimate of the log rate.</p></li>
<li><p>Our 95% confidence interval for the log-rate is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
-3.689 \pm 1.96 \times \sqrt{\frac{1}{0.025 \times 80}}
\]</div>
<ul class="simple">
<li><p>This gives a range of -5.075 to -2.303. This is an interval within which we are confident the log-rate lies. To obtain an interval on the original scale, we take the exponential transformation of each of these values:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
(e^{-5.075}=0.006, e^{-2.303}=0.0999).
\]</div>
<ul class="simple">
<li><p>So our estimated rate and 95% confidence interval is: 0.025 (95% CI 0.006 to 0.0999). We are 95% confident that the true rate of panic attacks per year per person lies between 0.006 and 0.0999.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="the-mean">
<h4>7.4.2 The mean<a class="headerlink" href="#the-mean" title="Permalink to this headline">¶</a></h4>
<p>In this subsection we consider estimating a population mean. Our observed data comprise <span class="math notranslate nohighlight">\(n\)</span> independent observations, <span class="math notranslate nohighlight">\(x_1, x_2, ..., x_n\)</span>. We consider two possibilities:</p>
<ul class="simple">
<li><p>Data are normally distributed, <span class="math notranslate nohighlight">\(X_i \sim Normal(\mu, \sigma^2),\)</span> for <span class="math notranslate nohighlight">\(i=1,..,n\)</span></p></li>
<li><p>Data are not normally distributed.</p></li>
</ul>
<p>In each case, the population mean and variance (the square of the population standard deviation) are:</p>
<div class="math notranslate nohighlight">
\[
E[X] = \mu, \ \ Var(X) = \sigma^2
\]</div>
<p>The sample mean is <span class="math notranslate nohighlight">\(\bar{x}\)</span> and the sample standard deviation is <span class="math notranslate nohighlight">\(s\)</span>. Our estimate of the population mean is just the sample mean: <span class="math notranslate nohighlight">\(\hat{\mu} = \bar{x}\)</span>. And, as we have seen, the standard error is
given by <span class="math notranslate nohighlight">\(\frac{\sigma}{\sqrt{n}}\)</span>.</p>
<p>There are various ways of constructing a 95% confidence interval, depending on the situation. These are shown in the table below.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:center head"><p>Approximate 95% Confidence Interval</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Small samples</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>- Normal distribution, known <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>- Normal distribution, unknown <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm t_{n-1} \times \frac{s}{\sqrt{n}}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Large samples</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>- Normal or not, known <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm 1.96 \times \frac{\sigma}{\sqrt{n}}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>- Normal or not, unknown <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm 1.96 \times \frac{s}{\sqrt{n}}\)</span></p></td>
</tr>
</tbody>
</table>
<p>where <span class="math notranslate nohighlight">\(t_{n-1}\)</span> is a number obtained from the t-distribution, which is similar to the standard normal distribution. This number is around 2 for small <span class="math notranslate nohighlight">\(n\)</span> and becomes very close to <span class="math notranslate nohighlight">\(1.96\)</span> for large samples.</p>
<p>For small samples, where the data are not normally distributed, the confidence interval assuming data are normally distributed often has reasonable performance, but other methods (e.g. bootstrap confidence intervals) may be advisable.</p>
</div>
<div class="section" id="comparing-two-groups">
<h4>7.4.3 Comparing two groups<a class="headerlink" href="#comparing-two-groups" title="Permalink to this headline">¶</a></h4>
<p>There are many ways of comparing outcomes between two groups. Two popular options are the difference in proportions for binary outcomes and the difference in means for continuous outcomes. Confidence intervals for other measures (e.g. the risk ratio, the odds ratio, the difference in medians, etc.) also can be obtained.</p>
<div class="tabbed-set docutils">
<input checked="checked" id="5629c224-3f4f-45ce-a664-418a069a3907" name="39004167-8046-45da-911c-64d552393744" type="radio">
</input><label class="tabbed-label" for="5629c224-3f4f-45ce-a664-418a069a3907">
Difference in proportions</label><div class="tabbed-content docutils">
<p>We are interested in the population difference in proportions from two observations from two binomial distributions. Suppose our observed data consist of two observations from <span class="math notranslate nohighlight">\(X_1 \sim binomial(n_1, \pi_1)\)</span>, with the realised values being <span class="math notranslate nohighlight">\(X_1=k_1\)</span> and <span class="math notranslate nohighlight">\(X_2 = k_2\)</span>. We want to estimate the difference <span class="math notranslate nohighlight">\(\delta = \pi_1 - \pi_2\)</span>.</p>
<p>We estimate the proportion in the first group by <span class="math notranslate nohighlight">\(\hat{\pi}_1 = \frac{k_1}{n_1}\)</span>. Similarly, we estimate the proportion in the second group by <span class="math notranslate nohighlight">\(\hat{\pi}_2 = \frac{k_2}{n_2}\)</span>. Then our estimate of the difference in proportions is <span class="math notranslate nohighlight">\(\hat{\delta} = \hat{\pi}_1 - \hat{\pi}_2\)</span>.</p>
<p>The standard error for the difference in proportions is:</p>
<div class="math notranslate nohighlight">
\[
\sqrt{\frac{\pi_1 (1-\pi_1)}{n_1} + \frac{\pi_2 (1-\pi_2)}{n_2}}
\]</div>
<p>And we can obtain an approximate 95% confidence interval as:</p>
<div class="math notranslate nohighlight">
\[\hat{\delta} \pm 1.96 \times \sqrt{\frac{\hat{\pi}_1 (1-\hat{\pi}_1)}{n_1} + \frac{\hat{\pi}_2 (1-\hat{\pi}_2)}{n_2}}\]</div>
</div>
<input id="30cf770d-beb4-4479-9b5f-4956389e3289" name="39004167-8046-45da-911c-64d552393744" type="radio">
</input><label class="tabbed-label" for="30cf770d-beb4-4479-9b5f-4956389e3289">
Difference in means</label><div class="tabbed-content docutils">
<p>We are interested in the difference in population means between two groups from <span class="math notranslate nohighlight">\(n_1\)</span> iid observations from a normal distribution from group 1 and <span class="math notranslate nohighlight">\(n_2\)</span> from group 2. Suppose our observed data are <span class="math notranslate nohighlight">\(n_1\)</span> observations drawn from <span class="math notranslate nohighlight">\(X_i \sim Normal(\mu_1, \sigma^2),\)</span> for <span class="math notranslate nohighlight">\(i=1,..,n_1\)</span> and <span class="math notranslate nohighlight">\(n_2\)</span> observations drawn from <span class="math notranslate nohighlight">\(X_i \sim Normal(\mu_2, \sigma^2),\)</span> for <span class="math notranslate nohighlight">\(i=1,..,n_2\)</span>. We want to estimate the difference <span class="math notranslate nohighlight">\(\delta = \mu_1 - \mu_2\)</span></p>
<p>The sample means are <span class="math notranslate nohighlight">\(\bar{x}_1\)</span> and <span class="math notranslate nohighlight">\(\bar{x}_2\)</span> and the sample standard deviations are <span class="math notranslate nohighlight">\(s_1\)</span> and <span class="math notranslate nohighlight">\(s_2\)</span>.</p>
<p>We can obtain a pooled estimate of the standard deviation, if we’re happy to assume that these are equal, as follows</p>
<div class="math notranslate nohighlight">
\[ 
s = \sqrt{\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_1^2 }{n_1 + n_2 - 2}}
\]</div>
<p>Our estimate of the difference in population means is: <span class="math notranslate nohighlight">\(\hat{\delta} = \hat{\mu}_1 - \hat{\mu}_2\)</span>. This has standard error:</p>
<div class="math notranslate nohighlight">
\[ 
SE(\hat{\delta}) =  \sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}
\]</div>
<p>Various confidence intervals can be obtained, depending on the setting. These are shown in the table below.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:center head"><p>Approximate 95% Confidence Interval</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Small samples</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>- Normal distribution, known <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm 1.96 \times \sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>- Normal distribution, unknown <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm t_{n_1+n_2-2} \times s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Large samples</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>- Normal or not, known <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm 1.96 \times s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>- Normal or not, unknown <span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\hat{\mu} \pm 1.96 \times s \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\)</span></p></td>
</tr>
</tbody>
</table>
<p>where <span class="math notranslate nohighlight">\(t_{n_1+n_2-2}\)</span> is a number obtained from the t-distribution with <span class="math notranslate nohighlight">\(n_1 + n_2 - 2\)</span> degrees of freedom. This will give a number that takes a value of around 2 for smaller samples and approximately 1.96 for larger samples.</p>
<p>Modified intervals that do not assume equality of standard deviation in the two groups also exist.</p>
</div>
</div>
</div>
</div>
<span id="document-07.f. Frequentist I"></span><div class="section" id="confidence-intervals-using-resampling">
<h3>7.5 Confidence Intervals using resampling<a class="headerlink" href="#confidence-intervals-using-resampling" title="Permalink to this headline">¶</a></h3>
<p>We saw that we can often create an approximate sampling distribution by resampling from our sample data. This is particularly useful in situations where there is no algebraic derivation for the sampling distribution.</p>
<p>We have seen that the important connection between sampling distributions and confidence intervals. So we would intuitively expect to be able to construct a confidence interval from the approximate sampling distribution we obtained using resampling. This is indeed possible. There are many ways of doing this, but the simplest and most intuitive method is the <strong>bootstrap percentile confidence interval</strong>.</p>
<p>The basic idea is very simple. We construct an approximate sampling distribution using bootstrap samples, as we did previously. Then we take the 2.5th and 97.5th percentiles of that distribution (the value such that 2.5% of the observations - the estimates across bootstrap samples - lie below the value; and the value such that 2.5% of observations lie above the value, respectively). These form the limits of our 95% confidence interval.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">78234</span><span class="p">)</span> 

<span class="c1"># Read in the sample of 10 ages</span>
<span class="n">ages</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">28.1</span><span class="p">,</span><span class="m">27.5</span><span class="p">,</span><span class="m">25</span><span class="p">,</span><span class="m">29.9</span><span class="p">,</span><span class="m">29.7</span><span class="p">,</span><span class="m">29.9</span><span class="p">,</span><span class="m">39.9</span><span class="p">,</span><span class="m">33.6</span><span class="p">,</span><span class="m">21.3</span><span class="p">,</span><span class="m">30.8</span><span class="p">)</span>

<span class="c1"># Draw bootstrap samples </span>
<span class="n">bootstrap_samples</span> <span class="o">&lt;-</span> <span class="nf">lapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">1039</span><span class="p">,</span> <span class="nf">function</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="nf">sample</span><span class="p">(</span><span class="n">ages</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">T</span><span class="p">))</span>

<span class="c1"># Calculate sample means in each bootstrap sample</span>
<span class="n">r.mean</span> <span class="o">&lt;-</span> <span class="nf">sapply</span><span class="p">(</span><span class="n">bootstrap_samples</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>

<span class="c1"># Obtain the 2.5th and 97.5th percentiles of the sample means across bootstrap samples</span>
<span class="p">(</span><span class="n">q</span><span class="o">&lt;-</span><span class="nf">quantile</span><span class="p">(</span><span class="n">r.mean</span><span class="p">,</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.025</span><span class="p">,</span> <span class="m">0.975</span><span class="p">)))</span>
         
<span class="c1"># Draw the approximate sampling distribution with the percentile confidence limits marked in red</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4.5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4.5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">r.mean</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution for mean \n with percentile 95% confidence limits&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Sample mean&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><dl class=dl-horizontal>
	<dt>2.5%</dt>
		<dd>26.798</dd>
	<dt>97.5%</dt>
		<dd>32.501</dd>
</dl>
</div><img alt="_images/07.f. Frequentist I_1_1.png" src="_images/07.f. Frequentist I_1_1.png" />
</div>
</div>
<p>The approximate 95% confidence interval for the mean age obtained by using the algebric approximation to the sampling distribution was: 26.6 to 32.5. The bootstrap percentile 95% confidence interval is: 26.8 to 32.5. We see that these intervals are very similar to one another, as we would expect.</p>
</div>
<span id="document-07.g. Frequentist I"></span><div class="section" id="summary-use-of-confidence-intervals">
<h3>7.6 Summary: Use of confidence intervals<a class="headerlink" href="#summary-use-of-confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>In this session we have discussed the concepts underlying confidence intervals and different interpretations of 95% confidence intervals.</p>
<p>We can construct a confidence interval for any estimate, including:</p>
<ul class="simple">
<li><p>means</p></li>
<li><p>proportions</p></li>
<li><p>differences in means</p></li>
<li><p>risk ratios</p></li>
<li><p>regression coefficients</p></li>
<li><p>etc.</p></li>
</ul>
<p>The way we construct confidence intervals can vary but the basic interpretation of the confidence interval remains the same.</p>
<p>While we have focused on 95% confidence intervals, we can construct other intervals, e.g. 99% confidence intervals. The use of 95% confidence intervals is largely convention.</p>
</div>
<span id="document-07.h. Frequentist I"></span><div class="section" id="further-resources">
<h3>Further resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h3>
<p>Note: further resources are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. All examinable material is contained within the notes.</p>
<p>Ashley I Naimi, Brian W Whitcomb, Can Confidence Intervals Be Interpreted?, American Journal of Epidemiology, Volume 189, Issue 7, July 2020, Pages 631–633, <a class="reference external" href="https://doi.org/10.1093/aje/kwaa004">https://doi.org/10.1093/aje/kwaa004</a></p>
</div>
</div>
</div>
<span id="document-08.a. Frequentist II"></span><div class="section" id="frequentist-ii-hypothesis-tests">
<h2>8. Frequentist II: Hypothesis tests<a class="headerlink" href="#frequentist-ii-hypothesis-tests" title="Permalink to this headline">¶</a></h2>
<p>In this session we continue with frequentist inference, exploring the concept of hypothesis testing and p-values. We discuss the general principle underlying a p-value, connections to confidence intervals and common misinterpretations and misuses of p-values.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>describe the meaning of the terms null and alternative hypotheses</p></li>
<li><p>describe what a p-value is</p></li>
<li><p>correctly interpret a p-value</p></li>
<li><p>explain the connection between 95% confidence intervals and p-values</p></li>
<li><p>describe the calculation of a p-value</p></li>
</ul>
</div>
<p>This session does not cover the mathematical derivation underlying hypothesis tests and common test statistics. Our intention is not to equip you with the ability to construct novel hypothesis tests. The purpose of this session is rather to convey an understanding of what a hypothesis test is, what a p-value is and how to interpret p-values correctly.</p>
<div class="toctree-wrapper compound">
<span id="document-08.b. Frequentist II"></span><div class="section" id="evidence-against-hypotheses">
<h3>8.1 Evidence against hypotheses<a class="headerlink" href="#evidence-against-hypotheses" title="Permalink to this headline">¶</a></h3>
<p>This session considers the concept of testing hypotheses.</p>
<div class="section" id="proving-and-disproving-hypotheses">
<h4>8.1.1 Proving and disproving hypotheses<a class="headerlink" href="#proving-and-disproving-hypotheses" title="Permalink to this headline">¶</a></h4>
<p>Let’s consider a very simple example. Suppose we believe that all men are over 120cm tall. We could:</p>
<ul class="simple">
<li><p><em>Prove</em> the hypothesis by finding every man and showing they are more than 120cm tall</p></li>
<li><p><em>Disprove</em> the hypothesis by finding a single man less than 120cm tall</p></li>
</ul>
<blockquote>
<div><p>It is easier to find evidence against a hypothesis than to prove it to be correct.</p>
</div></blockquote>
<p>The general approach we will take is as follows. We start with a <strong>null hypothesis</strong>, which is typically a statement about the population value of parameters. This will often be a statement of “no difference”. Some examples might be:</p>
<ul class="simple">
<li><p>Exposure to passive smoking is not associated with subsequent risk of lung cancer.</p></li>
<li><p>Treatment A does not improve survival compared with placebo</p></li>
<li><p>The mean body mass index (BMI) in England is the same as the mean BMI in Scotland.</p></li>
</ul>
<p>We assume that our null hypothesis holds, i.e. that our sample of data came from a population in which our null hypothesis is true. We then look for evidence, in our sample data, against the null hypothesis (i.e. to falsify the hypothesis).</p>
<p>For example, suppose our null hypothesis is that the mean BMI is the same in England and Scotland and that we have a random sample of adults from England and Scotland. If we assume our null hypothesis is true (the two populations have the same mean BMI), then we would expect our two samples to have similar means. If, in fact, we observed very different sample means in the two sample groups then we would take this as <em>evidence against our null hypothesis</em>.</p>
</div>
<div class="section" id="example">
<h4>8.1.2 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>To explore the concept of hypothesis testing, we will return to the example of emotional distress among violence researchers. The researchers were randomly assigned to receive an intervention (group debriefing aimed at reducing emotional distress) or control (nothing). At the end of the intervention, 22 researchers in the intervention group and 26 researchers in the control group filled in a questionnaire measuring emotional distress. The score gives a value of 0-20, with higher scores indicating higher distress.</p>
<p>The sample mean scores and their standard deviations in the two groups are:</p>
<ul class="simple">
<li><p>Control group (<span class="math notranslate nohighlight">\(n_0\)</span>=26), sample mean emotional distress score (sample standard deviation):  <span class="math notranslate nohighlight">\(\bar{x}_0 = 6.35\)</span>, (SD = 1.87)</p></li>
<li><p>Intervention group (<span class="math notranslate nohighlight">\(n_1\)</span>=22), sample mean emotional distress score (sample standard deviation):  <span class="math notranslate nohighlight">\(\bar{x}_1 = 5.45\)</span>, (SD = 1.87)</p></li>
</ul>
<p>The research question we consider in this session is:</p>
<blockquote>
<div><p>Is the true mean emotional distress score is different in the intervention and control group?</p>
</div></blockquote>
<p>The population parameter of interest is therefore the difference between the population mean emotional distress score in the intervention and control groups.</p>
<p>Aside: as is often the case, the population is a bit hard to define here. We can think about it as being the wider population of people who could be given the intervention (or not).</p>
<p>The code below reads in the data, obtains the sample means and SDs and draws histograms of the scores in each group.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read in data (emotional distress scores in control and intervention group)</span>
<span class="n">dist0</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">8</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">7</span><span class="p">)</span>
<span class="n">dist1</span><span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">3</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">8</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">3</span><span class="p">,</span>  <span class="m">5</span><span class="p">)</span>

<span class="c1"># Calculate sample means </span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Sample means: &quot;</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">dist0</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">dist1</span><span class="p">)</span>

<span class="c1"># Calculate sample standard deviations </span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Sample SDs: &quot;</span><span class="p">)</span>
<span class="nf">sqrt</span><span class="p">(</span><span class="nf">var</span><span class="p">(</span><span class="n">dist0</span><span class="p">))</span>
<span class="nf">sqrt</span><span class="p">(</span><span class="nf">var</span><span class="p">(</span><span class="n">dist1</span><span class="p">))</span>

<span class="c1"># Sample difference in means</span>
<span class="nf">print</span><span class="p">(</span><span class="s">&quot;Difference in sample means:&quot;</span><span class="p">)</span>
<span class="p">(</span><span class="n">delta.hat</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dist1</span><span class="p">)</span> <span class="o">-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dist0</span><span class="p">))</span>

<span class="c1"># Draw histograms of the scores in each group</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">dist0</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Distress score, control&quot;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">12</span><span class="p">))</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">dist1</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Distress score, intervention&quot;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">12</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Sample means: &quot;
</pre></div>
</div>
<div class="output text_html">6.34615384615385</div><div class="output text_html">5.45454545454545</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Sample SDs: &quot;
</pre></div>
</div>
<div class="output text_html">1.87493589634009</div><div class="output text_html">1.8702501163843</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;Difference in sample means:&quot;
</pre></div>
</div>
<div class="output text_html">-0.891608391608392</div><img alt="_images/08.b. Frequentist II_3_8.png" src="_images/08.b. Frequentist II_3_8.png" />
</div>
</div>
<p><strong>Statistical model:</strong>  We will let <span class="math notranslate nohighlight">\(Y_{0,1}, ...,Y_{0,26}\)</span> be random variables representing the emotional distress scores of the 26 sampled researchers in the control group and <span class="math notranslate nohighlight">\(Y_{1,1}, ...,Y_{1,22}\)</span> be random variables representing the emotional distress scores of the 22 sampled researchers in the intervention group. So the first subscript denotes the group (0=control, 1=intervention) and the second is an index for the person (i=1, …, 26 in the control group; i=1,…,22 in the intervention group).</p>
<p>We will assume that all random variables are independent of each other. The emotional distress scores in the control group are all drawn from the same normal distribution, with population mean <span class="math notranslate nohighlight">\(\mu_0\)</span> and population standard deviation  <span class="math notranslate nohighlight">\(\sigma\)</span>. For now, we suppose <span class="math notranslate nohighlight">\(\sigma\)</span> is a known value, with <span class="math notranslate nohighlight">\(\sigma = 1.75\)</span>. The emotional distress scores in the intervention group are assumed to be drawn from a normal distribution with population mean <span class="math notranslate nohighlight">\(\mu_1\)</span> and the same population standard deviation.</p>
<p>This model can be compactly written as follows</p>
<div class="math notranslate nohighlight">
\[ 
Y_{j,i} \overset{\small{iid}}{\sim} N(\mu_j, 1.75^2), \qquad i=1,2,...,n_j
\]</div>
<p><strong>Data:</strong> We will let <span class="math notranslate nohighlight">\(y_{0,1}, ...,y_{0,26}\)</span> and <span class="math notranslate nohighlight">\(y_{1,1}, ...,y_{1,22}\)</span> represent the realised values of these random variables (i.e. the observed emotional distress scores).</p>
<p><strong>Estimand, estimator and estimate:</strong> The population parameter (estimand) we are interested in is:</p>
<div class="math notranslate nohighlight">
\[
\delta = \mu_1 - \mu_0
\]</div>
<p>The obvious estimator for this is the sample difference in means:</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} = \bar{Y_1} - \bar{Y_0} = \frac{1}{n_1} \sum_{i=1}^{n_1} Y_{1,i} - \frac{1}{n_0} \sum_{i=1}^{n_0} Y_{0,i}
\]</div>
<p>And the sample estimate is:</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} =  \bar{y_1} - \bar{y_0} = \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1,i} - \frac{1}{n_0} \sum_{i=1}^{n_0} y_{0,i} = 5.4545 - 6.3462 = -0.892
\]</div>
<p><strong>Null and alternative hypotheses:</strong> The null hypothesis is that there is no difference in the population mean emotional distress score in the intervention and control groups. Formally, we write:</p>
<div class="math notranslate nohighlight">
\[
H_0:  \delta = 0
\]</div>
<p>The alternative hypothesis (sometimes written <span class="math notranslate nohighlight">\(H_1\)</span> or <span class="math notranslate nohighlight">\(H_A\)</span>) is that the null hypothesis is not true:</p>
<div class="math notranslate nohighlight">
\[
H_1: \delta \neq 0
\]</div>
<p>In our sample, we have seen that <span class="math notranslate nohighlight">\(\hat{\delta} = -0.892\)</span>. So the sample mean emotional distress score is lower in the intervention group (which is the direction we might be hoping for, since this group have received a form of counselling to reduce their emotional distress). However, the two sample means are very unlikely to be exactly equal, even if the true mean emotional distress score is the same in the two groups, due to sampling variability (i.e. due to random chance). So how should we interpret this sample difference in means? Does it constitute evidence against our null hypothesis?</p>
<p>In order to answer this question, we need to consider the sampling distribution of the difference in means. Unlike in previous sessions, where we used the sampling distribution to obtain confidence intervals, we are now interested in a subtly different sampling distribution: the sampling distribution that we would see <em>if the null hypothesis were true</em>.</p>
</div>
<div class="section" id="sampling-distribution-for-the-difference-in-sample-means">
<h4>8.1.3 Sampling distribution for the difference in sample means<a class="headerlink" href="#sampling-distribution-for-the-difference-in-sample-means" title="Permalink to this headline">¶</a></h4>
<p>Under the statistical models above, if <span class="math notranslate nohighlight">\(\sigma\)</span> is a known value it is straightforward to derive the sampling distribution of the estimator (the difference in sample means between groups).</p>
<blockquote>
<div><p>Linear combinations of independent normal distributions are also normal</p>
</div></blockquote>
<p>Thus the distribution of <span class="math notranslate nohighlight">\(\hat{\delta}\)</span> is normal. We can then calculate its expectation and variance using techniques from the <a class="reference external" href="https://lshtm-hds.github.io/Math-Refresher">Refresher</a> to obtain:</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} \sim N\left(\delta, \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_0} \right) \right)
\]</div>
<p>Substituting in the values <span class="math notranslate nohighlight">\(\sigma = 1.75\)</span>, <span class="math notranslate nohighlight">\(n_0 = 26\)</span> and <span class="math notranslate nohighlight">\(n_1 = 22\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} \sim N\left(\delta, 1.75^2 \left(\frac{1}{22} + \frac{1}{26} \right) \right) = N(\delta,  0.507^2)  
\]</div>
<p>So the expectation of the sampling distribution is the true population value <span class="math notranslate nohighlight">\(\delta\)</span> and the standard error is <span class="math notranslate nohighlight">\(0.507\)</span>. Remember that because we are considering the distribution of an estimator, we call the standard deviation of the estimator (the square root of the variance) the standard error.</p>
<p><strong>Sampling distribution under the null hypothesis</strong></p>
<p>We are interested in the distribution of the difference in sampling means would look like under repeated sampling <em>if the null hypothesis were true</em>. The null hypothesis states that <span class="math notranslate nohighlight">\(\delta = 0\)</span>. Therefore, under the null hypothesis,</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} \sim  N(0,  0.507^2) 
\]</div>
<p>The graph below shows this distribution. The code to draw the graph is suppressed, since it is not the focus here, but can be viewed by clicking the button.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample difference in means</span>
<span class="n">delta.hat</span> <span class="o">&lt;-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dist1</span><span class="p">)</span> <span class="o">-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">dist0</span><span class="p">)</span>

<span class="c1"># Randomly generate 10000 sample differences in means (following the sampling distribution under the null hypothesis)</span>
<span class="n">sample.diff.means</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0.507</span><span class="p">)</span>

<span class="c1"># Draw the approximate sampling distribution with the percentile confidence limits marked in red</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">sample.diff.means</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution for \n difference in sample means, \n UNDER THE NULL HYPOTHESIS&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Difference in sample means&quot;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-2.5</span><span class="p">,</span> <span class="m">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">0.8</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-2.5</span><span class="p">,</span> <span class="m">2.5</span><span class="p">,</span> <span class="m">0.025</span><span class="p">),</span> <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-2.5</span><span class="p">,</span> <span class="m">2.5</span><span class="p">,</span> <span class="m">0.025</span><span class="p">),</span> <span class="m">0</span><span class="p">,</span> <span class="m">0.507</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">delta.hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="nf">text</span><span class="p">(</span><span class="m">1.5</span><span class="p">,</span> <span class="m">0.2</span><span class="p">,</span> <span class="s">&quot;Null value&quot;</span><span class="p">)</span>
<span class="nf">text</span><span class="p">(</span><span class="m">-1.7</span><span class="p">,</span> <span class="m">0.4</span><span class="p">,</span> <span class="s">&quot;Observed value&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1.5</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.005</span><span class="p">,</span> <span class="m">0.18</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-1.8</span><span class="p">,</span> <span class="m">-0.9</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.38</span><span class="p">,</span> <span class="m">0.28</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/08.b. Frequentist II_7_0.png" src="_images/08.b. Frequentist II_7_0.png" />
</div>
</div>
<p>The sampling distribution above shows us the distribution of the differences in sample means that we could have seen under repeated sampling, i.e. if we had done the same study a very large number of times. The question we must ask now is: is the value we have seen consistent with this sampling distribution? Or is it surprising? A “surprising” result is taken as evidence against the null hypothesis. In order to clarify these ideas, consider two scenarios that could have happened.</p>
<p>Scenario 1: Suppose we had done exactly the same study, but had seen a difference in sample means of <span class="math notranslate nohighlight">\(\hat{\delta} = -3.5\)</span> (i.e. the intervention group sample mean score was 3.5 units lower than the control group mean).</p>
<p>Scenario 2: Suppose we had done this study, but had actually seen a difference in sample means of <span class="math notranslate nohighlight">\(\hat{\delta} = 0.02\)</span>.</p>
<p>What would we conclude in these scenarios? The graph below superimposes these two scenarios on the sampling distribution.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Draw the approximate sampling distribution with the percentile confidence limits marked in red</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">sample.diff.means</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Sampling distribution for \ndifference in sample means, \nUNDER THE NULL HYPOTHESIS&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Difference in sample means&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">0.8</span><span class="p">),</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-3.5</span><span class="p">,</span> <span class="m">3.5</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-3.5</span><span class="p">,</span> <span class="m">3.5</span><span class="p">,</span> <span class="m">0.025</span><span class="p">),</span> <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-3.5</span><span class="p">,</span> <span class="m">3.5</span><span class="p">,</span> <span class="m">0.025</span><span class="p">),</span> <span class="m">0</span><span class="p">,</span> <span class="m">0.507</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">delta.hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">-3.5</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.02</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="nf">text</span><span class="p">(</span><span class="m">-2.5</span><span class="p">,</span> <span class="m">0.42</span><span class="p">,</span> <span class="s">&quot;Scenario 1: \nObserved value&quot;</span><span class="p">)</span>
<span class="nf">text</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">0.22</span><span class="p">,</span>  <span class="s">&quot;Scenario 2: \nObserved value&quot;</span><span class="p">)</span>
<span class="nf">text</span><span class="p">(</span><span class="m">-1.9</span><span class="p">,</span> <span class="m">0.62</span><span class="p">,</span> <span class="s">&quot;Actual data: \nObserved value&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0.05</span><span class="p">,</span> <span class="m">1.5</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.005</span><span class="p">,</span> <span class="m">0.15</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-2.5</span><span class="p">,</span> <span class="m">-3.4</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.35</span><span class="p">,</span> <span class="m">0.2</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-1.8</span><span class="p">,</span> <span class="m">-0.9</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.55</span><span class="p">,</span> <span class="m">0.48</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/08.b. Frequentist II_9_0.png" src="_images/08.b. Frequentist II_9_0.png" />
</div>
</div>
<p><strong>Scenario 1</strong> We can see from the histogram that, under the null hypothesis, the probability of seeing a difference in sample means of -3.5 or less is incredibly low. In fact, this probability is less than 1 in 10,000. So if we did 10,000 studies we would expect only one of them to have a difference in sample means of -3.5 or less.</p>
<ul class="simple">
<li><p>So, have we been very unlucky and picked a very very unusual sample by chance?</p></li>
<li><p>Or is our initial premise incorrect? Is the null hypothesis wrong?</p></li>
</ul>
<p>This particular sample mean difference appears to be inconsistent with our null hypothesis. We interpret these “surprising” sample statistics as evidence against the null hypothesis.</p>
<p><strong>Scenario 2</strong> Again, the histogram shows quite clearly that, under the null hypothesis, many of the samples that we could have obtained would give us a sample mean difference close to zero. So this sample difference is completely consistent with the null hypothesis.</p>
<p>In this case, we would conclude that there is no evidence against the null hypothesis.</p>
<p><strong>Our actual observed data</strong> Our observed sample mean difference (-0.892) is somewhere in between. In fact, we can calculate the probability of observing a sample mean difference of -0.892 or lower (i.e. the proportion of the area of the histogram that lies to the left of -0.89): this turns out to be 4%. So under repeated sampling, if our null hypothesis is true and there is truly no difference between the mean emotional distress score in the intervention and control groups, then we would expect to see a difference at least this big 4% of the time.</p>
<p>In fact, we typically consider the proportion of samples in which we would get an estimate at least as extreme as the one we did get <em>in either direction</em>. In our case, this is the probability of seeing a sample mean difference of less than -0.892 or greater than +0.892. Under the null hypothesis, approximately 8% of samples would produce a sample mean difference at least as extreme as the one we have seen in our sample.</p>
<p>So we had around a 1 in 13 chance of ending up with this result, if the null hypothesis is true. We interpret this as  weak evidence against the null hypothesis.</p>
</div>
</div>
<span id="document-08.c. Frequentist II"></span><div class="section" id="the-p-value">
<h3>8.2 The p-value<a class="headerlink" href="#the-p-value" title="Permalink to this headline">¶</a></h3>
<p>The p-value is defined as the probability of observing the sample estimate or a more extreme one (in either direction) given that the null hypothesis is true.</p>
<p>The smaller the p-value, the lower the chance of getting a difference as big  as the one observed if the null hypothesis is true.</p>
<p>Therefore, the smaller the p-value,  the stronger the evidence against the null hypothesis.</p>
<div class="figure align-default" id="inference">
<a class="reference internal image-reference" href="_images/pvalues.png"><img alt="_images/pvalues.png" src="_images/pvalues.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Interpretation of p-values <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1119478/">(taken from Sterne &amp; Davey-Smith)</a></span><a class="headerlink" href="#inference" title="Permalink to this image">¶</a></p>
</div>
<p>The value of 0.05 has historically been used as a cut-off, with values of <span class="math notranslate nohighlight">\(p&lt;0.05\)</span> deemed “statistically significant” and values of <span class="math notranslate nohighlight">\(p\geq 0.05\)</span> “not significant”. As discussed further in a later sub-section, we do not recommend dichotomising p-values in this way.</p>
<blockquote>
<div><p>Note that:<br />
<br> - We have described what is called a <em>two-sided test</em>. Occasionally, a <em>one-sided test</em> might be used, where the p-value is the probability of results that are as extreme, or more extreme, <em>in the same direction</em> as the observed result. However, it is rare that it is justifiable to ignore sample statistics in one direction, so we will focus on two-sided tests.
<br><br> - When the sampling distribution is not symmetric, it can be hard to define what is <em>as extreme as</em> the estimate we have seen. In this case, there are various ways of obtaining the two-sided p-value. We do not pursue this further.</p>
</div></blockquote>
</div>
<span id="document-08.d. Frequentist II"></span><div class="section" id="connection-between-p-values-and-confidence-intervals">
<h3>8.3 Connection between p-values and confidence intervals<a class="headerlink" href="#connection-between-p-values-and-confidence-intervals" title="Permalink to this headline">¶</a></h3>
<p>Recall that we previously used the following fact:</p>
<blockquote>
<div><p>For a normal distribution, approximately 95% of observations are contained within 1.96 standard deviations of the mean.</p>
</div></blockquote>
<p>Which, applied to sampling distributions, tells us that:</p>
<blockquote>
<div><p>For a normally distributed sampling distribution that is centred around the true population value, 95% of the estimates obtained under repeated sampling would be contained within 1.96 standard errors of the true population value</p>
</div></blockquote>
<p>Applying this to the estimator <span class="math notranslate nohighlight">\(\hat{\delta}\)</span>, this leads to a 95% confidence interval of</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} \pm  1.96 \times SE(\delta)
\]</div>
<p>The graph below shows some possible values of <span class="math notranslate nohighlight">\(\hat{\delta}\)</span>, along with their 95% confidence intervals. We see that:</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\hat{\delta}\)</span> is exactly equal to the number <span class="math notranslate nohighlight">\(1.96 \times  SE(\delta)\)</span> then the 95% confidence interval just touches zero.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\hat{\delta} &gt; 1.96 \times  SE(\delta)\)</span> then the 95% confidence interval does not include zero - the whole interval lies above zero.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(0 &lt; \hat{\delta} &lt; 1.96\)</span>  then the 95% confidence interval does include zero.</p></li>
</ul>
<p>So what p-values would these values of <span class="math notranslate nohighlight">\(\hat{\delta}\)</span> result in?</p>
<ul class="simple">
<li><p>if <span class="math notranslate nohighlight">\(\hat{\delta} = 1.96 \times  SE(\delta)\)</span> then we know that 2.5% of the estimates lie above that point, so p=0.05.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\hat{\delta} &gt; 1.96 \times  SE(\delta)\)</span> then fewer than 2.5% of estimates lie above <span class="math notranslate nohighlight">\(\hat{\delta}\)</span>, so p&lt;0.05</p></li>
<li><p>if <span class="math notranslate nohighlight">\(0 &lt; \hat{\delta} &lt; 1.96 \times  SE(\delta)\)</span> then more than 2.5% of estimates lie above <span class="math notranslate nohighlight">\(\hat{\delta}\)</span>, so p&gt;0.05</p></li>
</ul>
<p>This leads us to the connection between 95% confidence intervals and p-values. When a 95% confidence interval and p-value are obtained from the same sampling distribution (which is typically the case when both are presented),</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>P-value</p></th>
<th class="head"><p>95% confidence interval</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(&lt;0.05\)</span></p></td>
<td><p>Excludes the null value</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\geq 0.05\)</span></p></td>
<td><p>Contains the null value</p></td>
</tr>
</tbody>
</table>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Labels for graph</span>
<span class="n">lab1</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="o">-</span> <span class="m">2</span><span class="o">*</span><span class="n">SE</span><span class="p">)</span>
<span class="n">lab2</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="o">-</span> <span class="m">1</span><span class="o">*</span><span class="n">SE</span><span class="p">)</span>
<span class="n">lab3</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="m">1</span><span class="o">*</span><span class="n">SE</span><span class="p">)</span>
<span class="n">lab4</span> <span class="o">&lt;-</span> <span class="nf">expression</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="n">SE</span><span class="p">)</span>

<span class="c1"># Draw sampling distribution</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-4</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">05</span><span class="p">),</span> <span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;none&quot;</span><span class="p">,</span>  <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Density&quot;</span><span class="p">,</span> 
     <span class="nf">dnorm</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">-4</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="n">.</span><span class="m">05</span><span class="p">),</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">)</span>
<span class="nf">axis</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nf">seq</span><span class="p">(</span><span class="m">-2</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="m">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="n">lab1</span><span class="p">,</span> <span class="n">lab2</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="n">lab3</span><span class="p">,</span> <span class="n">lab4</span><span class="p">))</span>

<span class="c1"># True population value</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="c1"># 1.96 SE from population value</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-1.96</span><span class="p">,</span> <span class="m">1.96</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>

<span class="c1"># Some 95% confidence intervals</span>
<span class="nf">points</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0.2</span><span class="p">,</span> <span class="m">1.96</span><span class="p">,</span> <span class="m">2.15</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.13</span><span class="p">,</span> <span class="m">0.03</span><span class="p">,</span> <span class="m">0.18</span><span class="p">),</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;orange&quot;</span><span class="p">)</span>

<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-1.76</span><span class="p">,</span> <span class="m">2.16</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.13</span><span class="p">,</span> <span class="m">0.13</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">3.92</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.03</span><span class="p">,</span> <span class="m">0.03</span><span class="p">),</span> <span class="n">col</span> <span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0.19</span><span class="p">,</span> <span class="m">4.17</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.18</span><span class="p">,</span> <span class="m">0.18</span><span class="p">),</span> <span class="n">col</span> <span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>

<span class="nf">text</span><span class="p">(</span><span class="m">2.75</span><span class="p">,</span> <span class="m">0.08</span><span class="p">,</span> <span class="nf">expression</span><span class="p">(</span><span class="nf">hat</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span><span class="o">==</span><span class="m">1.96</span><span class="o">*</span><span class="n">SE</span><span class="p">))</span>
<span class="nf">text</span><span class="p">(</span><span class="m">-2.6</span><span class="p">,</span> <span class="m">0.25</span><span class="p">,</span> <span class="nf">expression</span><span class="p">(</span><span class="nf">hat</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span><span class="o">&lt;</span><span class="m">1.96</span><span class="o">*</span><span class="n">SE</span><span class="p">))</span>
<span class="nf">text</span><span class="p">(</span><span class="m">2.95</span><span class="p">,</span> <span class="m">0.23</span><span class="p">,</span> <span class="nf">expression</span><span class="p">(</span><span class="nf">hat</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span><span class="o">&gt;</span><span class="m">1.96</span><span class="o">*</span><span class="n">SE</span><span class="p">))</span>

<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2.25</span><span class="p">,</span> <span class="m">3</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.185</span><span class="p">,</span> <span class="m">0.215</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2.05</span><span class="p">,</span> <span class="m">2.8</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.035</span><span class="p">,</span> <span class="m">0.065</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-2.4</span><span class="p">,</span> <span class="m">0.2</span><span class="p">),</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.23</span><span class="p">,</span> <span class="m">0.14</span><span class="p">),</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;black&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/08.d. Frequentist II_1_0.png" src="_images/08.d. Frequentist II_1_0.png" />
</div>
</div>
</div>
<span id="document-08.e. Frequentist II"></span><div class="section" id="other-mis-interpretations-of-p-values">
<h3>8.4 Other (mis-)interpretations of p-values<a class="headerlink" href="#other-mis-interpretations-of-p-values" title="Permalink to this headline">¶</a></h3>
<div class="section" id="p-values-as-decision-rules">
<h4>8.4.1 P-values as decision rules<a class="headerlink" href="#p-values-as-decision-rules" title="Permalink to this headline">¶</a></h4>
<p>Traditionally, hypothesis tests have been thought of as a means to make decisions. In this paradigm, a cut-off (typically p&lt;0.05) is chosen. If the p-value is smaller than the chosen cut-off, the null hypothesis is rejected. If the p-value is above the cut-off then the null hypothesis is accepted. This leads to the terminology of:</p>
<ul class="simple">
<li><p>“Type I error”, rejecting the null hypothesis when it is true</p></li>
<li><p>“Type II error”, accepting the null hypothesis when it is false</p></li>
</ul>
<p>Linked to this approach is the habit of labelling p-values &lt; 0.05 as “significant” and those larger as “non-significant”.</p>
<p>There are some instances where this decision-making paradigm seems appropriate. Some health data science research is indeed concerned with decision making. For example, we may wish to carry out a trial to assess whether a particular clinical decision support system improves the clinicians’ ability to detect malignant tumours. However much health data science research is not, at least directly, concerned with decision making. For example if we carry out an epidemiological study in which we relate risk of a particular disease to gender, we do this because we are interested in understanding the aetiology of the disease, not because we want to assess whether to modify gender! For this reason many researchers regard p-values as a measure of strength of evidence against the null hypothesis, rather than as an aid to decision
making.</p>
<p>In general, we do not advocate any approach which dichotomises p-values. There is very little difference, in terms of the information contained about the population parameter, between the two p-values of <span class="math notranslate nohighlight">\(p=0.049\)</span> and <span class="math notranslate nohighlight">\(p=0.051\)</span>. Therefore it seems counter-intuitive to make very different decisions based on these p-values.</p>
<p>P-values represent an area of substantial philosophical controversy in statistics. We choose to interpret the p-value as a measure of strength of evidence against the null hypothesis. It should, however, be pointed out that some statisticians advocate strongly against this interpretation.</p>
<p>In much health data science research, we are interested in knowing more about a particular population parameter. Many health data scientists, therefore, choose to focus on obtaining and interpreting estimates and confidence intervals rather than calculating p-values.</p>
</div>
<div class="section" id="misinterpretations-of-p-values">
<h4>8.4.2 Misinterpretations of p-values<a class="headerlink" href="#misinterpretations-of-p-values" title="Permalink to this headline">¶</a></h4>
<p>The p-value is the subject of a lot of argument, debate and controversy, both within the statistical world and beyond. The following warn against some common misinterpretations and mis-uses of p-values:</p>
<blockquote>
<div><p>Do not: <br> - believe that an association or effect exists just because it was statistically significant.
<br> - conclude that an association or effect is absent just because it was not statistically significant.
<br> - base conclusions solely on whether an association or effect was statistically significant or not.
<br> - conclude anything about scientific or practical importance based on statistical significance (or lack thereof).
<br> - interpret a p-value as the probability that chance alone produced the observed association or effect or the probability that the null hypothesis is true.</p>
</div></blockquote>
<p>Importantly, statistical significance was never meant to imply scientific or clinical importance. As well as the p-value, always consider the estimated effect of the population parameter of interest and its confidence interval. These will often provide more insight than the p-value alone.</p>
</div>
</div>
<span id="document-08.f. Frequentist II"></span><div class="section" id="calculating-p-values">
<h3>8.5 Calculating p-values<a class="headerlink" href="#calculating-p-values" title="Permalink to this headline">¶</a></h3>
<div class="section" id="example-calculation-of-the-p-value">
<h4>8.5.1 Example: Calculation of the p-value<a class="headerlink" href="#example-calculation-of-the-p-value" title="Permalink to this headline">¶</a></h4>
<p>In the emotional distress example, our difference in sample means is <span class="math notranslate nohighlight">\(\hat{\delta} = -0.892\)</span>.
We are interested in the distribution of the difference in sampling means would look like under repeated sampling <em>if the null hypothesis were true</em>. The null hypothesis states that <span class="math notranslate nohighlight">\(\delta = 0\)</span>. Therefore, under the null hypothesis,</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} \sim  N(0,  0.507^2) 
\]</div>
<p>The easiest way to do this calculation is to standardise the estimator to follow a standard normal distribution, i.e.</p>
<div class="math notranslate nohighlight">
\[
Z = \frac{\hat{\delta}}{0.507} \sim  N(0, 1) 
\]</div>
<p>In our sample, we get a value of <span class="math notranslate nohighlight">\(Z=-0.892/0.507 = -1.76\)</span>. The p-value is defined as</p>
<div class="math notranslate nohighlight">
\[
p = Pr( | \hat{\delta} | \geq  -0.892) = Pr( | Z | \geq   1.76)
\]</div>
<p>The standard normal distribution is symmetric, so this is equal to <span class="math notranslate nohighlight">\(2 \times P(Z \geq 1.76)\)</span>. This probability can be looked up using pre-calculated tables stored in all standard statistical software.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Manual calculation of p-value: </span>
<span class="m">2</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="nf">pnorm</span><span class="p">(</span><span class="m">1.76</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.0784078065749654</div></div>
</div>
</div>
<div class="section" id="approximate-tests-in-large-samples">
<h4>8.5.2 Approximate tests in large samples<a class="headerlink" href="#approximate-tests-in-large-samples" title="Permalink to this headline">¶</a></h4>
<p>More generally, suppose that the random variable used to calculate our p-value (above, the random variable was the  difference in sample means) is denoted by <span class="math notranslate nohighlight">\(R\)</span> and that it has an expected value and variance (under the null hypothesis) denoted by <span class="math notranslate nohighlight">\(E(R)\)</span> and <span class="math notranslate nohighlight">\(Var(R)\)</span>. Then define:</p>
<div class="math notranslate nohighlight">
\[
Z = \frac{R - E[R]}{\sqrt{Var(R)}} = \frac{R - E[R]}{SE(R)} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(SE(R)\)</span> is the standard error of <span class="math notranslate nohighlight">\(R\)</span> (the standard deviation of the sampling distribution; alternatively the square root of the variance of <span class="math notranslate nohighlight">\(R\)</span>). To simplify this even further, in many cases, as for the difference in sample means, <span class="math notranslate nohighlight">\(E(R) = 0\)</span>.</p>
<p>Thanks to the Central Limit Theorem, in almost all situations, as the sample size <span class="math notranslate nohighlight">\(n\)</span> becomes large, the distribution of <span class="math notranslate nohighlight">\(Z\)</span> tends towards a standard normal distribution.</p>
<div class="math notranslate nohighlight">
\[
lim_{n\rightarrow \infty} \ \  Z \sim N(0, 1). 
\]</div>
<p>The standard normal distribution can then be used to calculate the two-sided p-value, as above.</p>
</div>
<div class="section" id="the-two-sample-t-test">
<h4>8.5.3 The two-sample t-test<a class="headerlink" href="#the-two-sample-t-test" title="Permalink to this headline">¶</a></h4>
<p>Let us return to the comparison in population means between two groups. When, as is more typical, we do not know the value of <span class="math notranslate nohighlight">\(\sigma\)</span>, we need to replace it with an estimate from our sample, <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>. Typically we use an estimate based on the sample standard deviations in the two groups, <span class="math notranslate nohighlight">\(s_1\)</span> and <span class="math notranslate nohighlight">\(s_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{(n_1 - 1) s_1^2 + (n_0 - 1) s_0^2}{n_1 + n_0 - 2}
\]</div>
<p>For our sample of data, <span class="math notranslate nohighlight">\(\hat{\sigma} = 1.873\)</span>. The sampling distribution we used above involves the true population standard deviation</p>
<div class="math notranslate nohighlight">
\[
\hat{\delta} \sim N\left(\delta, \sigma^2 \left(\frac{1}{n_1} + \frac{1}{n_0} \right) \right)
\]</div>
<p>Similarly, the equivalent version of the sampling distribution (which we will find it easier to modify for our current purposes), is also no longer exactly true:</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\delta} - \delta}{\sigma \sqrt{\left(\frac{1}{n_1} + \frac{1}{n_0}\right) }}\sim N(0,1)
\]</div>
<p>This is only approximately true if we substitute the sample estimate <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> into the equation. A little more algebra (not shown here), however, gives us an exact distribution.</p>
<div class="math notranslate nohighlight">
\[
\frac{\hat{\delta} -\delta}{\hat{\sigma} \sqrt{\frac{1}{n_1} + \frac{1}{n_0}}} \sim t_{n_1 + n_0 - 2}
\]</div>
<p>Under the null hypothesis, <span class="math notranslate nohighlight">\(\delta = 0\)</span>, giving</p>
<div class="math notranslate nohighlight">
\[
T = \frac{\hat{\delta}}{\hat{\sigma} \sqrt{\frac{1}{n_1} + \frac{1}{n_0}}} \sim t_{n_1 + n_0 - 2}
\]</div>
<p>Substituting in the numbers from our sample of data,</p>
<div class="math notranslate nohighlight">
\[
T = \frac{-0.892}{1.873 \sqrt{\frac{1}{22} + \frac{1}{26}}} 
\]</div>
<p>gives <span class="math notranslate nohighlight">\(t = -1.644\)</span> (remembering that <span class="math notranslate nohighlight">\(T\)</span> is the random variable and <span class="math notranslate nohighlight">\(t\)</span> here is the realised (observed) value of that statistic). T-distributions are symmetric around zero, so we take <em>at least as extreme as</em> to mean less than -1.64 or greater than +1.64, which in turn is twice the probability of being less than -1.64. We simply need to calculate this probability for a t-distribution with 46 degrees of freedom (where we obtained 46 as <span class="math notranslate nohighlight">\(n_1 + n_0 - 2\)</span>).</p>
<p>The code below performs this calculation and then uses an inbuilt R package to obtain the same p-value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Manual calculation of p-value (two equivalent calculations)</span>
<span class="m">2</span><span class="o">*</span><span class="nf">pt</span><span class="p">(</span><span class="m">-1.644</span><span class="p">,</span> <span class="m">46</span><span class="p">)</span>

<span class="c1"># Read in data (emotional distress scores in control and intervention group)</span>
<span class="n">dist0</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">8</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span>  <span class="m">9</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">7</span><span class="p">)</span>
<span class="n">dist1</span><span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">3</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">8</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">7</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">5</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">4</span><span class="p">,</span>  <span class="m">6</span><span class="p">,</span>  <span class="m">3</span><span class="p">,</span>  <span class="m">5</span><span class="p">)</span>

<span class="c1"># T-test using inbuilt R package</span>
<span class="n">dist</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="n">dist0</span><span class="p">,</span> <span class="n">dist1</span><span class="p">)</span> 
<span class="n">gp</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">26</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">22</span><span class="p">))</span>

<span class="nf">t.test</span><span class="p">(</span><span class="n">dist</span><span class="o">~</span><span class="n">gp</span><span class="p">,</span> <span class="n">var.equal</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.106994541315052</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>	Two Sample t-test

data:  dist by gp
t = 1.6435, df = 46, p-value = 0.1071
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.2004223  1.9836391
sample estimates:
mean in group 0 mean in group 1 
       6.346154        5.454545 
</pre></div>
</div>
</div>
</div>
<p>Rounding to 2 decimal places, the p-value is 0.11.</p>
<p>In the output from the R package, the line</p>
<p><code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">1.6435,</span> <span class="pre">df</span> <span class="pre">=</span> <span class="pre">46,</span> <span class="pre">p-value</span> <span class="pre">=</span> <span class="pre">0.1071</span></code></p>
<p>tells us that the value of the statistic <span class="math notranslate nohighlight">\(T\)</span> above is <span class="math notranslate nohighlight">\(t=1.64\)</span> in this sample, the degrees of freedom tell us that we are looking at a t-distribution on 46 degrees of freedom. We are also given a 95% confidence interval for the population difference in means: (-0.20 to 1.98). As we noted above, when the p-value is &gt;0.05 then the null value (here, zero) will be included in the 95% confidence interval.</p>
</div>
<div class="section" id="other-hypothesis-tests">
<h4>8.5.4 Other hypothesis tests<a class="headerlink" href="#other-hypothesis-tests" title="Permalink to this headline">¶</a></h4>
<p>You will meet many types of hypothesis tests over your statistical studies. Many, like the t-test above, are constructed around a particular estimator and so there is a nice connection between the estimate, the 95% confidence interval and the p-value from the hypothesis test. Where this is the case, it is good practice to present the estimate and confidence interval alongside the p-value, since they contain much more information than the p-value alone.</p>
<p>In other cases, tests can be constructed without a specific parameter being estimated. The chi-squared test is a very commonly-used test. It tests the null hypothesis of no association between two unordered categorical variables. This test does not directly invoke the sampling distribution of an estimator, so typically only the p-value is presented, rather than also presenting an estimate and confidence interval.</p>
<p>In general, hypothesis testing is a controversial and widely misunderstood area of frequentist statistics. Where possible, focusing on estimating parameters along with confidence intervals can avoid some of the more damaging misuses of p-values.</p>
</div>
</div>
<span id="document-08.g. Frequentist II"></span><div class="section" id="further-resources">
<h3>Further resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h3>
<p>Note: further resources are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. All examinable material is contained within the notes.</p>
<p>Stang A, Poole C, Kuss O. The ongoing tyranny of statistical significance testing in biomedical research. Eur J Epidemiol. 2010;25(4):225-230. doi:10.1007/s10654-010-9440-x</p>
<p><a class="reference external" href="https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108">Ronald L. Wasserstein &amp; Nicole A. Lazar (2016) The ASA Statement on p-Values: Context, Process, and Purpose, The American Statistician, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108</a></p>
<p><a class="reference external" href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">Ronald L. Wasserstein, Allen L. Schirm &amp; Nicole A. Lazar (2019) Moving to a World Beyond “p &lt; 0.05”, The American Statistician, 73:sup1, 1-19, DOI: 10.1080/00031305.2019.1583913</a></p>
<p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1119478/">Sterne JA, Davey Smith G. Sifting the evidence-what’s wrong with significance tests? BMJ. 2001;322(7280):226-231. doi:10.1136/bmj.322.7280.226</a></p>
</div>
</div>
</div>
<span id="document-09.a. Bayesian Statistics I"></span><div class="section" id="bayesian-statistics-i">
<h2>9. Bayesian Statistics I<a class="headerlink" href="#bayesian-statistics-i" title="Permalink to this headline">¶</a></h2>
<p>So far in this module, we have looked at frequentist or classical statistical ideas, such as maximum likelihood estimation, hypothesis testing and p-values. Underlying the frequentist approach is the belief that there is a true state of reality, and that parameters have a fixed and true value. Probabilities are long-run frequencies; for example, the probability of a driver in London having a car accident is a fixed value between 0 and 1. A typical way of estimating this value is to take a sample of observations, construct a likelihood function for these observations, and to obtain the parameter value that maximizes the likelihood. When we take a Bayesian approach, the parameter we wish to estimate is considered to be a random variable, and probabilities may represent a subjective belief about the state of uncertainty, or there may be a data generating distribution underlying the random parameter. For example, you may have a prior belief about the probability of a driver in London having a car accident, and after collecting a sample of data, you combine your prior beliefs with the likelihood for those observations to construct an updated belief - the posterior. Your belief may change in light of the data.</p>
<p>Bayesian methods sometimes require numerical integration, and cheaper computing has made Bayesian approaches more feasible in the last 20 years. Bayesian approaches are likely to be an important part of working in Health Data Science. In the next two sessions, we introduce the fundamental principles.</p>
<p>The current session introduces the basic concepts underlying Bayesian inference and then applies the basic principles to a simple example using proportions.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>compare the notions of probability and likelihood in Bayesian and Frequentist paradigms</p></li>
<li><p>explain the notions of prior and posterior distributions</p></li>
<li><p>apply Bayes Theorem in the discrete case</p></li>
<li><p>Understand and apply the basic principles of Bayesian analysis using proportions, specifically:</p>
<ul>
<li><p>use the beta distribution as a prior and derive the posterior distribution</p></li>
<li><p>obtain credible HPD intervals for the parameter</p></li>
<li><p>obtain prior and posterior predictive distributions</p></li>
<li><p>explain the concept of conjugate priors</p></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<span id="document-09.b. Bayesian Statistics I"></span><div class="section" id="introduction-to-bayesian-inference">
<h3>9.1 Introduction to Bayesian Inference<a class="headerlink" href="#introduction-to-bayesian-inference" title="Permalink to this headline">¶</a></h3>
<div class="section" id="probability">
<h4>9.1.1 Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h4>
<p>In Session 2, we learned about probability in the frequentist sense: the proportion of times an event occurs in the long-run. Let’s have a look at the following two scenarios:</p>
<ol class="simple">
<li><p>A research group wishes to know the probability that a baby who is born in a particular hospital ward has cystic fibrosis. They look at the records on screening tests done at birth to investigate.</p></li>
<li><p>A 34 year old woman attends her GP practice, worried that she has cancer because she has had feelings of “fullness” and “bloating” as well as mild nausea for the last 2 weeks. The patient mentions ovarian, bowel and pancreatic cancer as concerns having read about her symptoms on the internet. The rest of the history as well as physical examination are unremarkable. If the GP’s assessment of the risk were above a certain level, the GP might refer the patient for tests (collect more data). In this case, the GP concludes that the current information about the patient suggests there is a very low risk that the patient has cancer.</p></li>
</ol>
<blockquote>
<div><p>What is the quantity that we trying to estimate in each scenario?<br />
What is the frequentist definition of probability in each of these settings? Does it make sense?</p>
</div></blockquote>
<p>A key problem with the frequentist paradigm is that the “long-run” frequency definition is not always relevant, or even appropriate, as we see in the second example above. Further, notice that the GP uses information from different sources to draw his/her conclusion about the probability that the patient has cancer. This synthesis of information can be incorporated into a Bayesian framework. A frequentist, in contrast, would tackle this problem by thinking about:</p>
<blockquote>
<div><p>a) the probability of the patient having these symptoms, given that she has cancer;<br />
b) the probability of the patient having these symptoms, given that she does not have cancer;</p>
</div></blockquote>
<p>and comparing the two probabilities. Note that this does not take into account the extra information about the context.</p>
</div>
<div class="section" id="bayesian-inference">
<h4>9.1.2 Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">¶</a></h4>
<p>The underlying concept for Bayesian inference essentially works as follows. We have some population parameter <span class="math notranslate nohighlight">\(\theta\)</span> which we wish to make inference on, and the likelihood <span class="math notranslate nohighlight">\(p(y|\theta)\)</span> which tells us how likely different values of <span class="math notranslate nohighlight">\(y\)</span> are, conditional on different parameter values <span class="math notranslate nohighlight">\(\theta\)</span>. In the frequentist approach, <span class="math notranslate nohighlight">\(\theta\)</span> is considered to be a fixed, but unknown, constant. Inference is then based on the likelihood <span class="math notranslate nohighlight">\(p(\mathbf{y}|\theta)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{y} = \left\{y_1, . . . ,y_n\right\}\)</span> is a sample of observations from the population. The frequentist approach looks at the distribution of the data given <span class="math notranslate nohighlight">\(\theta\)</span> to estimate <span class="math notranslate nohighlight">\(\theta\)</span> by using, for example, the maximum likelihood approach which we covered in Session 6.</p>
<p>In the Bayesian paradigm, we no longer assume that the parameters have a fixed true value, but consider <span class="math notranslate nohighlight">\(\theta\)</span> to be a random quantity with an unknown distribution, which we wish to estimate. This distribution is denoted by <span class="math notranslate nohighlight">\(p(\theta|y)\)</span>, and so we look at the distribution of the parameter, having seen data <span class="math notranslate nohighlight">\(y\)</span>. To achieve this, we will have to specify a prior probability distribution, denoted <span class="math notranslate nohighlight">\(p(\theta)\)</span>, which represents our initial beliefs about the distribution of <span class="math notranslate nohighlight">\(\theta\)</span>
prior to observing any data. In some situations, when we are trying to estimate a parameter <span class="math notranslate nohighlight">\(\theta\)</span> we have some knowledge, about the possible value of <span class="math notranslate nohighlight">\(\theta\)</span> before we take into account the data that we observe.</p>
<p>For example, consider the way a physician makes diagnostic decisions. A patient presents with a set of symptoms, concerned that they might have a certain disease. The physician assesses the probability that this patient has this disease, based on symptoms, family history, alternative explanations of symptoms and prevalence of the disease (their prior view that the patient has the disease). The physician might send the patient for a diagnostic test (collects some data) if her prior assessment of risk is above some threshold. Then the physician re-assesses the chance that the patient has this disease, taking account of the results and reliability of the diagnostic test (updates their prior in light of the data to get a posterior view on whether the patient has the disease). Depending on their certainty, the physician may then send the patient for further diagnostic tests. This thought process can be represented by the figure below and is analogous to Bayesian thinking.</p>
<div class="figure align-default" id="physician">
<a class="reference internal image-reference" href="_images/Physician.png"><img alt="_images/Physician.png" src="_images/Physician.png" style="height: 300px;" /></a>
</div>
<p>In this example, the physician is assessing the probability that the patient has the disease. It is the physician’s prior probability based on their own training, knowledge and experience; a colleague may have a different prior probability. Here, prior probability is being defined subjectively. The size of the probability represents the physician’s degree of belief about the occurrence of an event, i.e. their own personal assessment of how likely an event is, based on the evidence available to them before the test results are given. This definition corresponds more closely to the everyday, intuitive  usage of probability than a frequentist interpretation (where the probability of a particular event occurring can be interpreted as the proportion of times the event would/does occur in a large number of similar trials or situations). The prior probability of the event might come from direct data, known prevalance of disease in a population, or data from related populations. If such prior information does not exist, then it can be formally elicited from experts, but we would want to acknowledge the uncertainty in the experts’ knowledge.</p>
</div>
</div>
<span id="document-09.c. Bayesian Statistics I"></span><div class="section" id="bayes-theorem-recap">
<h3>9.2 Bayes Theorem (recap)<a class="headerlink" href="#bayes-theorem-recap" title="Permalink to this headline">¶</a></h3>
<p>Let’s remind ourselves of Bayes theorem for discrete events, which we met in Session 2 (probability):</p>
<p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are events, then</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{ P(B|A) P(A) } {P(B)} \propto P(B|A) P(A),
\]</div>
<p>or in words:</p>
<div class="math notranslate nohighlight">
\[
\mbox{posterior probability of A given B} \propto \mbox{the likelihood of B given A} \times \mbox{the prior probability of A}.
\]</div>
<p>Also, if <span class="math notranslate nohighlight">\(A_i\)</span> is a set of mutually exclusive and exhaustive events, i.e. <span class="math notranslate nohighlight">\( p( \bigcup\limits_i A_i ) = \sum\limits_i p(A_i) = 1\)</span> and <span class="math notranslate nohighlight">\(A_i \cap A_j = \emptyset\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
p(A_i|B) = \frac{ p(B|A_i) p(A_i) } {\sum\limits_j p(B|A_j) p(A_j) }.
\]</div>
<p>The calculation of the denominator is more difficult if we have continuous parameters as it requires integration over A; we will discuss this in the next section.</p>
<p>We will illustrate Bayes Theorem further with the diagnostic test example for Covid-19 below. We see Bayesian reasoning is purely probabilistic. Bayes theorem gives us a principled way to update prior probabilities on the basis of new data.</p>
<div class="section" id="example">
<h4>9.2.1 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://www.bmj.com/content/bmj/369/bmj.m1808.full.pdf">Watson (2020)</a> discusses some interesting issues around the interpretation of Covid-19 diagnostic tests. Typically, a clinician estimates a pre-test probability (a prior probability) of having Covid-19 for a particular area, which is derived from knowledge about local rates of Covid-19. Then, given a patient’s test result, the post-test probability (the posterior probability) of having Covid-19 is obtained. The posterior probability depends on the pre-test probability, as well as the sensitivity and specificity of the test, which are difficult to estimate; often, sensitivity is over-estimated. The article discusses how one can be fairly confident about a positive test result, but more caution is needed for a negative test result, as there may still be quite a high chance that a person has Covid-19. We illustrate this with Bayes’ theorem.</p>
<p>Suppose that, in a student hall of residence, the prevalence of Covid-19 if you have a persistent cough is <span class="math notranslate nohighlight">\(75\%\)</span>. Suppose we assume that the test will be positive in Covid-19 patients <span class="math notranslate nohighlight">\(70\%\)</span> of the time (sensitivity is 0.7), and it will be negative in non-Covid-19 patients <span class="math notranslate nohighlight">\(95\%\)</span> of the time (specificity is 0.95). Given that a student in this hall with a persistent cough tests negative, what is the probability that they have Covid-19? In other words, what is the probability of a false negative?</p>
<p>Let us denote by <span class="math notranslate nohighlight">\(C+\)</span> the event that a person has Covid-19, and <span class="math notranslate nohighlight">\(C-\)</span> the event that a person does not have Covid-19.  Further we denote by <span class="math notranslate nohighlight">\(T+\)</span> and <span class="math notranslate nohighlight">\(T-\)</span> the events that a person has a positive and a negative test, respectively. The information we are given is that:</p>
<div class="math notranslate nohighlight">
\[
p(C+)=0.75, \qquad p(T+|C+)=0.70, \qquad p(T-|C-)=0.95
\]</div>
<p>Now, what we want is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\mbox{false negative}) &amp;= p(C+|T-) = \frac{p(T-|C+)p(C+)}{p(T-)} \\
&amp;= \frac{p(T-|C+)p(C+)}{p(T-|C+)p(C+) + p(T-|C-)p(C-)} \\
&amp;= \frac{(1-0.7) \times 0.75}{(1-0.7) \times 0.75 + 0.95 \times 0.25} \\
&amp;= \frac{0.225}{0.4625} \\
&amp;= 0.4864
\end{align}
\end{split}\]</div>
<p>You can see that, despite the negative test result, due to the very high prevalence of Covid-19 in the hall of residence and the relatively low sensitivity rate, there is still a 48.64% chance that a person has Covid-19.</p>
<p>Suppose a different student has no symptoms. The prevalence of Covid-19 in asymptomatic people is 0.1. They use the same diagnostic test and the test result is positive. What is the probability that this student with a positive test result has Covid-19? In other words, what is <span class="math notranslate nohighlight">\(p(C+|T+)\)</span>?</p>
<p>Solution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(C+|T+) &amp;= \frac{p(T+|C+)p(C+)}{p(T+)} \\ &amp; = \frac{p(T+|C+)p(C+)}{p(T+|C+)p(H+) + p(T+|C-)p(C-)}  \\
&amp;= \frac{0.7 \times 0.1}{0.7 \times 0.1 + (1-0.95) \times 0.9} \\
&amp;= \frac{0.07}{0.115} \\
&amp;= 0.609
\end{align}
\end{split}\]</div>
<p>This means that, amongst all the people who test positive, <span class="math notranslate nohighlight">\(60.9\%\)</span> will actually have the disease. After a positive result from a test, the probability that you have Covid-19 increase from <span class="math notranslate nohighlight">\(10\%\)</span> to <span class="math notranslate nohighlight">\(61\%\)</span>.</p>
<p>Note that these results are specific to the the prevalence of Covid-19 in the area, as well as the sensitivity and specificity of the diagnostic test. The code below reproduces the leaf-plot from <a class="reference external" href="https://www.bmj.com/content/bmj/369/bmj.m1808.full.pdf">Watson (2020)</a>. The <span class="math notranslate nohighlight">\(x\)</span>-axis is the pre-test probability of having Covid-19. The corresponding <span class="math notranslate nohighlight">\(y\)</span>-values on the lower curve (lower leaf) are the post-test probabilities of having Covid-19, following a negative test result. The corresponding <span class="math notranslate nohighlight">\(y\)</span>-values on the upper curve (upper leaf) are the post-test probabilities of having Covid-19, following a positive test result. The correponding values on the diagonal (<span class="math notranslate nohighlight">\(y=x\)</span>) line represent probabilities if no test is carried out.</p>
<p>In our first example, the prevalence in symptomatic people is 0.75, so we follow the orange arrows to find that the post-test probability after a negative result 0.4864. In the second example, the prevalence in asymptomatic people is 0.1. We follow the purple arrows to find that the post-test probability after a positive result is 0.609. How do you think the shape of the lower and upper leaves would change, if sensitivity was higher? If specificity was lower? Re-run the code with different values to check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Function takes as arguments the sensitivitiy of the test (sensi) </span>
<span class="c1"># and the specificity (speci)</span>

<span class="n">leafplot</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">sensi</span><span class="p">,</span> <span class="n">speci</span><span class="p">){</span>
  
  <span class="n">pretest</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span> <span class="c1">#possible pre-test probabilities </span>
  
  <span class="c1">#probability of having Covid-19 after a positive test result </span>
  <span class="n">pos.test</span> <span class="o">&lt;-</span> <span class="n">sensi</span><span class="o">*</span><span class="n">pretest</span><span class="o">/</span><span class="p">(</span><span class="n">sensi</span><span class="o">*</span><span class="n">pretest</span><span class="o">+</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">speci</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">pretest</span><span class="p">))</span>
  
  <span class="c1">#probability of having Covid-19 after a negative test result </span>
  <span class="n">neg.test</span> <span class="o">&lt;-</span> <span class="p">((</span><span class="m">1</span><span class="o">-</span><span class="n">sensi</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">pretest</span><span class="p">))</span><span class="o">/</span><span class="p">((</span><span class="m">1</span><span class="o">-</span><span class="n">sensi</span><span class="p">)</span><span class="o">*</span><span class="n">pretest</span><span class="o">+</span><span class="n">speci</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">pretest</span><span class="p">))</span>
  
  <span class="c1">#plot leaves</span>
  <span class="nf">plot</span><span class="p">(</span><span class="n">pretest</span><span class="p">,</span> <span class="n">pos.test</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;darkgreen&quot;</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Pre-test Probability&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Post-test Probability&quot;</span><span class="p">)</span>
  <span class="nf">points</span><span class="p">(</span><span class="n">pretest</span><span class="p">,</span> <span class="n">neg.test</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;darkgreen&quot;</span><span class="p">)</span>
  <span class="nf">abline</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;darkgreen&quot;</span><span class="p">)</span>
  <span class="nf">legend</span><span class="p">(</span><span class="s">&quot;topleft&quot;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Positive Test&quot;</span><span class="p">,</span> <span class="s">&quot;Negative Test&quot;</span><span class="p">),</span>
        <span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Purple&quot;</span><span class="p">,</span> <span class="s">&quot;Orange&quot;</span><span class="p">),</span> <span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">bg</span><span class="o">=</span><span class="s">&quot;transparent&quot;</span><span class="p">)</span>
  
  <span class="c1">#plot arrows </span>
    <span class="c1">#we use pretest[11] to get the prevalence value of 0.1, and </span>
    <span class="c1">#pretest[76] to get the prevalence value of 0.75 in the vector &quot;pretest&quot;</span>
    
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">pretest</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">pos.test</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">)</span>
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">pos.test</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">pos.test</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">)</span>
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">pretest</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">neg.test</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">neg.test</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">neg.test</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
  
  <span class="p">}</span>

<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">6.5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5.5</span><span class="p">)</span>
<span class="nf">leafplot</span><span class="p">(</span><span class="n">sensi</span><span class="o">=</span><span class="m">0.7</span><span class="p">,</span> <span class="n">speci</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>


<span class="c1">#See what happens to the plot when you change sensitivity and specificity! </span>
<span class="c1">#leafplot(0.95, 0.8)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09.c. Bayesian Statistics I_3_0.png" src="_images/09.c. Bayesian Statistics I_3_0.png" />
</div>
</div>
</div>
</div>
<span id="document-09.d. Bayesian Statistics I"></span><div class="section" id="the-bayesian-paradigm-in-health-data-science-problems">
<h3>9.3 The Bayesian paradigm in Health data science problems.<a class="headerlink" href="#the-bayesian-paradigm-in-health-data-science-problems" title="Permalink to this headline">¶</a></h3>
<p>In this section we discuss the Bayesian approach in Health data science problems. Some features of the Bayesian paradigm are particularly useful in this context:</p>
<ol class="simple">
<li><p>Bayes theorem provides a statistically principled method for combining data. Thus, we can take into account the context within which the data are generated. For example, results of a diagnostic test may have a different interpretation/consequence if used in a symptomatic patient than in a general screening programme. The prior probability of disease would be higher in the former than the latter. Priors can then be updated by the test result to give an assessment of disease risk specific to the local prevalence.</p></li>
<li><p>For problems where there are multiple or diverse sources of data which must be combined, the Bayesian framework provides a natural environment for doing so. Examples where Bayesian synthesis of information is common are:<br />
•	models of biological systems, for example genetic and genomic pathways,<br />
•	models of the natural history of diseases over time and relationships with clinical events,<br />
•	economic models of disease trajectories and cost-effect trade-offs for interventions that interrupt the trajectories,<br />
•	ecological studies of pollutant emissions and effects on population health,<br />
•	demographic studies, for example to study migration,<br />
•	speech recognition software,<br />
•	other pattern recognition models such as medical imaging or search engines,<br />
•	epidemic modelling.<br />
In all these examples complex data is synthesised and/or used to update outputs.</p></li>
<li><p>Bayesian models fit well into decision theory methodology, providing we can also specify consequences of model outputs.</p></li>
<li><p>In many examples, especially those that aim to model complicated processes, some of the data inputs are very sparse, or even non-existent. In such cases, prior data may be formally elicited from an expert panel and incorporated in a Bayesian analysis. Examples include multiple evidence synthesis and identification of latent groups.</p></li>
<li><p>Bayesians are allowed to make direct probability statements about unknown quantities. Frequentists cannot make these direct probability statements because the unknown model parameters are assumed fixed.</p></li>
<li><p>In recent years the resources available to complete Bayesian analysis have increased, including bespoke software and packages within commercial statistical software.</p></li>
</ol>
<p>But Bayesian methods are not that widely used in statistics compared with more classical approaches because they have some limitations.</p>
<ol class="simple">
<li><p>Sometimes the need for a prior distribution is a barrier if little is known about a parameter and researchers fall back on priors that are weakly informative. In that case, it is not easy to see how much benefit comes from a Bayesian analysis.</p></li>
<li><p>Because of the need to use Bayesian updating via a prior distribution, the analysis almost always requires a parametric approach. This limits the structure of the analysis models. Although non-parametric Bayesian methods are available for some situations, they often have underlying parametric assumptions.</p></li>
<li><p>The numerical integration methods usually required for realistic problems are often computationally expensive. This is especially true if there are multiple sources of evidence to be combined.</p></li>
<li><p>Many statisticians are unfamiliar with the methods and associated software.</p></li>
</ol>
</div>
<span id="document-09.e. Bayesian Statistics I"></span><div class="section" id="bayes-thorem-for-discrete-and-continous-data">
<h3>9.4 Bayes thorem for discrete and continous data<a class="headerlink" href="#bayes-thorem-for-discrete-and-continous-data" title="Permalink to this headline">¶</a></h3>
<p>So far this session, we have looked at Bayes theorem in the discrete case. We turn to the more general case of Bayes thorem to make inference about an unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span>, which could be discrete or continuous.</p>
<p>The probability distribution for <span class="math notranslate nohighlight">\(\theta\)</span> reflects our uncertainty about it before seeing the data, <strong>prior distribution</strong>, <span class="math notranslate nohighlight">\(p(\theta)\)</span>. Once the data data <span class="math notranslate nohighlight">\(y\)</span> is known, we condition on it. Using Bayes theorem we obtain a conditional probability distribution for unobserved quantities of interest given the data. If <span class="math notranslate nohighlight">\(\theta\)</span> is continuous, we have:</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y)= \frac{ p(\theta)\, p(y \mid \theta)}{\int  p(\theta)\,p(y \mid \theta)\,d\theta},
\]</div>
<p>and <span class="math notranslate nohighlight">\(\theta\)</span> is discrete and takes values in the set <span class="math notranslate nohighlight">\(\Theta\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y)= \frac{ p(\theta)\, p(y \mid \theta)}{\sum_{\theta \in \Theta}  p(\theta) p(y \mid \theta) }.
\]</div>
<p>We call <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> the <strong>posterior distribution</strong>.</p>
<p>Note that the Bayesian approach is naturally synthetic in that it allows data from different sources to be combined, according to Bayes principles. This approach is most useful when there is informative prior information. We note that the Bayesian approach can be recursive, so <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> may be used as a prior when calculating <span class="math notranslate nohighlight">\(p(\theta \mid y, z)\)</span> for a second data set <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>The denominator, <span class="math notranslate nohighlight">\({\int  p(\theta)\,p(y \mid \theta)\,d\theta}\)</span> or <span class="math notranslate nohighlight">\(\sum_{\theta \in \Theta}  p(\theta) p(y \mid \theta)\)</span>, is a constant with respect to <span class="math notranslate nohighlight">\(\theta\)</span>. One of the challenges of using Bayesian approaches is that the integration can be analytically intractable, so that numerical methods are needed (for example, numerical integration or Markov Chain Monte Carlo methods). These methods are beyond the scope of the current module. In this introductory course, we will only look at examples where this constant need not be calculated, since the form of the posterior can be inferred by inspection once observing that the posterior is proportional to the product of the prior and likelihood:</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid y) \propto p(\theta)\,p(y \mid \theta).\]</div>
<p>We will see how this works for the inference of proportions.</p>
</div>
<span id="document-09.f. Bayesian Statistics I"></span><div class="section" id="bayesian-inference-on-proportions">
<h3>9.5 Bayesian inference on proportions<a class="headerlink" href="#bayesian-inference-on-proportions" title="Permalink to this headline">¶</a></h3>
<p>Consider a new drug being developed for the relief of chronic pain. To find out about its efficacy, we propose to run a single-arm early-phase clinical trial in which we give this drug to a number <span class="math notranslate nohighlight">\(n\)</span> of randomly selected patients. Because patients are independent of each other, so it seems reasonable to model the data using the Binomial distribution, <span class="math notranslate nohighlight">\(Y\sim Bin(n,\theta).\)</span> We have that <span class="math notranslate nohighlight">\(\theta\in [0,1]\)</span> is the probability of pain relief (success) in each patient, and this is unknown. We then make the observation that there are <span class="math notranslate nohighlight">\(y\)</span> successes out of <span class="math notranslate nohighlight">\(n\)</span> independent trials. As a reminder, the probability distribution function of the Binomial distribution is:</p>
<div class="math notranslate nohighlight">
\[
p \left(y \mid \theta \right) = {n \choose y} \theta^y (1-\theta)^{n-y}.
\]</div>
<p>To proceed, we need to have a prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>. Let us consider three possible prior distributions:</p>
<ol class="simple">
<li><p>An uninformative prior, where all values of <span class="math notranslate nohighlight">\(\theta\)</span> are equally probable.<br />
You essentially have no prior information about the effectiveness of the drug.</p></li>
<li><p>A symmetrical, concave prior that is centered at 0.5.<br />
You think that the drug is likely to be effective for patients around half of the time.</p></li>
<li><p>An asymmetrical prior with a spike at 0.1.<br />
You think that the drug is generally ineffective, and feel quite strongly about it.</p></li>
</ol>
<p>Now, the Beta distribution is a flexible distribution that can represent each of these prior beliefs by appropriate choice of its parameters. It is also convenient because it has a similar form to the Binomial distribution.</p>
<div class="section" id="the-beta-prior">
<h4>9.5.1 The Beta prior<a class="headerlink" href="#the-beta-prior" title="Permalink to this headline">¶</a></h4>
<p>The Beta distribution is a flexible two parameter distribution that is restricted to the interval between 0 and 1, and so it is a reasonable form for a probability distribution for a proportion. The two parameters, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, are often called  “shape” parameters. Given <span class="math notranslate nohighlight">\(\theta \sim \hbox{Beta}(a,b)\)</span>, the probability density function, expectation and variance of the distribution are as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\theta|a, b)  &amp;=  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \,\,
\theta^{a-1} \; (1-\theta)^{b-1}  \mbox{  where  } \theta \in (0,1) \\
{E}( \theta |a, b) &amp;=    \frac{a}{a+ b }     \\
{Var}( \theta |a, b) &amp;=    \frac{a b}{ (a+ b)^2 (a+ b+1)} 
\end{align}
\end{split}\]</div>
<p>The <em>Gamma function</em> <span class="math notranslate nohighlight">\(\Gamma(x)\)</span> is defined for positive integers as <span class="math notranslate nohighlight">\(\Gamma (x)=(x-1)!\)</span>, and has a more complex form for real numbers.</p>
<p>This prior distribution is very flexible. For example:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(a=1, b=1\)</span> results in the uniform distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(a=2, b=2\)</span> results in a symmetrical distribution centered on <span class="math notranslate nohighlight">\(p=0.5\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a=2, b=9\)</span> results in an asymmetrical distribution with a spike at <span class="math notranslate nohighlight">\(p=0.1\)</span>.</p></li>
</ol>
<p>These are the priors we specified earlier; they are plotted below. Note that the higher the values of <span class="math notranslate nohighlight">\(a, b,\)</span> the
smaller the variance of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">7</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(1,1) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(2,2) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">9</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(2,9) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09.f. Bayesian Statistics I_1_0.png" src="_images/09.f. Bayesian Statistics I_1_0.png" />
</div>
</div>
</div>
<div class="section" id="posterior">
<h4>9.5.2 Posterior<a class="headerlink" href="#posterior" title="Permalink to this headline">¶</a></h4>
<p>Now, we apply Bayes theorem to obtain the posterior distribution using a <span class="math notranslate nohighlight">\(Beta(a,b)\)</span> distribution for the prior:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\theta \mid y) &amp;= \frac{ p(\theta)\, p(y \mid \theta)} {\int  p(\theta)\,p(y \mid \theta)\,d\theta}\\
         &amp;\propto  p(\theta)\, p(y \mid \theta)
         \end{align}
\end{split}\]</div>
<p>Substituting in the appropriate distributions gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align} 
p(\theta \mid y) &amp;= \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1}  {n \choose y} \theta^y (1-\theta)^{n-y} \\
         &amp;\propto \theta^{a-1} (1-\theta)^{b-1} \theta^y (1-\theta)^{n-y} \\
         &amp;\propto \theta^{a+y-1} (1-\theta)^{b+n-y-1} 
         \end{align}
\end{split}\]</div>
<p>Now by inspection, we can see that this is in the form of a Beta distribution: we have that the posterior is proportional to <span class="math notranslate nohighlight">\(\theta^{a+y-1} (1-\theta)^{b+n-y-1}\)</span>. In other words, the posterior is <span class="math notranslate nohighlight">\(Beta(a+y, b+n-y).\)</span> This distribution has mean given by: <span class="math notranslate nohighlight">\(\frac{a+y}{a+b+n}\)</span> and variance <span class="math notranslate nohighlight">\(\frac{(a+y)(b+n-y)}{(a+b+n)^2(a+b+n+1)}.\)</span></p>
<p>Suppose the data we observe is <span class="math notranslate nohighlight">\(y=4\)</span> successes out of a total of <span class="math notranslate nohighlight">\(10\)</span> patients. Then:</p>
<ol class="simple">
<li><p>With the uniform <span class="math notranslate nohighlight">\(Beta(1,1)\)</span> prior, our posterior is <span class="math notranslate nohighlight">\(Beta(5, 7)\)</span>.</p></li>
<li><p>With the symmetrical <span class="math notranslate nohighlight">\(Beta(2, 2)\)</span> prior, our posterior is <span class="math notranslate nohighlight">\(Beta(6, 8)\)</span>.</p></li>
<li><p>With the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, our posterior is <span class="math notranslate nohighlight">\(Beta(6, 15)\)</span>.</p></li>
</ol>
<p>We plot the possible distibutions below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">7</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span>  <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(5, 7) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">8</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(6, 8) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(6, 15) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09.f. Bayesian Statistics I_3_0.png" src="_images/09.f. Bayesian Statistics I_3_0.png" />
</div>
</div>
<p>We can see that the uninformative prior leads to the posterior with the highest variance amongst the three. The narrow prior in the third example shifts the posterior distribution to the right. We can see that different choices of prior lead to different results. For this reason, it is often recommended to repeat analyses with different priors to see how much the results change: this is called <em>sensitivity analysis</em>.</p>
</div>
</div>
<span id="document-09.g. Bayesian Statistics I"></span><div class="section" id="summarising-posteriors">
<h3>9.6 Summarising Posteriors<a class="headerlink" href="#summarising-posteriors" title="Permalink to this headline">¶</a></h3>
<p>We often display the posterior distribution graphically to get a sense of the information that we have about the parameter. However, other ways to summarize the distribution can be helpful. We may also wish to summarise the posterior distribution by a credible interval.</p>
<p>Remember that a classical 95%  confidence interval is defined such that, if the data collection process is repeated again and again, then in the long run, 95% of the confidence intervals formed would contain the true parameter value.</p>
<p>A Bayesian 95% <strong>credible interval</strong> is an interval which contains 95% of the posterior distribution of the parameter.</p>
<p>There may be several different credible intervals such that the interval contains 95% of the distribution. The 95%  <strong>Highest Posterior Density (HPD)</strong> interval is the credible interval with the smallest range of values for <span class="math notranslate nohighlight">\(\theta\)</span> (providing the posterior is concave). Algebraically, this is the region <span class="math notranslate nohighlight">\([\theta_L, \theta_U]\)</span> that contains <span class="math notranslate nohighlight">\(95\%\)</span> of the probability, such that:</p>
<div class="math notranslate nohighlight">
\[
P(\theta \in [\theta_L,\theta_U])= 0.95 \mbox{ such that for all } \theta_O \notin  [\theta_L,\theta_U] \mbox{ and all  } \theta_I\in[\theta_L,\theta_U], p(\theta_O|y) &lt; p(\theta_I|y).
\]</div>
<p>In our previous example, when we used the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, our posterior was <span class="math notranslate nohighlight">\(Beta(6, 15)\)</span>. The posterior mean is  <span class="math notranslate nohighlight">\(\frac{6}{6+15}=0.286\)</span>. The 95% HPDI is (0.107,0.475). We plot the distribution below and check that the area between these two values gives us 0.95. Now, note that the interval (0.09, 0.465) also gives us an area of 0.95, but this interval is wider. In a sense, the HPDI is the “tightest” interval so that the area under the posterior distribution is 0.95.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">7</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(6, 15) Distribution  \n with 95% credible interval&quot;</span><span class="p">,</span>  <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.475</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.107</span><span class="p">,</span>  <span class="n">lty</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">)</span>

<span class="c1">#Area under the 95% HDPI</span>
<span class="nf">pbeta</span><span class="p">(</span><span class="m">0.475</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span><span class="o">-</span><span class="nf">pbeta</span><span class="p">(</span><span class="m">0.107</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span>

<span class="c1">#The interval (0.09, 0.465) also a 95% credible interval </span>
<span class="nf">pbeta</span><span class="p">(</span><span class="m">0.465</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span><span class="o">-</span><span class="nf">pbeta</span><span class="p">(</span><span class="m">0.09</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.949975144544822</div><div class="output text_html">0.951266598161814</div><img alt="_images/09.g. Bayesian Statistics I_1_2.png" src="_images/09.g. Bayesian Statistics I_1_2.png" />
</div>
</div>
<blockquote>
<div><p>Note: <br> <br> We have phrased the above discussion in terms of 95% confidence and credible intervals. However, there is nothing special about the level 95%. We can make the discussion more general by talking about <span class="math notranslate nohighlight">\(100(1−𝛼)\%\)</span>  confidence or credible intervals instead, with <span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span> (where <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> for 95% confidence or credible intervals but e.g. <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span> for 99% intervals).</p>
</div></blockquote>
</div>
<span id="document-09.h. Bayesian Statistics I"></span><div class="section" id="prior-predictions">
<h3>9.7 Prior Predictions<a class="headerlink" href="#prior-predictions" title="Permalink to this headline">¶</a></h3>
<p>Before observing a quantity <span class="math notranslate nohighlight">\(y\)</span>, we can provide its predictive distribution by integrating out the unknown parameter,</p>
<div class="math notranslate nohighlight">
\[
p(y) = \int p(y|\theta) p(\theta) d\theta.
\]</div>
<p>Predictions are useful in many settings, for example forecasting, cost-effectiveness models and design of
studies. In the trial described earlier in this section, we had 10 patients. Suppose we are interested in predicting the number of patients who will have a positive response. Recall that the Beta distribution is a suitable prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>, the proportion of positive responses. We have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\theta &amp;\sim \hbox{Beta}(a,b) \\
	y &amp;\sim \hbox{Binomial}(\theta,n)
\end{align*}
\end{split}\]</div>
<p>The exact predictive distribution <span class="math notranslate nohighlight">\(p(y)\)</span> can be computed analytically and is known as the <em>Beta-Binomial</em> distribution. It has the complex form with three parameters,  number of trials <span class="math notranslate nohighlight">\(n\)</span> and shape parameters, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(y) &amp;=  \frac{ \Gamma (a+ b)}{ \Gamma (a) \Gamma (b) }  {n \choose y}  \frac{\Gamma (a+ y) \Gamma (b+n-y)}{\Gamma (a+b+n)} \\
E(y) &amp;=  n \frac{a}{a+b}
\end{align*}
\end{split}\]</div>
<p>Given that we use the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, our predictive distribution would be:</p>
<div class="math notranslate nohighlight">
\[
p(y) =  \frac{ \Gamma (11)}{ \Gamma (2) \Gamma (9) }  {10 \choose y}  \frac{\Gamma (2+ y) \Gamma (19-y)}{\Gamma (21)},
\]</div>
<p>with <span class="math notranslate nohighlight">\(E(y) =  10 \, \frac{2}{11} = 1.81\)</span>. So, before observing any data, we would predict around 2 patients to have a positive response out of 10.</p>
<div class="section" id="posterior-prediction">
<h4>9.7.1 Posterior Prediction<a class="headerlink" href="#posterior-prediction" title="Permalink to this headline">¶</a></h4>
<p>Suppose that have observed <span class="math notranslate nohighlight">\(y\)</span>, and we want to predict future observations <span class="math notranslate nohighlight">\(z\)</span>, assuming that <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are independent, conditional on <span class="math notranslate nohighlight">\(\theta\)</span>. The posterior predictive distribution for <span class="math notranslate nohighlight">\(z\)</span> is given by,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(z|y) &amp;= \int p(z, \theta | y) d \theta \\
     &amp;= \int p(z |y, \theta) p(\theta |y ) d \theta \\ 
      &amp;= \int p(z | \theta) p(\theta |y ) d \theta 
\end{align*}
\end{split}\]</div>
<p>We are now weighting the probability distribution function for <span class="math notranslate nohighlight">\(z\)</span> with our posterior belief after having observed <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>For our example, we found that the posterior distribution <span class="math notranslate nohighlight">\(p(\theta |y ) \)</span> is a Beta(<span class="math notranslate nohighlight">\(a+y, b+n-y\)</span>) distribution. Thus our posterior predictive distribution is a Beta-binomial distribution with the number of trials <span class="math notranslate nohighlight">\(n_p\)</span> and shape parameters <span class="math notranslate nohighlight">\(a+y, b+n-y\)</span>.</p>
<p>Now, given that we use the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, and then observe that <span class="math notranslate nohighlight">\(y=4\)</span> patients out of <span class="math notranslate nohighlight">\(n=10\)</span> had a successful result, and we wish to predict how many sucesses <span class="math notranslate nohighlight">\(z\)</span> out of <span class="math notranslate nohighlight">\(n_p=20\)</span> to expect, our posterior predictive distribution is a Beta-binomial with parameters <span class="math notranslate nohighlight">\(20\)</span> and shape parameters <span class="math notranslate nohighlight">\(6\)</span> and <span class="math notranslate nohighlight">\(15\)</span>. The expectation of this distribution is <span class="math notranslate nohighlight">\(E(y) =  20 \frac{6}{21} \approx 6\)</span> patients.</p>
</div>
</div>
<span id="document-09.i. Bayesian Statistics I"></span><div class="section" id="conjugacy">
<h3>9.8 Conjugacy<a class="headerlink" href="#conjugacy" title="Permalink to this headline">¶</a></h3>
<p>In the example with the Beta-Binomial model, we found that using the Beta distribution for the prior lead us to a posterior distribution that is also a Beta distribution. This is not a coincidence. Often, a particular distributional family is chosen for the prior, so that the resulting posterior distribution belongs to the same family. This is called a conjugate prior. Below are the conjugate priors for some common likelihood models.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Likelihood</p></th>
<th class="text-align:left head"><p>Conjugate Prior</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Bernoulli</p></td>
<td class="text-align:left"><p>Beta</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Binomial</p></td>
<td class="text-align:left"><p>Beta</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Poisson</p></td>
<td class="text-align:left"><p>Gamma</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Geometric</p></td>
<td class="text-align:left"><p>Beta</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Normal</p></td>
<td class="text-align:left"><p>Normal, Gamma and a few others</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Exponential</p></td>
<td class="text-align:left"><p>Gamma</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Gamma</p></td>
<td class="text-align:left"><p>Gamma</p></td>
</tr>
</tbody>
</table>
<div class="section" id="exercise">
<h4>9.8.1 Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h4>
<p>Suppose that there is an experiment where <span class="math notranslate nohighlight">\(n\)</span> patients are asked to try different treatments each time they get a headache. We are interested in the number of different treatments a patient takes before they find one that is successful. For patient <span class="math notranslate nohighlight">\(i\)</span>, for <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span>, we denote by <span class="math notranslate nohighlight">\(y_i\)</span> the number of treatments tried before the first success. Note that <span class="math notranslate nohighlight">\(\left\{ y_1, y_2, ..., y_n \right\}\)</span> are a sample from a Geometric distribution: <span class="math notranslate nohighlight">\(y_i \sim Geom(\theta)\)</span>. The probability density function of a geometric distribution is:</p>
<div class="math notranslate nohighlight">
\[p(y | \theta) = \theta (\theta -1)^{y-1}\]</div>
<p>Suppose we wish to make inference on <span class="math notranslate nohighlight">\(\theta\)</span>. By specifying a Beta prior for <span class="math notranslate nohighlight">\(\theta\)</span>: <span class="math notranslate nohighlight">\(\theta \sim Beta(a, b)\)</span>, derive the posterior distribution of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Try the exercise and then click the button to reveal the solution.</p>
<div class="toggle docutils container">
<p>Solution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\theta \mid y_1, ..., y_n) 
         &amp;\propto  p(\theta) \prod_{i=1}^n p(y_i \mid \theta)\\
                &amp;\propto  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1} \prod_{i=1}^n \theta (\theta -1)^{y-1}\\
        &amp;\propto  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1}  \theta^n (\theta -1)^{\sum_{i=1}^n y_i-n}\\
        &amp;\propto  \theta^{a+n-1} (\theta -1)^{\sum_{i=1}^n y_i -n +b-1}
\end{align*}
\end{split}\]</div>
<p>This is a Beta distribution with parameters <span class="math notranslate nohighlight">\(a+n\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n y_i-n+b\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
<span id="document-10.a. Bayesian Statistics II"></span><div class="section" id="bayesian-statistics-ii-normal-data">
<h2>10. Bayesian Statistics II: Normal data<a class="headerlink" href="#bayesian-statistics-ii-normal-data" title="Permalink to this headline">¶</a></h2>
<p>In the previous session, we looked at Bayesian inference for proportions. We now consider continuous data and explore Bayesian inference for data when they are assumed to follow a Normal distribution.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>Find the posterior for a Normally distributed mean when the variance of the data is known</p></li>
<li><p>Find credible and HPD intervals for a Normally distributed mean</p></li>
<li><p>Find the Bayesian predictive distributions for Normal data and data summaries.</p></li>
</ul>
</div><p>The next sessions calculate the posterior for the mean of a Normal distribution, obtain HPD credible intervals and use the posterior to make predictions.</p>
<div class="toctree-wrapper compound">
<span id="document-10.b. Bayesian Statistics II"></span><div class="section" id="example-cd4-cell-counts">
<h3>10.1 Example: CD4 cell counts<a class="headerlink" href="#example-cd4-cell-counts" title="Permalink to this headline">¶</a></h3>
<p>In this session, we will use a dataset on CD4 cell counts which is available in R through the <em>boot</em> package. CD4 cells are in our blood as part of our immune system. Since these cells die in people who have HIV, CD4 cell counts are used in HIV patients to determine the health of their immune system and susceptibility to opportunistic infections.</p>
<p>In this dataset, there are 20 patients with HIV. Their CD4 cell counts are recorded before and after they were put on treatment. We wish to investigate whether this treatment increased their CD4 cell counts.</p>
<p>We install the <em>boot</em> package where the data is stored and we look at the data. Note that the unit of CD4 cell count is 100 <span class="math notranslate nohighlight">\(cells/mm^3\)</span>. We are interested in the difference in CD4 cell counts before and after treatment. We look at the summary statistics of the difference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">boot</span><span class="p">)</span>
<span class="n">ydata</span> <span class="o">&lt;-</span> <span class="n">cd4</span><span class="o">$</span><span class="n">oneyear</span> <span class="o">-</span> <span class="n">cd4</span><span class="o">$</span><span class="n">baseline</span>
<span class="n">data</span> <span class="o">&lt;-</span> <span class="nf">cbind</span><span class="p">(</span><span class="n">cd4</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">ydata</span><span class="p">)</span>
<span class="n">data</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">ydata</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 20 × 3</caption>
<thead>
	<tr><th></th><th scope=col>baseline</th><th scope=col>oneyear</th><th scope=col>y</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>2.12</td><td>2.47</td><td> 0.35</td></tr>
	<tr><th scope=row>2</th><td>4.35</td><td>4.61</td><td> 0.26</td></tr>
	<tr><th scope=row>3</th><td>3.39</td><td>5.26</td><td> 1.87</td></tr>
	<tr><th scope=row>4</th><td>2.51</td><td>3.02</td><td> 0.51</td></tr>
	<tr><th scope=row>5</th><td>4.04</td><td>6.36</td><td> 2.32</td></tr>
	<tr><th scope=row>6</th><td>5.10</td><td>5.93</td><td> 0.83</td></tr>
	<tr><th scope=row>7</th><td>3.77</td><td>3.93</td><td> 0.16</td></tr>
	<tr><th scope=row>8</th><td>3.35</td><td>4.09</td><td> 0.74</td></tr>
	<tr><th scope=row>9</th><td>4.10</td><td>4.88</td><td> 0.78</td></tr>
	<tr><th scope=row>10</th><td>3.35</td><td>3.81</td><td> 0.46</td></tr>
	<tr><th scope=row>11</th><td>4.15</td><td>4.74</td><td> 0.59</td></tr>
	<tr><th scope=row>12</th><td>3.56</td><td>3.29</td><td>-0.27</td></tr>
	<tr><th scope=row>13</th><td>3.39</td><td>5.55</td><td> 2.16</td></tr>
	<tr><th scope=row>14</th><td>1.88</td><td>2.82</td><td> 0.94</td></tr>
	<tr><th scope=row>15</th><td>2.56</td><td>4.23</td><td> 1.67</td></tr>
	<tr><th scope=row>16</th><td>2.96</td><td>3.23</td><td> 0.27</td></tr>
	<tr><th scope=row>17</th><td>2.49</td><td>2.56</td><td> 0.07</td></tr>
	<tr><th scope=row>18</th><td>3.03</td><td>4.31</td><td> 1.28</td></tr>
	<tr><th scope=row>19</th><td>2.66</td><td>4.37</td><td> 1.71</td></tr>
	<tr><th scope=row>20</th><td>3.00</td><td>2.40</td><td>-0.60</td></tr>
</tbody>
</table>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-0.6000  0.2675  0.6650  0.8050  1.3775  2.3200 
</pre></div>
</div>
</div>
</div>
<p>In the classical framework, we could use a paired t-test to see if the mean change in CD4 cell counts is significantly different from the null hypothesis value of zero (<span class="math notranslate nohighlight">\(H_0: \mu = E[Y]=0)\)</span>.</p>
<p>For our Bayesian analysis, we will assume these measurements come from a Normal distribution with an unknown mean <span class="math notranslate nohighlight">\(\mu\)</span>,
which represents the mean change in CD4 counts. We will assume that the variance is known to be <span class="math notranslate nohighlight">\(\sigma^2 = 0.7\)</span>. This is slightly artificial as, in a real example, we may not know what the true variance is; however, we might be able to infer the variability of CD4 counts from earlier studies. Having both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> unknown requires a more complicated analysis which we will not cover in this course.</p>
<p>The Bayesian analysis involves constructing a likelihood for the data, specifying an appropriate prior distribution and combining them to obtain a posterior distribution. We will then describe how credible intervals for <span class="math notranslate nohighlight">\(\mu\)</span>, and prior and posterior predictive distributions can be found.</p>
</div>
<span id="document-10.c. Bayesian Statistics II"></span><div class="section" id="calculating-the-posterior-for-the-mean-of-a-normal-distribution">
<h3>10.2 Calculating the posterior for the mean of a Normal distribution<a class="headerlink" href="#calculating-the-posterior-for-the-mean-of-a-normal-distribution" title="Permalink to this headline">¶</a></h3>
<p>In this section, we obtain the posterior for the mean of a Normal distribution with known variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>Suppose we have <span class="math notranslate nohighlight">\(n\)</span> observed independent data points, each assumed to come from the Normal distribution: <span class="math notranslate nohighlight">\(y_1,\dots,y_n \sim N(\mu,\sigma^2)\)</span>. Recall that the Normal distribution has probability density function given by</p>
<div class="math notranslate nohighlight">
\[
p(y \mid \mu, \sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{1/2} 
\exp\left\{-\frac{1}{2\sigma^2}(y-\mu)^2\right\}.
\]</div>
<p>Note that some authors will parameterize the Normal distribution with the <em>precision</em> instead of the variance: <span class="math notranslate nohighlight">\(\eta=\frac{1}{\sigma^2}\)</span>.</p>
<div class="section" id="likelihood">
<h4>10.2.1 Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this headline">¶</a></h4>
<p>For convenience, we will drop the conditioning on <span class="math notranslate nohighlight">\(\sigma^2\)</span>, since we are assuming this is a known number. Since we assume all observations are independent, the likelihood is the product of the <span class="math notranslate nohighlight">\(n\)</span> individual p.d.f.s:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(y_1,\dots,y_n \mid \mu) 
&amp;= p(y_1 \mid \mu) p(y_2 \mid \mu) \dots p(y_n \mid \mu) \\
&amp;= \prod_{i=1}^n p(y_i \mid \mu) \\
&amp;=
\left( \frac{1}{2\pi \sigma^2}\right)^{n/2} \exp\left\{
-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2 \right\}
\end{align*}
\end{split}\]</div>
<p>Notice that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{i=1}^n (y_i - \mu)^2  &amp;= \sum_{i=1}^n (y_i - \bar
y + \bar y  -
\mu)^2 \\
&amp; = \sum_{i=1}^n (y_i-\bar y)^2 + n(\bar y - \mu)^2, \mbox{ since }
\sum_{i=1}^n (y_i-\bar y)=0,\\
&amp;= (n-1)s^2 + n(\bar y - \mu)^2,
\end{align*}
\end{split}\]</div>
<p>where (as usual) <span class="math notranslate nohighlight">\(s^2 = \sum_{i=1}^n (y_i - \bar y)^2 /(n-1).\)</span></p>
<p>Thus the Likelihood can be written:</p>
<div class="math notranslate nohighlight">
\[
p(y_1,\dots,y_n \mid \mu) = \left(\frac{1}{2\pi \sigma^2}\right)^{n/2} \exp\left\{
-\frac{1}{2\sigma^2}\left[(n-1)s^2 + n(\bar y - \mu)^2 \right]   \right\}.
\]</div>
<p>Since we are interested in the posterior for <span class="math notranslate nohighlight">\(\mu,\)</span> we can drop all terms not involving <span class="math notranslate nohighlight">\(\mu,\)</span> so the likelihood is proportional to</p>
<div class="math notranslate nohighlight">
\[
p(y_1,\dots,y_n \mid \mu) \propto \exp\left\{ -\frac{n}{2\sigma^2} (\bar y - \mu)^2 \right\}.
\]</div>
<p>Notice that this also has the same form of a Normal distribution for the mean <span class="math notranslate nohighlight">\(\bar{y}\)</span>, specifically, <span class="math notranslate nohighlight">\(\bar{y} \sim N(\mu, \frac{\sigma^2}{n})\)</span>.</p>
</div>
<div class="section" id="prior">
<h4>10.2.2 Prior<a class="headerlink" href="#prior" title="Permalink to this headline">¶</a></h4>
<p>We noted in the previous session that the Normal distribution is a conjugate prior when the likelihood is a Normal distribution. Thus, for convenience, we will use a Normal distribution as a prior for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mu  \sim N(\phi, \tau^{2}),
\]</div>
<p>as the posterior distribution will conveniently be a Normal distribution as well. The prior parameters <span class="math notranslate nohighlight">\(\phi\)</span> and <span class="math notranslate nohighlight">\(\tau^2\)</span> should be specified based on prior knowledge of <span class="math notranslate nohighlight">\(\mu\)</span> and the uncertainty around this prior knowledge. It may come from previous research or formally elicited from investigators. If no prior evidence is available, we assign an appriopriately large value to <span class="math notranslate nohighlight">\(\tau\)</span>.</p>
</div>
<div class="section" id="posterior">
<h4>10.2.3 Posterior<a class="headerlink" href="#posterior" title="Permalink to this headline">¶</a></h4>
<p>To derive the posterior for the mean <span class="math notranslate nohighlight">\(\mu\)</span>, we need to find the  distribution of that parameter conditional on the data (both the empirical data and prior distribution). In the following calculation, we are only interested in the parts of the p.d.f. that depend on <span class="math notranslate nohighlight">\(\mu\)</span>. Any terms not involving <span class="math notranslate nohighlight">\(\mu\)</span> are part of the <em>normalisation constant</em>. This is part of the p.d.f., but does not affect the shape of the density.</p>
<p>The posterior is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mu \mid y_1,\dots,y_n) 
&amp;\propto p(y_1,\dots,y_n \mid \mu) p(\mu) \\
&amp;\propto \exp\left\{ -\frac{n}{2\sigma^2} (\bar y - \mu)^2 \right\}
\exp\left\{-\frac{1}{2\tau^2}(\mu-\phi)^2\right\} \\ 
 &amp; = \exp\left\{ -\frac{n}{2\sigma^2}(\bar{y}-\mu)^2
-\frac{1}{2\tau^2}(\mu-\phi)^2\right\} \\
\end{align*}
\end{split}\]</div>
<p>Expanding the brackets and retaining only terms containing <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(\mu \mid y_1,\dots,y_n) 
 \propto \exp\left\{ -\frac{n}{2\sigma^2 \tau^2}  (-2n\bar{y}\mu\tau^2 - n\mu^2\tau^2+\mu^2\sigma^2-2\mu\phi\sigma^2)
\right\} 
\]</div>
<p>Completing the squared term for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(\mu \mid y_1,\dots,y_n) 
\propto \exp\left\{ -\frac{\tau^2 n + \sigma^2}{2 \sigma^2 \tau^2}\left(\mu - \frac{ \tau^2 n\bar{y} -\sigma^2\phi}{\tau^2n+\sigma^2}\right)^2\right\}
\]</div>
<p>We can recognise this has the form the p.d.f. of the Normal distribution, therefore we see that</p>
<div class="math notranslate nohighlight">
\[
\mu \vert y_1,\dots,y_n \sim N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} \right\}.
\]</div>
<p>We see that:</p>
<ol class="simple">
<li><p>the Normal prior is <em>conjugate</em> for a Normal Likelihood, as the posterior is also Normal.</p></li>
<li><p>The posterior mean, <span class="math notranslate nohighlight">\(\frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}\)</span> is a weighted average of the data <span class="math notranslate nohighlight">\(\bar y \)</span> and the prior mean <span class="math notranslate nohighlight">\(\phi\)</span>: we can write it as <span class="math notranslate nohighlight">\(w \bar{y} + (1-w) \phi\)</span>, where <span class="math notranslate nohighlight">\(w= \frac{\tau^2 n}{\tau^2 n + \sigma^2}\)</span> . Hence the posterior combines the information from the likelihood (data) and prior (a priori belief).</p></li>
<li><p>The variance of the posterior is  <span class="math notranslate nohighlight">\(\frac{\sigma^2\tau^2}{\tau^2n+\sigma^2}\)</span>. In a larger study, since <span class="math notranslate nohighlight">\(n\)</span> becomes very large, we have <span class="math notranslate nohighlight">\(\tau^2 &gt;&gt; \frac{\sigma^2}{n}\)</span>, so the posterior variance tends to zero.</p></li>
<li><p>In smaller studies, <span class="math notranslate nohighlight">\(\tau^2 &lt;&lt; \frac{\sigma^2}{n}\)</span>, the posterior mean is closer to <span class="math notranslate nohighlight">\(\phi\)</span> and the posterior variance depends both on the prior and sampling variance <span class="math notranslate nohighlight">\(\frac{\sigma^2\tau^2}{\tau^2n+\sigma^2}\)</span>.</p></li>
</ol>
</div>
</div>
<span id="document-10.d. Bayesian Statistics II"></span><div class="section" id="credible-intervals">
<h3>10.3 Credible Intervals<a class="headerlink" href="#credible-intervals" title="Permalink to this headline">¶</a></h3>
<p>We saw in the previous session that a Bayesian <span class="math notranslate nohighlight">\(95\%\)</span> credible interval is an interval which contains <span class="math notranslate nohighlight">\(95\% \)</span> of the posterior distribution of the parameter, and the <span class="math notranslate nohighlight">\(95 \%\)</span> Highest Posterior Density (HPD) interval is the credible interval with the smallest range of values for <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Given that the posterior distribution has mean <span class="math notranslate nohighlight">\(\psi\)</span> and variance <span class="math notranslate nohighlight">\(\gamma^{2}\)</span>, the <span class="math notranslate nohighlight">\(95\%\)</span> HPD interval is given by
<span class="math notranslate nohighlight">\(\psi \pm 1.96 \times \gamma\)</span>. Thus, for a standard Normal posterior, the 95% HPD interval is <span class="math notranslate nohighlight">\((-1.96,1.96).\)</span></p>
<div class="section" id="cd4-cell-counts-example">
<h4>10.3.1 CD4 cell counts example:<a class="headerlink" href="#cd4-cell-counts-example" title="Permalink to this headline">¶</a></h4>
<p>In the CD4 cell count example, suppose that we have very strong prior information that suggests the treatment is not effective, and we expect that the difference in cell counts is approximately zero. Let us denote by <span class="math notranslate nohighlight">\(y\)</span> the difference in CD4 cell counts. We set <span class="math notranslate nohighlight">\(\mu \sim N(0, 0.1)\)</span> to reflect that there is only about <span class="math notranslate nohighlight">\(2.5\%\)</span> chance that the treatment increases mean CD4 counts by more than 0.62 (1.96 <span class="math notranslate nohighlight">\(\times \sqrt{0.1}\)</span>) and a <span class="math notranslate nohighlight">\(50\%\)</span> chance that it will actually decrease the mean CD4 count).</p>
<p>Summarizing the information we have:</p>
<blockquote>
<div><p>sample size <span class="math notranslate nohighlight">\(n = 20\)</span><br />
mean of data <span class="math notranslate nohighlight">\(\bar{y} = 0.805\)</span><br />
variance of data (assumed known) <span class="math notranslate nohighlight">\(\sigma^2 = 0.7\)</span><br />
prior mean <span class="math notranslate nohighlight">\( \phi = 0\)</span><br />
prior variance <span class="math notranslate nohighlight">\(\tau^2= 0.1\)</span></p>
</div></blockquote>
<p>We find the posterior distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mu \vert y_1,\dots,y_n &amp;\sim N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} \right\} \\
 &amp;\sim N\left\{ \frac{ 0.1 \times 20 \times 0.805 + 0 }{0.1 \times 20 + 0.7}, \frac{0.7 \times 0.1}{0.1 \times 20 +0.7 } \right\} \\
  &amp;\sim N\left\{ 0.596, 0.0259 \right\}
\end{align*}
\end{split}\]</div>
<p>We plot below the prior distribution (in blue), the distribution of <span class="math notranslate nohighlight">\(\bar{y}\)</span> (red) and the posterior distribution (purple). We observe that the mean of the posterior distribution is in between the mean of the prior and that of the likelihood. Note that in R, the Normal distribution is parameterized by the standard deviation rather than the variance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">7</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">-2</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>
<span class="c1">#plot the prior </span>
<span class="n">y1</span> <span class="o">&lt;-</span> <span class="nf">dnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="m">0.1</span><span class="p">))</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Density&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">mu</span><span class="p">))</span>
<span class="nf">legend</span><span class="p">(</span><span class="s">&quot;topleft&quot;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Prior distribution&quot;</span><span class="p">,</span> <span class="s">&quot;Distribution of mean of y&quot;</span><span class="p">,</span> <span class="s">&quot;Posterior distribution&quot;</span><span class="p">),</span>
       <span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="s">&quot;purple&quot;</span><span class="p">),</span> <span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
<span class="c1">#plot the observed distribution </span>
<span class="n">y2</span> <span class="o">&lt;-</span> <span class="nf">dnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="m">0.805</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="m">0.7</span><span class="o">/</span><span class="m">20</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">&lt;-</span> <span class="nf">dnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="m">0.596</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="nf">sqrt</span><span class="p">(</span><span class="m">0.0259</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/10.d. Bayesian Statistics II_2_0.png" src="_images/10.d. Bayesian Statistics II_2_0.png" />
</div>
</div>
<p>The  <span class="math notranslate nohighlight">\(95\%\)</span> HPD interval can be calculated as <span class="math notranslate nohighlight">\(0.596 \pm 1.96 \times \sqrt{0.0259} = (0.281, 0.911)\)</span>. This interval lies wholly above zero, so we can state that we have a strong posterior belief that there is an increase in CD4 cell counts.</p>
</div>
</div>
<span id="document-10.e. Bayesian Statistics II"></span><div class="section" id="predictions">
<h3>10.4 Predictions<a class="headerlink" href="#predictions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="prior-predictive-distributions">
<h4>10.4.1 Prior predictive distributions<a class="headerlink" href="#prior-predictive-distributions" title="Permalink to this headline">¶</a></h4>
<p>Finding the predictive distribution for a new patient <span class="math notranslate nohighlight">\(y\)</span> before making any observations involves finding the following distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
p(y | \sigma^2, \phi, \tau^2) = \int p(y, \mu | \sigma^2, \phi, \tau^2) d \mu\\
= \int p(y | \mu, \sigma^2, \phi, \tau^2) p(\mu |  \phi, \tau^2) d \mu
\end{split}\]</div>
<p>This calculation involves a lot of algebra. We instead use a different approach: note that we can write the observation as <span class="math notranslate nohighlight">\(y = \mu + \epsilon\)</span>, where <span class="math notranslate nohighlight">\(\mu \sim N(\phi, \tau^2)\)</span> and <span class="math notranslate nohighlight">\(\epsilon \sim N(0, \sigma^2)\)</span>. Then, since <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> are independent, we can use this result:</p>
<blockquote>
<div><p>If X and Y be independent random variables that are Normally distributed, <span class="math notranslate nohighlight">\(X\sim N(\mu _{X},\sigma _{X}^{2})\)</span> and <span class="math notranslate nohighlight">\(Y\sim N(\mu _{Y},\sigma _{Y}^{2})\)</span>, then their sum is also Normally distributed: <span class="math notranslate nohighlight">\(X + Y \sim N(\mu _{X}+\mu _{Y},\sigma _{X}^{2}+\sigma _{Y}^{2})\)</span>.</p>
</div></blockquote>
<p>Thus we have that <span class="math notranslate nohighlight">\(y \sim N(\phi, \tau^2 + \sigma^2)\)</span>.</p>
<p>In our example, before collecting any data, suppose we wish to predict the probability that the difference in cell counts is greater than 0.3 (30 <span class="math notranslate nohighlight">\(cells/mm^3\)</span>). We have that <span class="math notranslate nohighlight">\(y \sim N(0, 0.1 + 0.7)\)</span>. We compute <span class="math notranslate nohighlight">\(p(y &gt; 0.3)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="m">1</span><span class="o">-</span><span class="nf">pnorm</span><span class="p">(</span><span class="m">0.3</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">0.8</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.368657838608209</div></div>
</div>
<p>Given our prior distribution alone, the probability that the change in CD4 count for a new patient will exceed 0.3 (30 <span class="math notranslate nohighlight">\(cells/mm^3\)</span>) is approximately 0.369.</p>
</div>
<div class="section" id="posterior-predictive-distributions">
<h4>10.4.2 Posterior predictive distributions<a class="headerlink" href="#posterior-predictive-distributions" title="Permalink to this headline">¶</a></h4>
<p>Suppose that have observed <span class="math notranslate nohighlight">\(y_1, ..., y_n \)</span>, and we want to predict future observations <span class="math notranslate nohighlight">\(z\)</span>, assuming that <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> are independent for all <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span>, conditional on <span class="math notranslate nohighlight">\(\mu\)</span>. The posterior predictive distribution for <span class="math notranslate nohighlight">\(z\)</span> is given by,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(z| y_1, ..., y_n,  \sigma^2, \phi, \tau^2) &amp;= \int p(z, \mu | y_1, ..., y_n,  \sigma^2, \phi, \tau^2) d \mu \\
     &amp;= \int p(z | y_1, ..., y_n,\mu,  \sigma^2) p(\mu |y_1, ..., y_n,\sigma^2, \phi, \tau^2  ) d \mu. \\ 
     \end{align*}
\end{split}\]</div>
<p>Again, this involves some fiddly algebra but we can use a similar method to that we used for the prior predictive distribution. We wish to know what the predictive distribution of a new patient <span class="math notranslate nohighlight">\(z\)</span> is, given the previous observations <span class="math notranslate nohighlight">\(y_1, ..., y_n\)</span>. We can write <span class="math notranslate nohighlight">\(z  = \mu + \epsilon\)</span>. We have that <span class="math notranslate nohighlight">\(\mu \vert y_1,\dots,y_n \sim N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} \right\}, \)</span> and <span class="math notranslate nohighlight">\(\epsilon \sim N(0, \sigma^2)\)</span>.</p>
<p>Using the result for the sum of two independent Normal distributions, the posterior predictive distribution has the form <span class="math notranslate nohighlight">\( N\left\{ \frac{ \tau^2 n\bar{y} + \sigma^2\phi }{\tau^2 n + \sigma^2}, \frac{\sigma^2\tau^2}{\tau^2n+\sigma^2} + \sigma ^2\right\}\)</span></p>
<p>In our example, based on both prior and observed data, the predictive distribution for cell counts in a new patient being greater than 0.3 (30 <span class="math notranslate nohighlight">\(cells/mm^3\)</span>) is <span class="math notranslate nohighlight">\(N(0.596, 0.0259 + 0.7)\)</span>. We can compute <span class="math notranslate nohighlight">\(f(z | y_1, ..., y_n &gt; 0.3)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="m">1</span><span class="o">-</span> <span class="nf">pnorm</span><span class="p">(</span><span class="m">0.3</span><span class="p">,</span> <span class="m">0.596</span><span class="p">,</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">0.7259</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.635861643314828</div></div>
</div>
<p>After having observed the data, the predictive probability that the next patient will have a difference in CD4 cell counts of greater than 0.3 (30 <span class="math notranslate nohighlight">\(cells/mm^3\)</span>) has increased substantially to 0.636.</p>
</div>
</div>
<span id="document-10.f. Bayesian Statistics II"></span><div class="section" id="multiparameter-models">
<h3>10.5 Multiparameter models<a class="headerlink" href="#multiparameter-models" title="Permalink to this headline">¶</a></h3>
<p>Suppose now that our likelihood has two unknown parameters, <span class="math notranslate nohighlight">\((\mu,\sigma^2).\)</span> In this case, we would need a prior distribution for both parameters, and our posterior distribution will now be bivariate. If desired we can summarise this by the mean and covariance matrix or by HPD contour maps. However, often in applications, interest focusses only on one parameter, say <span class="math notranslate nohighlight">\(\mu;\)</span> the other parameter is usually referred to as a nuisance parameter. In Bayesian inference, we typically use simulation to draw from the posterior distribution of <span class="math notranslate nohighlight">\((\mu,\sigma^2).\)</span> For marginal inference for <span class="math notranslate nohighlight">\(\mu,\)</span> we summarise the draws from <span class="math notranslate nohighlight">\(\mu\)</span> in the usual way, across all simulated values of <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Analytically, this is equivalent to integrating the posterior over <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | y) = \int p(\mu,\sigma^2 |y) \, d\sigma^2,
\]</div>
<p>where we have used Bayes’ theorem to obtain the posterior, i.e.<span class="math notranslate nohighlight">\(p(\mu,\sigma^2 |y).\)</span> This integral may be intractable (hence the preference for simulation approaches).</p>
</div>
<span id="document-10.g. Bayesian Statistics II"></span><div class="section" id="further-resources">
<h3>Further Resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h3>
<p>Note: further resources are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. All examinable material is contained within the notes.</p>
<div class="section" id="resources-for-learning">
<h4>Resources for learning<a class="headerlink" href="#resources-for-learning" title="Permalink to this headline">¶</a></h4>
<p>These textbooks are recommended for further learning and examples:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="http://www.stat.columbia.edu/~gelman/book/">Bayesian data analysis by Gelman et. al</a> can be downloaded in PDF format.</p></li>
<li><p><a class="reference external" href="https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-the-bugs-book/">The Bugs Book by Lunn et. al.</a> is available at the LSHTM library.</p></li>
<li><p><a class="reference external" href="http://jim-stone.staff.shef.ac.uk/BookBayes2012/HTML_BayesRulev5EbookHTMLFiles/ops/xhtml/ch01BayesJVSone.html">an introductory book by Jim Stone with nice examples</a> The first chapter is freely available online.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="examples-of-applications">
<h4>Examples of applications<a class="headerlink" href="#examples-of-applications" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://www.nature.com/articles/nrg2615">Article on Nature providing guidelines Bayesian analyses for genetic association studies</a>.</p></li>
<li><p>The potential benefits of incorporating prior information in the context of health care evaluation is discussed by <a class="reference external" href="https://projecteuclid.org/euclid.ss/1089808280">David Spiegelhalter in this article</a>.</p></li>
<li><p>We mentioned earlier that Bayesian approaches can be helpful for overcoming challenges with small sample sizes in clinical trials for rare diseases; you can read more about this <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2551510/pdf/bmj00623-0045.pdf">in an article by Lilford et. al.</a></p></li>
</ul>
</div></blockquote>
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-11. Investigation.Intro"></span><div class="section" id="investigations-and-the-role-of-regression-modelling">
<h2>Investigations and the role of regression modelling<a class="headerlink" href="#investigations-and-the-role-of-regression-modelling" title="Permalink to this headline">¶</a></h2>
<p>Data scientists don’t do statistics just for fun (although, clearly statistics is indeed fun!).  At the heart of each data science project is a question to be answered.</p>
<p>This section of the notes begins by thinking about the different types of investigation that might be carried out within a data science project. We consider three important classes of investigation type: <strong>description</strong>, <strong>prediction</strong> and <strong>causal</strong>.</p>
<p>We then move on to consider a commonly used family of statistical analysis: <strong>regression modelling</strong>.  Three sessions introduce linear regression, beginning with the simplest type which we call <strong>simple linear regression</strong> involving a single explanatory variable. We then extend this to incorporate multiple explanatory variables, through <strong>multivariable linear regression</strong> modelling. We explore how to model various types of explanatory variables, including continuous, binary and categorical covariates and discover how to include interactions and higher-order terms (which are need to model non-linear relationships) in the regression model. The last of the linear regression sessions explores diagnostics to assess whether the underlying assumptions of the linear model hold in a particular dataset.</p>
<p>These ideas are then extended to other settings in the remaining two sessions. First, we meet <strong>logistic regression</strong>, an extension of linear regression modelling to settings where the outcome variable is binary. Finally, we define the <strong>Generalised Linear Model (GLM)</strong>, which is a generalisation of linear regression to a wide range of settings and can be seen as a way of unifying linear, logistic and Poisson regression models, as well as many other types of regression model. We explore <strong>Poisson regression</strong> as an important example of a GLM.</p>
<p>We conclude this section of the notes by returning to the idea of investigations and – armed with our new knowledge about regression modelling – consider the role of regression modelling in different types of investigations.</p>
</div>
<span id="document-11.a. Types of Investigation"></span><div class="section" id="types-of-investigation">
<h2>11. Types of Investigation<a class="headerlink" href="#types-of-investigation" title="Permalink to this headline">¶</a></h2>
<p>This session introduces how to set up a research question, explores the different types of investigation and brings in key concepts like prediction, causality and confounding. This session demonstrates how methods learnt in previous sessions are applied in research.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>Describe three different types of investigations that arise in medical statistics and health data science.</p></li>
<li><p>Link a research question to an investigation type and compare the properties of different investigation types.</p></li>
<li><p>Explain how and why explanatory variables are used differently in prediction studies and in causal investigations.</p></li>
</ul>
</div><div class="toctree-wrapper compound">
<span id="document-11.b. Types of Investigation"></span><div class="section" id="specifying-research-questions">
<h3>11.1 Specifying research questions<a class="headerlink" href="#specifying-research-questions" title="Permalink to this headline">¶</a></h3>
<p>Specifying the research question or questions is a crucial starting point for an investigation. In some cases the research question will be highly specific, and in others could be more wide ranging with several components. The research question then informs the subsequent stages of the investigation, ranging from choice of study population; study design; data collection; monitoring and quality control; data analysis; presentation of conclusions; interpretation. Figure 11.1 illustrates one
way of representing the whole process of an investigation (which we saw earlier in the Introduction).</p>
<div class="figure align-default" id="figure-11-1">
<a class="reference internal image-reference" href="_images/01_intro_PPDAC_adapt.png"><img alt="_images/01_intro_PPDAC_adapt.png" src="_images/01_intro_PPDAC_adapt.png" style="height: 400px;" /></a>
</div>
<p>The statistician/data scientist plays an important role at all stages of an investigation, not just at
the data analysis phase. It is perhaps most usual for collaborators who are subject-matter experts
(e.g. clinicians) to pose the initial research question. However, the statistician very often plays a
key part in refining these initial ideas in order to translate them into something formal and clearly
specified.</p>
</div>
<span id="document-11.c. Types of Investigation"></span><div class="section" id="different-types-of-investigation">
<h3>11.2 Different types of investigation<a class="headerlink" href="#different-types-of-investigation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="classification">
<h4>11.2.1 Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h4>
<p>The research question informs what type of investigation is required. Investigations can be divided
broadly into the following types:</p>
<ol class="simple">
<li><p>Description</p></li>
<li><p>Prediction</p></li>
<li><p>Causality and explanation</p></li>
</ol>
<p>Hernan, Hsu &amp; Healy (Chance, 2019) set out to classify data science tasks and used three classifications: <em>Description</em>, <em>Prediction</em>, and <em>Counterfactual prediction</em> (meaning causality). Schmueli (Statistical Science, 2010) also described similar classifications: <em>Descriptive modelling</em>, <em>Predictive modelling</em>, and <em>Explanatory modelling</em>. See also Hand (Harvard Data Science Review, 2019) for a nice discussion on this topic.</p>
</div>
<div class="section" id="implications-of-investigation-type">
<h4>11.2.2 Implications of investigation type<a class="headerlink" href="#implications-of-investigation-type" title="Permalink to this headline">¶</a></h4>
<p>The distinction between the different types of investigation is crucial because it has a fundamental impact on the steps of the analysis and beyond. For example, the investigation type influences:</p>
<ul class="simple">
<li><p>How we decide what variables are to be included in the analysis</p></li>
<li><p>What analysis methods to use</p></li>
<li><p>How we assess the fit/performance of the model or oter analysis approach used</p></li>
<li><p>How we present the results from the analysis</p></li>
<li><p>How the findings might be used in practice</p></li>
<li><p>How we need to work with other experts at different stages</p></li>
</ul>
</div>
<div class="section" id="the-role-of-study-design">
<h4>11.2.3 The role of study design<a class="headerlink" href="#the-role-of-study-design" title="Permalink to this headline">¶</a></h4>
<p>The different types of investigation may be performed using data from studies of different design. Having posed a research question, we can consider (with input from collaborators) what data are required to answer it robustly, including whether new data collection is needed, or whether there are existing data that could be used to address the question. This process needs to take into account considerations of cost, timeliness, feasibility and ethics. For example, for some questions our ideal study could be a randomized controlled trial, but to perform one would require such long followup that it would be infeasible and unethical, and so we would turn to observational data to address the research question. There is a major emphasis in the recent biostatistical and epidemiological literature on the use of ‘found’ data from sources such as electronic health records, which present great opportunities to answer research questions using data on a large number of individuals, but also present challenges for analysis and interpretation. All three types of investigation may make use of observational data. Randomized controlled trials are designed to estimate treatment effects (i.e. for causal investigations), but secondary analyses of trial data can be used in other types of investigation, such as to develop a prediction model.</p>
</div>
</div>
<span id="document-11.d. Types of Investigation"></span><div class="section" id="properties-of-different-types-of-investigation">
<h3>11.3 Properties of different types of investigation<a class="headerlink" href="#properties-of-different-types-of-investigation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="description">
<h4>11.3.1 Description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h4>
<p>In a descriptive investigation the data are used to provide a quantitative summary of features of the
population of interest, or in other words the data are summarised in a compact way.</p>
<p>Simple descriptive analyses involve calculating proportions of individuals with a particular characteristic (e.g. males and females; smokers and non-smokers), or estimating features of the distribution of continuous variables (e.g. mean and variance of weight or blood pressure). The resulting information is then presented using tables and data visualisation.</p>
<p>Some descriptive analyses may extend to use of more complex methods of analysis. For example, the research question may concern how individuals within a population cluster together interms of their dietary habits, requiring clustering methods. It may be of interest to estimate theexpected survival time post-disease diagnosis in the presence of censored survival times, which
would require survival analysis techniques.</p>
<p>All investigations should start with some basic descriptive analysis to gain understanding of the features of the data at hand. It is at this stage that we can uncover challenges such as missing data, gain insights into how certain variables are distributed, and, where relevant, gain understanding of correlations between key variables, including to identify collinearities. Some investigations then go on to the main research question, which goes beyond description, and others may be entirely descriptive and not proceed onto other questions.</p>
<p>Huebner et al. (2019) provide useful guidance on ‘initial data analysis’. See also Spiegelhalter
(2019) for an accessible discussion of summarising and communicating descriptions of data.</p>
</div>
<div class="section" id="prediction">
<h4>11.3.2 Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h4>
<p>Prediction is about using data on some features of individuals to predict other features with the aim of predicting the outcome for new or future observations. More formally, prediction is concerned with mapping data on variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, <span class="math notranslate nohighlight">\(X_{2}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> to an outcome <span class="math notranslate nohighlight">\(Y\)</span> . The prediction model could be developed using statistical models such as regression, or approaches that would be described as machine learning algorithms.</p>
<p>Results from prediction investigations are used for a range of purposes: to inform people of their risk or prognosis; to identify people at high risk of an adverse event and hence take action such as more frequent screening (though the investigation will not tell us whether such screening would be effective).</p>
<p>Prediction models are typically developed using observational data. A well known example is the Framingham Risk Score, which provides predictions of a person’s 10-year of developing coronary heart disease (D’Agostino et al 2008).</p>
<p>There is a huge literature on prediction in the medical setting. See for example the books by Riley et al. (2019) and Steyerberg (2019).</p>
</div>
<div class="section" id="causality-and-explanation">
<h4>11.3.3 Causality and explanation<a class="headerlink" href="#causality-and-explanation" title="Permalink to this headline">¶</a></h4>
<p>In causal investigations we seek to understand the causal effect of one or more variables on an outcome. Hernan et al. (2019) describe this as “Using data to predict certain features of the world as if the world had been different”. For a simple example of a causal investigation, consider a continuous outcome <span class="math notranslate nohighlight">\(Y\)</span> (e.g. blood pressure) and a binary treatment variable <span class="math notranslate nohighlight">\(X\)</span>, where <span class="math notranslate nohighlight">\(X = 1\)</span> denotes treated and <span class="math notranslate nohighlight">\(X = 0\)</span> denotes untreated. A causal investigation asks how the mean of Y would be different if all individuals had <span class="math notranslate nohighlight">\(X = 1\)</span> compared with if all individuals had <span class="math notranslate nohighlight">\(X = 0\)</span>. In other words, if we could change <span class="math notranslate nohighlight">\(X\)</span> what would be the expected change in <span class="math notranslate nohighlight">\(Y\)</span> ?</p>
<p>Questions such as this can be arguably simple to answer using a randomized controlled trial, where there is no confounding of the treatment-outcome association. However, issues of drop-out and non-compliance are important to consider. Historically, some have considered answering causal questions to lie only in the domain of randomized experiments. However, randomized experiments are not feasible or ethical to address many important questions. It is now recognised that causality is often the goal of investigations using observational data. See for example the paper of Hernan (2018), who wrote “being explicit about the causal objective of a study reduces ambiguity in the scientific question, errors in the data analysis, and excesses in the interpretation of the results”. The field of ‘causal inference’ has developed in recent decades, with particular advances in recent years, to enable this.</p>
<p>Schmeuli (2010) equates causality with ‘explanation’, meaning explanation of mechanisms of how one (or more) variable affects another. However, Hernan et al. (2019) make the point that we may be able to say that <span class="math notranslate nohighlight">\(X\)</span> causes <span class="math notranslate nohighlight">\(Y\)</span> without understanding the underlying mechanism. For example we may find strong evidence from a trial that a drug is effective for a given outcome, but the precise biological mechanisms through which the effect is transmitted are not well understood.</p>
<p>The variable of interest in a causal investigation could be use of a medical treatment (a drug) or application of a procedure. More generally it could be an ‘exposure’ such as ‘smoking’ or ‘exercising for at least 30 minutes per day’. The ‘hypothetical intervention’ of interest should be (reasonably) well defined, even if we could never in reality intervene on it in the real world (e.g. it would be impractical, not to say unethical, to intervene on smoking status). See Hernan (2016) for a discussion of related issues.</p>
</div>
<div class="section" id="is-there-a-fourth-investigation-type">
<h4>11.3.4 Is there a fourth investigation type?<a class="headerlink" href="#is-there-a-fourth-investigation-type" title="Permalink to this headline">¶</a></h4>
<p>There is arguably a fourth investigation type which is concerned with exploring how several explanatory variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> are associated with an outcome <span class="math notranslate nohighlight">\(Y\)</span>. This might be described as an “exploration of risk factors” investigation. It may involve univariable analyses, looking at the association of each explanatory variable (“risk factor”) individually with the outcome, and multivariable analyses which look at association of several variables with the outcome in a single model. These types of analysis are typically carried out using observational data, and many (or perhaps most) epidemiological studies are investigations of this type, at least historically.</p>
<p>These types of investigation can be useful for understanding associations between variables in the population of interest and, as such, some may consider these analyses to be descriptive. However, as we all know, association is not causation! These types of investigation often do not consider the relative temporal ordering of explanatory variables, which means that interpretation of estimated associations as causal effects can be misleading. There is recent emphasis in the epidemiological literature on more principled investigations which are more explicit about the aim of the investigation.</p>
<p>Like in a prediction investigation, the interest is in several explanatory variables. However, unlike in a prediction investigation, the aim is to actually explore quantitatively the unconditional and conditional associations of the explanatory variables with <span class="math notranslate nohighlight">\(Y\)</span>, rather than being purely on predicting <span class="math notranslate nohighlight">\(Y\)</span>. Unlike in a causal investigation, there is not a particular focus on a single variable. However, there is often an attempt to discuss the associations as though they may be causal even though an explicit causal question has not been posed.</p>
<p>Investigators should be wary of over-interpreting findings from “exploration of risk factors” investigations. And if we are really interested in addressing a causal question we should be explicit about that and carry out our analysis and interpretations accordingly.</p>
</div>
</div>
<span id="document-11.e. Types of Investigation"></span><div class="section" id="an-example-stroke-in-women">
<h3>11.4 An example: stroke in women<a class="headerlink" href="#an-example-stroke-in-women" title="Permalink to this headline">¶</a></h3>
<p>Table 1 provides an example of the features of different investigation types. The overall topic is stroke in women. The table (taken from Hernan et al. 2019) provides an example research question, the features of data that would be required to answer it, and the types of analysis that could be used for investigations of three types: Description, Prediction and Causal inference.</p>
<p>Table 1: From Hernan, Hsu &amp; Healy 2019. Examples of Tasks Conducted by Data Scientists Working with Electronic Health Records</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:left head"><p>Description</p></th>
<th class="text-align:left head"><p>Prediction</p></th>
<th class="text-align:left head"><p>Causal inference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Example of scientific question</p></td>
<td class="text-align:left"><p>How can women aged 60-80 years with stroke history be partitioned in classes defined by their characteristics?</p></td>
<td class="text-align:left"><p>What is the probability of having a stroke next year for women with certain characteristics?</p></td>
<td class="text-align:left"><p>Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><br> Data</p></td>
<td class="text-align:left"><p><br> - Eligibility criteria <br> - Features (symptoms, clinical parameters … )</p></td>
<td class="text-align:left"><p><br> - Eligibility criteria <br>- Output (diagnosis of stroke over the next year)<br> - Inputs (age, blood pressure, history of stroke, diabetes at baseline)</p></td>
<td class="text-align:left"><p><br> - Eligibility criteria <br> - Outcome (diagnosis of stroke over the next year) <br> -Treatment (initiation of statins at baseline) <br> - Confounders <br> - Effect modifiers (optional)</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><br> Example of analytics</p></td>
<td class="text-align:left"><p><br> Cluster analysis</p></td>
<td class="text-align:left"><p><br> Regression <br> Decision trees <br> Random forests <br> Support vector machines <br> Neural networks</p></td>
<td class="text-align:left"><p><br> Regression <br> Matching <br> Inverse probability weighting <br> G-formula <br> G-estimation <br> Instrumental variable estimation</p></td>
</tr>
</tbody>
</table>
</div>
<span id="document-11.f. Types of Investigation"></span><div class="section" id="role-of-explanatory-variables-in-different-types-of-investigation">
<h3>11.5 Role of explanatory variables in different types of investigation<a class="headerlink" href="#role-of-explanatory-variables-in-different-types-of-investigation" title="Permalink to this headline">¶</a></h3>
<p>The role of explanatory variables in different types of investigation differs. We focus here on prediction investigations and causal investigations.</p>
<div class="section" id="prediction">
<h4>11.5.1 Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h4>
<p>In prediction investigations the aim is to use <span class="math notranslate nohighlight">\(X_{1}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> to predict <span class="math notranslate nohighlight">\(Y\)</span> . In this setting the <span class="math notranslate nohighlight">\(X_{1}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> are often referred to as the ‘predictors’ for obvious reasons. For a prediction problem we may well use all of the explanatory variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> in the prediction model or algorithm. Crucially, in prediction we are not interested in the inter-relationships between the explanatory variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> and their temporal ordering. The only aim is to achieve a good prediction of the outcome <span class="math notranslate nohighlight">\(Y\)</span>. It may be desirable to reduce the number of explanatory variables, particularly in settings where the number of potential predictors <span class="math notranslate nohighlight">\(p\)</span> is very large. Various principled procedures are available for reducing the number of predictor variables.</p>
</div>
<div class="section" id="causality-and-explanation">
<h4>11.5.2 Causality and explanation<a class="headerlink" href="#causality-and-explanation" title="Permalink to this headline">¶</a></h4>
<p>In investigations of causality, one of the explanatory variables is designated as the treatment or exposure of interest. Let’s suppose this is variable <span class="math notranslate nohighlight">\(X_{1}\)</span> and the research question is about how <span class="math notranslate nohighlight">\(X_{1}\)</span> affects <span class="math notranslate nohighlight">\(Y\)</span>. Or, in other words, if <span class="math notranslate nohighlight">\(X_{1}\)</span> had been different, how would <span class="math notranslate nohighlight">\(Y\)</span> have been different? Let’s consider the setting of an Randomized Controlled Trials and an observational study separately and think of the situation where <span class="math notranslate nohighlight">\(X_{1}\)</span> is a binary treatment variable</p>
<p><em>Randomized controlled trials (RCT)</em></p>
<p>Suppose individuals are randomized to receive treatment <span class="math notranslate nohighlight">\((X_{1} = 1)\)</span> or not <span class="math notranslate nohighlight">\((X_{1} = 0)\)</span>, and the outcome <span class="math notranslate nohighlight">\(Y\)</span> is observed after some period of follow-up. It is straightforward to estimate the treatment effect in this setting because of the randomization. For a continuous outcome, we would quantify the treatment effect using a difference in the mean outcome in the two treatment groups <span class="math notranslate nohighlight">\((E(Y |X_{1} = 1) − E(Y |X_{1} = 0))\)</span>. For a binary outcome we could quantify the treatment effect in terms of a risk difference <span class="math notranslate nohighlight">\((Pr(Y = 1|X_{1} = 1) − Pr(Y = 1|X_{1} = 0))\)</span>, risk ratio <span class="math notranslate nohighlight">\((Pr(Y = 1|X_{1} = 1)/Pr(Y = 1|X_{1} = 0))\)</span> or odds ratio <span class="math notranslate nohighlight">\(((Pr(Y = 1|X_{1} = 1)/Pr(Y = 0|X_{1} = 1))/(Pr(Y = 1|X_{1} = 0)/Pr(Y = 0|X_{1} = 0)))\)</span>, for example.</p>
<p>Some of the other explanatory variables <span class="math notranslate nohighlight">\(X_{2}\)</span>, … , <span class="math notranslate nohighlight">\(X_{p}\)</span> are likely to be associated with <span class="math notranslate nohighlight">\(Y\)</span>, but we do not need to use them to estimate the treatment effect due to the study design. Sometimes investigators will adjust for baseline variables, measured at the start of the trial prior to treatment. By the study design, baseline variables are not associated with the treatment. There can be advantages of adjusting for baseline variables that are predictors of the outcome. Though there are particular nuances to the interpretation of the resulting estimates depending on the types of outcome (continuous, binary, etc) and on how the treatment effect is quantified.</p>
<p>Of course, there are many important considerations surrounding the validity and interpretation of
treatment effects estimated using RCTs, such as whether the effect is a ‘per-protocol’ or ‘intentionto-treat’ effect, whether there is drop-out, non-adherence or treatment switching.</p>
<p><em>Observational studies</em></p>
<p>Suppose we have available observational data on the treatment variable <span class="math notranslate nohighlight">\(X_{1}\)</span> and the outcome <span class="math notranslate nohighlight">\(Y\)</span>, for example from electronic health records. In this setting the treatment in non-randomized, and there are very likely to be confounders of the association between the treatment and the outcome.</p>
<p>A confounder is a variable that affects both the treatment and the outcome. Confounding variables occur prior in time to both the treatment/exposure and the outcome. See VanderWeele and Schpitser (2013) for a formal statistical discussion of confounding.</p>
<p>To estimate the causal effect of <span class="math notranslate nohighlight">\(X_{1}\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> requires us to control for confounding. Consider a simple setting in which there is only one other variable at play, <span class="math notranslate nohighlight">\(X_{2}\)</span>, which in the observational setting affects whether a person gets the treatment <span class="math notranslate nohighlight">\(X_{1}\)</span> and also affects their outcome <span class="math notranslate nohighlight">\(Y\)</span>. For example, if <span class="math notranslate nohighlight">\(X_{1}\)</span> is a blood pressure-lowering medication and <span class="math notranslate nohighlight">\(Y\)</span> is blood pressure 1 year later, then <span class="math notranslate nohighlight">\(X_{2}\)</span> could be the person’s blood pressure at the time origin. The assumed relationships between the three variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, <span class="math notranslate nohighlight">\(X_{2}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are illustrated in Figure 2 using directed acyclic graphs (DAGs), contrasting the relationships in an RCT and in an observational study.</p>
<div class="figure align-default" id="figure-2">
<a class="reference internal image-reference" href="_images/Session_11_Figure_2.jpg"><img alt="_images/Session_11_Figure_2.jpg" src="_images/Session_11_Figure_2.jpg" style="height: 250px;" /></a>
</div>
<p>DAGs, also called ‘causal diagrams’, are used to graphically describe mechanistic relationships between variable using uni-directional arrows. An arrow connecting two variables indicates (potential) causation in the direction of the arrow and the absence of an arrow indicates an assumption that there is no direct causal effect of the first variable on the second. See Greenland et al. (Epidemiology, 1999) and Shrier and Platt (2008) for introductions to causal diagrams. Some other useful more recent articles on this are from Etminan et al. (2020) and Tenant et al. (2019). In simple situation such as this example, we don’t need a DAG to tell us that we need to account for the confounding by <span class="math notranslate nohighlight">\(X_{2}\)</span> in our analysis in order to estimate the effect of <span class="math notranslate nohighlight">\(X_{1}\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> . However, when there are lots of variables at play DAGs become very useful, and have formal theory attached.</p>
<p>In summary, in a causal investigation the variables on which the research question focuses are <span class="math notranslate nohighlight">\(X_{1}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> . However, depending on the study design, we may need to account for other variables in the analysis, though those other variables are not our main focus. The concept of confounding is not relevant in prediction investigations.</p>
</div>
</div>
<span id="document-11.g. Types of Investigation"></span><div class="section" id="summary">
<h3>11.6 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h3>
<p>We have placed some emphasis on how the investigation type affects what variables should be included in the analysis and on how the results might be interpreted. There are naturally many other things to consider which are beyond the scope of this session. The above example focused on regression. The next few sessions in this module will focus on regression models of different types. They are a fundamental part of the statistician’s toolbox and are used in investigations of different types. However, there are many other specialised methods available for specific tasks. For example, in descriptive analyses we may use clustering methods and principal components analysis. In prediction tasks, machine learning methods not based on regression are increasingly used. In studies of causal effects many specialised methods have been developed over recent years. Some of these involve regression and others not.</p>
<p>The type of investigation affects how we should assess the performance and assumptions of a model/analysis. For example, in prediction tasks we should assess how well the prediction model performs in terms of predicting the outcome for a new individual. This requires tools such as cross validation, and measures of predictive performance such as <span class="math notranslate nohighlight">\(R^2\)</span> , area under the curve, sensitivity and specificity. In causal analyses we are concerned with whether the assumptions of the models used are valid and whether the model is correctly specified, alongside the validity of untestable assumptions such as whether there are any important confounders that have not been accounted for in the analysis.</p>
<p>This session aimed to provide a broad overview of different types of investigation used in medical statistics/health data science, and which you are likely to encounter in your future careers. This topic has seen some recent emphasis in the literature. The statistical and epidemiological community is increasingly emphasising the need for researchers to ensure they conduct meaningful studies and interpret findings appropriately, particularly relating to the use of observational data. It is a wide topic, and we have only touched on some aspects here.</p>
</div>
<span id="document-11.h. Types of Investigation"></span><div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<p>NOTE: You are not expected to read all of these references! It is intended as a list of resources that you may find useful in the future or if you wish to follow-up on some of the topics discussed in more detail.</p>
<p>Bandoli G., Palmsten K., Chambers C.D., et al. Revisiting the Table 2 fallacy: A motivating example examining preeclampsia and preterm birth. Pediatric and Perinatal Epidemiology 2018; 32: 390-397.</p>
<p>D’Agostino R.B., Vasan R.S., Pencina M.J., et al. General Cardiovascular Risk Profile for Use in Primary Care: The Framingham Heart Study. Circulation 2008; 117: 743–753.</p>
<p>Etminan M, Collins GS, Mansournia MA. Using Causal Diagrams to Improve the Design and Interpretation of Medical Research. CHEST 2020; 158: Supplement S21-S28.</p>
<p>Greenland S., Pearl J., Robins J.M. Causal diagrams for epidemiological research. Epidemiology 1999; 10:37–48.</p>
<p>Hand D. What is the Purpose of Statistical Modelling? Harvard Data Science Review 2019 <a class="reference external" href="https://doi.org/10.1162/99608f92.4a85af74">https://doi.org/10.1162/99608f92.4a85af74</a></p>
<p>Hernan M.A. Does water kill? A call for less casual causal inferences. Annals of Epidemiology 2016; 26: 674-680.</p>
<p>Hernan M.A. The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data. Am J Public Health. 2018;108: 616–619.</p>
<p>Hernan M.A., Hsu J., Healy B.. A second chance to get causal inference right: a classification of data science tasks. Chance 2019; 32: 42-49.</p>
<p>Huebner M., le Cessie S., Schmidt C., Wach W. A Contemporary Conceptual Framework for Initial Data Analysis. Observational Studies 2019; 4: 171-192.</p>
<p>Riley R.D. et al. Prognosis Research in Healthcare: Concepts, Methods, and Impact. 2019. Oxford University Press.</p>
<p>Schmueli. To explain or to predict? Statistical Science 2010; 25: 289-310.</p>
<p>Schooling CM, Jones H. Clarifying questions about “risk factors”: predictors versus explanation. Emerging Themes in Epidemiology 2018; 15: 10.</p>
<p>Schrier I., Platt R.W. Reducing bias through directed acyclic graphs. BMC Medical Research Methodology 2008; 8: 70.</p>
<p>Spiegelhalter D. The Art of Statistics: Learning from Data. 2019. Penguin.</p>
<p>Steyerberg E. Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating. 2nd Edition. 2019. Springer.</p>
<p>Tennant PWG, Harrison WJ, Murray EJ, et al. Use of directed acyclic graphs (DAGs) in applied health research: review and recommendations. MedRxiv 2019.
<a class="reference external" href="https://www.medrxiv.org/content/10.1101/2019.12.20.19015511v1">https://www.medrxiv.org/content/10.1101/2019.12.20.19015511v1</a></p>
<p>VanderWeele T.J., Shpitser I. On the definition of a confounder. Annals of Statistics 2013; 41: 196-220.</p>
<p>Westreich D., Greenland S. The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients. American Journal of Epidemiology 2013; 177: 292-298.</p>
</div>
</div>
</div>
<span id="document-12.a. Linear Regression I"></span><div class="section" id="linear-regression-i">
<h2>12. Linear Regression I<a class="headerlink" href="#linear-regression-i" title="Permalink to this headline">¶</a></h2>
<p>This is the first of three sessions that explore linear regression modelling. These are models where the outcome of interest is a continuous variable.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session, you will be able to:</p>
<ul class="simple">
<li><p>explain, in general, the rationale behind parametric statistical models;</p></li>
<li><p>fit and interpret a linear regression model;</p></li>
<li><p>describe the main properties of ordinary least squares estimators;</p></li>
<li><p>explain confidence intervals and hypothesis testing for regression coefficients</p></li>
</ul>
</div><p><strong>Acknowledgements:</strong>  Thank you to Jennifer Nicholas and Chris Frost whose notes on linear regression were particularly useful in the development of the current lesson.</p>
<div class="toctree-wrapper compound">
<span id="document-12.b. Linear Regression I"></span><div class="section" id="introduction">
<h3>12.1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h3>
<p>A parametric statistical model is an algebraic description of how one or more <strong>outcome</strong> variables are influenced by <strong>covariates</strong>. Such models are widely used in medical research. Some examples of questions that we can investigate using statistical models include:</p>
<ul class="simple">
<li><p>Does birthweight increase with length of pregnancy?</p></li>
<li><p>Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?</p></li>
<li><p>Can we predict the risk of heart disease for our patients?</p></li>
</ul>
<p>In the above examples, the outcome variables are birthweight, inflammation and heart disease. In the first two examples, the length of pregnancy and drug use are covariates. In the third example, no covariates are explicitly mentioned. However, when answering the third question, researchers may want to consider a range of patient characteristics that are associated with the risk of heart disease as covariates in their model, for example: diet, exercise, comorbodities, medications etc.</p>
<p>Recall that statistical models contain <strong>population parameters</strong> and representations of <strong>uncertainty</strong>. The population parameters are unknown quantities that we want to estimate from our sample and the uncertainty is a measure of the variability in the outcome variable that is not explained by the covariates.</p>
<p>This is the first ofthe sessions on linear regression. In this session, we will learn how to define linear regression models, how to estimate their population parameters and how to estimate measures of uncertainty. We begin by introducing the <strong>simple linear regression model</strong> which includes one outcome and one covariate. In the second session, we introduce the <strong>multivariable linear regression model</strong>, which is an extension of the simple linear regression model to situations with multiple covariates. We explore linear regression models with categorical variables, interactions and non-linear terms. In the third session, we discuss the key assumptions underlying linear regression models and important model diagnostics. The optional material to the session explores how to conduct an <strong>analysis of variance</strong> of statistical models.</p>
<p>Before delving in, it is worth making a note of the different terminologies that you may come across in the medical literature. Here, I have already used the terms: outcome and covariates. Table 1 summarises alternatives terms that may be used to describe the same concepts.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Outcome</p></th>
<th class="head"><p>Covariates</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(Y\)</span>-variable</p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span>-variables</p></td>
</tr>
<tr class="row-odd"><td><p>Dependent variable</p></td>
<td><p>Independent variables</p></td>
</tr>
<tr class="row-even"><td><p>Response variable</p></td>
<td><p>Regressors</p></td>
</tr>
<tr class="row-odd"><td><p>Output variable</p></td>
<td><p>Input variables</p></td>
</tr>
<tr class="row-even"><td><p>(no direct analogy)</p></td>
<td><p>Explanatory variables</p></td>
</tr>
<tr class="row-odd"><td><p>(no direct analogy)</p></td>
<td><p>Predictor variables</p></td>
</tr>
</tbody>
</table>
<p>Table 1: Different terminology used for outcome and covariates</p>
<p>Finally, it is important to understand that statistical models make <strong>assumptions</strong> about the form of relationships between outcomes and covariates. Although we can examine our data to investigate the validity of these assumptions (using methods covered in the next session), we can never be certain that the model is correct.</p>
</div>
<span id="document-12.c. Linear Regression I"></span><div class="section" id="data-used-in-our-examples">
<h3>12.2 Data used in our examples<a class="headerlink" href="#data-used-in-our-examples" title="Permalink to this headline">¶</a></h3>
<p>For our examples we will use data on babies and their mothers. The data contains a random sample of 1,174 mothers and their newborn babies. The column Birth Weight contains the birth weight of the baby, in ounces; Gestational Days is the number of gestational days, that is, the number of days the baby was in the womb. There is also data on maternal age, maternal height, maternal pregnancy weight, and whether or not the mother was a smoker.</p>
<p>The following code can be used to download and look at the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Load data</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>

<span class="c1">#Look at the first 10 rows of the data</span>
<span class="nf">head</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 6 × 6</caption>
<thead>
	<tr><th></th><th scope=col>Birth.Weight</th><th scope=col>Gestational.Days</th><th scope=col>Maternal.Age</th><th scope=col>Maternal.Height</th><th scope=col>Maternal.Pregnancy.Weight</th><th scope=col>Maternal.Smoker</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>120</td><td>284</td><td>27</td><td>62</td><td>100</td><td>False</td></tr>
	<tr><th scope=row>2</th><td>113</td><td>282</td><td>33</td><td>64</td><td>135</td><td>False</td></tr>
	<tr><th scope=row>3</th><td>128</td><td>279</td><td>28</td><td>64</td><td>115</td><td>True </td></tr>
	<tr><th scope=row>4</th><td>108</td><td>282</td><td>23</td><td>67</td><td>125</td><td>True </td></tr>
	<tr><th scope=row>5</th><td>136</td><td>286</td><td>25</td><td>62</td><td> 93</td><td>False</td></tr>
	<tr><th scope=row>6</th><td>138</td><td>244</td><td>33</td><td>62</td><td>178</td><td>False</td></tr>
</tbody>
</table>
</div></div>
</div>
</div>
<div class="section" id="exploratory-analyses">
<h3>12.2.1 Exploratory analyses<a class="headerlink" href="#exploratory-analyses" title="Permalink to this headline">¶</a></h3>
<p>The simple linear regression model is used to model the relationship between one single variable (<span class="math notranslate nohighlight">\(X\)</span>) and a single outcome (<span class="math notranslate nohighlight">\(Y\)</span>). For example, suppose we are interested in investigating the following relationships in our birthweight data:</p>
<ol class="simple">
<li><p>Association between the length of pregnancy (i.e. number of gestational days) and birthweight.</p></li>
<li><p>Association between mother’s smoking status and birthweight.</p></li>
</ol>
<p>An important first step in an analysis is to summarise and display the data. Below is a scatterplot and boxplot displaying the relevant data for Examples 1 and 2 respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>

<span class="c1"># Set the plot area into a 1x2 array</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>

<span class="c1"># Example 1: Scatter Plot</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Example 1&quot;</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational Days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span>

<span class="c1"># Example 2: Box plot</span>
<span class="nf">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Example 2&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Mother smokes&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12.c. Linear Regression I_3_0.png" src="_images/12.c. Linear Regression I_3_0.png" />
</div>
</div>
<p><em>Example 1:</em> Birthweight and gestational days appear to be highly correlated, where an increase in gestational days is associated with increased birthweight.</p>
<p><em>Example 2:</em> It appears that mothers who do not smoke give birth to heavier babies, on average, than mothers who do smoke.</p>
</div>
<div class="section" id="determining-the-dependent-and-independent-variables">
<h3>12.2.2 Determining the dependent and independent variables<a class="headerlink" href="#determining-the-dependent-and-independent-variables" title="Permalink to this headline">¶</a></h3>
<p>Before defining a regression model, we have to decide which is the independent variable and which is the outcome (i.e. the dependent variable). In this context, it is natural to consider birthweight as the outcome: conceptually, it makes little sense to investigate how birthweight influences length of pregnancy or the mother’s smoking status.  However, it is not necessarily always as straightforward. Suppose we were investigating the association between age and weight. It is possible that we might be interested in age as a predictor of weight, or in weight as a predictor of age. The aim of the analysis will guide the choice of outcome.</p>
<p>While the outcome is the same in our two examples, an important difference is the type of independent variable. In Example 1, the independent variable (length of pregnancy) is a continuous variable, whereas in Example 2, the independent variable (mother’s smoking status) is binary (yes or no). Using these examples, we will later see how the two different types of variables are modelled differently in linear regression.</p>
</div>
<span id="document-12.d. Linear Regression I"></span><div class="section" id="the-simple-linear-regression-model">
<h3>12.3 The simple linear regression model<a class="headerlink" href="#the-simple-linear-regression-model" title="Permalink to this headline">¶</a></h3>
<p>The equation for the simple linear regression model, relating <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_0 + \beta_1 X + \epsilon 
\]</div>
<p>There are two components of this model: the <strong>linear predictor</strong> and the <strong>error term</strong>. The linear predictor represents the variation in <span class="math notranslate nohighlight">\(Y\)</span> that can be predicted using the model: <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 X\)</span>. The error term, denoted by <span class="math notranslate nohighlight">\(\epsilon\)</span>, represents the variation in <span class="math notranslate nohighlight">\(Y\)</span> that cannot be predicted (by a linear relationship with <span class="math notranslate nohighlight">\(X\)</span>). This variation is sometimes referred to as the <strong>random error</strong> or <strong>noise</strong>.</p>
<p>The subsequent two sections take a closer look at the linear predictor and error term, respectively.</p>
<div class="section" id="the-linear-predictor">
<h4>12.3.1 The linear predictor<a class="headerlink" href="#the-linear-predictor" title="Permalink to this headline">¶</a></h4>
<p>The linear predictor is an additive function of the independent variables. With a single variable, it is simply:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 + \beta_1 X
\]</div>
<p>In linear regression, the linear predictor represents the algebraic relationship between the mean of the outcome and the independent variable. When <span class="math notranslate nohighlight">\(X\)</span> takes a particular value, <span class="math notranslate nohighlight">\(X=x\)</span>, the value of the linear predictor, <span class="math notranslate nohighlight">\(\beta_0+\beta_1x\)</span>, is interpreted as the expected value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X\)</span> takes the value <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
E(Y|X=x) = \beta_0 + \beta_1 x.
\]</div>
<p>The specification of the linear predictor has two parameters: <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. These are interpreted as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept. It is the expected value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X\)</span> takes the value 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the slope (or gradient). It is the expected change in <span class="math notranslate nohighlight">\(Y\)</span> per one unit increase in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
<p>It is worth emphasising that this model assumes that <strong>the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is linear</strong>. It is important to note that it is possible to have more complex relationships between variables that do not meet this assumption (see examples in the plots below). When this is the case, simple linear regression would not be an appropriate method to use but we might be able to model the relationship well by including non-linear terms. We will pursue these ideas further in the next session.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">### Set random number generator</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">24082098</span><span class="p">)</span>

<span class="c1">#Set graphical display to show 2 plots in a row</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>

<span class="c1">#Simulate a linear X-Y relationship and plot</span>
<span class="n">x</span><span class="o">&lt;-</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
<span class="n">ylin</span><span class="o">&lt;-</span><span class="n">x</span><span class="o">+</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ylin</span><span class="p">,</span><span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">yaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Linear Association&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Y&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">xlab</span><span class="o">=</span><span class="s">&quot;X&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>

<span class="c1">#Simulate a non-linear X-Y relationship and plot</span>
<span class="n">ynonlin</span><span class="o">&lt;-</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ynonlin</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.5</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="n">yaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Non-linear Association&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Y&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">xlab</span><span class="o">=</span><span class="s">&quot;X&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12.d. Linear Regression I_1_0.png" src="_images/12.d. Linear Regression I_1_0.png" />
</div>
</div>
</div>
<div class="section" id="the-error-term">
<h4>12.3.2 The error term<a class="headerlink" href="#the-error-term" title="Permalink to this headline">¶</a></h4>
<p>The error term, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the variance in <span class="math notranslate nohighlight">\(Y\)</span> that cannot be predicted by the model. Individual values of the errors can be written as (Y -  (<span class="math notranslate nohighlight">\(\beta_0 + \beta_1X\)</span>)). These errors cannot be observed, since they involve the unknown population parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>We assume that <span class="math notranslate nohighlight">\(\epsilon\)</span> has a normal distribution with mean 0 and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, where <span class="math notranslate nohighlight">\(\sigma^2\)</span> is termed the <strong>residual variance</strong> (i.e. the variance of the residuals):</p>
<div class="math notranslate nohighlight">
\[
\epsilon \sim N(0,\sigma^2)
\]</div>
<p>Importantly, note that the errors must be independent of the independent variable <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
<div class="section" id="different-ways-of-expressing-the-simple-linear-regression-model">
<h4>12.3.3 Different ways of expressing the simple linear regression model<a class="headerlink" href="#different-ways-of-expressing-the-simple-linear-regression-model" title="Permalink to this headline">¶</a></h4>
<p>Suppose we have a sample size of <span class="math notranslate nohighlight">\(n\)</span> and we let <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(x_i\)</span> <span class="math notranslate nohighlight">\((i=1,...,n)\)</span> denote the observed outcome and value of <span class="math notranslate nohighlight">\(X\)</span> for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation, respectively. Then, we can write the simple linear regression model as:</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 x_{i}+ \epsilon_i \text{ where } \epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2).
\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(iid\)</span> means “identically, independently distributed”. A key assumption of linear regression model is that all of the observations are independent.</p>
<p>This relationship can equivalently be expressed using matrix algebra:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon} \text{ where }\epsilon \sim N(0,\mathbf{I}\sigma^2)
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{vmatrix}y_1\\y_2 \\. \\. \\. \\y_n \end{vmatrix}=\begin{vmatrix}1 &amp; x_1 \\ 1 &amp; x_2 \\1 &amp; . \\1 &amp; .  \\ 1&amp; . \\1 &amp; x_n \end{vmatrix}\begin{vmatrix} \beta_0 \\ \beta_1 \end{vmatrix}+\begin{vmatrix}\epsilon_1\\ \epsilon_2 \\ . \\ . \\. \\ \epsilon_n \end{vmatrix} 
\end{split}\]</div>
<p>In this formulation, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n \times 2\)</span> matrix, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> are vectors of length <span class="math notranslate nohighlight">\(n\)</span> whilst <span class="math notranslate nohighlight">\(\beta\)</span> is a vector of length 2.</p>
</div>
<div class="section" id="assumptions">
<h4>12.3.4 Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h4>
<p>It is worth emphasising the four key assumptions that we have made in the simple linear regression model:</p>
<ol class="simple">
<li><p><strong>Linearity</strong>: The relationship between <span class="math notranslate nohighlight">\(X\)</span> and the mean of <span class="math notranslate nohighlight">\(Y\)</span> is linear.</p></li>
<li><p><strong>Normality</strong>: The errors follow a normal distribution.</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of error terms are constant across all values of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><strong>Independence</strong>: All observations are independent of each other.</p></li>
</ol>
</div>
</div>
<span id="document-12.e. Linear Regression I"></span><div class="section" id="estimation-of-the-population-parameters">
<h3>12.4 Estimation of the population parameters<a class="headerlink" href="#estimation-of-the-population-parameters" title="Permalink to this headline">¶</a></h3>
<p>In the specification of the simple linear regression model there are three population parameters (<span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, and <span class="math notranslate nohighlight">\(\sigma\)</span>). Since we do not know these parameters, we need to estimate them based on a sample from our population. We will use the symbols <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, and <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> to represent the sample estimates of the true population parameters.</p>
<p>There are many different methods available for obtaining estimates of the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. In this section, we focus on an approach that works by minimising the amount of error in the model. These estimates are called the <strong>ordinary least squares estimates</strong> (the reason for this name will become clear in the next section).</p>
<div class="section" id="fitted-values-and-residuals">
<h4>12.4.1 Fitted values and residuals<a class="headerlink" href="#fitted-values-and-residuals" title="Permalink to this headline">¶</a></h4>
<p><strong>Fitted values:</strong> Once we have estimates <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, the fitted value for the <span class="math notranslate nohighlight">\(i{th}\)</span> observation (in other words, the predicted value of the outcome for that individual) is:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]</div>
<p><strong>Residuals:</strong> The residual for the <span class="math notranslate nohighlight">\(i{th}\)</span> observation is</p>
<div class="math notranslate nohighlight">
\[
\hat{\epsilon}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i) = y_i - \hat{y}_i
\]</div>
<p>Sometimes, the word residual is used to refer to both the residual we have defined here and the error term, in which case it is necessary to distinguish between the true and fitted/estimated residual. Here, we use the term error to refer to the deviation of the observed value from the true mean outcome and we use the term residual to refer to the deviation of the observed value from the fitted value, as defined above.</p>
</div>
<div class="section" id="ordinary-least-squares-estimates">
<h4>12.4.2 Ordinary least squares estimates<a class="headerlink" href="#ordinary-least-squares-estimates" title="Permalink to this headline">¶</a></h4>
<p>The ordinary least square (OLS) estimates are those which minimise the sum of squared deviations from the fitted regression line. The residuals, <span class="math notranslate nohighlight">\(\hat{\epsilon}\)</span>, measure deviations of the observed outcomes from the fitted regression line. Therefore This sum is sometimes called the <strong>residual sum of squares</strong>. It is often denoted by <span class="math notranslate nohighlight">\(SS_{RES}\)</span> (where “SS” stands for Sum of Squares and “RES” is shorthand for RESiduals).</p>
<p>Formally, the OLS estimators are the values of <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> that minimise:</p>
<div class="math notranslate nohighlight">
\[
SS_{RES} = \sum_{i=1}^n \hat{\epsilon}_i^2 = \sum_{i=1}^n (y_i - \hat{\beta_0} -\hat{\beta_1}x_i)^2.
\]</div>
<p>The ordinary least squares estimates of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are given by the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta_1} &amp;= \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{y}=\frac{\sum_{i=1}^n y_i}{n}\)</span> and <span class="math notranslate nohighlight">\(\bar{x} = \frac{\sum_{i=1}^n x_i}{n}\)</span>. A proof of this result is given at the end of this session.</p>
</div>
<div class="section" id="estimation-of-the-error-variance">
<h4>12.4.3 Estimation of the error variance<a class="headerlink" href="#estimation-of-the-error-variance" title="Permalink to this headline">¶</a></h4>
<p>The residual sum of squares can be thought of as the remaining unexplained variation in the outcome. Therefore, an intuitively appealing estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span> is given by dividing the residual sum of squares by the number of observations:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \sum_{i=1}^n \frac{\hat{\epsilon}_i^2}{n} = \sum_{i=1}^n \frac{(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2}{n}
\]</div>
<p>However, this is a biased estimator. The bias arises because the observed values tend, on average, to lie closer to the fitted line (defined by <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>) than they do to the true regression line (defined by <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>). This is an exact parallel to the way the variablility of a sample around its mean underestimates the variability around the population mean.</p>
<p>It can be shown that an unbiased estimator of the residual variance in the simple linear regression model is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2  = \sum_{i=1}^n \frac{\hat{\epsilon_i}^2}{n-2}=\sum_{i=1}^n \frac{(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2}{n-2}
\]</div>
<p>This quantity is referred to as the residual mean square. It is often denoted by <span class="math notranslate nohighlight">\(MS_{RES}\)</span>, where “MS” stands for Mean Square and “RES” is shorthand for residual. The denominator is <span class="math notranslate nohighlight">\((n-2)\)</span> because fitting the model first requires the estimation of two parameters (<span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>) and the estimation of these parameters is said to reduce the information about the variance by two degrees of freedom.</p>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h4>12.4.4 Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h4>
<p>An alternative approach to estimating the model parameters is maximum likelihood estimation. This approach selects the estimates which maximise the likelihood (or equivalently, the log-likelihood) of the parameter values. It can be shown that the ordinary least square estimates for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are also the maximum likelihood estimates (a proof of this result is at the end of the session).</p>
<p>The maximum likelihood estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span> is equal to the biased estimate given above, obtained by dividing the residual sum of squares by the number of observations.</p>
</div>
</div>
<span id="document-12.f. Linear Regression I"></span><div class="section" id="example-continuous-independent-variable">
<h3>12.5 Example: continuous independent variable<a class="headerlink" href="#example-continuous-independent-variable" title="Permalink to this headline">¶</a></h3>
<p>We now return to our first example, where we are interested in investigating the association between birthweight and length of pregnancy. We will fit a linear model to explore this association.</p>
<div class="section" id="the-model">
<h4>12.5.1 The model<a class="headerlink" href="#the-model" title="Permalink to this headline">¶</a></h4>
<p>The outcome is birthweight, which is measured in ounces (oz). The independent variable is length of pregnancy, <span class="math notranslate nohighlight">\(L\)</span> (i.e. number of gestational days).The following model defines our assumed relationship between the length of pregnancy (<span class="math notranslate nohighlight">\(L\)</span>) and a baby’s birthweight (<span class="math notranslate nohighlight">\(Y\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\text{Model 1: }y_i = \beta_0 + \beta_1 l_i +  \epsilon_i 
\]</div>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">lm()</span></code> to perform simple linear regressions in R. Click <a class="reference external" href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm">here</a> for details of how this command works.</p>
<p>The following code can be used to perform this linear regression in R:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model 1: Investigating the relationship between birthweight and length of pregancy</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="n">model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>There is a lot of information contained in this output. For the moment, we will focus on the estimates of the intercept and slope. These can be found under the column heading <code class="docutils literal notranslate"><span class="pre">Estimate</span></code>.</p>
<ul class="simple">
<li><p>The estimated intercept, <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> is equal to -10.75. This is interpreted as: the estimated mean birthweight of a child born after 0 gestational days is -10.75oz. Since there are no observations with 0 gestational days in the study, this is an extrapolation based on the observed data and an assumption of linearity. Estimates based on extrapolation should be interpreted with caution and in this case, the results make little sense because a negative birthweight is estimated. Moreover, no child is born after 0 gestational days and so this intercept is of little interest. Later on, we will discuss a technique called <strong>centering</strong> which is often used to make the intercept term more interpretable.</p></li>
<li><p>The estimated slope, <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is equal to 0.47. This is interpreted as: the mean birthweight of a baby is estimated to increase by 0.47oz for each daily increase in the gestational period.</p></li>
<li><p>The estimated residual standard error, <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> is equal to 16.74 (the residual variance is equal to <span class="math notranslate nohighlight">\(16.74^2\)</span>). This means that the observed outcomes are scattered around the fitted regression line with a standard deviation of 16.74oz.</p></li>
</ul>
<p>It is always useful to look at the data. The code below graphs the data and superimposes the fitted regression line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">with</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nf">plot</span><span class="p">(</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">Birth.Weight</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12.f. Linear Regression I_4_0.png" src="_images/12.f. Linear Regression I_4_0.png" />
</div>
</div>
</div>
</div>
<span id="document-12.g. Linear Regression I"></span><div class="section" id="inference-for-the-slope">
<h3>12.6 Inference for the slope<a class="headerlink" href="#inference-for-the-slope" title="Permalink to this headline">¶</a></h3>
<p>Most commonly, we wish to conduct statistical inference on the estimated slope. Consequently, we focus our attention here on <span class="math notranslate nohighlight">\(\beta_1\)</span>, but it is possible to apply the same methods to the intercept, <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<p>For our statistical inference, we will view the values of the independent variable (<span class="math notranslate nohighlight">\(x_1, x_2, ..., x_n\)</span>) as fixed quantities.</p>
<div class="section" id="sampling-distribution-of-estimated-slope">
<h4>12.6.1 Sampling distribution of estimated slope<a class="headerlink" href="#sampling-distribution-of-estimated-slope" title="Permalink to this headline">¶</a></h4>
<p>Knowing the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> allows us to perform hypothesis tests and construct confidence intervals for <span class="math notranslate nohighlight">\(\beta_1\)</span>. Therefore, we now obtain that sampling distribution. The estimated slope, <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, is a linear combination of the observed outcome values:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\hat{\beta}_1 &amp;= \sum_{i=1}^n \left(\frac{(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}(y_i-\bar{y})\right) 
\end{align}
\]</div>
<p>We can simplify this by substituting in <span class="math notranslate nohighlight">\((y_i - \bar{y})=\beta_1(x_i-\bar{x})+(\epsilon_i-\bar{\epsilon})\)</span>, allowing us to write the estimated slope as a function of the random error (<span class="math notranslate nohighlight">\(\epsilon\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\hat{\beta}_1  &amp;=\beta_1 + \sum_{i=1}^n \left(\frac{(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}(\epsilon_i-\bar{\epsilon})\right)
\end{align}
\]</div>
<p>Since the estimated parameter is a linear combination of the <span class="math notranslate nohighlight">\(\epsilon_i\)</span>, and <span class="math notranslate nohighlight">\(\epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2)\)</span>, the estimated parameter itself is normally distributed, with a distribution centred around the true value, <span class="math notranslate nohighlight">\(\beta_1\)</span>. More specifically,</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\hat{\beta}_1  \sim N\left(\beta_1, \frac{\sigma^2}{SS_{xx}}\right), \qquad \mbox{with} \qquad  SS_{xx} = \sum_{i=1}^n(x_i-\bar{x})^2
\end{align}
\]</div>
<p>So the standard error of the slope is <span class="math notranslate nohighlight">\(SE(\hat{\beta}_1) = \sqrt{\frac{\sigma^2}{SS_{xx}}}\)</span>. The standard error (and therefore also the variance) of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>:</p>
<ul class="simple">
<li><p>increases with the size of the error variance (as might be expected intuitively),</p></li>
<li><p>decreases with increasing sample size (larger sample sizes give more precise estimates) and</p></li>
<li><p>decreases as <span class="math notranslate nohighlight">\({SS}_{xx}^2\)</span> increases (a wider range of <span class="math notranslate nohighlight">\(x\)</span> values leads to more precision in the slope estimate).</p></li>
</ul>
<p>We have seen that the sampling distrbution of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> follows a normal distribution. We can convert this to a standard normal distribution:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\sigma^2/SS_{xx}}} \sim N(0,1) 
\end{align}
\]</div>
<p>Of course, we do not know the true value of the error variance, <span class="math notranslate nohighlight">\(\sigma\)</span>. When we replace this with our sample estimate, this changes the distribution above from a normal to a t-distribution. For large samples, these two distributions are very similar but for smaller samples the t-distribution has larger tails (suggesting that in smaller samples needing to estimate <span class="math notranslate nohighlight">\(\sigma\)</span> leads to more variability in the estimated slope, which makes sense intuitively).</p>
<p>Therefore, replacing <span class="math notranslate nohighlight">\(\sigma\)</span> by the estimate <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\hat{\sigma}^2/SS_{xx}}} \sim t_{n-2}
\end{align}
\]</div>
<p>This t-distribution is the one we will use to obtain p-values and confidence intervals for the slope parameter.</p>
</div>
<div class="section" id="hypothesis-testing">
<h4>12.6.2 Hypothesis testing<a class="headerlink" href="#hypothesis-testing" title="Permalink to this headline">¶</a></h4>
<p>Typically, we are interested in assessing whether there is a relationship, or association, between the independent variable <span class="math notranslate nohighlight">\(X\)</span> and outcome <span class="math notranslate nohighlight">\(Y\)</span>. Recall our model: <span class="math notranslate nohighlight">\(E[Y | X=x] = \beta_0 + \beta_1 X\)</span>. Within the framework of this linear model, no association between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> would be reflected by a value of <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>.</p>
<p>Therefore, we are typically interested in testing the <strong>null hypothesis</strong> <span class="math notranslate nohighlight">\(H_0: \beta_1=0\)</span> against the alternative <span class="math notranslate nohighlight">\(H_1: \beta_1 \neq 0\)</span>.</p>
<p>Under our null hypothesis, the following test statistic follows a t-distribution, as we saw above:</p>
<div class="math notranslate nohighlight">
\[
T = \frac{\hat{\beta}_1 - 0}{\sqrt{\hat{\sigma}^2/SS_{xx}}} \sim t_{n-2}
\]</div>
<p>We now follow the familiar process from the session in hypothesis testing. We evaluate <span class="math notranslate nohighlight">\(T\)</span> in our particular sample and then calculate the probability of obtaining that value or one more extreme for a <span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom.</p>
<p>Notes</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(T\)</span> is simply the estimate divided by its standard error. This is a familiar form of test statistic which we saw in the session about hypothesis testing.  <br> <br>
Although typically we are interested in the null hypothesis <span class="math notranslate nohighlight">\(H_0: \beta_1=0\)</span> we could use the same approach to test other null hypotheses, such as <span class="math notranslate nohighlight">\(H_0: \beta_1=5\)</span>. However, it is rare that we are interested in values other than 0. Therefore, p-values outputted by statistical software arise from testing the null hypothesis value of 0 by default.</p>
</div></blockquote>
<div class="section" id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h5>
<p>Returning to our linear model relating birthweight to gestational days in the baby dataset, we will now test the null hypothesis that there is no association between gestational days and birthweight, i.e. <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span> in Model 1.</p>
<p>First, we rerun the code to reproduce the linear model output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="n">model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>In the above output, the column <code class="docutils literal notranslate"><span class="pre">Std.Error</span></code> gives the standard errors of the estimated intercept and slope.</p>
<p>The columns <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">value</span></code> and <code class="docutils literal notranslate"><span class="pre">Pr(&gt;|t|)</span></code> give the test statistic and associated <span class="math notranslate nohighlight">\(p\)</span>-value for a hypothesis test, testing the null hypothesis that <span class="math notranslate nohighlight">\(H_0: \beta_0 = 0\)</span> in the top row and <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span> in the bottom row.</p>
<p>Note that the t-value for the slope (which we call <span class="math notranslate nohighlight">\(T\)</span> in the discussion above) is 15.28. You can check that this is equal to the estimate divided by the standard error (0.46656/0.03054 = 15.277014).</p>
<p>To test the null hypothesis that <span class="math notranslate nohighlight">\(H_0: \beta_1=0\)</span> against the alternative <span class="math notranslate nohighlight">\(H_1:\beta_1 \neq  0\)</span>, the test statistic is 15.28 and the associated <span class="math notranslate nohighlight">\(p\)</span>-value is <span class="math notranslate nohighlight">\(&lt;2\times10^{-16}\)</span>. This is a very small <span class="math notranslate nohighlight">\(p\)</span>-value and therefore the data provide strong evidence against the null hypothesis. Based on these results, we can conclude that birthweight is associated with length of pregnancy.</p>
</div>
</div>
<div class="section" id="confidence-intervals-for-the-regression-coefficients">
<h4>12.6.3 Confidence intervals for the regression coefficients<a class="headerlink" href="#confidence-intervals-for-the-regression-coefficients" title="Permalink to this headline">¶</a></h4>
<p>We saw above that:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\frac{\hat{\beta}_1 - \beta_1}{\hat{SE}(\hat{\beta}_1) } \sim t_{n-2}
\end{align}
\]</div>
<p>with <span class="math notranslate nohighlight">\(\hat{SE}(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2/SS_{xx}}\)</span>. This leads to the following 95% confidence interval for <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 \pm t_{0.025, n-2} \hat{SE}(\hat{\beta}_1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{0.025, n-2}\)</span> is the 97.5<span class="math notranslate nohighlight">\(^{th}\)</span> percentile of a <span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom. For large samples, this value will be approximately 1.96. For smaller samples it will be a slightly larger number (reflecting additional imprecision in our estimate).</p>
<p>If <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large, the t-distribution is well approximated by a normal distribution. In this case, a 95% confidence interval can be found by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 \pm 1.96 \times \hat{SE}(\hat{\beta}_1)
\]</div>
<p>Example: Calculate a 95% for  <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>  (using the values given in the R output above):</p>
<p>Click the button to reveal the solution.</p>
<div class="toggle docutils container">
<p>Solution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{\beta}_1 \pm 1.96 \times SE(\hat{\beta}_1) \\
0.46656 \pm 1.96 \times 0.03054
\end{split}\]</div>
<p>which gives a 95% CI from 0.407 to 0.526.</p>
<p>The data are consistent with an increase in birthweight per daily increase in gestational age between 0.41oz and 0.52oz.</p>
<p>Since 0 does not lie within the interval, we conclude that there is evidence of an association between birthweight and length of pregancy (as also indicated by the results of the hypothesis test).</p>
</div>
<p>Alternatively, we can obtain confidence intervals using <code class="docutils literal notranslate"><span class="pre">confint</span></code> in R. The option <code class="docutils literal notranslate"><span class="pre">parm</span></code> tells R which regression coefficients to provide confidence intervals for. Try omitting this option or changing it to value 1 to see what happens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Confidence intervals for the slope, beta_1</span>
<span class="nf">confint</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">parm</span><span class="o">=</span><span class="m">2</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 1 × 2 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>
</thead>
<tbody>
	<tr><th scope=row>Gestational.Days</th><td>0.4066435</td><td>0.5264702</td></tr>
</tbody>
</table>
</div></div>
</div>
</div>
</div>
<span id="document-12.h. Linear Regression I"></span><div class="section" id="example-binary-independent-variable">
<h3>12.7 Example: binary independent variable<a class="headerlink" href="#example-binary-independent-variable" title="Permalink to this headline">¶</a></h3>
<p>We now return to our second example, where we are interested in the association between birthweight and the mother’s smoking status. In exploratory analyses, we saw that  mothers who do not smoke give birth to heavier babies, on average, than mothers who do smoke. We will now use a simple linear regression model to further explore this association.</p>
<p>The outcome variable is the same as for our previous example. A key difference, however, is that the independent variable is a binary variable.</p>
<div class="section" id="the-model">
<h4>12.7.1 The model<a class="headerlink" href="#the-model" title="Permalink to this headline">¶</a></h4>
<p>We have a continuous outcome variable and a binary independent variable. To include this binary variable in the model, we create a <strong>dummy</strong> variable that takes the value 1 if the mother smokes and 0 if the mother doesn’t smoke:</p>
<div class="math notranslate nohighlight">
\[\begin{split} s_{i} = 
\begin{cases}
    1 &amp; \text{ if the $i^{th}$ baby's mother smokes} \\
    0 &amp; \text{ if the $i^{th}$ baby's mother does not smoke}
\end{cases} \end{split}\]</div>
<p>We then define the following linear regression model:</p>
<div class="math notranslate nohighlight">
\[ 
\text{Model 2: } y_i = \beta_0 + \beta_1 s_i + \epsilon_i
\]</div>
<p>When including binary (or categorical) variables in a linear regression in R, we can tell R to treat it as a factor variable using <code class="docutils literal notranslate"><span class="pre">factor()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example 2: Investigating the relationship between birthweight and mother&#39;s smoking status.</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="n">model2</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="nf">factor</span><span class="p">(</span><span class="n">Maternal.Smoker</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ factor(Maternal.Smoker), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-68.085 -11.085   0.915  11.181  52.915 

Coefficients:
                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                 123.0853     0.6645 185.221   &lt;2e-16 ***
factor(Maternal.Smoker)True  -9.2661     1.0628  -8.719   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 17.77 on 1172 degrees of freedom
Multiple R-squared:  0.06091,	Adjusted R-squared:  0.06011 
F-statistic: 76.02 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_0 = 123.09\)</span>. This is interpreted as the estimated mean birthweight (in oz) of a baby with “dummy” variable equal to 0, i.e. it is the estimated mean birthweight of babies whose mothers do not smoke.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_1=-9.23\)</span>. The mean birthweight is estimated to decrease by 9.23oz per unit increase in the “dummy” variable. A unit increase in the dummy variable equates to moving from the non-smoking group to the smoking group, so we can interpret this as the difference in mean birthweights between the two groups.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma}=17.77\)</span>. The observed outcomes are scattered around the fitted regression line with a standard deviation of 17.77oz.</p></li>
</ul>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="simple">
<li><p>Perform a hypothesis test of the null hypothesis that there is no association between maternal smoking and birthweight. Write down the null hypothesis, the test statistic and the p-value. interpret your p-value.</p></li>
<li><p>Calculate (manually or using R) a 95% confidence interval for the difference in mean birthweight between the group whose mothers smoke and those who don’t.</p></li>
</ol>
<p>Try the exercise and then click the button to reveal the solution.</p>
<div class="toggle docutils container">
<p>Solution:</p>
<ol class="simple">
<li><p>The null hypothesis is <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span>. The test statistic (from the R output) is <span class="math notranslate nohighlight">\(T=-8.72\)</span>. We can check this is the ratio of the estimated slope to its standard error (-9.2661/1.0628 = -8.72). The p-value is <span class="math notranslate nohighlight">\(p&lt;2e_16\)</span>. In health applications, we typically write <span class="math notranslate nohighlight">\(p&lt;0.001\)</span>. We interpret this as strong evidence against the null hypothesis. Therefore, we can conclude there is an association between birthweight and maternal smoking.</p></li>
<li><p>This is a large sample, so an approxiate 95% confidence interval can be obtained by the estimate plus-or-minus 1.96 times the standard error. This is:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\begin{align*}
-9.2661 \pm 1.96 \times 1.0628  = 
\end{align*}
\]</div>
<p>This gives an interval from -11.35 to -7.18.</p>
<p>Alternatively, in R:  <code class="docutils literal notranslate"><span class="pre">confint(model2,</span> <span class="pre">parm=2,</span> <span class="pre">level=0.95)</span></code></p>
</div>
</div>
</div>
</div>
<span id="document-12.i. Linear Regression I"></span><div class="section" id="additional-material">
<h3>12.8 Additional material<a class="headerlink" href="#additional-material" title="Permalink to this headline">¶</a></h3>
<p>This section contains additional material concerning confidence intervals for a fitted value and reference ranges. These are useful topics in regression, but will not be examinable.</p>
<div class="section" id="confidence-intervals-for-a-fitted-value">
<h4>12.8.1 Confidence intervals for a fitted value<a class="headerlink" href="#confidence-intervals-for-a-fitted-value" title="Permalink to this headline">¶</a></h4>
<p>So far we have only discussed conducting inference on the estimated regression coefficients. However, it may also be of interest to determine <strong>confidence intervals for the fitted outcomes</strong>, or <strong>prediction intervals</strong>. The subsequent two sections describe and illustrate these two concepts, respectively.</p>
<p>Rather than focusing on associations between variables and the outcome, we are sometimes interested in the expected value of the outcome at particular values of <span class="math notranslate nohighlight">\(X\)</span>, i.e. <span class="math notranslate nohighlight">\(y_x = E[Y | X=x] = \beta_0+\beta_1 x\)</span>.</p>
<p>The fitted value is our estimate of this quantity,</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_x = \hat{\beta}_0 + \hat{\beta}_1 x.
\]</div>
<p>The variance of the fitted value is given by:</p>
<div class="math notranslate nohighlight">
\[
V(\hat{y}_x) = \sigma^2 \left(\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(SS_{xx}=\sum_{i=1}^n(x_i-\bar{x})^2\)</span>, i.e. the sum of squares of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>The 95% confidence interval for the fitted value is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{y_x} \pm t_{n-2, 0.975}\hat{\sigma} \sqrt{\frac{1}{n}+ \frac{(x-\bar{x})^2}{SS_{xx}}}
\]</div>
<ul class="simple">
<li><p>95% confidence intervals can be obtained for values of the independent variable that do not arise in the data. However, the width of the confidence interval increases with the distance from the mean (as can be seem from the formula and figure given below).</p></li>
<li><p>Care must be taken when extrapolating outside the range of the observed data as this makes an un-testable assumption that linearity continues outside the observed data range.</p></li>
</ul>
<p><em>Example</em>. The R code below calculates a 95% confidence interval for the fitted value of birthweight of a baby born after 280 gestational days.</p>
<div class="section" id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h5>
<p>We return to our first example, exploring the association between birthweight and length of pregnancy (gestational days).Suppose we are interested in the expected birthweight for a baby who is born at 280 days’ gestation.</p>
<p>The code below refits our model and uses it to estimate the expected birthweight for this gestational age. It also provides a 95% confidence interval around that estimate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Refit model</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="n">model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Confidence interval for a fitted value </span>
<span class="n">new.data</span><span class="o">&lt;-</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">Gestational.Days</span><span class="o">=</span><span class="m">280</span><span class="p">)</span>
<span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new.data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;confidence&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 1 × 3 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>119.8818</td><td>118.9215</td><td>120.8421</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>We estimate that the expected (average) birthweight for babies born at 280 days’ gestation is 119.9oz. The 95% confidence interval for this estimate is is (118.9, 120.8). Informally, we can interpret this as: it is plausible that the true value of the expected birthweight, for babies born at 280 days’ gestation, lies between 118.9oz and 120.8oz.</p>
<p>We can extend this idea to graph the fitted values - estimated expected birthweight - and their confidence intervals across the range of gestational days. The code below does this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>

<span class="c1"># Plot the fitted regression line (the fitted values)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;95% Confidence intervals&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>

<span class="c1"># Add the confidence intervals for the fitted regression line</span>
<span class="n">conf_interval</span><span class="o">&lt;-</span><span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;confidence&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">3</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12.i. Linear Regression I_6_0.png" src="_images/12.i. Linear Regression I_6_0.png" />
</div>
</div>
<p>Notice how the confidence interval around the fitted line is narrowest in the centre of the x-axis, where most of our data are concentrated, and widest at the extremes.</p>
</div>
</div>
<div class="section" id="prediction-intervals">
<h4>12.8.2 Prediction intervals<a class="headerlink" href="#prediction-intervals" title="Permalink to this headline">¶</a></h4>
<p>The 95% confidence interval around the fitted line describes our certainty about where the fitted line is (i.e. where the expected value is).</p>
<p>Sometimes, we are not interested in the average outcome at a particular point, but the likely spread of values arount the average. In this case, we are interested in obtaining what is called a <strong>prediction interval</strong>, or <strong>reference range</strong>.</p>
<p>A 95% prediction interval, or 95% reference range, is an interval within which 95% of future observations are expected to lie.</p>
<p>The predicted value for an individual with <span class="math notranslate nohighlight">\(X=x\)</span> is the fitted value, as above. However, there are now two sourcces of uncertainty to take into account. (1) There is uncertainty about the fitted value (the expected value), as above. (2) There is random error around that point (<span class="math notranslate nohighlight">\(\sigma^2\)</span>). Thus, the variance in the individual prediction is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
V(\hat{y_x}) + \sigma^2 &amp;= \sigma^2 \left(\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}\right)+ \sigma^2 \\
 &amp; = \sigma^2\left(1+\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}\right)
\end{align*}
\end{split}\]</div>
<p>A 95% prediction interval is then given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{y_x} \pm t_{n-2, 0.975} \hat{\sigma} \sqrt{1+ \frac{1}{n}+ \frac{(x-\bar{x})^2}{S_{xx}}}
\]</div>
<div class="section" id="id1">
<h5>Example<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>The R code below calculates a 95% prediction interval for the birthweight of babies who are born at 280 days’ gestation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prediction interval</span>
<span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new.data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 1 × 3 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>119.8818</td><td>87.01496</td><td>152.7486</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The 95% prediction interval for babies born at 280 days’ gestation is (87.0, 152.7). This means that we would expect 95% of babies born after 280 gestational days to weigh between 87 and 152.7 ounces.</p>
</div>
</div>
<div class="section" id="comparing-intervals">
<h4>12.8.3 Comparing intervals<a class="headerlink" href="#comparing-intervals" title="Permalink to this headline">¶</a></h4>
<p>The code below produces two scatterplots of gestational days against birthweight with the linear regression line of best fit (obtained from Model 1) superimposed. The blue lines on the left-hand side plot represent the 95% confidence intervals for the fitted values across the entire range of gestational days. The blue lines on the right-hand side plot represent the 95% prediction intervals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Set the graphical space so that two plots are shown side-by-side in one row</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.width</span><span class="o">=</span><span class="m">8</span><span class="p">)</span>

<span class="c1">#Confidence intervals for predicted values</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;95% Confidence intervals&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>

<span class="n">conf_interval</span><span class="o">&lt;-</span><span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;confidence&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">3</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>

<span class="c1">#Reference ranges</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;95% Prediction intervals&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>

<span class="n">conf_interval</span><span class="o">&lt;-</span><span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">3</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12.i. Linear Regression I_13_0.png" src="_images/12.i. Linear Regression I_13_0.png" />
</div>
</div>
<p>As expected, we see that the prediction interval is much wider. Loosely speaking, the plot on the left shows a range of uncertainty about where the <em>average</em> line is. The plot on the right shows a range of uncertainty about where individual measurements will lie.</p>
</div>
</div>
</div>
</div>
<span id="document-13.a. Linear Regression II"></span><div class="section" id="linear-regression-ii">
<h2>13. Linear Regression II<a class="headerlink" href="#linear-regression-ii" title="Permalink to this headline">¶</a></h2>
<p>This is the second of three sessions that explore linear regression modelling. These are models where the outcome of interest is a continuous variable.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session, you will be able to:</p>
<ul class="simple">
<li><p>explain the difference between a univariable and multivariable linear regression model</p></li>
<li><p>fit and interpret a multivariable linear regression</p></li>
<li><p>describe the principles of centering</p></li>
<li><p>interpret categorical variables, quadratic terms and interaction terms included in a linear regression model</p></li>
</ul>
</div><p><strong>Acknowledgements:</strong>  Thank you to Jennifer Nicholas and Chris Frost whose notes on linear regression were particularly useful in the development of the current lesson.</p>
<div class="toctree-wrapper compound">
<span id="document-13.b. Linear Regression II"></span><div class="section" id="categorical-independent-variables">
<h3>13.1 Categorical independent variables<a class="headerlink" href="#categorical-independent-variables" title="Permalink to this headline">¶</a></h3>
<p>We have explored simple linear regression with a continuous independent variable and with a binary variable. We now extend these ideas to include a categorical independent variable.</p>
<p>We will return to the baby example and use linear regression to explore the association between maternal Body Mass Index (BMI) category and the baby’s birthweight.</p>
<div class="section" id="dummy-variables">
<h4>13.1.1 Dummy variables<a class="headerlink" href="#dummy-variables" title="Permalink to this headline">¶</a></h4>
<p>We have height measured in inches and weight measured in pounds. BMI is obtained using the formula <span class="math notranslate nohighlight">\(BMI = 703 \times weight (lb) / height (in)^2\)</span>. We will then categorise BMI according to the World Health Organisation’s classification.</p>
<p>We will define a categorical variable <span class="math notranslate nohighlight">\(c_{i}\)</span> denoting the <span class="math notranslate nohighlight">\(i^{th}\)</span> mother’s BMI category, defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
c_{i} =
\begin{cases}
    1 &amp; \text{ if the mother's BMI is less than 18.5 (underweight)} \\
    2 &amp; \text{ if the mother's BMI is at least 18.5 and less than 25 (normal)} \\
    3 &amp; \text{ if the mother's BMI is at least 25 and less than 30 (overweight)} \\
    4 &amp; \text{ if the mother's BMI is 30 or more (obese)} 
\end{cases} 
\end{split}\]</div>
<p>Our BMI categorical variable has four categories. To distinguish between all four categories we need <em>three</em> dummy variables.  We choose a <em>baseline</em> or <em>reference</em> group, which for us will be the underweight category. For each of the other categories, we create a dummy variable which indicates that the woman is in (or not) that category. Specifically, we define our dummy variables as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
w_{1i} =
\begin{cases}
    1 &amp; \text{ if } c_{i}=2\\
    0 &amp; \text{ if } c_{i} \neq 2
\end{cases} 
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
w_{2i} =
\begin{cases}
    1 &amp; \text{ if } c_{i}=3\\
    0 &amp; \text{ if } c_{i} \neq 3
\end{cases}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
w_{3i} =
\begin{cases}
    1 &amp; \text{ if } c_{i}=4\\
    0 &amp; \text{ if } c_{i} \neq 4
\end{cases}
\end{split}\]</div>
<p>The R code below read in the baby data and create variables containing the mother’s BMI (<code class="docutils literal notranslate"><span class="pre">Maternal.BMI</span></code>) and the mother’s BMI category (<code class="docutils literal notranslate"><span class="pre">Maternal.BMIcat</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="c1"># Calculate maternal BMI (with conversion factor due to measurement in lb and in)</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMI</span> <span class="o">&lt;-</span> <span class="m">703</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Pregnancy.Weight</span><span class="o">/</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="p">)</span><span class="o">**</span><span class="m">2</span>

<span class="c1"># Categorise the BMI values</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMIcat</span> <span class="o">&lt;-</span><span class="m">1</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMIcat</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMI</span><span class="o">&gt;=</span><span class="m">18.5</span> <span class="o">&amp;</span> <span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMI</span><span class="o">&lt;</span><span class="m">25</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">2</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMIcat</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMI</span><span class="o">&gt;=</span><span class="m">25</span>   <span class="o">&amp;</span> <span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMI</span><span class="o">&lt;</span><span class="m">30</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">3</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMIcat</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMI</span><span class="o">&gt;=</span><span class="m">30</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">4</span>

<span class="c1"># Tabulate the BMI categories</span>
<span class="nf">table</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.BMIcat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1   2   3   4 
 84 932 124  34 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-model">
<h4>13.1.1 The model<a class="headerlink" href="#the-model" title="Permalink to this headline">¶</a></h4>
<p>A linear regression model relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>, the outcome) to the three dummy variables (<span class="math notranslate nohighlight">\(W_1, W_2, W_3\)</span>) representing the mother’s BMI category (<span class="math notranslate nohighlight">\(C\)</span>, the categorical independent variable) is defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{Model 3: } y_i = \beta_0 + \beta_{1}w_{1,i} + \beta_{2}w_{2,i} + \beta_{3}w_{3,i}  + \epsilon_i 
\]</div>
<p>The equation above can also be written as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
y_i &amp;= \beta_0 + \epsilon_i \text{ if } c_i=1 \text{ (underweight mothers) }  \\
y_i &amp;= \beta_0 + \beta_1 +  \epsilon_i \text{ if } c_i=2 \text{ (normal weight mothers) } \\
y_i &amp;= \beta_0 + \beta_2 + \epsilon_i \text{ if } c_i=3 \text{ (overweight mothers) } \\
y_i &amp;= \beta_0 + \beta_3 + \epsilon_i \text{ if } c_i=4 \text{ (obese mothers) }  \\
\end{align}
\end{split}\]</div>
<p>This makes explicit the interpretation of the parameters in the model.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(C=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_1\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(C=2\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_1\)</span> is the difference in the expectation of <span class="math notranslate nohighlight">\(Y\)</span> between groups defined by <span class="math notranslate nohighlight">\(C=1\)</span> and <span class="math notranslate nohighlight">\(C=2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_2\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(C=3\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_2\)</span> is the difference in the expectation of <span class="math notranslate nohighlight">\(Y\)</span> between groups defined by <span class="math notranslate nohighlight">\(C=1\)</span> and <span class="math notranslate nohighlight">\(C=3\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_3\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(C=4\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_3\)</span> is the difference in the expectation of <span class="math notranslate nohighlight">\(Y\)</span> between groups defined by <span class="math notranslate nohighlight">\(C=1\)</span> and <span class="math notranslate nohighlight">\(C=4\)</span>.</p></li>
</ul>
<p>Notes</p>
<blockquote>
<div><p>In this parameterisation of the model, the group defined by <span class="math notranslate nohighlight">\(C=0\)</span> is often referred to as the baseline group. There is no statistical reason why one group rather than another should be chosen as the baseline group. It can sometimes be desirable to re-parameterise a model of this type to estimate parameters representating differences in mean levels from a particular baseline group. In this example, for instance, we might instead want the group with normal weight to be our baseline group, in which case we would need to redefine our first dummy variable. <br> <br>
Note that, in contrast to the models that we have met so far, this has more than one variable in it (even though the three dummy variables together measure a single characteristic). Therefore, this is no longer strictly a simple linear regression model. It is an example of a multivariable linear regression model. We will discuss general theory for this model later. Broadly speaking, the ideas we have met in the context of simple linear regression extend to this more general model very naturally.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model 3: Relating birthweight to length of pregnancy and mother&#39;s height group. </span>
<span class="n">model3</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="nf">factor</span><span class="p">(</span><span class="n">Maternal.BMIcat</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ factor(Maternal.BMIcat), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-64.828 -10.828   0.172  11.172  57.677 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               115.286      1.996  57.752   &lt;2e-16 ***
factor(Maternal.BMIcat)2    4.543      2.084   2.180   0.0295 *  
factor(Maternal.BMIcat)3    3.037      2.585   1.175   0.2404    
factor(Maternal.BMIcat)4    8.626      3.719   2.320   0.0205 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 18.3 on 1170 degrees of freedom
Multiple R-squared:  0.006152,	Adjusted R-squared:  0.003604 
F-statistic: 2.414 on 3 and 1170 DF,  p-value: 0.06511
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_0 = 115.29\)</span>. This is interpreted as the estimated mean birthweight (in oz) of a baby with all “dummy” variables equal to 0, i.e. it is the estimated mean birthweight of babies in our baseline category (those with mothers who are underweight).</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_1= 4.543\)</span>. The mean birthweight is estimated to increase by 4.5 oz per unit increase in the first “dummy” variable. A unit increase in the first dummy variable equates to moving from the underweight group to the normal weight group. So we can interpret this as the difference in mean birthweights between the group whose mothers have normal BMI and those whose mothers are underweight.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_2= 3.037\)</span>. The mean birthweight is estimated to increase by 3.0 oz per unit increase in the second “dummy” variable. A unit increase in the second dummy variable equates to moving from the underweight group to the overweight group. So we can interpret this as the difference in mean birthweights between the group whose mothers are overweight and those whose mothers are underweight.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_3= 8.626\)</span>. The mean birthweight is estimated to increase by 8.6 oz per unit increase in the third “dummy” variable. A unit increase in the first dummy variable equates to moving from the underweight group to the obese group. So we can interpret this as the difference in mean birthweights between the group whose mothers are obese and those whose mothers are underweight.</p></li>
</ul>
<p>Overall, we see a pattern of higher maternal BMI being associated with higher birthweights, particularly for the group with obese mothers.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma}=18.3\)</span>. The observed outcomes are scattered around the fitted regression line with a standard deviation of 18.3oz.</p></li>
</ul>
<p>We can obtain confidence intervals around the three BMI estimates as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 4 × 2 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>
</thead>
<tbody>
	<tr><th scope=row>(Intercept)</th><td>111.3691529</td><td>119.202276</td></tr>
	<tr><th scope=row>factor(Maternal.BMIcat)2</th><td>  0.4533602</td><td>  8.631864</td></tr>
	<tr><th scope=row>factor(Maternal.BMIcat)3</th><td> -2.0356770</td><td>  8.109410</td></tr>
	<tr><th scope=row>factor(Maternal.BMIcat)4</th><td>  1.3296865</td><td> 15.922414</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The R output from the model provides p-values for each of the three coefficients relating maternal BMI to birth weight. However, we are typically interested in the broad question of whether maternal BMI is related to birth weight, rather than whether an individual dummy variable is related to the outcome.</p>
<p>Therefore, when we have a categorical variable in a regression model, the hypothesis of interest usually relates to the combination of all dummy variables representing the categorical variable. An appropriate hypothesis test jointly tests the hypothesis that all coefficients for dummy variables are zero, i.e.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_1 = 0, \beta_2 = 0, \beta_3 = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \text{at least one of } \ \beta_1, \beta_2, \beta_3 \neq 0\)</span></p></li>
</ul>
<p>We use a partial <span class="math notranslate nohighlight">\(F\)</span>-test to test this hypothesis. Details are beyond the scope of the course, but are outlined in the appendix to this session. The R code to obtain the joint p-value is shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove maternal BMI from the model (i.e. a constant-only model)</span>
<span class="n">model3_without</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="m">1</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="nf">anova</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">model3_without</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A anova: 2 × 6</caption>
<thead>
	<tr><th></th><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>1170</td><td>391633.5</td><td>NA</td><td>       NA</td><td>      NA</td><td>        NA</td></tr>
	<tr><th scope=row>2</th><td>1173</td><td>394057.9</td><td>-3</td><td>-2424.344</td><td>2.414232</td><td>0.06510651</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>In this case we have a p-value of p=0.065, indicating some evidence against the null hypothesis of no association bewteen maternal BMI category and the baby’s birthweight.</p>
</div>
<div class="section" id="categorising-continuous-variables">
<h4>13.1.2 Categorising continuous variables<a class="headerlink" href="#categorising-continuous-variables" title="Permalink to this headline">¶</a></h4>
<p>In the above example, we have categorised an continuous variable (BMI) in order to demonstrate how a categorical variable should be included in a linear regression model. This is important to know, since there are many variables that are categorical by definition and may be required for a statistical analysis. For example: cancer stage, ethnicity, education level, etc. While these examples should be included as a categorical variable in a linear regression model, it is not, in general, recommended to categorise a continuous variable in a linear model. We did so above, purely for pedagogical reasons.</p>
<p>One of the problems with categorising continuous variables is that it is difficult to decide what the cut-off for each category should be. In the example above, however, there are widely used categorisations.</p>
<p>We often lose information by categorising continous variables. We can often obtain a better and more parsimonious fit (using fewer parameters to describe the relationship) by modelling the continuous variable without categorisation. We will return to these ideas later.</p>
</div>
</div>
<span id="document-13.c. Linear Regression II"></span><div class="section" id="multivariable-linear-regression">
<h3>13.2 Multivariable linear regression<a class="headerlink" href="#multivariable-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>Multivariable linear regression extends the simple linear regression model to situations in which we wish to relate two or more independent variables to one outcome. Where there are multiple independent variables, we will refer to them as <strong>covariates</strong>.</p>
<p>There can be a number of different reasons why we would want to add more covariates in our linear regression model. Recall these two examples of questions we might want to answer using statistical models (given at the beginning of this lesson):</p>
<ul class="simple">
<li><p>Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?</p></li>
<li><p>Can we predict the risk of heart disease for our patients?</p></li>
</ul>
<p>In the first example, we could use a statistical model with inflammation as the outcome and drug use as the independent variable of interest, but we may need to <strong>control</strong> (or <strong>adjust</strong>) for the <strong>confounding</strong> effects of other patient characteristics (age, gender, other medication use, etc.). In the second example, there are many different factors that could be associated with risk of heart disease (age, gender, lifestyle choices, etc.) and we may wish to include all such factors in a statistical model to predict heart disease.</p>
<p>Here, we introduce the multivariable linear regression model and describe how to estimate and interpret the parameters in the model using an example from the birthweight data.</p>
<p>Note</p>
<blockquote>
<div><p><em>A note on notation.</em> There can be some confusion between the terms <strong>multivariable</strong> models and <strong>multivariate</strong> models. Multivariate models are those which have more than one outcome variable. Such models are beyond the scope of this module; we focus our attention on <strong>univariate</strong> models which have only one outcome. Both simple linear regression models and multivariable linear regression models are considered as univariate.</p>
</div></blockquote>
<div class="section" id="the-multivariable-linear-regression-model">
<h4>13.2.1 The multivariable linear regression model<a class="headerlink" href="#the-multivariable-linear-regression-model" title="Permalink to this headline">¶</a></h4>
<p>Suppose we wish to relate an outcome (<span class="math notranslate nohighlight">\(Y\)</span>) to <span class="math notranslate nohighlight">\(p\)</span> predictor variables <span class="math notranslate nohighlight">\((X_1, X_2, ..., X_p)\)</span>. The appropriate multivariable linear regression model is a straightforward extension of the simple linear regression model:</p>
<div class="math notranslate nohighlight">
\[ 
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ,..., \beta_p x_{ip}+\epsilon_i \text{ with } \epsilon_i \overset{iid}{\sim} N(0,\sigma^2).
\]</div>
<p>where, <span class="math notranslate nohighlight">\(y_i\)</span> is the value of the dependent variable for the ith participant and <span class="math notranslate nohighlight">\(x_{ji}\)</span> is the value of the jth predictor variable for the ith participant.</p>
<p>The parameters in the model are interpreted as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept. It is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when all the <span class="math notranslate nohighlight">\(X_j\)</span>’s are zero.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_j\)</span> is the expected change in <span class="math notranslate nohighlight">\(Y\)</span> for a 1 unit increase in <span class="math notranslate nohighlight">\(X_j\)</span> <em>with all the other covariates held constant</em>.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(\beta_j\)</span>’s are the <strong>regression coefficients</strong> (otherwise known as <strong>partial regression coefficients</strong>). Each one measures the effect of one covariate controlled (or adjusted) for all of the others.</p>
</div>
<div class="section" id="the-multivariable-linear-regression-model-in-matrix-notation">
<h4>13.2.2 The multivariable linear regression model in matrix notation<a class="headerlink" href="#the-multivariable-linear-regression-model-in-matrix-notation" title="Permalink to this headline">¶</a></h4>
<p>Similarly to the simple linear regression model, the multivariable linear regression model can be expressed using matrix algebra.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon} \text{ where }\epsilon \sim N(0,\mathbf{I}\sigma^2)
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{vmatrix}y_1\\y_2 \\. \\. \\. \\y_n \end{vmatrix}=\begin{vmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\ 1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2p}  \\1 &amp; . \\1 &amp; .  \\ 1&amp; . \\1 &amp; x_{p1} &amp; x_{p} &amp; ... &amp; x_{pn} \end{vmatrix}\begin{vmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ ... \\ \beta_p \end{vmatrix}+\begin{vmatrix}\epsilon_1\\ \epsilon_2 \\ . \\ . \\. \\ \epsilon_n \end{vmatrix} 
\end{split}\]</div>
<p>In this formulation, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n \times (p+1)\)</span> matrix, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> are vectors of length <span class="math notranslate nohighlight">\(n\)</span> whilst <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> is a vector of length <span class="math notranslate nohighlight">\((p+1)\)</span>.</p>
</div>
<div class="section" id="estimation-of-the-parameters">
<h4>13.2.3 Estimation of the parameters<a class="headerlink" href="#estimation-of-the-parameters" title="Permalink to this headline">¶</a></h4>
<p>The regression coefficients in multivariable linear regression can be estimated by minimising the residual sum of squares:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
SS_{RES} &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 = \sum_{i=1}^n (y_i-\hat{y})^2 \\
&amp;= \sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}_1x_{1i}-...-\hat{\beta}_px_{pi})^2 
\end{align}
\end{split}\]</div>
<p>The closed form solution, obtained by solving the <span class="math notranslate nohighlight">\((p+1)\)</span> simultaneous equations that result from setting the partial derivatives of the above equation with respect to each parameter estimate to zero, can be written succinctly using matrix notation:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{\beta}}= (\mathbf{X'X})^{-1}X'Y
\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{\hat{\beta}}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>. Its distribution is as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{\beta}} \sim \mathbf{N(\beta, (X'X)^{-1}\sigma^2)}.
\]</div>
<p>This expresses the fact that the elements of <span class="math notranslate nohighlight">\(\mathbf{\hat{\beta}}\)</span> follow a multivariate normal distribution whose variances and covariances are given by <span class="math notranslate nohighlight">\(\mathbf{(X'X)^{-1}\sigma^2}\)</span>.</p>
<p>It can also be shown that the following is an unbiased estimator for <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\sigma}^2 &amp;= \sum_{i=1}^n \frac{\hat{\epsilon_i}^2}{(n-(p+1))}\\
              &amp;=\sum_{i=1}^n \frac{(y_i - \hat{\beta_0} - \hat{\beta}_1x_{1i}- ... - \hat{\beta_p}x_{ip})^2}{(n-(p+1))}
\end{align}
\end{split}\]</div>
<p>While it is useful to know how these parameters are estimated, in practice they are often obtained using statistical software. Next, we demonstrate how to perform multivariable regression in R using the birthweight data and discuss the interpretation of the estimated regression coefficients.</p>
</div>
</div>
<span id="document-13.d. Linear Regression II"></span><div class="section" id="including-multiple-covariates">
<h3>13.3 Including multiple covariates<a class="headerlink" href="#including-multiple-covariates" title="Permalink to this headline">¶</a></h3>
<p>We are interested in investigating a model that relates birthweight to length of pregnancy and mother’s height. We will use the following multivariable linear regression model:</p>
<div class="math notranslate nohighlight">
\[
\text{Model 4: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i  + \epsilon_i 
\]</div>
<p>The outcome <span class="math notranslate nohighlight">\(y_i\)</span> denotes the birthweight (in oz) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby. The predictors <span class="math notranslate nohighlight">\(l_i\)</span> and <span class="math notranslate nohighlight">\(h_i\)</span> denote the length of pregnancy (i.e. number of gestational days), and the height of the mother (in inches), for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby, respectively.</p>
<p>The linear regression can be conducted in R using the <code class="docutils literal notranslate"><span class="pre">lm()</span></code> command:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model 4: Relating birthweight to length of pregnancy and mother&#39;s height</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="n">model4</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">Maternal.Height</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days + Maternal.Height, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -88.51993   14.31910  -6.182 8.73e-10 ***
Gestational.Days   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p><strong>Interpretation of the regression coefficients</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_1=0.45\)</span>. This is the estimated regression coefficient for number of gestational days. It is interpreted as: the expected increase in a baby’s birthweight for each gestational day, <em>amongst babies whose mothers were of the same height</em>, is 0.45 ounces.</p></li>
</ul>
<p>It may be tempting to make causal inference from regression models such as Model 4, i.e. “longer pregnancies <strong>cause</strong> an increase in birthweight”. However, this is far from straightforward. Based on the results presented above, it would be reasonable to say that “birthweight increases with length of pregnancy”. However, it is much less reasonable to claim that higher birthweight is caused by longer pregnancies (based on these results alone), because there may be an unobserved third variable that is the “real” cause of both increased length of pregancy and birthweight. Causal statements require more than just the results of a statistical model to make them plausible; this is a topic that we return to in the next lesson.</p>
<p><em>Excerise</em>: What is the interpretation of <span class="math notranslate nohighlight">\(\hat{\beta}_2?\)</span></p>
<p><strong>Interpretation of the intercept</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_0=-88.52\)</span>. The interpretation is that the estimated mean birthweight for a child who was born after 0 gestastional days and whose mother’s height is 0 inches is -88.52 ouces. Clearly this is an absurd value to estimate because no babies are born that quickly and no mothers are that short. If we wish to obtain a more reasonable intercept, we can use a technique called <strong>centering</strong>.</p></li>
</ul>
</div>
<span id="document-13.e. Linear Regression II"></span><div class="section" id="centering">
<h3>13.4 Centering<a class="headerlink" href="#centering" title="Permalink to this headline">¶</a></h3>
<p>In many analyses, interpreting the intercept is not as important as interpreting the estimated regression coefficients and so it does not matter if our intercept is an absurd value (as in the example above). However, if we do wish to obtain an interpretable intercept, we can <strong>center</strong> the independent variables.</p>
<p>Centering a variable means subtracting a constant from every value of the  variable. This essentially shifts the scale of the predictor (the point 0 is shifted to the chosen constant), but does not affect the units of the variable. Consequently, the new interpretation of the intercept would be the mean of <span class="math notranslate nohighlight">\(Y\)</span> when the independent variable is equal to the constant. The estimated regression coefficient of the independent variable is not affected.</p>
<p>As as example, we will repeat the analysis above, but center each of the covariates on their mean value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># What are the mean gestational days and mothers height in our data?</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="p">)</span>

<span class="c1"># Create new (centered) variables in our data</span>
<span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days.Centered</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">)</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Centered</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="p">)</span>

<span class="c1"># Redefine Model 4 using the centered variables</span>
<span class="n">model4</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days.Centered</span><span class="o">+</span><span class="n">Maternal.Height.Centered</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  148.0   272.0   280.0   279.1   288.0   353.0 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  53.00   62.00   64.00   64.05   66.00   72.00 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days.Centered + Maternal.Height.Centered, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               119.46252    0.47980 248.983  &lt; 2e-16 ***
Gestational.Days.Centered   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>Now the intercept (<span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>) is equal to 119.46. This is interpreted as: the estimated mean birthweight for a child who was born after 279.1 gestastional days and whose mother’s height is 64.05 inches is 119.46 ouces. Additionally, notice that the estimated regression coefficients for gestational days and mother’s height, and their associated standard errors have not changed.</p>
</div>
<span id="document-13.f. Linear Regression II"></span><div class="section" id="including-higher-order-terms">
<h3>13.6  Including higher-order terms<a class="headerlink" href="#including-higher-order-terms" title="Permalink to this headline">¶</a></h3>
<p>As we have already discussed, linear regression assumes that the relationship between the outcome and the independent variables is linear. As we already know, this is not always the case in real data. For example, suppose we are interested in the association between weight and age. On average, the weight of young adults will increase with age. However, at a certain age, the average weight may start to decrease. In this case, the association between weight and age would follow a non-linear (upside-down) <span class="math notranslate nohighlight">\(u\)</span>-shape. It could still be possible to model this relationship within the linear regression framework, by adding a <strong>second-order term</strong> to the model. This procedure is known as <strong>quadratic regression</strong>.</p>
<div class="section" id="the-quadratic-regression-model">
<h4>13.6.1 The quadratic regression model<a class="headerlink" href="#the-quadratic-regression-model" title="Permalink to this headline">¶</a></h4>
<p>The quadratic regression model is a multivariable regression model with two independent variables where the second variable is the square of the first variable. Algebraically:</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon \text{ where }  \epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2).
\]</div>
<p>Despite the fact that one of the variables is the square of the other, this is still a linear regression model because the expectation of the outcome is a linear function of both parameters.</p>
<p>The figure below shows two scatter plots of the data used in Scenario A above. The plot on the left-hand side includes the fitted values of a linear regression model (with no higher-order terms included) and the right-hand side plot includes the fitted values of a quadratic regression model. By comparing the plots, we can see that the quadratic regression model does have a better fit, particularly at the extreme values of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<div class="figure align-default" id="quadratic-example">
<a class="reference internal image-reference" href="_images/quadratic_example.png"><img alt="_images/quadratic_example.png" src="_images/quadratic_example.png" style="height: 600px;" /></a>
</div>
<p>Unfortunately, interpreting <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_2\)</span> is not as straightforward as in most linear models. The reason for this is that it is not possible to change <span class="math notranslate nohighlight">\(X^2\)</span> by 1 unit whilst holding <span class="math notranslate nohighlight">\(X\)</span> constant.</p>
</div>
<div class="section" id="example">
<h4>13.6.2 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>Suppose the outcome, birthweight, is denoted <span class="math notranslate nohighlight">\(Y\)</span> and length of pregnancy (gestational days) is denoted by <span class="math notranslate nohighlight">\(L\)</span>. The original linear model we considered was:</p>
<div class="math notranslate nohighlight">
\[
\text{Model 1: } y_i = \beta_0 + \beta_1 l_i + \epsilon_i
\]</div>
<p>We now extend this to allow a quadratic relationship between <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\text{Model 5: } y_i = \beta_0 + \beta_1 l_i + \beta_2 l^2_i + \epsilon_i
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># What are the mean gestational days and mothers height in our data?</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>

<span class="c1"># Add quadratic term:</span>
<span class="n">model5</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="nf">I</span><span class="p">(</span><span class="n">Gestational.Days</span><span class="o">**</span><span class="m">2</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days + I(Gestational.Days^2), 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.527 -10.980   0.190   9.973  69.655 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)           -6.901e+01  5.043e+01  -1.368    0.171  
Gestational.Days       8.962e-01  3.678e-01   2.436    0.015 *
I(Gestational.Days^2) -7.890e-04  6.731e-04  -1.172    0.241  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.74 on 1171 degrees of freedom
Multiple R-squared:  0.1671,	Adjusted R-squared:  0.1656 
F-statistic: 117.4 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>The estimated coefficient for the quadratic term is very small (-0.000789). The p-value, testing the null hypothesis that <span class="math notranslate nohighlight">\(H_0: \beta_2 = 0\)</span> is p=0.241, suggesting no evidence against the null hypothesis. Therefore, we conclude there is no evidence of a quadratic relationship.</p>
</div>
<div class="section" id="more-complex-models">
<h4>13.6.3 More complex models<a class="headerlink" href="#more-complex-models" title="Permalink to this headline">¶</a></h4>
<p>Quadratic regression models are limited in terms of describing relationships between variables in most medical applications. Quadratic functions either increase to a maximum and the decline, or fall to a minimum and then increase. Further, the behaviour of a quadratic is symmetric about the turning point. Such relationships in medical research are rarely plausible.</p>
<p>There are a number of alternative approaches that can be used to model complex relationships between continuous independent variables and the outcome within a linear regression model. Some are discussed below.</p>
<p>The quadratic regression model belongs to a family of <strong>polynomial regression models</strong> and is the simplest model in that family. Further power terms can be added to the regression model in order to increase complexity. For example, a cubic regression model is one which includes a cubic term as well as a squared term.</p>
<p>An even more flexible approach is to use a <strong>piecewise polynomial model</strong>, which allows for a different polynomial function in different ranges of the observed values of <span class="math notranslate nohighlight">\(X\)</span>, defined according to specified <strong>knots</strong>. The flexibility of the model (and therefore its ability to model more complex relationships) can be increased by increasing the degree of polynomial and/or the number of knots. However, highly flexible models may overfit the data and make the model difficult to interpret. In general, it is a good idea to consider an appropriate trade-off between flexibility and interpretability</p>
<p>A related idea is to use <strong>splines</strong> to flexibly model the relationship. These are a type of piecewise polynomial model, where the adjacent polynomials are constrained to meet at the join points (the knots).</p>
</div>
</div>
<span id="document-13.g. Linear Regression II"></span><div class="section" id="modelling-interaction-terms">
<h3>13.7  Modelling interaction terms<a class="headerlink" href="#modelling-interaction-terms" title="Permalink to this headline">¶</a></h3>
<p>Suppose we fit a multivariable linear regression model relating the outcome of weight to the covariates age, sex and height, for adults in the general population. In this case, the estimated regression coefficient for height represents the effect of a unit increase in height on weight in people of the same age and sex. The model assumes that the coefficient relating weight to height is the same for all people of all ages and sexes. For example, that it is the same for twenty year old men as in ninety-three year old women. But this is not necessarily true! It could be that the slope of the association between weight and height differs by sex and by age. If this is the case, we say there is an <strong>interaction</strong> between height and sex and between height and age.</p>
<p>The term <strong>interaction</strong> is used to describe situations in which the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> differs according to the level of one or more other covariates.</p>
<div class="section" id="linear-regression-with-an-interaction-term">
<h4>13.7.1 Linear regression with an interaction term<a class="headerlink" href="#linear-regression-with-an-interaction-term" title="Permalink to this headline">¶</a></h4>
<p>Suppose we wish to relate an outcome (<span class="math notranslate nohighlight">\(Y\)</span>) to two covariates (<span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>), but we want to allow the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> to differ according to the value of <span class="math notranslate nohighlight">\(X_2\)</span>. To allow for this we fit an interaction model that contains an additional variable (<span class="math notranslate nohighlight">\(X_3\)</span>) that is the product of <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
y_i =  \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i \text{ where }  \epsilon_i \overset{\small{iid}}{\sim} N(0, \sigma^2).
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> = value of the outcome for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{1i}\)</span> = value of the first covariate for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{2i}\)</span> =  value of the second covariate for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{3i}\)</span> = <span class="math notranslate nohighlight">\(x_{1i} \times x_{2i}\)</span></p></li>
</ul>
<p>To understand why this model allows the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> to vary according to <span class="math notranslate nohighlight">\(X_2\)</span>, we can consider the form of the equation when we fix <span class="math notranslate nohighlight">\(X_2\)</span> to have a particular value, say <span class="math notranslate nohighlight">\(X_2=k\)</span>. In this situation the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> is as follows:</p>
<div class="math notranslate nohighlight">
\[
y_i = (\beta_0 + \beta_2k) + (\beta_1 + \beta_3k)x_{1i} + \epsilon_i.
\]</div>
<p>In other words, when <span class="math notranslate nohighlight">\(x_2=k\)</span> the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> is a linear one with both slope and intercept dependent upon <span class="math notranslate nohighlight">\(k\)</span>. The intercept is <span class="math notranslate nohighlight">\(\beta_0+\beta_2k\)</span> and the slope is <span class="math notranslate nohighlight">\(\beta_1 +  \beta_3k\)</span>.</p>
<p>By allowing the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> to vary according to <span class="math notranslate nohighlight">\(X_2\)</span>, we have also allowed the slope for the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> to vary according to <span class="math notranslate nohighlight">\(X_1\)</span>. If we look at the form of the model when <span class="math notranslate nohighlight">\(X_1\)</span> takes particular value, say <span class="math notranslate nohighlight">\(X_1=m\)</span>, we find:</p>
<div class="math notranslate nohighlight">
\[
y_i = (\beta_0+\beta_1m) + (\beta_2+\beta_3m)x_{2i} + \epsilon_i.
\]</div>
<p>Again, the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> is a linear one with both slope and intercept dependent upon <span class="math notranslate nohighlight">\(m\)</span>.</p>
</div>
<div class="section" id="interaction-between-a-continuous-variable-and-a-binary-variable">
<h4>13.7.2 Interaction between a continuous variable and a binary variable<a class="headerlink" href="#interaction-between-a-continuous-variable-and-a-binary-variable" title="Permalink to this headline">¶</a></h4>
<p>The interaction model is particularly easy to interpret when one of the covariates (say <span class="math notranslate nohighlight">\(X_2\)</span>) is a binary, taking the values 0 and 1 (i.e. a dummy variable). The linear regression model then becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
&amp;y_i = \beta_0 + \beta_1x_{1i} + \epsilon_i \text{ when } x_2=0\\ 
&amp;y_i = (\beta_0 + \beta_2) + (\beta_1+\beta_3)x_{1i} + \epsilon_i \text{ when } x_2=1 
\end{align}
\end{split}\]</div>
<p>The interpretation of each of the parameters is as follows.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept when <span class="math notranslate nohighlight">\(X_2=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_2\)</span> is the intercept when <span class="math notranslate nohighlight">\(X_2=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span> is the difference in intercepts between the two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the slope when <span class="math notranslate nohighlight">\(X_2=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1+\beta_3\)</span> is the slope when <span class="math notranslate nohighlight">\(X_2=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_3\)</span> is the difference in slopes between the two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
</ul>
<p><em>Example.</em> To demonstrate the impact of adding an interaction term, we will consider two models: (1) relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>) to length of pregnancy (<span class="math notranslate nohighlight">\(X_1\)</span>) and mother’s smoking status (<span class="math notranslate nohighlight">\(X_2\)</span>) and (2) relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>) to length of pregnancy (<span class="math notranslate nohighlight">\(X_1\)</span>), mother’s smoking status (<span class="math notranslate nohighlight">\(X_2\)</span>) and their interaction (<span class="math notranslate nohighlight">\(X_3\)</span>). In these models, <span class="math notranslate nohighlight">\(X_2=1\)</span> indicates that the mother smokes and <span class="math notranslate nohighlight">\(X_2=0\)</span> indicates that the mother does not smoke.</p>
<p>We first consider the model with no interaction term. The code below defines the model in R, summarises the results and produces a scatter plot of birthweight against gestational days, with the fitted values superimposed. The blue points (and line) on the scatter plot are observations in the group of babies whose mothers do not smoke and the red points (and line) are observations in the group of babies whose mothers do smoke.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="c1">#Define a dummy variable for maternal.smoker</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="o">&lt;-</span><span class="m">0</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker</span><span class="o">==</span><span class="s">&quot;True&quot;</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">1</span>

<span class="c1">#Model without the interaction term</span>
<span class="n">no_int_model</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)</span>

<span class="c1">#Scatterplot</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
     <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;Birthweight&quot;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="n">no_int_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">1</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="n">no_int_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">),</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">coef</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)[</span><span class="m">3</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)[</span><span class="m">2</span><span class="p">],</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + factor(data$Maternal.Smoker2))

Residuals:
    Min      1Q  Median      3Q     Max 
-50.789 -11.035  -0.211  10.053  52.412 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    -3.18492    8.32945  -0.382    0.702    
data$Gestational.Days           0.45117    0.02968  15.200   &lt;2e-16 ***
factor(data$Maternal.Smoker2)1 -8.37440    0.97346  -8.603   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.25 on 1171 degrees of freedom
Multiple R-squared:  0.2157,	Adjusted R-squared:  0.2143 
F-statistic:   161 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
<img alt="_images/13.g. Linear Regression II_3_1.png" src="_images/13.g. Linear Regression II_3_1.png" />
</div>
</div>
<p>As can be seen from the above figure, the fitted values from the model with no interaction term form two straight lines with a common slope 0.45 ounces and intercepts -3.18 ounces for the non-smoking group and -3.18-8.37=-11.55 ounces for the smoking group. This type of model (no interactions) is sometimes known as a <strong>parallel lines</strong> regression model, because it restricts the lines to be parallel. It permits adjustment of the effect of one covariate for the effects of others, but forces the effects of a unit change in each covariate to be constant, whatever the level of the other covariate. This restriction is not appropriate if the slope effect of one covariate depends on the value of another covariate. Adding an interaction term removes this restriction.</p>
<p>Below, we fit the second model which includes an interaction term, and produce a second scatter plot with the fitted values from our new model superimposed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create the interaction term</span>
<span class="n">data</span><span class="o">$</span><span class="n">int1</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span>

<span class="c1">#Include the interaction term in our model</span>
<span class="n">int_model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">)</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">int1</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)</span>

<span class="c1">#Scatter plot</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
     <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;Birthweight&quot;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="n">int_model1</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">1</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="n">int_model1</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">),</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">3</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span> <span class="o">+</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">4</span><span class="p">],</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + factor(data$Maternal.Smoker2) + 
    data$int1)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.023 -11.078  -0.084   9.995  50.499 

Coefficients:
                                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                     19.63964   10.29098   1.908 0.056580 .  
data$Gestational.Days            0.36962    0.03671  10.069  &lt; 2e-16 ***
factor(data$Maternal.Smoker2)1 -72.68713   17.23243  -4.218 2.65e-05 ***
data$int1                        0.23085    0.06176   3.738 0.000194 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.16 on 1170 degrees of freedom
Multiple R-squared:  0.2249,	Adjusted R-squared:  0.2229 
F-statistic: 113.2 on 3 and 1170 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
<img alt="_images/13.g. Linear Regression II_5_1.png" src="_images/13.g. Linear Regression II_5_1.png" />
</div>
</div>
<p>In our new model, the intercept and slope among the non-smoking group are 19.64 ounces and 0.37 ounces respectively. The intercept and slope among the smoking group are 19.64-72.69=-53.05 ounces and  0.37+0.23=0.60 ounces respectively. The interaction term has <span class="math notranslate nohighlight">\(p\)</span>=0.0001, so there is evidence that the slopes are different.</p>
</div>
</div>
</div>
</div>
<span id="document-14.a. Linear Regression III"></span><div class="section" id="linear-regression-iii">
<h2>14. Linear Regression III<a class="headerlink" href="#linear-regression-iii" title="Permalink to this headline">¶</a></h2>
<p>This is the final session on Linear Regression. In this session we focus on the assumptions underlying this model.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>explain the assumptions underlying multivariable linear regression</p></li>
<li><p>apply a range of graphical techniques to investigate the assumptions of the linear regression model;</p></li>
</ul>
</div><p><strong>Acknowledgements:</strong> Thank you to Jennifer Nicholas, Chris Frost and Ruth Keogh whose notes on linear regression and generalised linear models were particularly useful in the development of the current session.</p>
<div class="toctree-wrapper compound">
<span id="document-14.b. Linear Regression III"></span><div class="section" id="assumptions">
<h3>14.1 Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h3>
<p>The linear regression model makes a number of assumptions. All inferences made from a model are contingent on these assumptions being correct. It is therefore important that we have statistical techniques (or <strong>diagnostic tools</strong>) to investigate these assumptions.</p>
<p>In practice, it is rare for all the assumptions of a statistical procedure to hold exactly. We may have evidence in the data, or prior knowledge about the data, that lead us to believe that the assumptions made by the model do not hold. This does not necessarily mean that the results from the model should be disregarded, since statistical procedures are <strong>robust</strong> to departures from assumptions in many settings. When conducting statistical analyses, it is a good idea to first try to establish to what extent assumptions hold and then consider whether the methods used can be adapted to improve the extent to which assumptions hold. If adaptations cannot be made, it is necessary to consider to what extent the results of an analysis can be trusted.</p>
<p>In this section we largely focus on diagnostic tools that can be used to identify assumption violations. Some pointers are given to possible adaptations and alternative techniques that can be used when assumptions are violated, however issues of robustness are not considered in great detail. It is worth noting that, broadly speaking, the central limit theorem implies that departures from assumptions are less important for large datasets than for small ones, and so assumption violations are less of a concern when working with big data.</p>
</div>
<div class="section" id="assumptions-of-the-linear-regression-model">
<h3>14.1.1 Assumptions of the linear regression model<a class="headerlink" href="#assumptions-of-the-linear-regression-model" title="Permalink to this headline">¶</a></h3>
<p>The assumptions made by the linear regression model are as follows:</p>
<ol class="simple">
<li><p><strong>Linearity:</strong> There is a linear relationship between the dependent variable <span class="math notranslate nohighlight">\(Y\)</span> and each of the independent variables. Here we are contrasting a linear relationship with a non-linear relationship, not with no relationship. A model in which one of the regression coefficients is zero can satisfy the assumptions of linear regression.</p></li>
<li><p><strong>Normality:</strong> The error terms follow a normal distribution.</p></li>
<li><p><strong>Homoscedasticity:</strong> The error variance is constant i.e. the scatter of points around the true regression line has the same variance, irrespective of the value of <span class="math notranslate nohighlight">\(x_i\)</span>. The converse of this feature is termed <strong>heteroscedasticity</strong>.</p></li>
<li><p><strong>Independence:</strong> The observations of <span class="math notranslate nohighlight">\(y_i\)</span> are independent.</p></li>
</ol>
<p>In this session we will focus on the first three assumptions. Violations of the independence assumption are often more apparant from the context of a study than from the data itself. For example, if we carry out a study in which the blood pressure of 100 people are each measured twice, and then treat the 200 measurements as independent in the statistical analysis it is clear that the assumption of independence is violated.</p>
<p>Notice that the normality and homoscedasticity assumptions concern the error terms, which can be thought of as the <em>true</em> residuals, defined in terms of deviations from the model defined by population parameters. Since these errors or true residuals can never be observed in practice, we have to use the observed residuals (obtained by replacing the population parameters with their estimates). In fact, observed residuals are neither independent nor do they have constant variance, but in most settings the departures from independence and homoscedasticity are very small. Consequently, we can proceed as if the observed residuals were the true residuals when investigating assumptions.</p>
</div>
<span id="document-14.c. Linear Regression III"></span><div class="section" id="investigating-assumptions-using-plots">
<h3>14.2 Investigating assumptions using plots<a class="headerlink" href="#investigating-assumptions-using-plots" title="Permalink to this headline">¶</a></h3>
<p>It is a good idea to explore your data using a number of simple plots. Here we will introduce the most useful plots for both simple and multivariable linear regression models.</p>
<div class="section" id="scatter-plots-of-the-outcome-against-independent-variables">
<h4>14.2.1 Scatter plots of the outcome against independent variables<a class="headerlink" href="#scatter-plots-of-the-outcome-against-independent-variables" title="Permalink to this headline">¶</a></h4>
<p>For simple linear regression models, a scatter plot of the outcome against the independent variable can usually make serious violations of assumptions apparent. Such plots are particularly good for identfiying non-linearity, heteroscedasticity and <strong>outliers</strong> (points which lie atypically far from the regression line).</p>
<p>Let our outcome and independent variable be denoted by <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, respectively. The figure below depicts four different scenarios where various assumptions are violated. In Scenario A, there is a slight curvature in the scatter of points between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, suggesting a non-linear relationship which violates the linearity assumption. In Scenario B, the variance of <span class="math notranslate nohighlight">\(Y\)</span> is larger for larger values of <span class="math notranslate nohighlight">\(X\)</span>, violating the homoscedasticity assumption. In Scenario C, the linearity and homoscedasticity assumptions appear to hold, but there is a possible outlier (circled in red). Scenario D depicts an ideal situation for simple linear regression, where there appears to be no violations.</p>
<div class="figure align-default" id="simple-plots">
<a class="reference internal image-reference" href="_images/simple_plots.png"><img alt="_images/simple_plots.png" src="_images/simple_plots.png" style="height: 600px;" /></a>
</div>
<p>For multivariable linear regression, the linearity assumption requires that the relationship between the outcome and each independent variable is linear <em>conditional on the other covariates in the model</em>. So, there is no requirement that the relationship between the outcome and each individual covariate is linear when other covariates are ignored. This means that assessment of the fit of a multivariable linear regression cannot be inferred from a series of scatter plots relating the outcome to each covariate. Such plots can be useful for detecting points with extreme values, but the residual plots considered next are more useful for multivariable models.</p>
</div>
<div class="section" id="plots-of-residuals-against-fitted-values-or-covariates">
<h4>14.2.2  Plots of residuals against fitted values or covariates<a class="headerlink" href="#plots-of-residuals-against-fitted-values-or-covariates" title="Permalink to this headline">¶</a></h4>
<p>Plots of the observed residuals against the fitted values are useful for investigating the assumptions of linearity and homoscedasticity. For linearity: if a non-linear relationship is present, then the residuals will not be equally distributed above and below zero across the range of fitted values. For homoscedasticity: if there is heterogeneity in the residuals, the variance of residuals will not be constant across the range of fitted values.</p>
<p>The figure below uses the same data from Scenarios A-D above, but displays the observed residuals against fitted values. We can see that linearity is violated in Scenario A, since the scatter points are not equally distributed above and below the line at <span class="math notranslate nohighlight">\(\epsilon=0\)</span>. Furthermore, in Scenario B we can see that the variance of residuals increase with increasing <span class="math notranslate nohighlight">\(\hat{y}\)</span>, indicating a violation of homoscedasticity.</p>
<div class="figure align-default" id="residual-plots">
<a class="reference internal image-reference" href="_images/residual_plots.png"><img alt="_images/residual_plots.png" src="_images/residual_plots.png" style="height: 600px;" /></a>
</div>
<p>It can also be useful to plot residuals against each covariate, as a futher check for a linear relationship between <span class="math notranslate nohighlight">\(Y\)</span> and each of the independent variables (conditional on the other covariates in the model). If there are only a small number of covariates in the model, then these plots can be done for all variables. However, if the model is very complex, it may be judged sufficient to only plot residuals against fitted values and residuals against the most important covariates.</p>
</div>
<div class="section" id="normal-plots-of-residuals">
<h4>14.2.3  Normal plots of residuals<a class="headerlink" href="#normal-plots-of-residuals" title="Permalink to this headline">¶</a></h4>
<p>Normal plots (such as the <strong>Q-Q plot</strong>) provide the best means of visually detecting departures from normality. The normal Q-Q plot plots observed values against a standard normal distribution with the same number of points. If the data are perfectly normally distributed, the points on a Q-Q plot would lie on the line <span class="math notranslate nohighlight">\(Y=X\)</span>. Deviations from this line indicate deviations from normality. Q-Q plots of residuals can be used to investigate the normality assumption.</p>
<p>As previously mentioned, the observed residuals do not have constant variance even when true residuals do. Therefore, some authors suggest using <strong>standardised</strong> residuals in the normal plots (since standardised residuals do have constant variance). On the other hand, some prefer to work with the observed residuals since these have the same units as the outcome. In practice, the differences between the two approaches are minor.</p>
<p>The figure below depicts normal Q-Q plots for the standardised residuals in Scenarios A-D. In such plots we might expect to see some deviation from the straight line in the extreme values of the residuals and so the variation in the tails are not of great concern. In Scenario A however, there is deviation away from the line <span class="math notranslate nohighlight">\(Y=X\)</span> towards the middle, indicating a violation of the normality assumption. Furthermore, the outlier in Scenario C may need further investigation (we discuss outliers further in a subsequent section).</p>
<div class="figure align-default" id="normal-residual-plots">
<a class="reference internal image-reference" href="_images/qqplots.png"><img alt="_images/qqplots.png" src="_images/qqplots.png" style="height: 600px;" /></a>
</div>
</div>
<div class="section" id="plots-based-on-cook-s-distance">
<h4>14.2.4  Plots based on Cook’s distance<a class="headerlink" href="#plots-based-on-cook-s-distance" title="Permalink to this headline">¶</a></h4>
<p>Cook’s distance is a measure of the <strong>influence</strong> of an observation. An influential observation is one that has a large impact on the model parameter estimates. It is worth checking the influence of observations, particularly potential outliers, to see if they are having a much larger impact on model fit than we would expect.</p>
<p>For a model with <span class="math notranslate nohighlight">\(p\)</span> parameters (with estimated residual variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>), the Cook’s distance for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation (<span class="math notranslate nohighlight">\(D_i\)</span>) is obtained by refitting the model excluding this observation and obtaining new fitted values (<span class="math notranslate nohighlight">\(\hat{y_{j(i)}}\)</span>) for all <span class="math notranslate nohighlight">\(n\)</span> observations (including the omitted one). <span class="math notranslate nohighlight">\(D_i\)</span> is then defined as:</p>
<div class="math notranslate nohighlight">
\[D_i = \frac{\sum_{i=1}^n(\hat{y}_{j(i)}-\hat{y}_i)^2}{(p+1)\hat{\sigma}^2}\]</div>
<p>The higher the value of <span class="math notranslate nohighlight">\(D_i\)</span>, the more influential the observation.</p>
<p>It can be informative to display Cook’s distances graphically. The figure below plots the Cook’s distances for each observation in Scenarios A-D.</p>
<div class="figure align-default" id="cooks-distance">
<a class="reference internal image-reference" href="_images/cooks_distance.png"><img alt="_images/cooks_distance.png" src="_images/cooks_distance.png" style="height: 600px;" /></a>
</div>
<p>In Scenario C the outlier identified in the previous plots and potentially problematic has a much higher Cook’s distance than the other observations, indicating that it is highly influential and worth further investigation.</p>
</div>
</div>
<span id="document-14.d. Linear Regression III"></span><div class="section" id="statistical-tests-of-assumptions">
<h3>14.2 Statistical tests of assumptions<a class="headerlink" href="#statistical-tests-of-assumptions" title="Permalink to this headline">¶</a></h3>
<p>It might be anticipated that the assumptions of the linear regression model can be investigated using formal hypothesis tests. Indeed there exist a number of statistical tests for normalilty including the Kolmorogorov-Smirnov test and the Shapiro-Wilk test. Further, there exist statistical tests for heteroscedasticity of rediduals.</p>
<p>However, these tests suffer from the drawback that they tend to only have statistical power to detect model violations when datasets are large and when datasets are large the central limit theorem means that the consequences of these violations of are less important than in small datasets. With large datasets, tests of normality and heteroscedasticity can often be statistically significant, but the impact of these violations may be practically unimportant. For these reasons, the tests are considered by many statisticians to be of limited practical use and so details of these procedures will not be given here.</p>
<div class="section" id="examples-using-the-birthweight-data">
<h4>14.2.1 Examples using the birthweight data<a class="headerlink" href="#examples-using-the-birthweight-data" title="Permalink to this headline">¶</a></h4>
<p>We will use some of the graphical tools discussed above to assess the validity of assumptions in the multivariable model defined in the previous session (Model 4). Recall Model 4 was defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{Model 4: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i + \epsilon_i 
\]</div>
<p>The outcome <span class="math notranslate nohighlight">\(y_i\)</span> denotes the birthweight (in oz) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby. The covariates <span class="math notranslate nohighlight">\(l_i\)</span> and <span class="math notranslate nohighlight">\(h_i\)</span> denote the length of pregnancy (i.e. number of gestational days), and the height of the mother (in inches) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby, respectively.</p>
<p>The code below fits Model 4 to the birthweight data, and then produces (1) a plot of residuals against fitted values (2) a Q-Q plot of the standardised residuals and (3) a plot of Cook’s distances by observation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Load the data</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>

<span class="c1">#Fit Model 3 to the data</span>
<span class="n">model4</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">Maternal.Height</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="c1">#Set the graphical space so that two plots are shown side-by-side in one row</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="c1">#Plot the residuals against the fitted values</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">model4</span><span class="o">$</span><span class="n">fitted.values</span><span class="p">,</span> <span class="n">model4</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">main</span> <span class="o">=</span> <span class="s">&quot;Residual plot&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Fitted values&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Residuals&quot;</span><span class="p">,</span> <span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">0</span><span class="p">)</span>

<span class="c1">#Obtain the standardised residuals</span>
<span class="n">Standardised.Residuals</span><span class="o">&lt;-</span><span class="nf">rstandard</span><span class="p">(</span><span class="n">model4</span><span class="p">)</span>

<span class="c1">#Normal Q-Q plot of the standardised residuals </span>
<span class="nf">qqnorm</span><span class="p">(</span><span class="n">Standardised.Residuals</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Normal Q-Q plot&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Expected normal value&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Standardised Residuals&quot;</span><span class="p">)</span>
<span class="nf">qqline</span><span class="p">(</span><span class="n">Standardised.Residuals</span><span class="p">)</span>

<span class="c1">#Plot of Cook&#39;s distance</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">model4</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/14.d. Linear Regression III_2_0.png" src="_images/14.d. Linear Regression III_2_0.png" />
</div>
</div>
<p>We make the following observations:</p>
<ol class="simple">
<li><p><strong>Linearity:</strong> The residuals are equally distributed above and below zero in the “Residual plot”</p></li>
<li><p><strong>Normality:</strong> There do not appear to be any serious departures from normality, based on the “Normal Q-Q plot”</p></li>
<li><p><strong>Homoscedasticity:</strong> The variance of residuals are constant across the fitted values (based on the “Residual plot”)</p></li>
</ol>
<p>However, the Cook’s distance plot reveals that observation 239 is highly influential, compared to the remaining observations. Observations 405 and 820 also have a relatively high Cook’s distance. Sensitivity analyses may be required to assess model fit with and without these observations (this is discussed in Section 3.5.3).</p>
<p>Finally, we can assume that the independence assumption holds, since the birthweight of a baby from one mother is not expected to be associated with the birthweight of a baby from a different mother. Therefore, we can reasonably conclude that all the assumptions are met in this model (but there are some potentially problematic observations in terms of influence).</p>
<p>Next, we briefly introduce some of the statisical solutions available for when assumptions are not met.</p>
</div>
</div>
<span id="document-14.e. Linear Regression III"></span><div class="section" id="dealing-with-violations-of-assumptions">
<h3>14.3 Dealing with violations of assumptions<a class="headerlink" href="#dealing-with-violations-of-assumptions" title="Permalink to this headline">¶</a></h3>
<p>So far, we have discussed diagnostic tools that are useful for identifying possible violations of the assumptions of a linear model. Identification of potential violations of concern is only the first, and arguably the easiest, aspect of an exploration of the robustness of the results of fitting a model. Here, we briefly describe some approaches that can be used to deal with violations. When these approaches do not work, then more complex methods (beyond the scope of this lesson) may be needed to analyse the data.</p>
<div class="section" id="checking-the-data">
<h4>14.3.1 Checking the data<a class="headerlink" href="#checking-the-data" title="Permalink to this headline">¶</a></h4>
<p>Clearly, it is important that errors in data are eliminated as far as possible. In practice, ensuring that a large dataset is 100% error free may be impossible. Observations with large standardised residuals can potentially arise through data entry or coding errors and so a useful first step is to check such values with the data provider or original source of data, if available.</p>
</div>
<div class="section" id="transformations">
<h4>14.3.2 Transformations<a class="headerlink" href="#transformations" title="Permalink to this headline">¶</a></h4>
<p>Sometimes it can be useful to transform either the outcome variable and/or one or more of the covariates. The transformed variables are then used in the analysis in replacement of the original variables. There are a number of possible motivations for this:</p>
<ol class="simple">
<li><p>Transformations can be used to convert a non-linear relationship into a linear one. For example:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
y_i = \alpha(x_i)^{\beta} ⇒log(y_i) = log (\alpha)+\beta log (x_i).
\]</div>
<ol class="simple">
<li><p>Transformations can be used to improve the normality of residuals. For example, the Box-Cox transformation is a power transformation for this purpose.</p></li>
<li><p>Transformations can help stabilise the variance of residuals. For example, if <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is proportional to <span class="math notranslate nohighlight">\([E(Y)]^2\)</span> then <span class="math notranslate nohighlight">\(y^*=log(y)\)</span> is a useful variance-stabilising transformation. Alternatively, if <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is proportional to <span class="math notranslate nohighlight">\([E(Y)]^3\)</span> then <span class="math notranslate nohighlight">\(y^*=1/\sqrt{y}\)</span> can be used.</p></li>
</ol>
</div>
<div class="section" id="sensitivity-analyses">
<h4>14.3.3 Sensitivity analyses<a class="headerlink" href="#sensitivity-analyses" title="Permalink to this headline">¶</a></h4>
<p>If we observe potentially problematic outliers, sensitivity analyses can be used to assess how problematic they are. This involves repeating the analysis after omitting the outlier (or group of outliers) and considering the extent to which the results are altered.</p>
<p>However, even if the outlier affects the results (and/or assumptions) it is not a good idea to simply drop the data point. If it is not a data error, then it is a legitimate observation that should be included and understanding the reasons why it is an outlier could be important. In most cases, it is preferable to report the results including all data points, but discuss the impact removing the outlier had on the results.</p>
</div>
</div>
<span id="document-14.f. Linear Regression III"></span><div class="section" id="collinearity">
<h3>14.5 Collinearity<a class="headerlink" href="#collinearity" title="Permalink to this headline">¶</a></h3>
<p>A potential issue that can arise from including higher-order terms or interaction terms in a linear regression model is collinearity. Collinearity occurs when there is correlation between one or more of the independent covariates. If the degree of correlation between covariates is high enough, it can cause the following problems:</p>
<ol class="simple">
<li><p>Estimated coefficients can swing in either direction: known important variables may have surprisingly small coefficient estimates and known less important variables may have surprisingly large coefficient estimates.</p></li>
<li><p>Increased variance of estimated coefficients, therefore reducing the statistical power of the model.</p></li>
</ol>
<p>Including higher-order terms or interaction terms can result in collinearity due to the inevitable correlation between variables, their powers and the interaction terms involving them. Having said that, collinearity is only a concern in particular situations.</p>
<p>Collinearity only affects the specific independent variables that are correlated. Therefore, if the aim of the analysis is to estimate the way <span class="math notranslate nohighlight">\(X\)</span> influences <span class="math notranslate nohighlight">\(Y\)</span> after adjusting for <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, and there is only correlation between <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, then collinearity is not a concern. Moreover, collinearity rarely affects the predicted outcomes, so if the aim of the analysis is to predict <span class="math notranslate nohighlight">\(Y\)</span> using data from <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, then collinearity between any of the covariates is not a concern. Finally, the severity of the problems caused by collinearity increases with the degree of correlation. Therefore, if only moderate or weak correlation is present, then collinearity is not a concern.</p>
<p>If we are in a situation where collinearity is causing problems, then we could either remove some of the highly correlated variables, or transform one of them. Examples of transformations include:</p>
<ol class="simple">
<li><p>Instead of using systolic and diastolic blood pressure as collinear predictor variables, use diastolic blood pressure and (systolic-diastolic blood pressure).</p></li>
<li><p>Instead of using height and weight as predictor variables, use height and body mass index (weight/height<span class="math notranslate nohighlight">\(^2\)</span>).</p></li>
<li><p>When fitting a quadratic regression model, use <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\((X-\bar{X})^2\)</span>, rather than <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X^2\)</span> as covariates.</p></li>
</ol>
</div>
<span id="document-14.g. Linear Regression III"></span><div class="section" id="optional-reading-analysis-of-variance">
<h3>14.6 Optional Reading: Analysis of Variance<a class="headerlink" href="#optional-reading-analysis-of-variance" title="Permalink to this headline">¶</a></h3>
<div class="section" id="partitioning-variance">
<h4>14.6.1 Partitioning variance<a class="headerlink" href="#partitioning-variance" title="Permalink to this headline">¶</a></h4>
<p>The total variation in <span class="math notranslate nohighlight">\(Y\)</span> is equal to:</p>
<div class="math notranslate nohighlight">
\[ 
SS_{TOT} = \sum_{i=1}^n (Y_i-\bar{Y})^2
\]</div>
<p>This is often referred to as the sum of squares of the <span class="math notranslate nohighlight">\(Y\)</span>’s. This represents all of the variation in <span class="math notranslate nohighlight">\(Y\)</span> about its overall mean value. We can think about two components of this total variation:</p>
<ul class="simple">
<li><p>the predictable variation in <span class="math notranslate nohighlight">\(Y\)</span> (predicted by the variables included in the model) and</p></li>
<li><p>the unpredictable variation in <span class="math notranslate nohighlight">\(Y\)</span> (the remaining “noise”).</p></li>
</ul>
<p>The predictable variation represents the variation of the predicted values <span class="math notranslate nohighlight">\(\hat{Y}\)</span> about the mean. We can measure this as:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
SS_{REG}= \sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 
\end{align}
\]</div>
<p>The unpredictable variation represents the variation of the observed values about their predicted values:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
SS_{RES} =  \sum_{i=1}^n(Y_i-\hat{Y_i})^2
\end{align}
\]</div>
<p>Note that in the equations above, SS stands for sums of squares, REG for regression and RES for residual.</p>
<p>A key result for ANOVA is that the total variation in <span class="math notranslate nohighlight">\(Y\)</span> can be partitioned into the predictable variation, explained by the regression model, and the unpredictable (residual) variation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
SS_{TOT} &amp;= SS_{REG}+SS_{RES} \\
\rightarrow \sum_{i=1}^n (Y_i-\bar{Y})^2 &amp;= \sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 + \sum_{i=1}^n(Y_i-\hat{Y_i})^2
\end{align}
\end{split}\]</div>
<div class="section" id="the-coefficient-of-determination">
<h5>14.6.1.1 The coefficient of determination<a class="headerlink" href="#the-coefficient-of-determination" title="Permalink to this headline">¶</a></h5>
<p>Using the sums of squares defined above, we can calculate the proportion of variance explained by the statistical model, known as the <strong>coefficient of determination</strong>.</p>
<p>The proportion of variation which is explained by a statistical model is denoted by <span class="math notranslate nohighlight">\(R^2\)</span> and is given by:</p>
<div class="math notranslate nohighlight">
\[
R^2 = \frac{SS_{REG}}{SS_{TOT}}.
\]</div>
</div>
<div class="section" id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h5>
<p>The coefficient of determination is given in the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> output for a linear regression in R. In Model 1, <span class="math notranslate nohighlight">\(R^2=0.1661\)</span> (see output below). This means that Model 1 explains 16.6% of the total variation in <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#The coefficient of determination</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
<span class="n">model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>While <span class="math notranslate nohighlight">\(R^2\)</span> is sometimes used an overall measure of goodness-of-fit (or predictive performance), it isn’t used to formally compare models. This is because <span class="math notranslate nohighlight">\(R^2\)</span> will never decrease when new covariates are added to a model (provided that the number and identity of observations remains the same). Therefore, using <span class="math notranslate nohighlight">\(R^2\)</span> for model comparisons, we would always conclude that the more complex model is at least as good a fit as the simpler model, even if this is not true.</p>
<p>An adjusted <span class="math notranslate nohighlight">\(R^2\)</span> has been proposed to account for this issue. Above, for example, the <span class="math notranslate nohighlight">\(R^2\)</span> is 0.166 but the adjusted R-squared is a little smaller <span class="math notranslate nohighlight">\(R^2 = 0.165\)</span>. However, we would not recommend using the <span class="math notranslate nohighlight">\(R^2\)</span> for formal model comparison.</p>
</div>
</div>
<div class="section" id="comparing-models">
<h4>14.6.2 Comparing models<a class="headerlink" href="#comparing-models" title="Permalink to this headline">¶</a></h4>
<p>When fitting statistical models, we may wish to compare how well two models fit the data, to see which is most appropriate. Consider the following three models:</p>
<ul class="simple">
<li><p>Model 1 (birthweight~length of pregnancy)</p></li>
<li><p>Model 2 (birthweight~mother’s smoking status)</p></li>
<li><p>Model 4 (birthweight~length of pregnancy+mothers height)</p></li>
</ul>
<p>We might want to make the following comparisons between models:</p>
<ul class="simple">
<li><p>Comparison 1: Model 1 vs Model 4</p></li>
<li><p>Comparison 2: Model 2 vs Model 4</p></li>
</ul>
<p>In these examples, Comparison 1 is much simpler than Comparison 2, because the models in Comparison 1 are <strong>nested</strong>.</p>
<p>Statistical models are said to be <strong>nested</strong> when one model (the simpler model) contains a subset of the covariates in the other one (the complex model) and no other additional variables. In Comparison 2, the models are not nested because the simpler model (Model 2) contains mother’s smoking status as a variable, which is not included in Model 4.</p>
<p>Nested models can be compared using <strong>Analysis of Variance (ANOVA)</strong> (the comparison of non-nested models is much more complicated and is beyond the scope of this module).</p>
<p>The main idea of ANOVA is that: if the complex model better describes the data than the simpler model, then we would expect a reasonably large amount of the residual variation that is unexplained by the simpler model to be explained by the complex one. ANOVA provides a statistical framework that can formally test this.</p>
<p>We will first consider ANOVA in the context of simple linear regression, where the simpler model assumes no association between the outcome and the independent variable (the <strong>null</strong> model). We will then consider ANOVA in the context of multivariable linear regression and we end by learning how ANOVA can be used to test for differences between groups in a categorical variable.</p>
</div>
<div class="section" id="the-anova-table">
<h4>14.6.3 The ANOVA table<a class="headerlink" href="#the-anova-table" title="Permalink to this headline">¶</a></h4>
<p><strong>Sums of squares (SS):</strong> The first step is to partition the total variation into the regression (predictable) and residual (unpredictable) components. Variation is measured by sums of squares (SS). So we partition the total sum of squares (<span class="math notranslate nohighlight">\(SS_{TOT}\)</span>) into  (<span class="math notranslate nohighlight">\(SS_{REG}\)</span> and <span class="math notranslate nohighlight">\(SS_{RES}\)</span>)</p>
<p><strong>Degrees of freedom:</strong> Each of these sum of squares have an associated degrees of freedom (d.f.). The d.f. for the total sum of squares is <span class="math notranslate nohighlight">\((n-1)\)</span>, since the variance of <span class="math notranslate nohighlight">\(Y\)</span> is <span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_i-\bar{Y})^2/(n-1)\)</span>. The d.f. for the regression sum of squares in the number of covariates in the regression model (when a simple linear regression model is used this is equal to 1). The residual d.f. is found by subtracting the regression d.f. from the total d.f.</p>
<p><strong>Mean squares (MS)</strong> The sums of squares also have associated mean squares, which are obtained by dividing each sum of squares by its associated degrees of freedom (note that the residual mean square is then equal to <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>).</p>
<p>These statistics are typically summarised in an ANOVA table. The table for simple linear regression is shown below.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Regression</p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG}=\frac{SS_{REG}}{1}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Residual</p></td>
<td><p>n-2</p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{RES}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{RES}=\frac{SS_{RES}}{n-2}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p>n-1</p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Notes</p>
<blockquote>
<div><p>Very loosely speaking, degrees of freedom are “bits of information”. We start with <span class="math notranslate nohighlight">\(n\)</span> bits of information. Every time we estimate something we “use” a bit of information (and so lose a degree of freedom. Therefore, when we calculate the overall variation in <span class="math notranslate nohighlight">\(Y\)</span>, we lose one of the <span class="math notranslate nohighlight">\(n\)</span> bits of information because we need to calculate the overall mean to obtain the sum of squares of <span class="math notranslate nohighlight">\(Y\)</span>. Therefore, we have <span class="math notranslate nohighlight">\(n-1\)</span> bits of information overall.  In a simple linear regression model, we estimate two parameters, so we’re using two bits of information, but one of these is essentially the same bit we lost from calculating the overall mean, so we say that the regression model is using 1 degree of freedom. We started with n-1 df and the regression model used 1 of them so there are n-2 left for the remaining component, the residual SS.</p>
</div></blockquote>
<div class="section" id="hypothesis-testing-using-anova">
<h5>14.6.3.1 Hypothesis testing using ANOVA<a class="headerlink" href="#hypothesis-testing-using-anova" title="Permalink to this headline">¶</a></h5>
<p>The values in the ANOVA table can be used to conduct formal hypothesis tests.</p>
<p>ANOVA is used to test the null hypothesis that the simpler of the two nested models better fits the data. In simple linear regression, the simpler model is the null model, in which case:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> The null model is a better fit</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1:\)</span> The simple linear regression model is a better fit</p></li>
</ul>
<p>To test the null hypothesis defined above, we use an <span class="math notranslate nohighlight">\(F\)</span> statistic, defined as:</p>
<div class="math notranslate nohighlight">
\[
F = \frac{MS_{REG}}{MS_{RES}}
\]</div>
<p>This ratio measures how much more variation in <span class="math notranslate nohighlight">\(Y\)</span> is explained by the model than would be expected by chance. If the model does not fit the data well, then we would expect this ratio to be equal to 1. The larger the value of <span class="math notranslate nohighlight">\(F\)</span>, the stronger the evidence that the complex model is a better fit. To obtain a <span class="math notranslate nohighlight">\(p\)</span>-value for a formal hypothesis test, <span class="math notranslate nohighlight">\(F\)</span> can be compared to the <span class="math notranslate nohighlight">\(F_{1,(n-2)}\)</span> distribution (where 1 and (n-2) are the relevant degrees of freedom for the mean squares).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># F-test using anova()</span>
<span class="nf">anova</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A anova: 2 × 5</caption>
<thead>
	<tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>Gestational.Days</th><td>   1</td><td> 65449.51</td><td>65449.5131</td><td>233.4293</td><td>3.395226e-48</td></tr>
	<tr><th scope=row>Residuals</th><td>1172</td><td>328608.34</td><td>  280.3825</td><td>      NA</td><td>          NA</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-statistic is equal to 233.4 with <span class="math notranslate nohighlight">\(p\)</span>-value <span class="math notranslate nohighlight">\(&lt;2.2\times10^{-16}\)</span>. With such a small <span class="math notranslate nohighlight">\(p\)</span>-value there is strong evidence against the null hypothesis. Therefore we conclude that the model which includes gesational days is a better fit.</p>
</div>
<div class="section" id="connection-between-f-tests-and-t-tests-in-simple-linear-regression">
<h5>14.6.3.2 Connection between F tests and t-tests in simple linear regression<a class="headerlink" href="#connection-between-f-tests-and-t-tests-in-simple-linear-regression" title="Permalink to this headline">¶</a></h5>
<p>Above, we used a <span class="math notranslate nohighlight">\(F\)</span>-test to compare the model for birthweight (Y) including gestational days (L):</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0  + \beta_1 l_i + \epsilon_i
\]</div>
<p>with a model including just a constant</p>
<div class="math notranslate nohighlight">
\[
y_i = \alpha_0  + \epsilon_i
\]</div>
<p>In other words, we have just used a <span class="math notranslate nohighlight">\(F\)</span>-test to test the null hypothesis <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span>. This is exactly the same hypothesis test we tested previously using a <span class="math notranslate nohighlight">\(t\)</span>-test.</p>
<p>In other words, the <span class="math notranslate nohighlight">\(F\)</span>-test for a simple linear regression model is the same as the <span class="math notranslate nohighlight">\(t\)</span>-test of the null hypothesis that the slope parameter is equal to 0. Below, we perform the <span class="math notranslate nohighlight">\(t\)</span>-test again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># F-test</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>We see that the t-statistic for the slope is <span class="math notranslate nohighlight">\(t=15.28\)</span>, with p-value <span class="math notranslate nohighlight">\(p&lt;2e-16\)</span>. Previously, we had <span class="math notranslate nohighlight">\(F=233.4=15.24^2=t^2\)</span>. The p-value for the <span class="math notranslate nohighlight">\(F\)</span>-test was identical to the <span class="math notranslate nohighlight">\(t\)</span>-test.</p>
<p>The two tests are equivalent, with <span class="math notranslate nohighlight">\(F=t^2\)</span>, providing identical p-values. Consequently, it is not particularly common to use <span class="math notranslate nohighlight">\(F\)</span>-tests in the simple linear regression model, they are more useful for assessing more complex models with multiple covariates.</p>
</div>
</div>
<div class="section" id="anova-for-multivariable-linear-regression">
<h4>14.6.4 ANOVA for multivariable linear regression<a class="headerlink" href="#anova-for-multivariable-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>In the context of multivariable linear regression, ANOVA can be used to test whether a more complex model is a better fit than the null model (<strong>the Global F test</strong>), or whether a more complex model is a better fit than a simpler model that includes a subset of the covariates in the complex model (<strong>the partial F test</strong>). Each test requires slight modifications to the ANOVA table defined above and we will discuss these in turn.</p>
<div class="section" id="the-global-f-test">
<h5>14.6.4.1 The Global F test<a class="headerlink" href="#the-global-f-test" title="Permalink to this headline">¶</a></h5>
<p>The general formulation of the ANOVA table (suitable for simple and multivariable linear regression models) is given below. <span class="math notranslate nohighlight">\(p\)</span> is the number of covariates in the model.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Regression</p></td>
<td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG}=\frac{SS_{REG}}{p}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Residual</p></td>
<td><p><span class="math notranslate nohighlight">\(n-(p+1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{RES}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{RES}=\frac{SS_{RES}}{n-p-1}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\(n-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Note that this is equivalent to the previous table (for simple linear regression) when <span class="math notranslate nohighlight">\(p=1\)</span>.</p>
<p>The Global F test tests the null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) that the null model is a better fit than the more complex model against the alternative hypothesis (<span class="math notranslate nohighlight">\(H_1\)</span>) that the complex model is a better fit. Or, equivalently:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> All slope parameters in the complex model are equal to 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1:\)</span> At least one of the slope parameters in the complex model is not equal to 0.</p></li>
</ul>
<p>The appropriate <span class="math notranslate nohighlight">\(F\)</span> statistic is the ratio</p>
<div class="math notranslate nohighlight">
\[
F = \frac{MS_{REG}}{MS_{RES}}
\]</div>
<p>Under the null hypothesis, <span class="math notranslate nohighlight">\(F\)</span> follows an <span class="math notranslate nohighlight">\(F_{p,(n-(p+1))}\)</span> distribution.</p>
</div>
<div class="section" id="id1">
<h5>Example<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>We can use <code class="docutils literal notranslate"><span class="pre">summary()</span></code> to conduct a global <span class="math notranslate nohighlight">\(F\)</span>-test for Model 4, our model relating birthweight to both length of pregnancy and maternal height.</p>
<p>The null and alternative hypotheses, for the global <span class="math notranslate nohighlight">\(F\)</span>-test, are defined as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0\)</span>: the regression coefficients for both gestational days and mother’s height are equal to 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1\)</span>: the regression coefficient for either gestational days or mother’s height (or both) is not equal to 0.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># ANOVA for Model 4 </span>
<span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days.Centered</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">)</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Centered</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="p">)</span>

<span class="n">model4</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days.Centered</span><span class="o">+</span><span class="n">Maternal.Height.Centered</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days.Centered + Maternal.Height.Centered, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               119.46252    0.47980 248.983  &lt; 2e-16 ***
Gestational.Days.Centered   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(F\)</span> statistic is 143.5 with a <span class="math notranslate nohighlight">\(p\)</span>-value <span class="math notranslate nohighlight">\(&lt;2.2 \times 10^{-16}\)</span>. Therefore, there is strong evidence against the null and we can conclude that at least one of the estimated regression coefficients is non-zero (i.e Model 4 is a better fit than the null model).</p>
</div>
<div class="section" id="the-partial-f-test">
<h5>14.6.4.2 The partial F test<a class="headerlink" href="#the-partial-f-test" title="Permalink to this headline">¶</a></h5>
<p>The global <span class="math notranslate nohighlight">\(F\)</span>-test is a joint test of the statistical signficance of all the slope parameters in a linear regression model. On the other hand, the partial <span class="math notranslate nohighlight">\(F\)</span>-test compares the fit of:</p>
<ul class="simple">
<li><p>Model A (complex model, with  <span class="math notranslate nohighlight">\(p\)</span> predictors)</p></li>
<li><p>Model B (simpler nested model with <span class="math notranslate nohighlight">\(p-k\)</span> predictors).</p></li>
</ul>
<p>The key to the partial <span class="math notranslate nohighlight">\(F\)</span>-test is the construction of an Analysis of Variance table that partitions the sum of squares explained by the complex model into that explained by the simple model and the extra sum of squares only explained by the complex model. Using the notation that <span class="math notranslate nohighlight">\(SS_{REG_A}\)</span> denotes the sum of squares explained by the complex model, whilst <span class="math notranslate nohighlight">\(SS_{REG_B}\)</span> denotes the sum of squares explained by the simpler model, the ANOVA table is as shown below.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Explained by Model B</p></td>
<td><p><span class="math notranslate nohighlight">\(p-k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG_B}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG_B}=\frac{SS_{REG_B}}{p-k}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Explained by Model A</p></td>
<td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG_A}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG_A}=\frac{SS_{REG_A}}{p}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Extra explained by Model A</p></td>
<td><p><span class="math notranslate nohighlight">\(k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG_A}-SS_{REG_B}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG_X}=\frac{(SS_{REG_A}-SS_{REG_B})}{k}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Residual from Model A</p></td>
<td><p><span class="math notranslate nohighlight">\(n-(p+1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{RES_A}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{RES}=\frac{SS_{RES_A}}{n-(p+1)}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\((n-1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>The partial <span class="math notranslate nohighlight">\(F\)</span>-test tests the following null hypothesis:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \)</span> all of the slope parameters included in Model A but omitted from Model B are equal to zero.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \)</span> at least one of the additional parameters in Model A is not equal to 0.</p></li>
</ul>
<p>The appropriate test statistic (<span class="math notranslate nohighlight">\(F\)</span>) is the ratio of extra mean sum of squares in Model A to the mean residual sum of squares from Model A. Under the null hypothesis, this test statistic follows an <span class="math notranslate nohighlight">\(F\)</span>-distribution:</p>
<div class="math notranslate nohighlight">
\[
\text{Under } H_0: F = \frac{MS_{REG_X}}{MS_{RES}} \sim F_{k,(n-(p+1))}
\]</div>
<p><em>Example</em>: We can use <code class="docutils literal notranslate"><span class="pre">anova()</span></code> to conduct a partial F-test to compare Models 1 and 3:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> Model 1 is the better fit</p></li>
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> Model 3 is the better fit</p></li>
</ul>
<div class="section" id="id2">
<h6>Example<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h6>
<p>We can use <code class="docutils literal notranslate"><span class="pre">anova()</span></code> to conduct a partial <span class="math notranslate nohighlight">\(F\)</span>-test to compare Models 1 and 4:</p>
<ul class="simple">
<li><p>Model 1 (birthweight~length of pregnancy)</p></li>
<li><p>Model 4 (birthweight~length of pregnancy+mothers height)</p></li>
</ul>
<p>Model 1 is nested within Model 4. Model 4 is our complex model.</p>
<p>In this case, the two models only differ by one variable (mother’s height) and so the hypotheses being tested within the partial <span class="math notranslate nohighlight">\(F\)</span>-test could be written as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_2=0\)</span>, where <span class="math notranslate nohighlight">\(\beta_2\)</span> is the regression coefficient for mother’s height.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_2 \neq 0\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">anova</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">model4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A anova: 2 × 6</caption>
<thead>
	<tr><th></th><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>1172</td><td>328608.3</td><td>NA</td><td>      NA</td><td>      NA</td><td>          NA</td></tr>
	<tr><th scope=row>2</th><td>1171</td><td>316482.2</td><td> 1</td><td>12126.13</td><td>44.86728</td><td>3.266475e-11</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-statistic is 44.87 with a <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(3.23\times 10^{-11}\)</span>. This is strong evidence against the null hypothesis. Hence the data indicates that Model 4 (the more complex model) is the better fit.</p>
<p>When the two models being compared only differ by one variable, the partial F test is equivalent to the t test of the null hypothesis that the regression coefficient for that variable is equal to 0. Notice in our example that the results of the partial F test are the same as the t-test for <span class="math notranslate nohighlight">\(\beta_2=0\)</span>, with <span class="math notranslate nohighlight">\(F=t^2\)</span>.</p>
<p>For this reason, partial F tests are more useful in situations where we wish to compare models that differ by more than one variable. The approach is identical to that shown above.</p>
</div>
</div>
</div>
<div class="section" id="anova-for-models-with-categorical-independent-variables">
<h4>14.6.5 ANOVA for models with categorical independent variables<a class="headerlink" href="#anova-for-models-with-categorical-independent-variables" title="Permalink to this headline">¶</a></h4>
<p>Another useful application of ANOVA is to test for differences in means between categories of a categorical variable.</p>
<p>Suppose we are interested in the association between an outcome <span class="math notranslate nohighlight">\(Y\)</span> and a categorical variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(K\)</span> groups. We have already seen how to define a multivariable linear regression model using dummy variables for this situation. An alternative model, often termed the <strong>ANOVA model</strong>, is as follows:</p>
<p>Let <span class="math notranslate nohighlight">\(y_{ki}\)</span> be the value of the outcome for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation in the <span class="math notranslate nohighlight">\(k^{th}\)</span> group (<span class="math notranslate nohighlight">\(i=1,...,n_k\)</span> and <span class="math notranslate nohighlight">\(k=1,...,K\)</span>). The ANOVA model is then defined as:</p>
<div class="math notranslate nohighlight">
\[
y_{ki}=\mu_k + \epsilon_{ki} \text{, where } \epsilon_{ki} \overset{iid}{\sim} N(0,\sigma^2)
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mu_k\)</span> is the mean of the outcome in the <span class="math notranslate nohighlight">\(k^{th}\)</span> group. With this representation, the null and alternative hypothesis are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \mu_k= \mu\)</span> (i.e. the means in all groups defined by the categorical variables are equal to a common value).</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \mu_k \neq \mu\)</span> (i.e. the group means are not all equal).</p></li>
</ul>
<div class="section" id="sum-of-squares-for-models-with-categorical-variables">
<h5>14.6.5.1 Sum of squares for models with categorical variables<a class="headerlink" href="#sum-of-squares-for-models-with-categorical-variables" title="Permalink to this headline">¶</a></h5>
<p>For models with a single independent categorical variable the fitted values are simply the group means (<span class="math notranslate nohighlight">\(\bar{y_k}\)</span>). Under the null hypothesis that the group means are all equal, the fitted values are all equal to the overall mean (<span class="math notranslate nohighlight">\(\bar{y}\)</span>). This leads to new terminology for the residual sum of squares (<span class="math notranslate nohighlight">\(SS_{RES}\)</span>) and the sum of squares explained by the model (<span class="math notranslate nohighlight">\(SS_{REG}\)</span>):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(SS_{RES} = \sum_{k=1}^K \sum_{i=1}^{n_k} (y_{ki} - \bar{y}_k)^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(SS_{REG} = \sum_{k=1}^K \sum_{i=1}^{n_k} (\bar{y}_k - \bar{y})^2 = \sum_{k=1}^K n_k (\bar{y}_k - \bar{y})^2 \)</span></p></li>
</ul>
<p>In this case, the residual sum of squares is often termed the <strong>within group sum of squares <span class="math notranslate nohighlight">\((SS_{Within})\)</span></strong> and the regression sum of squares is often termed the <strong>between group sum of squares <span class="math notranslate nohighlight">\((SS_{Between})\)</span></strong>.</p>
</div>
<div class="section" id="id3">
<h5>14.6.5.2 The ANOVA table<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<p>When there are <span class="math notranslate nohighlight">\(K\)</span> groups, the degrees of freedom for the within groups sum of squares is <span class="math notranslate nohighlight">\(n-K\)</span> (because the model includes <span class="math notranslate nohighlight">\(K\)</span> parameters) and the degrees of freedom for the between groups sum of squares is <span class="math notranslate nohighlight">\(K-1\)</span> (because the null model contains a single parameter, the overall mean). Hence the ANOVA table is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Between groups</p></td>
<td><p><span class="math notranslate nohighlight">\(K-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{Between}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{Between}=\frac{SS_{Between}}{(K-1)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>WIthin Groups</p></td>
<td><p><span class="math notranslate nohighlight">\(n-K\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{Within}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{Within}=\frac{SS_{RES}}{n-K}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\(n-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="the-f-test">
<h5>14.6.5.3 The F-test<a class="headerlink" href="#the-f-test" title="Permalink to this headline">¶</a></h5>
<p>To test the null hypothesis that the means in all groups are equal to a common value, the appropriate <span class="math notranslate nohighlight">\(F\)</span>-statistic is:</p>
<div class="math notranslate nohighlight">
\[
F = \frac{MS_{Between}}{MS_{Within}} \sim F_{(K-1), n-K} \text{ under } H_0.
\]</div>
<p>If this test obtains a small <span class="math notranslate nohighlight">\(p\)</span>-value, then we have evidence that the means in the groups are not all the same. However, it does not tell us which of the group means differed from which other group means. For this reason, if we do find evidence of difference in means on an <span class="math notranslate nohighlight">\(F\)</span>-test, we may want to follow up with further analysis. Such further analysis may include pair-wise comparisons of means through analysis restricted to two groups.</p>
<div class="section" id="id4">
<h6>Example<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h6>
<p>We conduct an <span class="math notranslate nohighlight">\(F\)</span>-test to compare the average birthweights between babies whose mothers smoke and whose mothers don’t smoke using the birthweight data.</p>
<p>Let <span class="math notranslate nohighlight">\(\mu_1\)</span> and <span class="math notranslate nohighlight">\(\mu_0\)</span> denote the mean birthweight for babies whose mothers do smoke and don’t smoke, respectively. Then, the relevant hypotheses are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \mu_1= \mu_0\)</span> (i.e. the birthweight of a baby does not depend on whether the mother smoked)</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \mu_1\neq \mu_0\)</span></p></li>
</ul>
<p>Recall that we previously defined Model 2 to related birthweight and mother’s smoking status:</p>
<div class="math notranslate nohighlight">
\[ 
y_i = \alpha_0 + \alpha_1 s_i + \epsilon_i
\]</div>
<p>Where <span class="math notranslate nohighlight">\(Y\)</span> denotes the birthweight and</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
s_{i} =
\begin{cases}
    1 &amp; \text{ if the mother smokes} \\
    0 &amp; \text{ if the mother does not smoke}
\end{cases} 
\end{split}\]</div>
<p>We can rewrite this equation using the ANOVA model as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    y_{1i} &amp;=\mu_1 + \epsilon_{1i}  \text{ if the mother smokes} \\
    y_{0i} &amp;=\mu_0 + \epsilon_{0i}  \text{ if the mother does not smoke}
\end{align} \end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(y_{ki}\)</span> is the mean birthweight in the <span class="math notranslate nohighlight">\(k^{th}\)</span> group (groups are defined by mother’s smoking status), <span class="math notranslate nohighlight">\(\mu_1 = \beta_0 + \beta_1\)</span> and <span class="math notranslate nohighlight">\(\mu_0=\beta_0\)</span> (in other words, our null hypothesis can be rewritten as: <span class="math notranslate nohighlight">\(\beta_1=0\)</span>).</p>
<p>We can use either <code class="docutils literal notranslate"><span class="pre">anova()</span></code> or <code class="docutils literal notranslate"><span class="pre">summary()</span></code> to conduct the test in R:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model2</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="nf">factor</span><span class="p">(</span><span class="n">Maternal.Smoker</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">anova</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A anova: 2 × 5</caption>
<thead>
	<tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>factor(Maternal.Smoker)</th><td>   1</td><td> 24002.06</td><td>24002.0638</td><td>76.0167</td><td>9.461068e-18</td></tr>
	<tr><th scope=row>Residuals</th><td>1172</td><td>370055.79</td><td>  315.7473</td><td>     NA</td><td>          NA</td></tr>
</tbody>
</table>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ factor(Maternal.Smoker), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-68.085 -11.085   0.915  11.181  52.915 

Coefficients:
                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                 123.0853     0.6645 185.221   &lt;2e-16 ***
factor(Maternal.Smoker)True  -9.2661     1.0628  -8.719   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 17.77 on 1172 degrees of freedom
Multiple R-squared:  0.06091,	Adjusted R-squared:  0.06011 
F-statistic: 76.02 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>In the ANOVA table, the <code class="docutils literal notranslate"><span class="pre">factor(Maternal.Smoker)</span></code> row gives the between groups results and the <code class="docutils literal notranslate"><span class="pre">Residuals</span></code> row gives the within group results.</p>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-statistic is equal to 76.02 with a <span class="math notranslate nohighlight">\(p\)</span>-value equal to <span class="math notranslate nohighlight">\(9.46\times10^{-18}\)</span>. This evidence suggests that there is a difference in the mean birthweight between the two groups defined by mother’s smoking status.</p>
</div>
</div>
</div>
</div>
<span id="document-14.h. Linear Regression III"></span><div class="section" id="proofs">
<h3>14.7 Proofs<a class="headerlink" href="#proofs" title="Permalink to this headline">¶</a></h3>
<p>This section contains two important proofs. These are not examinable.</p>
<div class="section" id="proof-for-the-ordinary-least-squares-estimates-in-simple-linear-regression">
<h4>14.7.1 Proof for the ordinary least squares estimates in simple linear regression<a class="headerlink" href="#proof-for-the-ordinary-least-squares-estimates-in-simple-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>Recall the ordinary least square (OLS) estimates of the intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) and slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) in simple linear regression are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_0}\bar{x} \\
\hat{\beta_1} &amp;= \frac{\sum_{i=1}^2 (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x_i})^2}
\end{align}
\end{split}\]</div>
<p>Proof:</p>
<p>To solve for the value of <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimises <span class="math notranslate nohighlight">\(SS_{RES}\)</span>, we differentiate <span class="math notranslate nohighlight">\(SS_{RES}\)</span> with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and set the derivative to zero:</p>
<div class="math notranslate nohighlight">
\[ 
\frac{d(SS_{RES})}{d(\hat{\beta}_0)} = \sum_{i=1}^n -2(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)=0
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\sum_{i=1}^n (y_i)=n\bar{y}\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n(x_i)=n\bar{x}\)</span>, we can simplify to:</p>
<div class="math notranslate nohighlight">
\[
-n\bar{y}+n\hat{\beta}_0+n\hat{\beta}_1\bar{x}=0
\]</div>
<p>Rearranging the above and divide by <span class="math notranslate nohighlight">\(n\)</span> to give:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}.
\]</div>
<p>To solve for the value of <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimises <span class="math notranslate nohighlight">\(SS_{RES}\)</span>, we have to differentiate with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>. First, we substitute in our solution for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
SS_{RES}=\sum_{i=1}^n(y_i-(\bar{y}-\hat{\beta}_1\bar{x})-\hat{\beta}_1x_i)^2=\sum_{i=1}^n ((y_i-\bar{y})-\hat{\beta}_1(x_i-\bar{x}))^2
\]</div>
<p>Now differentiating the above with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> and setting the differential to zero gives:</p>
<div class="math notranslate nohighlight">
\[
\frac{d(SS_{RES})}{d(\hat{\beta}_1)} = \sum_{i=1}^n -2(x_i-\bar{x})(y_i-\bar{y})+2\hat{\beta}_1(x_i-\bar{x})^2=0
\]</div>
<p>Rearranging gives:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1\sum_{i=1}^n (x_i-\bar{x})^2 = \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})
\]</div>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]</div>
</div>
<div class="section" id="proof-that-the-ols-estimates-are-also-the-maximum-likelihood-estimates">
<h4>14.7.2 Proof that the OLS estimates are also the maximum likelihood estimates<a class="headerlink" href="#proof-that-the-ols-estimates-are-also-the-maximum-likelihood-estimates" title="Permalink to this headline">¶</a></h4>
<p>If <span class="math notranslate nohighlight">\(Y_i \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>, the log likelihood function is:</p>
<div class="math notranslate nohighlight">
\[
l(\mu | y_1,...,y_n) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu)^2
\]</div>
<p>So, for the simple linear regression model:</p>
<div class="math notranslate nohighlight">
\[
l(\beta_0, \beta_1 | y_1,...,y_n) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \beta_1x_i-\beta_0)^2.
\]</div>
<p>Therefore, for any fixed positive value for <span class="math notranslate nohighlight">\(\sigma^2\)</span>, maximising the log likelihood function is equivalent to minimising <span class="math notranslate nohighlight">\(SS_{RES}\)</span> and so the OLS estimates are also maximum likelihood estimates of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
</div>
</div>
</div>
</div>
<span id="document-15.a. Logistic Regression"></span><div class="section" id="logistic-regression">
<h2>15 Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>In this session, we continue exploring regression modelling. We now extend the ideas encountered in the context of linear regression models and apply them to a setting where the outcome of interest is a binary variable.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session, you will be able to:</p>
<ul class="simple">
<li><p>explain the rationale and the structure behind the logistic regression</p></li>
<li><p>apply a logistic regression model to real data</p></li>
<li><p>interpret the results of a logistic regression</p></li>
<li><p>interpret output from key diagnosis tools used for logistic regression</p></li>
</ul>
</div><div class="toctree-wrapper compound">
<span id="document-15.b. Logistic Regression"></span><div class="section" id="regression-modelling-for-binary-outcomes">
<h3>15.1 Regression modelling for binary outcomes<a class="headerlink" href="#regression-modelling-for-binary-outcomes" title="Permalink to this headline">¶</a></h3>
<p>In many health applications, the outcome of interest is a binary outcome. Examples are 30-day mortality following surgery, 5-year survival following diagnosis of breast cancer, whether or not a particular side effect was experienced after taking a particular.</p>
<p>In this session, we explore a very commonly used technique in health data science: logistic regression. This can be seen as an extension of the linear regression model we have been exploring in the last few sessions. We will see that many aspects of linear regression modelling extend quite naturally to logistic regression modelling.</p>
</div>
<div class="section" id="why-cant-we-just-use-linear-regression">
<h3>15.1.2 Why can’t we just use Linear Regression?<a class="headerlink" href="#why-cant-we-just-use-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>We developed our linear regression model for a continuous outcome. As we have seen, the linear regression model relies on a number of assumptions. Two of them, normality of residuals and homoscedasticity (constant residual variance) are particularly relevant to this discussion.</p>
<p><strong>Normality of errors:</strong> The usual inference procedures (calculating confidence intervals and p-values) for linear regression assume that the errors follow a normal distribution. This assumption will be violated with binary outcome data.</p>
<p><strong>Homoscedasticity:</strong> The error variance is constant. With binary data the variance is a function of the mean. Therefore, the variance would change as the mean changes. Therefore, this assumption is unlikely to hold.</p>
<p>An additional problem arises when we attempt to use linear regression to model binary outcome data, which relates to the fitted values.</p>
<p><strong>Fitted values can be impossible:</strong> The mean of a binary outcome is the probability that it is a “success”, using the terminology of Bernoulli trials. This mean, because it is a probability, must lie between 0 and 1. If we tried to use linear regression to model our binary outcome we may find that some of the fitted values are below 0 or greater than 1. This problem arises largely because of the assumption that the mean is modelled by the linear predictor, an additive function of the covariates. This assumption is unlikely to be valid.</p>
<p>Logistic regression avoids these problems by relating the covariates (through the linear predictor) to a <em>function of</em> the mean, instead of the mean.</p>
</div>
<span id="document-15.c. Logistic Regression"></span><div class="section" id="data-used-in-our-examples">
<h3>15.2 Data used in our examples<a class="headerlink" href="#data-used-in-our-examples" title="Permalink to this headline">¶</a></h3>
<p>We will use a dataset that is simulated to represent data from electronic health records for 200,000 patients. The outcome we will consider is whether or not a patient is diagnosed with dementia. In this example, there is an additional complexity because patients were followed up for different amounts of time. A longer follow-up will naturally lead to a higher probability of being diagnosed with dementia. In later modules, we will encounter survival analysis which allows the aspect of time to be accounted for. For now, we will ignore this aspect.</p>
<p>The code below reads in the dataset and displays the first few rows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># we load the dataset and display its first lines</span>
<span class="n">dementia</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/Dementia/dementia2.csv&quot;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">dementia</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 6 × 30</caption>
<thead>
	<tr><th></th><th scope=col>id</th><th scope=col>prac</th><th scope=col>pr_lcd</th><th scope=col>sex</th><th scope=col>age</th><th scope=col>bmi</th><th scope=col>bmi_category</th><th scope=col>consultations</th><th scope=col>agegp</th><th scope=col>alcohol</th><th scope=col>⋯</th><th scope=col>mortality</th><th scope=col>date_death</th><th scope=col>timetodementia</th><th scope=col>dementia</th><th scope=col>date_dementia</th><th scope=col>end_date</th><th scope=col>dob</th><th scope=col>rsample</th><th scope=col>vitd</th><th scope=col>lp</th></tr>
	<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>⋯</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td> 23189</td><td>142</td><td>08dec2009</td><td>1</td><td>53</td><td>20.4</td><td><span style=white-space:pre-wrap>Normal (18.5-&lt;25)     </span></td><td>12</td><td>50</td><td>3-6 units/day</td><td>⋯</td><td>0</td><td></td><td>NA</td><td>0</td><td></td><td>08dec2009</td><td>01nov1941</td><td>1</td><td><span style=white-space:pre-wrap>      NA</span></td><td>-0.8153054</td></tr>
	<tr><th scope=row>2</th><td> 92186</td><td>132</td><td>03feb2003</td><td>0</td><td>73</td><td>21.5</td><td><span style=white-space:pre-wrap>Normal (18.5-&lt;25)     </span></td><td> 4</td><td>70</td><td>&lt;2 units/day </td><td>⋯</td><td>0</td><td></td><td>NA</td><td>0</td><td></td><td>03feb2003</td><td>16jan1928</td><td>1</td><td><span style=white-space:pre-wrap>      NA</span></td><td>-1.2268275</td></tr>
	<tr><th scope=row>3</th><td>187963</td><td> 43</td><td>06jul2001</td><td>0</td><td>40</td><td>27.1</td><td><span style=white-space:pre-wrap>Overweight (25-&lt;30)   </span></td><td> 0</td><td>40</td><td>&lt;2 units/day </td><td>⋯</td><td>0</td><td></td><td>NA</td><td>0</td><td></td><td>06jul2001</td><td>18jun1961</td><td>1</td><td><span style=white-space:pre-wrap>      NA</span></td><td>-0.6602434</td></tr>
	<tr><th scope=row>4</th><td>148379</td><td>215</td><td>08mar2012</td><td>1</td><td>40</td><td>20.9</td><td><span style=white-space:pre-wrap>Normal (18.5-&lt;25)     </span></td><td> 3</td><td>40</td><td>&lt;2 units/day </td><td>⋯</td><td>0</td><td></td><td>NA</td><td>0</td><td></td><td>08mar2012</td><td>10feb1952</td><td>1</td><td>23.22692</td><td>-0.9507329</td></tr>
	<tr><th scope=row>5</th><td> 44194</td><td>225</td><td>02feb2011</td><td>1</td><td>92</td><td>32.5</td><td>Obese class I (30-&lt;35)</td><td>10</td><td>90</td><td><span style=white-space:pre-wrap>Non drinker  </span></td><td>⋯</td><td>0</td><td></td><td>NA</td><td>0</td><td></td><td>02feb2011</td><td>09dec1912</td><td>1</td><td><span style=white-space:pre-wrap>      NA</span></td><td> 1.0403746</td></tr>
	<tr><th scope=row>6</th><td>169915</td><td>175</td><td>02nov2011</td><td>1</td><td>55</td><td>26.3</td><td><span style=white-space:pre-wrap>Overweight (25-&lt;30)   </span></td><td> 3</td><td>55</td><td>3-6 units/day</td><td>⋯</td><td>0</td><td></td><td>NA</td><td>0</td><td></td><td>02nov2011</td><td>06oct1946</td><td>1</td><td><span style=white-space:pre-wrap>      NA</span></td><td>-0.1080445</td></tr>
</tbody>
</table>
</div></div>
</div>
</div>
<div class="section" id="exploratory-analyses">
<h3>15.2.1 Exploratory analyses<a class="headerlink" href="#exploratory-analyses" title="Permalink to this headline">¶</a></h3>
<p>The variables we will use during this session are:</p>
<ul class="simple">
<li><p>id: a variable that identifies a patient</p></li>
<li><p>sex: a factor variable that gives the sex of the patient (<span class="math notranslate nohighlight">\(0\)</span> for men, <span class="math notranslate nohighlight">\(1\)</span> for women)</p></li>
<li><p>age: age in years of the patient at study baseline</p></li>
<li><p>bmi: Body Mass Index of the patient at study baseline</p></li>
<li><p>dementia: an indicator variable that equals <span class="math notranslate nohighlight">\(1\)</span> if the patient is diagnosed with dementia during follow-up, <span class="math notranslate nohighlight">\(0\)</span> if not.</p></li>
</ul>
<p>In this session the outcome of interest is dementia diagnosis, which we will treat as a binary variable. We are interested in modelling the relationship between dementia diagnosis and age, sex and BMI. Generally, we would expect older people to have a higher risk of being diagnosed with dementia. Females typically have higher risk. The relationship with BMI is less well understood.</p>
<p>The code below tabulates dementia and sex and draws box-plots of age and BMI, separately by dementia diagnosis status.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tabulate dementia diagnosis versus sex (dementia = right-hand column)</span>
<span class="p">(</span><span class="n">table</span><span class="o">&lt;-</span><span class="nf">table</span><span class="p">(</span><span class="n">dementia</span><span class="o">$</span><span class="n">sex</span><span class="p">,</span> <span class="n">dementia</span><span class="o">$</span><span class="n">dementia</span><span class="p">))</span>
<span class="nf">prop.table</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>

<span class="c1"># Box plot of age by dementia diagnosis</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">boxplot</span><span class="p">(</span><span class="n">dementia</span><span class="o">$</span><span class="n">age</span> <span class="o">~</span> <span class="n">dementia</span><span class="o">$</span><span class="n">dementia</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Age&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Dementia diagnosis&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Baseline age (years)&quot;</span><span class="p">)</span>
<span class="nf">boxplot</span><span class="p">(</span><span class="n">dementia</span><span class="o">$</span><span class="n">bmi</span> <span class="o">~</span> <span class="n">dementia</span><span class="o">$</span><span class="n">dementia</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;BMI&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Dementia diagnosis&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Baseline BMI&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   
         0      1
  0 107981   1707
  1  88132   2180
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   
             0          1
  0 0.98443768 0.01556232
  1 0.97586146 0.02413854
</pre></div>
</div>
<img alt="_images/15.c. Logistic Regression_3_2.png" src="_images/15.c. Logistic Regression_3_2.png" />
</div>
</div>
<p>From the output above, we see that dementia is fairly rare in this study population, with 1.6% of males receiving a dementia diagnosis during follow-up compared to a slightly higher 2.4% among females.</p>
<p>The box-plots show that patients who received a dementia diagnosis during follow-up generally had a much higher age at baseline, as expected. The second box-plot perhaps hints at a slightly lower BMI among those diagnosed with dementia, but there is a less evident relationship than for age.</p>
</div>
<span id="document-15.d. Logistic Regression"></span><div class="section" id="the-logistic-regression-model">
<h3>15.3 The logistic regression model<a class="headerlink" href="#the-logistic-regression-model" title="Permalink to this headline">¶</a></h3>
<p>Throughout this session we will assume that the outcome <span class="math notranslate nohighlight">\(Y\)</span> is binary. Further, we assume that <span class="math notranslate nohighlight">\(Y\)</span> takes a value of 0 (“failure”) or 1 (“success”). As we discussed earlier, the terminology of success and failure does not imply success is a good thing; in health applications “success” often refers to a bad outcome such as death.</p>
<p>We will initially consider the simple situation with a single independent variable of interest, <span class="math notranslate nohighlight">\(X\)</span>. We assume that conditional on <span class="math notranslate nohighlight">\(X\)</span>, the outcome <span class="math notranslate nohighlight">\(Y\)</span> follows a Bernoulli distribution:</p>
<div class="math notranslate nohighlight">
\[
Y | X=x \sim Bernoulli(\pi_x)
\]</div>
<p>Then <span class="math notranslate nohighlight">\(\pi_x\)</span> is the conditional probability of sucess, given <span class="math notranslate nohighlight">\(X=x\)</span>. It also represents the conditional expectation of the outcome, given <span class="math notranslate nohighlight">\(X=x\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\pi_x = E[Y | X=x] = P(Y=1 | X=x)
\]</div>
<p>Typically, our research question involves relating this probability to the covariate(s).</p>
<div class="section" id="components-of-the-model">
<h4>15.3.1 Components of the model<a class="headerlink" href="#components-of-the-model" title="Permalink to this headline">¶</a></h4>
<div class="section" id="the-logit-function">
<h5>The logit function<a class="headerlink" href="#the-logit-function" title="Permalink to this headline">¶</a></h5>
<p>As we have discussed, we do not wish to directly model <span class="math notranslate nohighlight">\(\pi\)</span>, because fitted values can lie outside the possible range of values. Instead, we will first transform <span class="math notranslate nohighlight">\(\pi\)</span>. In other words, we will model a function of <span class="math notranslate nohighlight">\(\pi\)</span>. We want a one-to-one function (so we can back-transform to the original scale, if we wish) that maps a probability <span class="math notranslate nohighlight">\(\pi\)</span> to the whole real line.</p>
<p>The function that we use in logistic regression is called the <strong>logit function</strong>. Specifically,</p>
<div class="math notranslate nohighlight">
\[
logit(\pi) = log\left(\frac{\pi}{1-\pi}\right)
\]</div>
<p>The probability <span class="math notranslate nohighlight">\(\pi\)</span> lies in the interval <span class="math notranslate nohighlight">\([0,1]\)</span> but the transformed value, <span class="math notranslate nohighlight">\(logit(\pi)\)</span> lies in the range <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>.</p>
<p>It will also be useful to know how to back-transform. If <span class="math notranslate nohighlight">\(logit(\pi) = L\)</span> then</p>
<div class="math notranslate nohighlight">
\[
\pi = \frac{exp(L)}{1 + exp(L)}
\]</div>
<p>This relationship will allow us to obtain fitted probabilities from our logistic regression model.</p>
<div class="section" id="odds">
<h6>Odds<a class="headerlink" href="#odds" title="Permalink to this headline">¶</a></h6>
<p>Suppose we have a binary outcome, where the probability of success is <span class="math notranslate nohighlight">\(\pi\)</span>, i.e. <span class="math notranslate nohighlight">\(P(Y=1) = \pi\)</span>. Then the <strong>odds</strong> of success are given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\pi}{1-\pi}
\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(logit(\pi)\)</span> is the logarithm of the odds, or the log-odds. We will see below that using the logit function leads to the parameters of the regression model being interpreted in terms of odds and odds ratios.</p>
</div>
</div>
<div class="section" id="the-linear-predictor">
<h5>The linear predictor<a class="headerlink" href="#the-linear-predictor" title="Permalink to this headline">¶</a></h5>
<p>Just as for linear regression models, the linear predictor is an additive function of the independent variables. With a single covariate, it is simply:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 + \beta_1 X
\]</div>
</div>
</div>
<div class="section" id="the-basic-logistic-regression-model">
<h4>15.3.2 The basic logistic regression model<a class="headerlink" href="#the-basic-logistic-regression-model" title="Permalink to this headline">¶</a></h4>
<p>The equation for a logistic regression model with, relating <span class="math notranslate nohighlight">\(X\)</span> to a binary outcome <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
logit(\pi_x) = \beta_0 + \beta_1 X
\]</div>
<p>Note that, unlike linear regression, there is no explicit error term in the logistic regression model.</p>
<div class="section" id="interpreting-the-parameters">
<h5>Interpreting the parameters<a class="headerlink" href="#interpreting-the-parameters" title="Permalink to this headline">¶</a></h5>
<p>Suppose that our single covariate <span class="math notranslate nohighlight">\(X\)</span> is binary, taking values 1 (exposed, say) and 0 (unexposed). Our model is then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
logit(\pi_x) =
\begin{cases} \beta_0 &amp;\text{when $X$=0 (unexposed group)} \\
\beta_0 + \beta_1 &amp;\text{when $X$=1 (exposed group)}
\end{cases}
\end{split}\]</div>
<p>In other words, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\beta_0 &amp;\qquad \text{is the log-odds of the outcome in the unexposed group} \\
\beta_0 + \beta_1 &amp;\qquad \text{is the log-odds of the outcome in the exposed group}
\end{align*}
\end{split}\]</div>
<p>Taking the exponential, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
e^{\beta_0} &amp;\qquad \text{is the odds of the outcome in the unexposed group} \\
e^{\beta_0 + \beta_1} &amp;\qquad \text{is the odds of the outcome in the exposed group}
\end{align*}
\end{split}\]</div>
<p>Now we have that <span class="math notranslate nohighlight">\(e^{\beta_0 + \beta_1}  = e^{\beta_0} \times e^{\beta_1}\)</span>. Therefore, <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span> also represents the multiplicative increase in the odds, going from the unexposed group to the exposed group. This multiplicative increase is known as the <strong>odds ratio</strong>.  Therefore,  we can also write:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
e^{\beta_0} &amp;\qquad \text{is the odds of the outcome in the unexposed group} \\
e^{\beta_1} &amp;\qquad \text{is the odds ratio of the outcome, comparing the exposed group to the unexposed group}
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="general-interpretation">
<h5>General interpretation<a class="headerlink" href="#general-interpretation" title="Permalink to this headline">¶</a></h5>
<p>This leads us to the following interpretation of the model:</p>
<div class="math notranslate nohighlight">
\[
logit(\pi_x) = \beta_0 + \beta_1 X
\]</div>
<ul class="simple">
<li><p>The intercept, <span class="math notranslate nohighlight">\(\beta_0\)</span> is the log-odds among those with <span class="math notranslate nohighlight">\(X=0\)</span>. This is often called the <strong>baseline log-odds</strong>. Alternatively, the exponential <span class="math notranslate nohighlight">\(e^{\beta_0}\)</span> is the odds among those with <span class="math notranslate nohighlight">\(X=0\)</span>.</p></li>
<li><p>The slope, <span class="math notranslate nohighlight">\(\beta_1\)</span>, is the difference in the log-odds associated with a one-unit increase in <span class="math notranslate nohighlight">\(X\)</span>. Equivalently, <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span> is the odds ratio associated with a one-unit increase in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<span id="document-15.e. Logistic Regression"></span><div class="section" id="estimating-the-parameters">
<h3>15.4 Estimating the parameters<a class="headerlink" href="#estimating-the-parameters" title="Permalink to this headline">¶</a></h3>
<p>Having specified our model, we now want to use a sample of data to obtain estimates of the model parameters.</p>
<div class="section" id="statistical-model-and-observed-data">
<h4>15.4.1 Statistical model and observed data<a class="headerlink" href="#statistical-model-and-observed-data" title="Permalink to this headline">¶</a></h4>
<p><strong>Data:</strong> Suppose we have a sample of <span class="math notranslate nohighlight">\(n\)</span> people. Person <span class="math notranslate nohighlight">\(i\)</span> has an observed <span class="math notranslate nohighlight">\(X\)</span> value of <span class="math notranslate nohighlight">\(x_i\)</span> and an observed outcome <span class="math notranslate nohighlight">\(y_i\)</span>. Therefore, our sample of data consists of: <span class="math notranslate nohighlight">\(\{ (x_i, y_i); i=1,2,...,n\}\)</span>.</p>
<p><strong>Statistical model:</strong> Our statistical model assumes that these observations are independent (between people) and are drawn from the distribution:</p>
<div class="math notranslate nohighlight">
\[
Y_i | X=x_i \sim Bernoulli(\pi_i)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\pi_i = P(Y_i=1 | X=x_i)
\]</div>
<p>We further have a model relating the outcome to the independent variable:</p>
<div class="math notranslate nohighlight">
\[
logit(\pi_i) = \beta_0 + \beta_1 x_i \qquad \text{or, equivalently:} \qquad \pi_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}
\]</div>
<p>We could put these aspects all together to write our statistical model concisely as:</p>
<div class="math notranslate nohighlight">
\[
Y_i | X=x_i \sim Bernoulli\left(  \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} \right)
\]</div>
<p>Note</p>
<blockquote>
<div><p>In the previous section, we used the notation <span class="math notranslate nohighlight">\(\pi_x\)</span> in order to emphasise that this probability is conditional on the value of <span class="math notranslate nohighlight">\(X\)</span>. Now we are applying the distribution to a sample of people so we have changed to <span class="math notranslate nohighlight">\(\pi_i\)</span> to emphasise that the probability is conditional on whatever value <span class="math notranslate nohighlight">\(X\)</span> takes for person <span class="math notranslate nohighlight">\(i\)</span>.</p>
</div></blockquote>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h4>15.4.2 Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h4>
<p>We first need to derive the likelihood of the model. we assume that observations (people) are independent of each other, thus:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
L(\beta_0, \beta_1) &amp;= \prod_{i=1}^n Pr(Y_i = y_i|X_i=x_i) 
\end{align*}
\]</div>
<p>We can write this as</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
L(\beta_0, \beta_1) &amp;= \prod_{i=1}^n Pr(Y_i = 1 | X_i = x_i)^{y_i}Pr(Y_i = 0 | X_i = x_i)^{1-y_i}
\end{align*}
\]</div>
<p>Because when the observed outcome is 1, the second term above is 1 (recall <span class="math notranslate nohighlight">\(x^0\)</span>=1 for any <span class="math notranslate nohighlight">\(x\)</span>) so we just have <span class="math notranslate nohighlight">\(Pr(Y_i = 1|X_i=x_i)\)</span>, which is equal to <span class="math notranslate nohighlight">\(Pr(Y_i = y_i|X_i=x_i)\)</span> when <span class="math notranslate nohighlight">\(y_i = 1\)</span>. Conversely, when <span class="math notranslate nohighlight">\(y_i = 0\)</span>, the first term becomes 1 and we are left with just the second term.</p>
<p>Now <span class="math notranslate nohighlight">\(Pr(Y_i = 1 | X_i = x_i)\)</span> is just the fraction within the Bernoulli distribution above, so we can substitute this in to get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L(\beta_0, \beta_1) &amp;= \prod_{i=1}^n \left(  \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}} \right)^{y_i} \left( \frac{1}{1 + e^{\beta_0 + \beta_1 x_i}} \right)^{1-y_i} \\ &amp; = \prod_{i=1}^n (e^{\beta_0 + \beta_1 x_i})^{y_i} \times \left(\frac{1}{1 + e^{\beta_0 + \beta_1 x_i}} \right) 
\end{align*}
\end{split}\]</div>
<p>Taking the log of the above likelihood, we derive the following log-likelihood</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
l(\beta_0, \beta_1) &amp;= \sum_{i=1}^n \{ y_i log(e^{\beta_0 + \beta_1 x_i})  - log \left(1 + e^{\beta_0 + \beta_1 x_i} \right)  \} \\
 &amp;= \sum_{i=1}^n \{ y_i (\beta_0 + \beta_1 x_i)  - log \left(1 + e^{\beta_0 + \beta_1 x_i} \right)  \}
\end{align*}
\end{split}\]</div>
<p>By maximizing this log-likelihood over the parameters <span class="math notranslate nohighlight">\((\beta_0, \beta_1)\)</span>, we can obtain the maximum likelihood estimates of the parameters: <span class="math notranslate nohighlight">\((\hat{\beta}_0, \hat{\beta}_1)\)</span>. There is no closed-form solution to this optimisation problem. Therefore, the maximisation over the parameters is done numerically.</p>
</div>
</div>
<span id="document-15.f. Logistic Regression"></span><div class="section" id="examples">
<h3>15.5 Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<div class="section" id="dementia-and-sex">
<h4>15.5.1 Dementia and sex<a class="headerlink" href="#dementia-and-sex" title="Permalink to this headline">¶</a></h4>
<p>We now return to the dementia dataset and explore the relationship between sex and diagnosis of dementia during the study period. In this is example, our outcome <span class="math notranslate nohighlight">\(Y\)</span> is the binary variable of whether the patient was diagnosed with dementia during follow-up (1=yes, 0=no). Our single independent variable <span class="math notranslate nohighlight">\(S\)</span> is sex (0=male, 1=female). The logistic regression model we will fit is:</p>
<div class="math notranslate nohighlight">
\[ 
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 s_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_i=E(Y| S=s_i)\)</span>.</p>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">glm()</span></code> to perform simple linear regressions in R. Click <a class="reference external" href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm">here</a> for details of how this command works.</p>
<p>The following code can be used to perform this logistic regression in R. We need to specify the formula for the model, which is very similar to the syntax used in linear regression modelling. In addition, we now need to tell R that we are using the <code class="docutils literal notranslate"><span class="pre">logit</span></code> function and that we are assuming that the data are assumed to follow a Bernoulli distribution (which, recall is a special case of the Binomial distribution).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dementia</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/Dementia/dementia2.csv&quot;</span><span class="p">)</span>
<span class="n">dementia1</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">dementia</span> <span class="o">~</span> <span class="n">sex</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">dementia</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">dementia1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = dementia ~ sex, family = binomial(link = &quot;logit&quot;), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2211  -0.2211  -0.1771  -0.1771   2.8855  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -4.14722    0.02439 -170.01   &lt;2e-16 ***
sex          0.44771    0.03264   13.72   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 38143  on 199998  degrees of freedom
AIC: 38147

Number of Fisher Scoring iterations: 7
</pre></div>
</div>
</div>
</div>
<p>We interpret the two estimated coefficients as follows:</p>
<ul class="simple">
<li><p>The estimated log-odds of dementia diagnosis among males (the “baseline” group, with <span class="math notranslate nohighlight">\(S=0\)</span>) is -4.147.</p></li>
<li><p>The estimated log odds ratio for females, compared with males, is 0.4477.</p></li>
</ul>
<p>For a slightly more intuitive interpretation, we will take the exponential transformation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">exp</span><span class="p">(</span><span class="nf">coefficients</span><span class="p">(</span><span class="n">dementia1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.dl-inline {width: auto; margin:0; padding: 0}
.dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}
.dl-inline>dt::after {content: ":\0020"; padding-right: .5ex}
.dl-inline>dt:not(:first-of-type) {padding-left: .5ex}
</style><dl class=dl-inline><dt>(Intercept)</dt><dd>0.0158083366516896</dd><dt>sex</dt><dd>1.5647202094567</dd></dl>
</div></div>
</div>
<p>Now we can equivalently, and perhaps more intuitively, interpret the coefficients as follows:</p>
<ul class="simple">
<li><p>The estimated odds of dementia diagnosis among males is 0.0158.</p></li>
<li><p>The estimated odds ratio for females, compared with males, is 1.576. In other words, the odds of dementia diagnosis among females is estimated to be 1.576 times higher than among males.</p></li>
</ul>
</div>
<div class="section" id="dementia-and-age">
<h4>15.5.2 Dementia and age<a class="headerlink" href="#dementia-and-age" title="Permalink to this headline">¶</a></h4>
<p>We now explore the relationship of dementia diagnosis and age, measured in years. In this is example, our outcome <span class="math notranslate nohighlight">\(Y\)</span> remains dementia diagnosis, as above, but our single independent variable <span class="math notranslate nohighlight">\(A\)</span> is age, measured in years. The logistic regression model we will fit is:</p>
<div class="math notranslate nohighlight">
\[ 
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 a_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_i=E(Y| A=a_i)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dementia2</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">dementia</span> <span class="o">~</span> <span class="n">age</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">dementia</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">dementia2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = dementia ~ age, family = binomial(link = &quot;logit&quot;), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9935  -0.1989  -0.1140  -0.0721   3.5947  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -10.533958   0.103139 -102.13   &lt;2e-16 ***
age           0.101865   0.001402   72.66   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 31876  on 199998  degrees of freedom
AIC: 31880

Number of Fisher Scoring iterations: 8
</pre></div>
</div>
</div>
</div>
<p>We interpret the two estimated coefficients as follows:</p>
<ul class="simple">
<li><p>The estimated log-odds of dementia diagnosis among people aged 0 is is -10.53. Of course, this is not a meaningful quantity. As for linear regression, we could center the age variable to provide an interpretable intercept.</p></li>
<li><p>The estimated log odds ratio for each increase of one year in age is 0.101.</p></li>
</ul>
<p>For a slightly more intuitive interpretation, we will take the exponential transformation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">exp</span><span class="p">(</span><span class="nf">coefficients</span><span class="p">(</span><span class="n">dementia2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.dl-inline {width: auto; margin:0; padding: 0}
.dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}
.dl-inline>dt::after {content: ":\0020"; padding-right: .5ex}
.dl-inline>dt:not(:first-of-type) {padding-left: .5ex}
</style><dl class=dl-inline><dt>(Intercept)</dt><dd>2.66170781376369e-05</dd><dt>age</dt><dd>1.10723429559233</dd></dl>
</div></div>
</div>
<p>Now we can interpret the two estimated coefficients as follows:</p>
<ul class="simple">
<li><p>The estimated odds of dementia diagnosis among people aged 0 is is 2.66.</p></li>
<li><p>The estimated odds ratio for each increase of one year in age is 1.107. In other words, the estimated odds of dementia diagnosis is multiplied by 1.11 (or, increased by 11%) with each increase in year of age at study baseline.</p></li>
</ul>
</div>
</div>
<span id="document-15.g. Logistic Regression"></span><div class="section" id="inference">
<h3>15.6 Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h3>
<p>We have fitted the following logistic regression model:</p>
<div class="math notranslate nohighlight">
\[
logit(\pi_i) = \beta_0 + \beta_1 x_i 
\]</div>
<p>Having estimated the parameters of the logistic regression model using maximum likelihood estimation, we would like to obtain 95% confidence intervals for the parameters and perform hypothesis testing. We will now explore options available to do those things.</p>
<p>A sketch of the relevant statistical theory is provided in the optional reading in the appendix to this session.</p>
<div class="section" id="confidence-intervals">
<h4>15.6.1 Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">¶</a></h4>
<p>A number of approximate confidence intervals can be obtained. Two commonly used confidence intervals are the Wald-type confidence intervals and profile confidence intervals.</p>
<p><strong>Wald-type confidence interval:</strong> This confidence interval takes a familiar form. For slope parameter <span class="math notranslate nohighlight">\(\beta_1\)</span>, an approximate 95% confidence inteval is given by</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 \pm 1.96 \hat{SE}(\hat{\beta}_1)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is the maximum likelihood estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\hat{SE}(\hat{\beta}_1)\)</span> is its standard error.</p>
<p><strong>Profile likelihood confidence intervals</strong>  These intervals are based on the log-likelihood-ratio. For each parameter of interest, a <strong>profile</strong> likelihood is constructed, which treats all other parameters as nuisances and removes them from the likelihood (by setting to their values which maximise the likelihood for each value of the parameter of interest). Then confidence intervals are constructed based on the profile likelihood. The Wald-type confidence intervals provide an approximation to this process. Profile likelihod confidence intervals are provided in R using the command <code class="docutils literal notranslate"><span class="pre">confint</span></code>.</p>
</div>
<div class="section" id="hypothesis-tests">
<h4>15.6.2 Hypothesis tests<a class="headerlink" href="#hypothesis-tests" title="Permalink to this headline">¶</a></h4>
<p>Often, the  hypothesis we are interested in testing is that the independent variable <span class="math notranslate nohighlight">\(X\)</span> is <em>not associated with</em> the outcome. Therefore, the null and alternative hypotheses are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \beta_1 \neq 0\)</span></p></li>
</ul>
<p>This is the null hypothesis tested, by default, in regression output provided in R.</p>
<p>There are three important type of tests available. These are all approximate tests and are asymptotically equivalent to one another. So in large samples, we would expect to see similar p-values from each test.</p>
<p><strong>Likelihood ratio test</strong> This test is based directly on the approximate distribution of the log-likelihood-ratio.</p>
<p><strong>Wald test</strong>  This test is based on a quadratic approximation to the log-likelihood-ratio. As such, it can be less accurate than the likelihood ratio test, particularly if the null value is a long way from the maximum likelihood estimate. However, in this case all tests are likely to provide small p-values and similar qualitative conclusions.</p>
<p>The Wald test is used to obtain the p-values automatically displayed in regression output for GLMs in R and many other software platforms. This is because Wald tests are computationally less intensive than likelihood ratio tests.</p>
<p><strong>Score test</strong> These tests are based on a slightly different quadratic approximation to the log-likelihood-ratio. This type of test is much less used than the other types, so we do not pursue this further here. Early tests used in epidemiology tended to be score tests, since they are less computationally intensive than the other approaches.</p>
</div>
<div class="section" id="example">
<h4>15.6.3 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>We return to our model exploring the association between sex and diagnosis of dementia. We first perform a hypothesis test investigating the null hypothesis that sex is not associated with dementia diagnosis. Then we obtain 95% confidence intervals for our two parameters of interest.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dementia</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/Dementia/dementia2.csv&quot;</span><span class="p">)</span>
<span class="n">dementia1</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">dementia</span> <span class="o">~</span> <span class="n">sex</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">dementia</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">dementia1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = dementia ~ sex, family = binomial(link = &quot;logit&quot;), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2211  -0.2211  -0.1771  -0.1771   2.8855  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -4.14722    0.02439 -170.01   &lt;2e-16 ***
sex          0.44771    0.03264   13.72   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 38143  on 199998  degrees of freedom
AIC: 38147

Number of Fisher Scoring iterations: 7
</pre></div>
</div>
</div>
</div>
<p>The p-value for sex is <span class="math notranslate nohighlight">\(p&lt;0.001\)</span>, providing strong evidence against the null hypothesis that sex is not associated with the odds of being diagnosed with dementia.</p>
<p>Now we will obtain the profile confidence intervals for the two estimated regression coefficients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint</span><span class="p">(</span><span class="n">dementia1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Waiting for profiling to be done...
</pre></div>
</div>
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 2 × 2 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>
</thead>
<tbody>
	<tr><th scope=row>(Intercept)</th><td>-4.1954026</td><td>-4.0997726</td></tr>
	<tr><th scope=row>sex</th><td> 0.3838153</td><td> 0.5117587</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>In fact, these are more easily interpreted on the exponentiated scale, as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">cbind</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="nf">coefficients</span><span class="p">(</span><span class="n">dementia1</span><span class="p">)),</span> <span class="nf">exp</span><span class="p">(</span><span class="nf">confint</span><span class="p">(</span><span class="n">dementia1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Waiting for profiling to be done...
</pre></div>
</div>
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 2 × 3 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>
</thead>
<tbody>
	<tr><th scope=row>(Intercept)</th><td>0.01580834</td><td>0.01506468</td><td>0.01657644</td></tr>
	<tr><th scope=row>sex</th><td>1.56472021</td><td>1.46787427</td><td>1.66822252</td></tr>
</tbody>
</table>
</div></div>
</div>
<ul class="simple">
<li><p>The estimated odds in males is 0.0158 (95% CI 0.01506, 0.01657). We are 95% confident that thee odds of dementia diagnosis among males lies within this range.</p></li>
<li><p>The estimated odds ratio for females, compared with males, is 1.56 (95% CI 1.47, 1.67). We estimate that the odds of dementia diagnosis is 1.56 times higher among females than among males. The data are consistent with this value being as low as 1.47 or as high as 1.67.</p></li>
</ul>
<p>Below is the code to obtain Wald test confidence intervals. Comparing these with the (unexponentiated) confidence intervals above, we see these are very similar, as we would expect.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint.default</span><span class="p">(</span><span class="n">dementia1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 2 × 2 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>
</thead>
<tbody>
	<tr><th scope=row>(Intercept)</th><td>-4.1950299</td><td>-4.0994058</td></tr>
	<tr><th scope=row>sex</th><td> 0.3837405</td><td> 0.5116735</td></tr>
</tbody>
</table>
</div></div>
</div>
</div>
</div>
<span id="document-15.h. Logistic Regression"></span><div class="section" id="multivariable-logistic-regression">
<h3>15.7 Multivariable logistic regression<a class="headerlink" href="#multivariable-logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>Suppose we wish to relate a binary outcome (<span class="math notranslate nohighlight">\(Y\)</span>) to <span class="math notranslate nohighlight">\(p\)</span> predictor variables <span class="math notranslate nohighlight">\((X_1, X_2, ..., X_p)\)</span>. The appropriate multivariable logistic regression model is a straightforward extension of the simple logistic regression model:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ,..., \beta_p x_{ip}
\]</div>
<p>where, <span class="math notranslate nohighlight">\(x_{ji}\)</span> is the value of the jth predictor variable for the ith participant and <span class="math notranslate nohighlight">\(\pi_i = P(Y_i = 1 | X_1=x_1, ..., X_p=x_p)\)</span>.</p>
<p>The parameters in the model are interpreted as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept. It is the estimated log-odds of <span class="math notranslate nohighlight">\(Y\)</span> when all the <span class="math notranslate nohighlight">\(X_j\)</span>’s are zero.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_j\)</span> is the expected change in the log-odds of <span class="math notranslate nohighlight">\(Y\)</span> for a 1 unit increase in <span class="math notranslate nohighlight">\(X_j\)</span> <em>with all the other covariates held constant</em>.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(\beta_j\)</span>’s are the <strong>regression coefficients</strong> (otherwise known as <strong>partial regression coefficients</strong>). Each one measures the effect of one covariate controlled (or adjusted) for all of the others.</p>
<p>The maximum likelihood estimation process outlined earlier can be naturally extended to the multivariable model above.</p>
<div class="section" id="example">
<h4>15.7.1 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>We consider an example using the dementia dataset. This time, our interest lies in modeling the relationship between the odds of being diagnosed with dementia during study follow-up and to sex (<span class="math notranslate nohighlight">\(S\)</span>), age (<span class="math notranslate nohighlight">\(A\)</span>) and BMI (<span class="math notranslate nohighlight">\(B\)</span>) at study baseline.</p>
<p>Our multivariable logistic regression model is:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{logit}(\pi_i) = \beta_0 + \beta_1 s_i + \beta_2 a_i + \beta_3 b_i
\]</div>
<p>This model can be estimated in <code class="docutils literal notranslate"><span class="pre">R</span></code>using the <code class="docutils literal notranslate"><span class="pre">glm</span></code> function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dementia</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/Dementia/dementia2.csv&quot;</span><span class="p">)</span>
<span class="n">dementia2</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">dementia</span> <span class="o">~</span> <span class="n">sex</span> <span class="o">+</span> <span class="n">age</span> <span class="o">+</span> <span class="n">bmi</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">dementia</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">dementia2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = dementia ~ sex + age + bmi, family = binomial(link = &quot;logit&quot;), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1067  -0.1959  -0.1134  -0.0732   3.6917  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.783837   0.152138 -64.309  &lt; 2e-16 ***
sex          0.306798   0.033773   9.084  &lt; 2e-16 ***
age          0.098682   0.001413  69.826  &lt; 2e-16 ***
bmi         -0.025619   0.003596  -7.124 1.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 31732  on 199996  degrees of freedom
AIC: 31740

Number of Fisher Scoring iterations: 8
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">cbind</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="nf">coefficients</span><span class="p">(</span><span class="n">dementia2</span><span class="p">)),</span> <span class="nf">exp</span><span class="p">(</span><span class="nf">confint</span><span class="p">(</span><span class="n">dementia2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Waiting for profiling to be done...
</pre></div>
</div>
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 4 × 3 of type dbl</caption>
<thead>
	<tr><th></th><th scope=col></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>
</thead>
<tbody>
	<tr><th scope=row>(Intercept)</th><td>5.635516e-05</td><td>4.179134e-05</td><td>7.587428e-05</td></tr>
	<tr><th scope=row>sex</th><td>1.359066e+00</td><td>1.272090e+00</td><td>1.452170e+00</td></tr>
	<tr><th scope=row>age</th><td>1.103716e+00</td><td>1.100675e+00</td><td>1.106790e+00</td></tr>
	<tr><th scope=row>bmi</th><td>9.747061e-01</td><td>9.678335e-01</td><td>9.815740e-01</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>We can interpret the parameters as follows:</p>
<ul class="simple">
<li><p><em>sex</em>: Females are estimated to have 1.36 times higher odds of being diagnosed with dementia than men <em>who have the same age and BMI at study baseline</em>. The data are consistent with the true odds ratio lying between 1.27 and 1.45. The p-value, <span class="math notranslate nohighlight">\(p&lt;0.001\)</span>, provides strong evidence against the null hypothesis of no association between sex and dementia <em>after adjusting for age and BMI</em>. <br><br></p></li>
<li><p><em>age</em>: The odds of being diagnosed with dementia is estimated to increase 1.1-fold for each increase in year of age at study baseline. The data are consistent with the true odds ratio lying between 1.1006 and 1.107.  The p-value, <span class="math notranslate nohighlight">\(p&lt;0.001\)</span>, provides strong evidence against the null hypothesis of no association between age and dementia <em>after adjusting for sex and BMI</em>. <br><br></p></li>
<li><p><em>bmi</em>: The odds of being diagnosed with dementia is estimated to reduce by 0.97 times for each increase in unit of BMI, suggesting an inverse association between BMI and odds of dementia diagnosis.  The p-value, <span class="math notranslate nohighlight">\(p&lt;0.001\)</span>, provides strong evidence against the null hypothesis of no association between BMI and dementia <em>after adjusting for sex and age</em>. <br><br></p></li>
</ul>
</div>
</div>
<span id="document-15.i. Logistic Regression"></span><div class="section" id="interactions-and-higher-order-terms">
<h3>15.8 Interactions and higher-order terms<a class="headerlink" href="#interactions-and-higher-order-terms" title="Permalink to this headline">¶</a></h3>
<p>We are often interested in exploring whether associations between an independent variable and our outcome differ depending on the value taken by another independent variable, i.e. we are interested in <em>interactions</em>.</p>
<p>Similarly, we may be modelling the relationship between the odds of the outcome and a continuous covariate, and wish to explore whether the relationshp is linear on the log-odds scale.</p>
<p>The good news is that the same techniques that we met in linear regression modelling can be applied here. We can add interactions, quadratic or higher order terms or splines to our logistic regression model.</p>
</div>
<span id="document-15.j. Logistic Regression"></span><div class="section" id="model-diagnostics">
<h3>15.9 Model diagnostics<a class="headerlink" href="#model-diagnostics" title="Permalink to this headline">¶</a></h3>
<p>This material is not examinable and is provided for your information.</p>
<p>Many model diagnostics are available for logistic regression models. We touch on a few very briefly here.</p>
<div class="section" id="goodness-of-fit">
<h4>15.9.1 Goodness-of-fit<a class="headerlink" href="#goodness-of-fit" title="Permalink to this headline">¶</a></h4>
<div class="section" id="deviance">
<h5>Deviance<a class="headerlink" href="#deviance" title="Permalink to this headline">¶</a></h5>
<p>The deviance of a model <span class="math notranslate nohighlight">\(M\)</span> is a measure of the goodness-of-fit of the model. It is defined as</p>
<div class="math notranslate nohighlight">
\[
D = -2(l_M - l_S)
\]</div>
<p>where <span class="math notranslate nohighlight">\(l_M\)</span>  is the log-likelihood of model <span class="math notranslate nohighlight">\(M\)</span> and <span class="math notranslate nohighlight">\(l_S\)</span> is the log-likelihood of the saturated model (one which uses the maximum possible number of parameters without redundancies; this is the model with the best possible fit).</p>
<p>In general, higher values of deviance indicate worse model fit to the data. Two deviance statistics are often produced in output following logistic regression:</p>
<ul class="simple">
<li><p>Null deviance: the deviance computed for the null model, i.e. the minimal model containing only an intercept.</p></li>
<li><p>Residual deviance: the deviance computed for the model that has just been estimated.</p></li>
</ul>
<p>Note</p>
<blockquote>
<div><p>When computing deviances of different models for the same dataset, the log-likelihood of the saturated model <span class="math notranslate nohighlight">\(l_S\)</span> is constant. Therefore, statistical software (including the output from <code class="docutils literal notranslate"><span class="pre">glm</span></code>) often provides the deviance in a simplified form: as <span class="math notranslate nohighlight">\(-2 l_M\)</span>.</p>
</div></blockquote>
</div>
<div class="section" id="akaike-information-criterion">
<h5>Akaike information criterion<a class="headerlink" href="#akaike-information-criterion" title="Permalink to this headline">¶</a></h5>
<p>The Akaike information criterion (AIC) quantifies model fit as a function of the likelihood and the number of parameters being estimated. It is defined as <span class="math notranslate nohighlight">\(AIC = 2k - 2 l(\hat{\beta})\)</span> where <span class="math notranslate nohighlight">\(k\)</span> is the number of parameter in the model and <span class="math notranslate nohighlight">\(l(\hat{\beta})\)</span> the log-likelihood of the model computed at the estimated parameter values <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>The AIC is mainly used as a way to compare different models. The best model, in the scale of the AIC, is the one with the lowest AIC. (Note that sometimes, contrarily to the <code class="docutils literal notranslate"><span class="pre">glm</span></code> package, the AIC is computed as <span class="math notranslate nohighlight">\(AIC = -2k + 2l(\hat{\beta})\)</span> in which case the best model would be the one with the highest AIC value.)</p>
<p>The AIC is actually minus the sum of the deviance and twice the number of the parameters. By including the number of parameters, the AIC penalizes models that have too many parameters, thus avoiding the selection of overfitted models.</p>
</div>
<div class="section" id="mcfadden-pseudo-r-2">
<h5>McFadden pseudo-<span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#mcfadden-pseudo-r-2" title="Permalink to this headline">¶</a></h5>
<p>For the linear regression model, the coefficient of determination (<span class="math notranslate nohighlight">\(R^2\)</span>) measures how much variability is explained by the model.</p>
<p>For the logistic regression model, several generalization of the <span class="math notranslate nohighlight">\(R^2\)</span> measure have been proposed. Here, we will focus on the McFadden’s pseudo-<span class="math notranslate nohighlight">\(R^2\)</span>. The McFadden <span class="math notranslate nohighlight">\(R^2\)</span> is defined as follow:</p>
<div class="math notranslate nohighlight">
\[
R^2_{McFadden} = 1 - \frac{l_M}{l_0}
\]</div>
<p>where <span class="math notranslate nohighlight">\(l_M\)</span> is the log-likelihood of the estimated model and <span class="math notranslate nohighlight">\(l_0\)</span> is the log-likelihood of the null model (containing an intercept only).</p>
<p>The rationale behind this measure is that when the estimated model does not explain correctly the variability, its log-likelihood will be close to the null log-likelihood so that the ratio will be close to <span class="math notranslate nohighlight">\(1\)</span> and the McFadden’s pseudo-<span class="math notranslate nohighlight">\(R^2\)</span> close to <span class="math notranslate nohighlight">\(0\)</span>. Conversely, when the model correctly explains the variability of the model, the likelihood will be close to <span class="math notranslate nohighlight">\(1\)</span> and therefore <span class="math notranslate nohighlight">\(l_M\)</span> will be close to <span class="math notranslate nohighlight">\(0\)</span> so that the McFadden’s pseudo-<span class="math notranslate nohighlight">\(R^2\)</span> will be close to <span class="math notranslate nohighlight">\(1\)</span>. However, when applied to a classic linear regression model, the McFadden’s pseudo-<span class="math notranslate nohighlight">\(R^2\)</span> is not equivalent to the classic <span class="math notranslate nohighlight">\(R_2\)</span>.</p>
</div>
<div class="section" id="the-hosmer-lemeshow-test">
<h5>The Hosmer-Lemeshow test<a class="headerlink" href="#the-hosmer-lemeshow-test" title="Permalink to this headline">¶</a></h5>
<p>The Hosmer-Lemeshow test is a classic approach to assess the goodness-of-fit of a logistic regression model. The rationale of this test is to divide the vector of predicted probabilites <span class="math notranslate nohighlight">\(\hat{\pi} = (\hat{\pi}_i)\)</span> with <span class="math notranslate nohighlight">\(i=1,\dots,n\)</span> into <span class="math notranslate nohighlight">\(G\)</span> groups, e.g. based on the quantiles, with <span class="math notranslate nohighlight">\(n_g\)</span> subjects. In each group, the mean of the predicted probabilites <span class="math notranslate nohighlight">\(\bar{\pi}_g\)</span> is compared to the proportion of observed success. Formally, for the group <span class="math notranslate nohighlight">\(g=1,\dots,G\)</span>, we have that</p>
<ul class="simple">
<li><p>the observed values are</p>
<ul>
<li><p>for Y = 1: <span class="math notranslate nohighlight">\(y_g\)</span></p></li>
<li><p>for Y = 0: <span class="math notranslate nohighlight">\(n_g - y_g\)</span></p></li>
</ul>
</li>
<li><p>the predicted values are</p>
<ul>
<li><p>for Y = 1: <span class="math notranslate nohighlight">\(\bar{\pi}_gn_g\)</span></p></li>
<li><p>for Y = 0: <span class="math notranslate nohighlight">\(n_g(1 - \bar{\pi}_g)\)</span></p></li>
</ul>
</li>
</ul>
<p>The Hosmer-Lemeshow test statistics is based on the chi-square statistics computed over all groups and all possible values for <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[\sum_{g=1}^G\sum_{l=0}^1 \frac{(o_{gl} - e_{gl})^2}{e_{gl}} = \sum_{g=1}^G \frac{(n_g\bar{\pi}_g - y_g)^2}{n_g\bar{\pi}_g(1-\bar{\pi}_g)}\]</div>
<p>and has been shown to follow asymptotically a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with <span class="math notranslate nohighlight">\(g-2\)</span> degrees of freedom under the null hypothesis of a correctly specified model. However, we insist on the fact that this test if often criticized for several reasons. First it is known to have low power. Secondly, its results can be sensible to the choice of the number of groups <span class="math notranslate nohighlight">\(G\)</span> and this is even worst for small sample sizes.</p>
<p>The Hesmer-Lemeshow test statistics has not been implemented into the <code class="docutils literal notranslate"><span class="pre">glm</span></code> package but is available on the <code class="docutils literal notranslate"><span class="pre">ResourceSelection</span></code> package through the <code class="docutils literal notranslate"><span class="pre">hoslem.test</span></code> function.</p>
</div>
</div>
</div>
<span id="document-15.k. Logistic Regression"></span><div class="section" id="common-pitfalls">
<h3>15.12 Common pitfalls<a class="headerlink" href="#common-pitfalls" title="Permalink to this headline">¶</a></h3>
<div class="section" id="perfect-separation">
<h4>15.12.1 Perfect separation<a class="headerlink" href="#perfect-separation" title="Permalink to this headline">¶</a></h4>
<p>Perfect separation happens when the outcome can be directly predicted from one of the predictor variables. For example, let say that we model an outcome <span class="math notranslate nohighlight">\(Y\)</span> using one explanatory standard gaussian variable <span class="math notranslate nohighlight">\(X_1\)</span> and that <span class="math notranslate nohighlight">\(Y\)</span> is such that <span class="math notranslate nohighlight">\(Y=0\)</span> whenever <span class="math notranslate nohighlight">\(X_1\leq0\)</span> and <span class="math notranslate nohighlight">\(Y=1\)</span> whenever <span class="math notranslate nohighlight">\(X_1&gt;0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">x1</span> <span class="o">&lt;=</span> <span class="m">0</span><span class="p">)</span><span class="o">*</span><span class="m">1</span>

<span class="n">data_sep</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">x1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us try to estimate this logistic regression model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model_sep</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">y</span> <span class="o">~</span> <span class="n">x1</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data_sep</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning message:
“glm.fit: algorithm did not converge”
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning message:
“glm.fit: fitted probabilities numerically 0 or 1 occurred”
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">R</span></code> detects the perfect separation and prompts an error that states that <code class="docutils literal notranslate"><span class="pre">fitted</span> <span class="pre">probabilities</span> <span class="pre">numerically</span> <span class="pre">0</span> <span class="pre">or</span> <span class="pre">1</span> <span class="pre">occurred</span></code>. The reason of this error is that, due to the perfect separation, the maximum likelihood of the parameter <span class="math notranslate nohighlight">\(\beta_1\)</span> for the variable <span class="math notranslate nohighlight">\(X_1\)</span> cannot be estimated as its value is actually infinite. Options to consider when facing this issue include:</p>
<ul class="simple">
<li><p>removing the problematic variable from the model</p></li>
<li><p>setting <span class="math notranslate nohighlight">\(\beta_1\)</span> at an arbitrary high value and estimate the model</p></li>
<li><p>changing the model or manipulating the data</p></li>
</ul>
<p>Note that, in practice, perfect separation is not very likely to happen. However, <em>quasi_perfect</em> separation is totally possible and needs to be tackled. For more details about how to handle separation, one can read the following articles:</p>
<blockquote>
<div><p><em>Heinze, G., &amp; Schemper, M. (2002). A solution to the problem of separation in logistic regression. Statistics in Medicine</em></p>
<p><em>Firth, D. (1993). Bias Reduction of Maximum Likelihood Estimates. Biometrika</em></p>
</div></blockquote>
</div>
<div class="section" id="low-events-per-variable">
<h4>15.12.2 Low events per variable<a class="headerlink" href="#low-events-per-variable" title="Permalink to this headline">¶</a></h4>
<p>A common issue when estimating logistic regression model is the problem of the ratio between the number of events and the number of predictive variables. This ratio is known as <em>Events Per Variable</em>. When this ratio is low, it can lead to biased estimation and models with poor predictive abilities.</p>
<p>In the biomedical literature, the so-called <em>ten events per variable rule</em> is commonly used. However, we emphasize here the absence of theoretical justification and even the lack of actual evidence that this rule gives good results. If you want more information about the issues raised by this commonly used rule, you can read the following article:</p>
<blockquote>
<div><p><em>Smeden, M., de Groot, J.A., Moons, K.G. et al. (2016) No rationale for 1 variable per 10 events criterion for binary logistic regression analysis. BMC Med Res Methodol</em>.</p>
</div></blockquote>
</div>
<div class="section" id="influential-values">
<h4>15.12.3 Influential values<a class="headerlink" href="#influential-values" title="Permalink to this headline">¶</a></h4>
<p>Another aspect to take into account when estimating a logistic regression model is the presence of influential values among the observations which, as their names indicates, might have a huge impact on the estimation of the model. The Cook’s distance is a useful measure to assess how influential an observation is. It measures how much the outcome would be modifier by removing this observation from the data.</p>
<p>In, <code class="docutils literal notranslate"><span class="pre">R</span></code>, the Cook’s distance can be easily plotted and directly plotted by specifying <code class="docutils literal notranslate"><span class="pre">which</span> <span class="pre">=</span> <span class="pre">4</span></code> as an argument to the <code class="docutils literal notranslate"><span class="pre">plot</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">dementia</span> <span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;Practicals/Datasets/Dementia/dementia2.csv&quot;</span><span class="p">)</span>
<span class="n">dementia2</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">dementia</span> <span class="o">~</span> <span class="n">sex</span> <span class="o">+</span> <span class="n">age</span> <span class="o">+</span> <span class="n">bmi</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">dementia</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="nf">binomial</span><span class="p">(</span><span class="n">link</span><span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">dementia2</span><span class="p">)</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">5</span><span class="p">,</span> <span class="n">repr.plot.width</span><span class="o">=</span><span class="m">5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">dementia2</span><span class="p">,</span> <span class="n">which</span> <span class="o">=</span> <span class="m">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = dementia ~ sex + age + bmi, family = binomial(link = &quot;logit&quot;), 
    data = dementia)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1067  -0.1959  -0.1134  -0.0732   3.6917  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.783837   0.152138 -64.309  &lt; 2e-16 ***
sex          0.306798   0.033773   9.084  &lt; 2e-16 ***
age          0.098682   0.001413  69.826  &lt; 2e-16 ***
bmi         -0.025619   0.003596  -7.124 1.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 38333  on 199999  degrees of freedom
Residual deviance: 31732  on 199996  degrees of freedom
AIC: 31740

Number of Fisher Scoring iterations: 8
</pre></div>
</div>
<img alt="_images/15.k. Logistic Regression_5_1.png" src="_images/15.k. Logistic Regression_5_1.png" />
</div>
</div>
<p>As you can see from the above example, some observations seems to have higher influence than the other. However, if we look at the y-axis scale, this difference is not huge.</p>
<p>If some observations appear to have a lot of influence on the estimated regression coefficients, it is important to assess the robustness of your conclusions to these observations. This is typically done using <em>sensitivity analysis</em>, i.e. performing the analysis including and excluding the problematic observations.</p>
<p>Note that we would not recommend excluding observations from an analysis entirely just because they are influential or outlying.</p>
</div>
</div>
<span id="document-15.l. Logistic Regression"></span><div class="section" id="further-resources">
<h3>15.13 Further resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h3>
<p>Note:  resources below are for you to deepen your understanding of the subject if you wish to do so. This is entirely optional. The extensions below (conditional logistic regression, multinomial regression, ordinal logistic regression and neural networks) are not examinable. These comments are provided for your interest.</p>
<p>The following book contains a much more detailed presentation of logistic regression. Chapter 7, 8 and 11 are particularly useful.</p>
<blockquote>
<div><p>Agresti, A., Categorical Data Analysis, 3rd edition, 2012</p>
</div></blockquote>
<p>The following also provide more detail on the subjects we have covered:</p>
<blockquote>
<div><p>Hosmer D. W., Lemeshow S. and Rodney X. S. Applied logistic regression. Wiley, 2013.</p>
</div></blockquote>
<blockquote>
<div><p><em>Hosmer, D. W., Lemeshow, S. (1980) Goodness of fit tests for the multiple logistic regression model. Communications in Statistics – Theory and Methods</em></p>
</div></blockquote>
<div class="section" id="conditional-logistic-regression">
<h4>15.13.1  Conditional logistic regression<a class="headerlink" href="#conditional-logistic-regression" title="Permalink to this headline">¶</a></h4>
<p>Conditional logistic regression is specifically designed for grouped data and in particular for pair-matched studies where each group is a pair of two matched subjects.</p>
<p>To analyse a pair-matched study, a naïve idea would be to include into a logistic regression model a parameter specific to each data stratum. However, for matched studies, it would mean having as many parameters as pairs. This raised two main issues. First, each of this stratum specific parameter would be estimated only from the information contained in a unique pair, i.e. very little information. Secondly, as the number of pairs increases, the number of parameters would also increase and maximum likelihood theory would fail to provide valid estimation.</p>
<p>The idea of conditional logistic regression is to remove the dependence upon the stratum specific parameters by conditioning the probability on sufficient exposure information. This way, because the stratum specific parameters vanish from the equation, there is no violation of the assumptions underlying maximum likelihood theory  and the other model parameters can be estimated consistently using classic techniques.</p>
<p>We note that in some cases (e.g. where the matching was on measurable characteristics, such as age and sex only), standard logistic regression adjusting for the matching variables is valid.</p>
</div>
<div class="section" id="multinomial-logistic-regression">
<h4>15.13.2  Multinomial logistic regression<a class="headerlink" href="#multinomial-logistic-regression" title="Permalink to this headline">¶</a></h4>
<p>The multinomial logistic regression model is a generalization of the logistic regression model for outcomes that have more than <span class="math notranslate nohighlight">\(2\)</span> categories, <span class="math notranslate nohighlight">\(Y\in\{1,\dots,J\}\)</span> with <span class="math notranslate nohighlight">\(J\geq 2\)</span> a natural number. In this case, the conditional distribution of <span class="math notranslate nohighlight">\(Y_i\)</span> given the covariates <span class="math notranslate nohighlight">\(X_i\)</span> is the multinomial distribution. Among the <span class="math notranslate nohighlight">\(J\)</span> categories, a reference one is chosen, e.g. the first category, and for <span class="math notranslate nohighlight">\(j=2,\dots,J\)</span></p>
<div class="math notranslate nohighlight">
\[\log\left( \frac{P(Y_i=j|X_i)}{P(Y_i=1|X_i)} \right) = \beta_{0,j} + \sum_{k=1}^p \beta_{k,j}X_{i,k}\]</div>
<p>The model is estimated simultaneously for all values of <span class="math notranslate nohighlight">\(j\)</span>. For a fixed <span class="math notranslate nohighlight">\(j\)</span>, the interpretation of the parameters is similar to the logistic regression model.</p>
</div>
<div class="section" id="ordinal-logistic-regression">
<h4>15.13.3 Ordinal logistic regression<a class="headerlink" href="#ordinal-logistic-regression" title="Permalink to this headline">¶</a></h4>
<p>The ordinal logistic regression is designed for outcomes that have more than <span class="math notranslate nohighlight">\(2\)</span> categories, <span class="math notranslate nohighlight">\(Y\in\{1,\dots,J\}\)</span> with <span class="math notranslate nohighlight">\(J\geq 2\)</span>, and whose categories have an explicit ordering. In the ordinal logistic regression, the modelled quantity is <span class="math notranslate nohighlight">\(\mathrm{logit}(P(Y_i\geq j|X_i))\)</span> for <span class="math notranslate nohighlight">\(j\geq 2\)</span>. Indeed, when <span class="math notranslate nohighlight">\(j=1\)</span>, t
<span class="math notranslate nohighlight">\(P(Y\geq 1)=1\)</span> and does not need to be modelled. As the categories are ordered, a fundamental assumption made by ordinal logistic regression is that the effect of the covariates are homogenous between the different categories. Therefore, the model is written</p>
<div class="math notranslate nohighlight">
\[\mathrm{logit}(P(Y_i\geq j|X_i)) = \beta_{0j} + \sum_{k=1}^n \beta_{k}X_{i,k}\]</div>
<p>where only the intercept term depends upon the category. However, it is important to carefully check for violations of this assumption.</p>
</div>
<div class="section" id="neural-networks">
<h4>15.13.4 Neural networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h4>
<p>Artificial neural networks are a class of model widely used for data classification in machine learning. Artificial neural networks are at the heart of <em>deep learning</em> methods used to develop computer vision, speech recognition, audio recognition, etc. Actually, the basic logistic regression model happens to be a special case of artificial neural network. If you are interested in this subject and want to have more insight on the relation between these two models, you might want to read the following article:</p>
<blockquote>
<div><p>Dreiseitl, S., &amp; Ohno-Machado, L. (2002). Logistic regression and artificial neural network classification models: a methodology review. Journal of Biomedical Informatics</p>
</div></blockquote>
</div>
</div>
<span id="document-15.m. Logistic Regression"></span><div class="section" id="additional-reading">
<h3>15.14 Additional reading<a class="headerlink" href="#additional-reading" title="Permalink to this headline">¶</a></h3>
<p>The following notes sketch out the statistical theory underlying confidence interval construction and hypothesis testing using the log-likelihood ratio. This material is not examinable.</p>
<p>These notes are intended to draw connections between the previous material surrounding maximum likelihood and the material concerning frequentist inference.</p>
<div class="section" id="likelihood-for-simple-logistic-regression">
<h4>15.14.1 Likelihood for simple logistic regression<a class="headerlink" href="#likelihood-for-simple-logistic-regression" title="Permalink to this headline">¶</a></h4>
<p><strong>Data:</strong> We have a sample of <span class="math notranslate nohighlight">\(n\)</span> binary observations <span class="math notranslate nohighlight">\(y_1, ..., y_n\)</span>. We will consider a very simple situation, with no covariates of interest.</p>
<p><strong>Statistical model:</strong> We assume our sample arises from <span class="math notranslate nohighlight">\(n\)</span> independent variables <span class="math notranslate nohighlight">\(Y_1, ..., Y_n\)</span>, with <span class="math notranslate nohighlight">\(Y_i \sim Bernoulli(\pi)\)</span>. Our logistic regression model is:</p>
<div class="math notranslate nohighlight">
\[
logit(\pi) = \beta
\]</div>
<p><strong>Likelihood:</strong> Following the notes in the main text, we can obtain the likelihood function:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
L(\beta)  = e^{k \beta}  \times \left(\frac{1}{1 + e^{\beta}} \right)^n
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(k = \sum_i y_i\)</span>.</p>
<p><strong>Log-likelihood:</strong> Taking the log of the above likelihood, we derive the following log-likelihood</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
l(\beta) = k \beta   - n log \left(1 + e^{\beta} \right) 
\end{align*}
\]</div>
<p><strong>Likelihood ratio:</strong> This is the likelihood function divided through by the likelihood function evaluated at its maximum point (i.e. at the maximum likelihood estimator, <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>. Therefore, this is simply the likelihood scaled to have a maximum of 1:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
LR = \frac{L(\beta)}{L(\hat{\beta})}  = \frac{e^{k \beta} \left/ \left(1 + e^{\beta} \right. \right)^{n}}{e^{k \hat{\beta}}  \left/ \left(1 + e^{\hat{\beta}} \right.  \right)^{n}}
\end{align*}
\]</div>
<p><strong>Log likelihood ratio:</strong> The log of the likelihood ratio is:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
llr(\beta) = k \beta   - n log \left(1 + e^{\beta} \right)  - \left\{ k \hat{\beta}   - n log \left(1 + e^{\hat{\beta}} \right)  \right\}
\end{align*}
\]</div>
<p><strong>Maximum likelihood estimate:</strong> We take the derivative of the log-likelihood and evaluate it at zero to obtain the maximum likelihood estimator, <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\frac{d l(\beta)}{d \beta} = \frac{d}{d \beta} \left\{ k \beta   - n log \left(1 + e^{\beta} \right)  \right\} = k - n \frac{e^{\beta}}{\left(1 + e^{\beta} \right)}
\end{align*}
\]</div>
<p>Setting this to zero gives</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\hat{\beta} = log\left( \frac{\bar{y}}{1 - \bar{y}} \right) 
\end{align*}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{y} = k/n\)</span> is the sample proportion of successes.</p>
<p>The figure below shows the Likelihood function and the log-likelihood at the top and the likelihood ratio and log-likelihood ratio on the bottom. We see that all four have a maximum at the same value of <span class="math notranslate nohighlight">\(\beta\)</span>. The two graphs on the log scale (on the right hand side) are flatter and more symmetric. The likelihood ratio is simply the likelihood but scaled so the maximum value is 1. The log-likelihood ratio is scaled so the maximum value is 0.</p>
<p>The code is suppressed to focus on the output but you can click to see the code.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Observed data </span>
<span class="n">n</span><span class="o">&lt;-</span> <span class="m">20</span>
<span class="n">y</span><span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">)</span>

<span class="c1"># MLE</span>
<span class="n">k</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">ybar</span> <span class="o">&lt;-</span> <span class="n">k</span><span class="o">/</span><span class="n">n</span>
<span class="n">beta_hat</span> <span class="o">&lt;-</span> <span class="nf">log</span><span class="p">(</span><span class="n">ybar</span><span class="o">/</span><span class="p">(</span><span class="m">1</span> <span class="o">-</span> <span class="n">ybar</span><span class="p">))</span>

<span class="c1"># Set range of betas over which to evaluate functions</span>
<span class="n">beta</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">-1.5</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="m">0.004</span><span class="p">)</span>

<span class="c1"># Likelihood and loglikelihoods function</span>
<span class="n">L</span> <span class="o">&lt;-</span> <span class="nf">exp</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span><span class="o">^</span><span class="n">n</span>
<span class="n">l</span> <span class="o">&lt;-</span> <span class="n">beta</span><span class="o">*</span><span class="n">k</span> <span class="o">-</span> <span class="n">n</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="m">1</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>

<span class="c1"># Likelihood ratio and log likelihood ratio</span>
<span class="n">LR</span> <span class="o">&lt;-</span> <span class="n">L</span><span class="o">/</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">beta_hat</span><span class="o">*</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">))</span><span class="o">^</span><span class="n">n</span><span class="p">)</span>
<span class="n">llr</span> <span class="o">&lt;-</span> <span class="n">l</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta_hat</span><span class="o">*</span><span class="n">k</span> <span class="o">-</span> <span class="n">n</span><span class="o">*</span><span class="nf">log</span><span class="p">(</span><span class="m">1</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">)))</span>


<span class="c1"># Graph the four functions</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">))</span> 

<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Likelihood&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;L(beta)&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Log-likelihood&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;l(beta)&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">LR</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Likelihood ratio&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;LR(beta)&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">llr</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Log-likelihood ratio&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;llr(beta)&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15.m. Logistic Regression_3_0.png" src="_images/15.m. Logistic Regression_3_0.png" />
</div>
</div>
</div>
<div class="section" id="confidence-intervals-based-on-the-likelihood">
<h4>15.14.2 Confidence intervals based on the likelihood<a class="headerlink" href="#confidence-intervals-based-on-the-likelihood" title="Permalink to this headline">¶</a></h4>
<p>Remember that the likelihood is a measure of how consistent the different values of the parameter are with the observed data. The most consistent value is at the maximum, i.e. the maximum likelihood estimator. We can also see that values with a much lower likelihood are much less consistent with the data.</p>
<p>This suggests the idea of obtaining a confidence interval by taking all values that have a likelihood within a certain range of the maximum.</p>
<p>In fact, when we have a single parameter of interest (which we will call <span class="math notranslate nohighlight">\(\beta_0\)</span>) then it turns out that for an independent sample (under a number of “regularity” conditions not stated here), we have the following asymptotic distribution:</p>
<div class="math notranslate nohighlight">
\[
-2 llr(\beta_0) = -2 (l(\beta_0) - l(\hat{\beta})) \sim \chi^2_1    \qquad \text{as} \ n \rightarrow \infty
\]</div>
<p>A <span class="math notranslate nohighlight">\(\chi^2_1\)</span> distribution has 5% of the distribution above the value 3.84. Therefore, this means that</p>
<div class="math notranslate nohighlight">
\[
P(-2 llr(\beta_0) \geq 3.84) = 0.95 \rightarrow P(llr(\beta_0) \geq  -1.92) = 0.95
\]</div>
<p>leading to the 95% confidence interval of all values of <span class="math notranslate nohighlight">\(\beta\)</span> that have a log-likelihood ratio at most 1.92 units lower than the maximum:</p>
<div class="math notranslate nohighlight">
\[
\{ \beta s.t. l(\beta) - l(\hat{\beta}) \geq -1.92\}
\]</div>
<p>The plot below shows the line -1.92. Our 95% confidence interval is formed by all values of <span class="math notranslate nohighlight">\(\beta\)</span> which have a log-likelihood falling above this line. This is approximately the iterval: (-1.34, 0.48). The MLE and confidnence limits are shown in orange dashed lines.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">))</span> 
<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">llr</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;CI from log-likelihood ratio&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;llr(beta)&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">-1.92</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>

<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">-1.34</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.48</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15.m. Logistic Regression_5_0.png" src="_images/15.m. Logistic Regression_5_0.png" />
</div>
</div>
</div>
<div class="section" id="quadratic-approximation">
<h4>15.14.2 Quadratic approximation<a class="headerlink" href="#quadratic-approximation" title="Permalink to this headline">¶</a></h4>
<p>There is often no closed form solution to obtain the exact values at which the log-likelihood ratio takes value -1.92. An often simpler approach is to work instead with a quadratic approximation to the log-likelihood ratio, for which there is a simple closed-form solution.</p>
<p>We will now make a quadratic approximation to the log likelihood ratio. In the plot above, we see that this graph is not quite symmetric but looks fairly quadratic near the maximum.</p>
<p>To obtain our quadratic approximation, we will look for a function of the (quadratic) form:</p>
<div class="math notranslate nohighlight">
\[
f(\beta) = -\frac{1}{2} \left( \frac{\beta - M}{S} \right)^2
\]</div>
<p>We want our quadratic approximation to</p>
<ul class="simple">
<li><p>have the same maximum</p></li>
<li><p>have the same curvature near the maximum</p></li>
</ul>
<p>The first condition above means that we need <span class="math notranslate nohighlight">\(f(\hat{\beta}) = 0\)</span>. This fixes <span class="math notranslate nohighlight">\(M = \hat{\beta}\)</span>.</p>
<p>The second condition means that we need the second derivatives of <span class="math notranslate nohighlight">\(f(\beta)\)</span> and <span class="math notranslate nohighlight">\(llr(\beta)\)</span> to be equal at the MLE, <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, since curvature is measured by the second derivative. In fact, we will consider making the curvature (second derivatives) of <span class="math notranslate nohighlight">\(f(\beta)\)</span> and <span class="math notranslate nohighlight">\(l(\beta)\)</span> to be equal at the MLE, since this is algebraically a little simpler. From the plots above we can see that the curvature of <span class="math notranslate nohighlight">\(l(\beta)\)</span> and <span class="math notranslate nohighlight">\(llr(\beta)\)</span> are identical.</p>
<p>Differentiating <span class="math notranslate nohighlight">\(f(\beta)\)</span> twice shows that <span class="math notranslate nohighlight">\(f''(\beta) = -1/S^2\)</span> for any value of <span class="math notranslate nohighlight">\(\beta\)</span>. Thus we set</p>
<div class="math notranslate nohighlight">
\[
S^2 = -\frac{1}{l''(\hat{\beta})}
\]</div>
<p>It also turns out that the resulting value for <span class="math notranslate nohighlight">\(S\)</span> is also the standard error of <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, i.e. <span class="math notranslate nohighlight">\(S = SE(\hat{\beta})\)</span>.</p>
<p>This gives us our required quadratic approximation to the log-likelihood ratio:</p>
<div class="math notranslate nohighlight">
\[
f(\beta) = -\frac{1}{2} \left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \qquad  \text{with SE obtained as: } SE^2 = -\frac{1}{l''(\hat{\beta})}
\]</div>
<div class="section" id="example">
<h5>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h5>
<p>Returning to our example, we take the second derivative of the log-likelihood to obtain <span class="math notranslate nohighlight">\(S\)</span>. First, we have already taken the first derivative to obtain our MLE:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\frac{d l(\beta)}{d \beta} = \frac{d}{d \beta} \left\{ k \beta   - n log \left(1 + e^{\beta} \right)  \right\} = k - n \frac{e^{\beta}}{\left(1 + e^{\beta} \right)}
\end{align*}
\]</div>
<p>Taking the derivative of this, we get:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\frac{d^2 l(\beta)}{d \beta^2} = \frac{d}{d \beta} \left\{ k - n \frac{e^{\beta}}{\left(1 + e^{\beta} \right)} \right\} = -\frac{n e^\beta}{\left(1 + e^{\beta} \right)^2}
\end{align*}
\]</div>
<p>Thus we set</p>
<div class="math notranslate nohighlight">
\[
S^2 = -\frac{1}{l''(\hat{\beta})} =  \frac{\left(1 + e^{\hat{\beta}} \right)^2}{n e^\hat{\beta}}
\]</div>
<p>The figure below shows the log likelihood ratio and the quadratic approximation.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain S (i.e. the SE(beta_hat))</span>
<span class="n">SE</span> <span class="o">&lt;-</span> <span class="nf">sqrt</span><span class="p">((</span><span class="n">n</span><span class="o">*</span><span class="nf">exp</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="m">1</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">))</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">-1</span><span class="p">))</span>

<span class="c1"># Obtain quadratic approximation to log-likelihood ratio:</span>
<span class="n">f</span><span class="o">&lt;-</span> <span class="m">-1</span><span class="o">/</span><span class="m">2</span><span class="o">*</span><span class="p">((</span><span class="n">beta</span> <span class="o">-</span> <span class="n">beta_hat</span><span class="p">)</span><span class="o">/</span><span class="n">SE</span><span class="p">)</span><span class="o">^</span><span class="m">2</span> 

<span class="c1"># Plot</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">))</span> 
<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">llr</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Quadratic approxmiation (green) to \n log-likelihood ratio (blue)&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;llr(beta)&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">)</span>

<span class="c1"># indicate MLE</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="c1"># indicate log likelihood ratio confidence interval</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">-1.92</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">-1.34</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.48</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15.m. Logistic Regression_7_0.png" src="_images/15.m. Logistic Regression_7_0.png" />
</div>
</div>
<p>The quadratic approximation is very good near to the maximum. The horizontal red line indicates the 95% confidence interval obtained using the log likelihood ratio. The quadratic approximation starts to deviate from the log likelihood ratio at that point, but not by much. overall, this plot suggests that the quadratic approximation will provide us with a 95% confidence interval very close to the one obtained directly from the log likelihood ratio.</p>
</div>
</div>
<div class="section" id="id1">
<h4>15.14.3 Quadratic approximation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>If our quadratic approximation is a good approximation to the log-likelihood ratio then we will have, approximately</p>
<div class="math notranslate nohighlight">
\[
-2 f(\beta) \sim \chi^2_1
\]</div>
<p>Thus</p>
<div class="math notranslate nohighlight">
\[
-2 f(\beta) = -2 \times -\frac{1}{2} \left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \sim \chi^2_1
\]</div>
<p>In other words,</p>
<div class="math notranslate nohighlight">
\[
\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \sim \chi^2_1
\]</div>
<p>A <span class="math notranslate nohighlight">\(\chi^2_1\)</span> distribution has 5% of the distribution above the value 3.84, so</p>
<div class="math notranslate nohighlight">
\[
P\left(\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \geq 3.84\right)  = 0.95
\]</div>
<p>Noticing that <span class="math notranslate nohighlight">\(\sqrt{3.84} = 1.96\)</span>, this gives the 95% confidence interval:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta} \pm 1.96 \times SE(\hat{\beta})
\]</div>
<p>This is sometimes called a <strong>Wald-type</strong> confidence interval.</p>
<p>The plot below graphs the interval constructed in this way (indicated by the purple dashed lines), along with the previous  confidence interval calculated from the log-likelihood ratio.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">))</span> 
<span class="nf">plot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">llr</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Two types of 95% CI&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;llr(beta)&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;green&quot;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;llr(beta)&quot;</span><span class="p">)</span>

<span class="c1"># indicate MLE</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">beta_hat</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>

<span class="c1"># indicate log likelihood ratio confidence interval</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">-1.92</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">-1.34</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.48</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>

<span class="c1"># indicate Wald-type confidence interval (from quadratic approximation)</span>
<span class="n">lower_lim</span> <span class="o">&lt;-</span> <span class="n">beta_hat</span> <span class="o">-</span> <span class="m">1.96</span><span class="o">*</span><span class="n">SE</span>
<span class="n">upper_lim</span> <span class="o">&lt;-</span> <span class="n">beta_hat</span> <span class="o">+</span> <span class="m">1.96</span><span class="o">*</span><span class="n">SE</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">lower_lim</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">upper_lim</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15.m. Logistic Regression_10_0.png" src="_images/15.m. Logistic Regression_10_0.png" />
</div>
</div>
<p>In the plot above, the two confidence intervals are very similar.</p>
</div>
<div class="section" id="hypothesis-testing">
<h4>15.14.4 Hypothesis testing<a class="headerlink" href="#hypothesis-testing" title="Permalink to this headline">¶</a></h4>
<p>Suppose we wish to test the null hypothesis:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta = \beta_0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \beta \neq \beta_0\)</span></p></li>
</ul>
<p><strong>Likelihood ratio test</strong></p>
<p>Under the null hypothesis,</p>
<div class="math notranslate nohighlight">
\[
-2 llr(\beta_0) \sim \chi^2_1
\]</div>
<p>Tests can be based on this distribution, giving a p-value.</p>
<p><strong>Wald test</strong></p>
<p>Using the quadratic approximation,</p>
<div class="math notranslate nohighlight">
\[
\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)^2  \sim \chi^2_1
\]</div>
<p>Or, equivalently:</p>
<div class="math notranslate nohighlight">
\[
\left( \frac{\beta - \hat{\beta}}{SE(\hat{\beta})} \right)  \sim N(0,1)
\]</div>
<p>This is exactly the form of hypothesis tests we encountered in the session about hypothesis testing.</p>
</div>
<div class="section" id="additional-comments">
<h4>15.14.5 Additional comments<a class="headerlink" href="#additional-comments" title="Permalink to this headline">¶</a></h4>
<p>Notes</p>
<blockquote>
<div><p>We often construct confidence intervals on the log scale (e.g. the log odds ratios). Confidence intervals based on the log likelihood ratio are transformation invariant, but Wald-type intervals are not. Often, basing calculations on a log scale improves the approximations made above. <br><br>
We have focused on situations with a single unknown parameter. With more than one unknown parameter, things are a little more complex. The profile likelihood, which treats some parameters as “nuisance” parameters and <em>removes</em> them from the likelihood, using a process called profiling, is beyond the scope of these notes. The fundamental principles remain the same.</p>
</div></blockquote>
</div>
</div>
</div>
</div>
<span id="document-16.a. Generalised Linear Model (GLM)"></span><div class="section" id="generalised-linear-models-glms">
<h2>16. Generalised Linear Models (GLMs)<a class="headerlink" href="#generalised-linear-models-glms" title="Permalink to this headline">¶</a></h2>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session, you will be able to:</p>
<ul class="simple">
<li><p>explain what Generalised Linear Model is;</p></li>
<li><p>describe the role of a link function;</p></li>
<li><p>apply a GLM to Poisson distributed data and evaluate the findings;</p></li>
<li><p>demonstrate how to explore the goodness of fit.</p></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<span id="document-16.b. Generalised Linear Model (GLM)"></span><div class="section" id="introduction-to-generalised-linear-models-glms">
<h3>16.1 Introduction to Generalised Linear Models (GLMs)<a class="headerlink" href="#introduction-to-generalised-linear-models-glms" title="Permalink to this headline">¶</a></h3>
<p>The term Generalised Linear Model (GLM) refers to a large class of models popularised by McCullagh and Nelder in 1982. It should not be confused with the similarly named method, General Linear Model (which was covered in sessions 12 to 14).</p>
<p>GLMs can be seen as a extension to the familiar regression models you have already been introduced to in previous chapters. GLM allows for the outcome variable to have an error distribution other than the normal distribution. The name comes from the method which generalises linear regression by allowing the linear model to be related to the outcome variable via something called a link function. This means that GLMs can model outcomes with distributions in the exponential family with a link function which varies linearity with the predictors (covariates) rather than assuming the outcome itself must vary linearly.</p>
</div>
<span id="document-16.c. Generalised Linear Model (GLM)"></span><div class="section" id="generalised-linear-model-components">
<h3>16.2 Generalised Linear Model Components<a class="headerlink" href="#generalised-linear-model-components" title="Permalink to this headline">¶</a></h3>
<p>A generalised linear model consists of three components:</p>
<div class="section" id="a-random-component">
<h4>A random component<a class="headerlink" href="#a-random-component" title="Permalink to this headline">¶</a></h4>
<p>This refers to the probability distribution of the outcome variable <span class="math notranslate nohighlight">\(Y_{i}\)</span> (for the ith of n independently sampled observations).  It specifies the conditional distribution of the outcome given the values of the predictors (covariates) in the model. <span class="math notranslate nohighlight">\(Y_{i}\)</span> is generally formulated as distribution from the exponential family, however subsequent work has extended GLMs to multivariate exponential families, to certain non-exponential families and to also to situations where the distribution of <span class="math notranslate nohighlight">\(Y_{i}\)</span> is not completely specified. Within this chapter we will only explore the application to distributions from the exponential family (i.e. Normal, Gamma, Poisson, Bernoulli etc.)</p>
</div>
<div class="section" id="a-systematic-component-the-linear-predictor">
<h4>A systematic component  (the linear predictor)<a class="headerlink" href="#a-systematic-component-the-linear-predictor" title="Permalink to this headline">¶</a></h4>
<p>This is the linear function of the predictors (covariates) in the model</p>
<div class="math notranslate nohighlight">
\[
\eta_{i} = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}
\]</div>
<p>Just as in  linear and logistic regression models, the predictors (covariates) <span class="math notranslate nohighlight">\(X_{ij}\)</span> may be continuous and/or categorical.</p>
</div>
<div class="section" id="a-link-function">
<h4>A link function<a class="headerlink" href="#a-link-function" title="Permalink to this headline">¶</a></h4>
<p>This function transforms the expectation of the predictors (covariates) to be linear with the outcome variable.</p>
<p>Suppose we let <span class="math notranslate nohighlight">\(\mu_{i} = E[Y_{i}]\)</span>. Then the <em>link function</em> is a function <span class="math notranslate nohighlight">\(g(.)\)</span> with</p>
<div class="math notranslate nohighlight">
\[
g(\mu_{i} ) = \eta_{i} 
\]</div>
<p>Or in other words,</p>
<div class="math notranslate nohighlight">
\[
g(\mu_{i} ) = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}
\]</div>
<p>This can be inverted so</p>
<div class="math notranslate nohighlight">
\[
\mu_{i}  = g^{-1}(\eta_{i}) = g^{-1}(\beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}).
\]</div>
<p>The inverse link <span class="math notranslate nohighlight">\(g^{-1}(.)\)</span> is often called the <em>mean function</em> as it  gives the expected value of the outcome.</p>
</div>
<div class="section" id="an-example-logistic-regression">
<h4>16.2.1 An example - logistic regression<a class="headerlink" href="#an-example-logistic-regression" title="Permalink to this headline">¶</a></h4>
<p>Suppose we wish to fit a logistic regression model (which we will see is one particular type of GLM) for a binary outcome <span class="math notranslate nohighlight">\(Y\)</span> and a single covariate <span class="math notranslate nohighlight">\(X\)</span>. Normally we write <span class="math notranslate nohighlight">\(\pi_i\)</span> for the expected outcome but here we will use <span class="math notranslate nohighlight">\(\mu_i\)</span> instead, so you can see how the model we had previously connects with the more general notation above. Thus, we let <span class="math notranslate nohighlight">\(\mu_i = E[Y_i]\)</span> (=<span class="math notranslate nohighlight">\(\pi_i\)</span> in previous sessions). We have:</p>
<div class="math notranslate nohighlight">
\[
Y_i \overset{iid}{\sim} Bernouilli(\mu_i), \qquad \text{for} \ i=1, 2, ..., n
\]</div>
<p>The linear predictor is given by:</p>
<div class="math notranslate nohighlight">
\[
\eta_i = \beta_0 + \beta_1 X_i 
\]</div>
<p>The link function can be defined generically. In the equation below, <span class="math notranslate nohighlight">\(z\)</span> has no intrinsic meaning; it is just used here to enable us to define a function. The link function for logistic regression is the logit function:</p>
<div class="math notranslate nohighlight">
\[
g(z) = log \left\{ \frac{z}{1-z} \right\}
\]</div>
<p>Setting this equal to <span class="math notranslate nohighlight">\(\eta_i\)</span>, as per the definition above, we get:</p>
<div class="math notranslate nohighlight">
\[
g(\mu_i) = log \left\{ \frac{\mu_i}{1-\mu_i} \right\} = \beta_0 + \beta_1 X_i 
\]</div>
<p>which is the logistic regression model we met previously.</p>
</div>
</div>
<span id="document-16.d. Generalised Linear Model (GLM)"></span><div class="section" id="glm-assumptions">
<h3>16.3 GLM Assumptions<a class="headerlink" href="#glm-assumptions" title="Permalink to this headline">¶</a></h3>
<p>To successful apply a GLM, a number of assumptions about the data must be met.</p>
<ol class="simple">
<li><p>The data must be independently distributed.</p></li>
<li><p>The outcome variable <span class="math notranslate nohighlight">\(Y_{i}\)</span> does not have to be normally distributed but should typically form a distribution from the exponential family</p></li>
<li><p>GLM’s must assume a linear relationship between the transformed outcome in terms of the link function and the predictor (covariate) variables.</p></li>
<li><p>The homogeneity of variance does not need to be satisfied. Generally the model structure, and overdispersion (when the observed variance is larger than what the model assumes) can be present.</p></li>
<li><p>Errors need to be independent but not normally distributed.</p></li>
<li><p>GLM’s use maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations.</p></li>
</ol>
</div>
<span id="document-16.e. Generalised Linear Model (GLM)"></span><div class="section" id="link-functions">
<h3>16.4 Link Functions<a class="headerlink" href="#link-functions" title="Permalink to this headline">¶</a></h3>
<p>The link function provides the relationship between the systematic component and the mean of the distribution. There are many commonly used link functions, the table below lists only three examples with their distributions and mean functions. Here we use matrix notation where <span class="math notranslate nohighlight">\(\mu_{i}  = \beta_{0} + \beta_{1}X_{i1} + \beta_{2}X_{i2} + … + \beta_{k}X_{ik}\)</span>  is represented by <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{\beta}\)</span>.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Distribution</p></th>
<th class="text-align:left head"><p>Data</p></th>
<th class="text-align:left head"><p>Link Name</p></th>
<th class="text-align:left head"><p>Link function</p></th>
<th class="text-align:left head"><p>Mean function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Normal</p></td>
<td class="text-align:left"><p>real: (-<span class="math notranslate nohighlight">\(\infty\)</span> , + <span class="math notranslate nohighlight">\(\infty\)</span>)</p></td>
<td class="text-align:left"><p>Identity <span class="math notranslate nohighlight">\(     \)</span></p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{\beta} =\mu\)</span></p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\( \mu = \mathbf{X}\mathbf{\beta} \)</span>  <span class="math notranslate nohighlight">\(     \)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Poisson</p></td>
<td class="text-align:left"><p>integer: 0,1,2,…</p></td>
<td class="text-align:left"><p>Log</p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{\beta} =ln( \mu)\)</span></p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\( \mu = exp(\mathbf{X}\mathbf{\beta} )\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Binomial</p></td>
<td class="text-align:left"><p>integer: 0,1,2,…N</p></td>
<td class="text-align:left"><p>Logit</p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{\beta}=ln(\frac{\mu}{n-\mu})\)</span></p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\( \mu =\frac{exp(\mathbf{X}\mathbf{\beta})}{1 + exp(\mathbf{X}\mathbf{\beta}} \)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Gamma</p></td>
<td class="text-align:left"><p>real: (0, + <span class="math notranslate nohighlight">\(\infty\)</span>)</p></td>
<td class="text-align:left"><p>Negative Inverse</p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{\beta} = -\mu^{-1}\)</span></p></td>
<td class="text-align:left"><p><span class="math notranslate nohighlight">\( \mu = - (\mathbf{X}\mathbf{\beta} )^{-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>It is important to note that both linear regression which is covered in sessions 12 to 14 and logistic regression in sessions 15 can be reproduced through a GLM.</p>
<p>Recall that a linear regression assumes data is  normal distributed so using the identity link function for a normal distribution within the GLM framework will give the same estimated regression coefficients. However, the inference (p-values and confidence intervals) is slightly better using ordinary least squares compared to than maximum likelihood estimation thus we prefer to fit linear regression models using OLS.</p>
<p>In logistic regression if you use the logit function for a binomial family (recalling that Bernoulli is a special type of binomial distribution)  you will be able to reproduce the same results as obtained through standard logistic regression modelling. For binary outcomes, the GLM has the extra flexibility compared to the logistic regression model. You can also use other link functions, for example the Probit, the Log-Log and the Complementary log-log functions. These will give similar results but adjust for slight differences from data collection situations to improve the transformation of the expectation of the outcome to the systematic component. In this module we only focus on the logit link, however if you wish to explore further, more information can be found here: <a class="reference external" href="https://aip.scitation.org/doi/pdf/10.1063/1.5139815">https://aip.scitation.org/doi/pdf/10.1063/1.5139815</a></p>
</div>
<span id="document-16.f. Generalised Linear Model (GLM)"></span><div class="section" id="programming-glm-s-in-r">
<h3>16.5 Programming GLM’s in R<a class="headerlink" href="#programming-glm-s-in-r" title="Permalink to this headline">¶</a></h3>
<p>To fit a GLM in R you will need to use the glm() function where we tell R what the distribution of the errors and linear predictor should be.</p>
<p>The function syntax is as followed:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>glm(formula, family = gaussian, data, weights, subset,
na.action, start = NULL, etastart, mustart, offset,
control = list(…), model = TRUE, method = &quot;glm.fit&quot;,
x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, …)
</pre></div>
</div>
<p>The minimal inputted parameters required is the <em>formula</em> and the <em>family</em>. The <em>formula</em> tells R in a symbolic description what model is required to be fitted and the <em>family</em> describes what error distribution and link function are required.</p>
</div>
<span id="document-16.g. Generalised Linear Model (GLM)"></span><div class="section" id="introduction-to-poisson-generalised-linear-modelling-poisson-regression">
<h3>16.6 Introduction to Poisson Generalised Linear Modelling (Poisson Regression)<a class="headerlink" href="#introduction-to-poisson-generalised-linear-modelling-poisson-regression" title="Permalink to this headline">¶</a></h3>
<p>In the previous session we met an important type of GLM - logistic regression. Now we will now consider another important GLM - Poisson regression.</p>
<div class="section" id="poisson-distribution-recap">
<h4>16.6.1 Poisson Distribution Recap<a class="headerlink" href="#poisson-distribution-recap" title="Permalink to this headline">¶</a></h4>
<p>The Poisson distribution was first published by Siméon Denis Poisson in 1838. Poisson was a French mathematician, engineer, and physicist, his name is one of 72 engraved on the Eiffel Tower in Paris. The Poisson distribution is a skewed, discrete distribution restricted to non-negative numbers. The shape of the distribution is defined by the shape parameter <span class="math notranslate nohighlight">\(\lambda\)</span> which represents the average number of events in the given time interval. As <span class="math notranslate nohighlight">\(\lambda\)</span> increases the distribution looks more and more like the normal distribution. When <span class="math notranslate nohighlight">\(\lambda\)</span> is about 10 or greater, then a normal distribution is a good approximation.</p>
</div>
<div class="section" id="why-can-t-we-just-use-ordinary-linear-regression">
<h4>16.6.2 Why can’t we just use Ordinary Linear Regression?<a class="headerlink" href="#why-can-t-we-just-use-ordinary-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>One of the main assumptions required for fitting an ordinary linear regression (OLR) is that the residual errors must follow a normal distribution. For this to be achieved with data from a skewed distribution, a transformation must be applied however with discrete data this can be very problematic (making the interpretation of the findings unfeasibly difficult) or impossible (for example, a high number of 0’s could prevent normality from being achieved). Another issue is that an OLR has the ability to create negative predicted values which would be theoretically impossible. For these reasons it is better to apply a method which actually reflects the natural distribution instead of trying to make the distribution reflect the method. This is why a Poisson regression is generally more suited to count data than OLR.</p>
</div>
<div class="section" id="poisson-regression">
<h4>16.6.3 Poisson Regression<a class="headerlink" href="#poisson-regression" title="Permalink to this headline">¶</a></h4>
<p>A GLM for Poisson distributed outcome is commonly known as Poisson regression but is sometimes referred to as a log-linear model.</p>
<div class="section" id="random-component">
<h5>Random component<a class="headerlink" href="#random-component" title="Permalink to this headline">¶</a></h5>
<p>Supppose we have an outcome <span class="math notranslate nohighlight">\(Y\)</span> representing counts of events over a fixed time period <span class="math notranslate nohighlight">\(T\)</span>. For simplicity, we will let <span class="math notranslate nohighlight">\(T=1\)</span>, i.e. we have followed our individuals up for a single unit of time (e.g. one year).</p>
<p>Suppose we are happy to assume that <span class="math notranslate nohighlight">\(Y\)</span> follows a Poisson distribution. We wish to model the relationship between a vector of <span class="math notranslate nohighlight">\(p\)</span> covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and the expectation of <span class="math notranslate nohighlight">\(Y\)</span> (and therefore also the variance of <span class="math notranslate nohighlight">\(Y\)</span>, since the mean is equal to the variance for a Poisson variable).</p>
<p>We let <span class="math notranslate nohighlight">\(\mu = E[Y | X]\)</span> and assume:</p>
<div class="math notranslate nohighlight">
\[ 
\mathbf{Y} \overset{iid}{\sim} Poisson(\mu)
\]</div>
</div>
<div class="section" id="systematic-component-linear-predictor">
<h5>Systematic component (linear predictor)<a class="headerlink" href="#systematic-component-linear-predictor" title="Permalink to this headline">¶</a></h5>
<p>The linear predictor is a linear function of the covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + .... + \beta_p X_p 
\]</div>
<p>Using vector notation to simplify the maths, this is equivalent to:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}^T\mathbf{\beta}
\]</div>
<p>(Note that we assume the vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> contains a constant, so <span class="math notranslate nohighlight">\(\mathbf{X}^T = (1, X_1, X_2, ..., X_p)\)</span>.</p>
</div>
<div class="section" id="link-function">
<h5>Link function<a class="headerlink" href="#link-function" title="Permalink to this headline">¶</a></h5>
<p>We want an equation that connects the expected outcome <span class="math notranslate nohighlight">\(\mu\)</span> (which must be positive) to the linear predictor <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{\beta}\)</span>  (which can be any real value, positive or negative). The link function is applied to the expected outcome, therefore in this case an appropriate link function must map the positive real values to all real values. An obvious candidate is the natural logarithm. This is the default (often called the <em>canonical</em>) link function for Poisson variables. Then, applying the link function to the expected value of the outcome, we have:</p>
<div class="math notranslate nohighlight">
\[
ln(\mu) = \mathbf{X}^T\mathbf{\beta}
\]</div>
<p>This is the <strong>Poisson regression</strong> model.</p>
</div>
<div class="section" id="regression-coefficients">
<h5>Regression coefficients<a class="headerlink" href="#regression-coefficients" title="Permalink to this headline">¶</a></h5>
<p>In the model above, <span class="math notranslate nohighlight">\(\beta\)</span> is a vector of regression coefficients. An element of <span class="math notranslate nohighlight">\(\beta\)</span> represents the expected change in the natural <span class="math notranslate nohighlight">\(log\)</span> of the mean per unit change of one explanatory variable in <span class="math notranslate nohighlight">\(X\)</span> (constraining the other elements to not change).</p>
<p>We can interpret <span class="math notranslate nohighlight">\(\mu\)</span> as the expected rate of the outcome (remember we assume the fixed time period being considered is <span class="math notranslate nohighlight">\(T=1\)</span>). Consider a simpler example with a single binary covariate <span class="math notranslate nohighlight">\(X\)</span>. Then the model above becomes</p>
<div class="math notranslate nohighlight">
\[
ln(\mu) = \beta_0 + \beta_1 X
\]</div>
<p>This says that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
ln(\mu) &amp;=
\begin{cases} 
\beta_0 &amp;\mbox{if } X=0 \\
\beta_0 + \beta_1  &amp;\mbox{if } X=1 \\
\end{cases}
\end{align*}
\end{split}\]</div>
<p>The expeted rates in the two groups <span class="math notranslate nohighlight">\((X=0)\)</span> and <span class="math notranslate nohighlight">\((X=1)\)</span> are then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mu &amp;=
\begin{cases} 
e^{\beta_0} &amp;\mbox{if } X=0 \\
e^{\beta_0 + \beta_1}  &amp;\mbox{if } X=1 \\
\end{cases}
\end{align*}
\end{split}\]</div>
<p>So the rate ratio comparing the rate in the exposed <span class="math notranslate nohighlight">\((X=1)\)</span> with the rate in the unexposed <span class="math notranslate nohighlight">\((X=0)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
RR = \frac{e^{\beta_0 + \beta_1}}{e^{\beta_0}} = e^{\beta_1}
\end{align*}
\]</div>
<p>Therefore, <span class="math notranslate nohighlight">\(e^{\beta_1}\)</span> can be interpreted as the (incidence) rate ratio comparing the exposed with the unexposed groups. Or <span class="math notranslate nohighlight">\(\beta_1\)</span> can be interpreted as the log rate ratio.</p>
<p>More generally, a regression coefficient can be intepreted as the log rate ratio associated with a unit change of that covariate. Its exponential is the analogous rate ratio.</p>
</div>
<div class="section" id="eestimating-the-regression-coefficients">
<h5>Eestimating the regression coefficients<a class="headerlink" href="#eestimating-the-regression-coefficients" title="Permalink to this headline">¶</a></h5>
<p>As for all GLMs, the regression coefficients, <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>, are estimated by maximum likelihood estimation.</p>
</div>
<div class="section" id="the-mean-and-variance">
<h5>The mean and variance<a class="headerlink" href="#the-mean-and-variance" title="Permalink to this headline">¶</a></h5>
<p>To obtain an equation for the mean of the outcome, we need to apply the inverse link function:</p>
<div class="math notranslate nohighlight">
\[
E[\mathbf{Y}|\mathbf{X}] = \mu = g^{-1} (\mathbf{X}^T\mathbf{\beta})
\]</div>
<p>Which here is equal to</p>
<div class="math notranslate nohighlight">
\[
E[\mathbf{Y}|\mathbf{X}]  = \mu = e^{\mathbf{X^T\beta}},
\]</div>
<p>Similarly the variance of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>  is written:</p>
<div class="math notranslate nohighlight">
\[ 
Var[\mathbf{Y}|\mathbf{X}] = Var[\mu] =Var[ g^{-1} (\mathbf{X}^T\mathbf{\beta})].
\]</div>
</div>
</div>
<div class="section" id="offsets-optional">
<h4>16.6.4 Offsets  [optional]<a class="headerlink" href="#offsets-optional" title="Permalink to this headline">¶</a></h4>
<p>In this short sub-section, we explore an extension of the Poisson regression model above which allows us to take into account the fact that the observations in our data may represent counts from different lengths of observation time. This is a common occurrence in practice. We handle this through something called an <strong>offset term</strong>.</p>
<p>Above, we simplified the model by assuming each individual was observed for the same period of time. But suppose that was not he case. Suppose, for example, we are counting the number of asthma attacks experienced by school-aged children over time. Some children are followed up for one year and others are followed up for up to five years. Naturally, we would expect those followed up for longer to experience more asthma attacks, on average.</p>
<p>Suppose individual <span class="math notranslate nohighlight">\(i\)</span> is followed up for <span class="math notranslate nohighlight">\(T_i\)</span> years and experiences <span class="math notranslate nohighlight">\(Y_i\)</span> asthma attacks. We have:</p>
<div class="math notranslate nohighlight">
\[ 
\mathbf{Y_i} \overset{iid}{\sim} Poisson(\lambda_i T_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_i\)</span> is the annual rate of asthma attacks for individual <span class="math notranslate nohighlight">\(i\)</span>. The expected number of attacks for individual <span class="math notranslate nohighlight">\(i\)</span> is <span class="math notranslate nohighlight">\(\mu_i = \lambda_i T_i\)</span>. We might wish to propose the following model for the rate (based on the Poisson regression model we met previously):</p>
<div class="math notranslate nohighlight">
\[
ln(\lambda_i) = \mathbf{X}_i^T\mathbf{\beta}
\]</div>
<p>This implies the following model for the expected number of attacks:</p>
<div class="math notranslate nohighlight">
\[
ln(\mu_i) = ln(\lambda_i T_i) =  \mathbf{X}_i^T\mathbf{\beta} + ln(T_i)
\]</div>
<p>This is very similar to our previous model. The only difference is that we now have a covariate (<span class="math notranslate nohighlight">\(ln(T_i)\)</span>) appearing in the model without a regression coefficient. In other words, the regression coefficient for <span class="math notranslate nohighlight">\(ln(T_i)\)</span> is constrained to be equal to 1.</p>
<p>This is exactly what an offset term is: a covariate in a regression model with its regression coefficient constrained to be equal to 1.</p>
</div>
</div>
<span id="document-16.h. Generalised Linear Model (GLM)"></span><div class="section" id="poisson-regression-example">
<h3>16.7 Poisson Regression Example<a class="headerlink" href="#poisson-regression-example" title="Permalink to this headline">¶</a></h3>
<div class="section" id="example-data-set">
<h4>16.7.1 Example data set<a class="headerlink" href="#example-data-set" title="Permalink to this headline">¶</a></h4>
<p>For the purpose of illustration, we will simulate some data and pretend it comes from a clinical trial. We generate 100 participants ( 𝑛 ) and three variables. The first is a count variable representing the number of hospital admissions (<em>counts</em>) a participant has had in a year and it is created from a Poisson distribution with  𝑙𝑎𝑚𝑏𝑑𝑎=2 . The second is a categorical variable (<em>country</em>) with 4 groups representing the country a participant lives in (England, Northern Ireland, Scotland, Wales) and the last is a binary variable (<em>treatment</em>) representing which treatment arm the participant was randomised to. Let’s start with simulating the data and looking at some descriptive statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">## Simulate Data</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
<span class="n">n</span><span class="o">&lt;-</span><span class="m">100</span>
<span class="n">lambda</span><span class="o">&lt;-</span><span class="m">6</span>
<span class="n">counts</span> <span class="o">&lt;-</span> <span class="nf">rpois</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
<span class="n">country</span> <span class="o">&lt;-</span>  <span class="nf">factor</span><span class="p">(</span><span class="nf">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">T</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;England&quot;</span><span class="p">,</span><span class="s">&quot;Northern Ireland&quot;</span><span class="p">,</span><span class="s">&quot;Scotland&quot;</span><span class="p">,</span><span class="s">&quot;Wales&quot;</span><span class="p">))</span>
<span class="n">treatment</span> <span class="o">&lt;-</span> <span class="nf">factor</span><span class="p">(</span><span class="nf">gl</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="n">n</span><span class="o">/</span><span class="m">2</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Active Arm&quot;</span><span class="p">,</span> <span class="s">&quot;Placebo Arm&quot;</span><span class="p">))</span>
<span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">treatment</span><span class="p">,</span> <span class="n">country</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Assume we wish to model <em>counts</em> using a GLM with <em>treatment</em> and <em>country</em> as predictors. We already know the admissions count variable follows a Poisson distribution as we have simulated the data directly from the distribution without adding noise, therefore we know a Poisson regression is suitable. To fit the model, we call the glm() function with the family set to “poisson” and use the summary command to look at the output.</p>
<p>Note: We have used the option <code class="docutils literal notranslate"><span class="pre">family=poisson</span></code>. We could be more explicit and state the link function we want R to use by replacing this with <code class="docutils literal notranslate"><span class="pre">family=poisson(link=log)</span></code>. Try re-running the command using <code class="docutils literal notranslate"><span class="pre">family=poisson(link=identity)</span></code>. What is this doing? Is this a sensible/useful model?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">m1</span> <span class="o">&lt;-</span> <span class="nf">glm</span><span class="p">(</span><span class="n">counts</span> <span class="o">~</span> <span class="n">treatment</span> <span class="o">+</span> <span class="n">country</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">poisson</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
glm(formula = counts ~ treatment + country, family = poisson, 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.2467  -0.5783   0.0477   0.6381   2.2260  

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)              1.82733    0.08448  21.631  &lt; 2e-16 ***
treatmentPlacebo Arm    -0.23187    0.08226  -2.819  0.00482 ** 
countryNorthern Ireland  0.17001    0.10822   1.571  0.11620    
countryScotland          0.14936    0.12396   1.205  0.22822    
countryWales             0.06669    0.11640   0.573  0.56670    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 130.13  on 99  degrees of freedom
Residual deviance: 120.53  on 95  degrees of freedom
AIC: 483.69

Number of Fisher Scoring iterations: 5
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="example-glm-output">
<h4>16.7.2 Example GLM output<a class="headerlink" href="#example-glm-output" title="Permalink to this headline">¶</a></h4>
<p>The first part of the output gives information on deviance residuals. We would expect to see the deviance residuals to be approximately normally distributed if the model is correctly specified. Here we can see the median is close to 0 (0.05) and there does not appear to be any skewness as Q1 (quartile 1 = -0.58) and Q3 (quartile 3 = 0.64) have a similar distance from the median and so are the minimum and maximum.</p>
<p>The second part of the output gives the Poisson regression coefficients for each variable with their standard errors, z values, p-values. We interpret Poisson regression coefficients as if there was a one unit change in the predictor variable (if a continuous variable otherwise change from the reference category to the category listed) the regression coefficient tells us the effect on the logs of the expected counts (admission counts in our example), given the other variables in the model are held constant. The coefficient for treatment is -0.23 which tells use the expected log admissions count for being randomised to the active arm compared to the placebo arm is -0.23. The expected log admissions count for the other countries compared to England are all positive.</p>
<p>We can also see the regression estimate when all the variables in the model are evaluated at zero (or categorical reference group) and this is called the constant and labelled “(Intercept)”. In our model this would represent the expected log admissions count for participants in the placebo arm who live in England.</p>
<p>The standard errors are given which are used to calculate the z-value which in turn is used to calculate the p value. The null hypothesis for each p value is that the corresponding regression coefficient is zero given the rest of the variables in the model. The z value here is just the ratio of the coefficient to the standard error for example treatment we can see the estimate/standard error equals the z value: -0.23187/0.08226=-2.819. The z value follows a normal distribution and is tested against a two-sided alternative hypothesis that the coefficient is not equal to zero. We can see for treatment the p value is 0.005 and if we set out alpha significant level at  𝛼=0.05  we would reject the null hypothesis and conclude the Poisson regression coefficient for treatment is statistically different from zero, given country is in the model.</p>
<p>Lastly, at the bottom of the output, we have information on the residual deviance which can be used to perform a goodness of fit test for the overall model.</p>
</div>
<div class="section" id="poisson-regression-goodness-of-fit-example">
<h4>16.7.3 Poisson Regression Goodness of Fit Example<a class="headerlink" href="#poisson-regression-goodness-of-fit-example" title="Permalink to this headline">¶</a></h4>
<p>At the bottom of the output we see the null deviance and residual deviance from the model. The residual deviance is 120.53 on 95 degrees of freedom (df). There are 100 observations in our model and 5 estimates which gives us 95 df (100-1df for treatment- 3df for each country – 1df for the constant) . To calculate the p-value for the deviance goodness of fit test we simply calculate the probability to the right of the deviance value for the chi-squared distribution on 95 df</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">pchisq</span><span class="p">(</span><span class="n">m1</span><span class="o">$</span><span class="n">deviance</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">m1</span><span class="o">$</span><span class="n">df.residual</span><span class="p">,</span> <span class="n">lower.tail</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.0395607905595946</div></div>
</div>
<p>The null hypothesis is that our model is correctly specified. Here we can see the p value is 0.0396 which is significant if we set our level of significant at 0.05. We therefore have strong evidence to reject the null hypothesis. This result is expected as when creating the simulated data we made no relationship between any of the variables in the model, so we would expect a poor fit.</p>
</div>
</div>
<span id="document-16.i. Generalised Linear Model (GLM)"></span><div class="section" id="common-problems-in-poisson-regression">
<h3>16.8 Common Problems in Poisson Regression<a class="headerlink" href="#common-problems-in-poisson-regression" title="Permalink to this headline">¶</a></h3>
<div class="section" id="problems">
<h4>16.8.1 Problems<a class="headerlink" href="#problems" title="Permalink to this headline">¶</a></h4>
<p>There are two frequent common problems when applying Poisson Regression to count data and both are caused by the deviations from the Poisson distribution assumptions. The first problem is overdispersion and the second is zero inflation.</p>
</div>
<div class="section" id="overdispersion">
<h4>16.8.2 Overdispersion<a class="headerlink" href="#overdispersion" title="Permalink to this headline">¶</a></h4>
<p>Overdispersion happens with then the variance is no longer equal to the mean but larger which violates the Poisson distribution principle. There are two main ways to handle overdispersion, the first is through using a negative binomial distribution (not covered here) instead and the second is to implement something called a quasi-likelihood through a GLM also called a Quasi-Poisson regression.</p>
</div>
<div class="section" id="quasi-poisson-regression">
<h4>16.8.3 Quasi-Poisson regression<a class="headerlink" href="#quasi-poisson-regression" title="Permalink to this headline">¶</a></h4>
<p>A Quasi-Poisson regression is often fitted to handle over-dispersion, it uses the same mean regression function and variance function from Poisson regression but allows the dispersion parameter <span class="math notranslate nohighlight">\(\phi\)</span> to be unrestriced from 1. In Poisson regression <span class="math notranslate nohighlight">\(\phi\)</span> is assumed to be fixed at 1 to make the mean and variance equal, in Quasi-Poisson regression <span class="math notranslate nohighlight">\(\phi\)</span> is not fixed and is estimated from the data. Quasi- Poisson regression leads to the same coefficient estimates as the Poisson regression model but inference are adjusted for the over-dispersion through the standard errors. To run a Quasi-Poisson regression in R we just tell the glm() function that the family is “quasipoisson”</p>
</div>
<div class="section" id="zero-inflation">
<h4>15.3.5 Zero inflation<a class="headerlink" href="#zero-inflation" title="Permalink to this headline">¶</a></h4>
<p>Zero inflation happens when the distribution contains a large number of zero’s. For example, if you were to count how many occasions people drank alcohol in a month but included a large number of non-drinkers you will expect to have multiple counts of 0. A Zero-Inflated Poisson (ZIP) distribution can be thought of being generated by two processes, the first generates zeros and the second is generated by the Poisson distribution (which will contain zeros). The two processes look like this:</p>
<p><span class="math notranslate nohighlight">\(P[\mathbf{Y}=0] = \pi (1-\pi)e^{- \lambda }\)</span>,</p>
<p><span class="math notranslate nohighlight">\(P[\mathbf{Y}=k] = (1-\pi)\frac{\lambda^{k}e^{-\lambda}}{k!}\)</span>,</p>
<p>Where <span class="math notranslate nohighlight">\(k\)</span> is a non-negative integer value, <span class="math notranslate nohighlight">\(\lambda\)</span> is the expected Poisson count and <span class="math notranslate nohighlight">\(\pi\)</span> is the probability of extra zeros. The mean of a ZIP is <span class="math notranslate nohighlight">\((1-\pi)\lambda\)</span> and the variance is <span class="math notranslate nohighlight">\(\lambda (1-\pi) (1+\pi \lambda)\)</span>.</p>
<p>Unfortunately the glm() function is incapable of running a ZIP regression to run, you will need to use the “pscl” package which fits a GLM with a binomial logit link to predict the excess zeros and a GLM with a Poisson log link to model the rest of the distribution.</p>
</div>
</div>
</div>
</div>
<span id="document-17. Investigations round up"></span><div class="section" id="the-role-of-regression-in-different-types-of-investigation">
<h2>17. The role of regression in different types of investigation<a class="headerlink" href="#the-role-of-regression-in-different-types-of-investigation" title="Permalink to this headline">¶</a></h2>
<p>At the start of this section of the notes, we considered different types of investigation that might be of interest within a health data science project. We grouped these investigations into three classes: descriptive, predictive and causal.</p>
<p>Having explored various types of regression modelling, we now revisit the idea of the underlying investigation and consider the role of the regression model in different types of investigation.</p>
<p>We often use the same statistical tools to address research questions for investigations of different types. Regression is a key tool for analyses in all the types of investigation. In this short session we illustrate how the same regression model could be used in prediction and causal investigations, but that the output from the regression should be used and interpreted differently.</p>
<div class="section" id="simple-example">
<h3>17.1  Simple example<a class="headerlink" href="#simple-example" title="Permalink to this headline">¶</a></h3>
<p>We focus on a simple (fictitious) observational study involving three variables: two binary explanatory variables ‘maternal smoking status’ (<span class="math notranslate nohighlight">\(X_{1}\)</span> = 1: smoker, <span class="math notranslate nohighlight">\(X_{1}\)</span> = 0: non-smoker) and maternal socioeconomic status (<span class="math notranslate nohighlight">\(X_{2}\)</span> = 1: low, <span class="math notranslate nohighlight">\(X_{2}\)</span> = 0: high), and a continuous outcome ‘birth weight’ (measured in grams). The assumed relationships between the three variables are summarised in the causal diagram in the figure below.</p>
<p>For the purposes of a simple illustration, we suppose that these are the only three variables at play in this ‘system’. In reality of course there are many other maternal and other characteristics that affect a baby’s birthweight, such as genetics, maternal diet and alcohol consumption, mother’s access to prenatal care, and other features of the environment.</p>
<p>Consider a linear regression of <span class="math notranslate nohighlight">\(Y\)</span> on <span class="math notranslate nohighlight">\(X_{1}\)</span>, <span class="math notranslate nohighlight">\(X_{2}\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[
Y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \epsilon
\]</div>
<div style="text-align: right"> (1) </div>
<p>The estimated regression coefficients and corresponding 95% confidence intervals for this fictitious example are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\beta_{0}} &amp;= 3227 \ \ (95\% \text{CI}:   \  1603, 4851) \\
\hat{\beta_{1}} &amp;= −341 \ \  (95\% \text{CI}:  \  −513, −169) \\
\hat{\beta_{2}} &amp;= −214 \ \  (95\%  \text{CI}:  \  −410, −18)
\end{align*}
\end{split}\]</div>
<p>We now consider how the output from this regression could be used in different investigation types.</p>
</div>
<div class="section" id="prediction">
<h3>17.2 Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h3>
<p>If the aim is to predict birth weight based on the two characteristics of the mother, this model allows us to do this. We could obtain the expected value of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X_{1}\)</span> and <span class="math notranslate nohighlight">\(X_{2}\)</span> (in this very simple example there are only 4 possible combinations).</p>
<p>In this prediction setting we do not, however, particularly care about the estimates of the regression coefficients. We should instead be concerned with the predictive performance of the model. This could be measured, for example, using <span class="math notranslate nohighlight">\(R^2\)</span>, which measures the proportion of variability in the outcome that is explained by the statistical model.</p>
<p>There are many details about how to appropriately assess and quantify the predictive performance of a prediction model which we do not discuss here.</p>
</div>
<div class="section" id="causality-and-explanation">
<h3>17.3 Causality and explanation<a class="headerlink" href="#causality-and-explanation" title="Permalink to this headline">¶</a></h3>
<p>Suppose instead that the aim is to assess the causal effect of maternal smoking (<span class="math notranslate nohighlight">\(X_{1}\)</span>) on birth weight (<span class="math notranslate nohighlight">\(Y\)</span>). In the simple setting shown in the causal diagram above, maternal socioeconomic status (<span class="math notranslate nohighlight">\(X_{2}\)</span>) is the only confounder of the association between <span class="math notranslate nohighlight">\(X_{1}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>The regression model in equation (1) adjusts for <span class="math notranslate nohighlight">\(X_{2}\)</span> and hence the coefficient for <span class="math notranslate nohighlight">\(X_{1}\)</span> can be interpreted as the conditional causal effect of <span class="math notranslate nohighlight">\(X_{1}\)</span> on <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>We can make the interpretation that if all mothers in the study population had smoked, the mean birthweight would have been 341 grams lower than had all mothers in the study population not smoked. This is referred to as an ‘average causal effect’. The 95% confidence interval can be used to provide information about how precisely we believe we have estimated this causal effect. In this case, the 95% confidence interval excludes 0 and includes only negative numbers, running from -513 to -169.</p>
<p>Here, we have not given any interpretation of the estimate of <span class="math notranslate nohighlight">\(\beta_{2}\)</span> because it wasn’t relevant for our research question, even though it was important to adjust for <span class="math notranslate nohighlight">\(X_{2}\)</span> to adjust for confounding. In a more realistic setting, there will be many other variables that confound the association between <span class="math notranslate nohighlight">\(X_{1}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> and which would need to be accounted for to enable a causal interpretation of <span class="math notranslate nohighlight">\(\beta_{1}\)</span>.</p>
</div>
<div class="section" id="the-table-2-fallacy">
<h3>17.4 The “Table 2 Fallacy”<a class="headerlink" href="#the-table-2-fallacy" title="Permalink to this headline">¶</a></h3>
<p>After adjusting for maternal socioeconomic status, maternal smoking was associated with a lowering of 341 grams in mean birthweight. After adjusting for maternal smoking, low maternal socioeconomic status was associated with a lowering of 214 grams in mean birthweight. However, <span class="math notranslate nohighlight">\(\beta_{1}\)</span> and <span class="math notranslate nohighlight">\(\beta_{2}\)</span> in model (1) do not have the same type of interpretation. This  is due to the relationships between the three variables.</p>
<ul class="simple">
<li><p><em>Interpreting <span class="math notranslate nohighlight">\(\beta_2\)</span></em>: According to the causal diagram above, maternal smoking status is on the causal pathway from socioeconomic status to birth weight. Hence the parameter <span class="math notranslate nohighlight">\(\beta_2\)</span> represents the effect of socioeconomic status on birth weight that does not go through smoking status - this is a ‘direct effect’ rather than a ‘total effect’. <br><br></p></li>
<li><p><em>Interpreting <span class="math notranslate nohighlight">\(\beta_1\)</span></em>: By contrast, <span class="math notranslate nohighlight">\(\beta_{1}\)</span> represents the total effect of smoking status on birth weight. We do not go into details about definitions of different types of effect. The aim here is simply to point out that the correct interpretation of the coefficients in the regression model in (1) depends on assumptions about the inter-relationships between the three variables, including how they are ordered in time.</p></li>
</ul>
<p>In some (or perhaps many) epidemiological investigations that involve exploration of risk factors, estimates of regression coefficients from multivariable models such as that in (1) (and versions with many more explanatory variables) are presented alongside one another in a table, together with confidence intervals and p-values. They may then be interpreted as though all coefficients had the same meaning, ignoring possible inter-relationships between the variables and temporal ordering. As we have seen from the above example, this could be misleading. This problem has been referred to in the literature the ‘Table 2 fallacy’, because the estimates of regression coefficients are often presented in ‘Table 2’ in a paper (where ‘Table 1’ is usually a table of descriptive statistics). See Westreich and Greenland (2013) for a description of the Table 2 fallacy. Bandoli et al. (2018) provide an example in the context of preeclampsia and preterm birth.</p>
</div>
<div class="section" id="other-analysis-approaches">
<h3>17.5 Other analysis approaches<a class="headerlink" href="#other-analysis-approaches" title="Permalink to this headline">¶</a></h3>
<p>Regression modelling is a fundamental part of the statistician’s toolbox and is used in many investigations of different types.  We have used regression modelling to illustrate the connection between the analysis method and the underlying aim of the investigation.</p>
<p>However, regression models are not the only tool available. You will come across many other types of analysis method, such as clustering or neural networks, which can also be used in various types of investigation.</p>
</div>
</div>
<span id="document-18. Statistics for HDS round up"></span><div class="section" id="statistics-and-health-data-science">
<h2>Statistics and Health Data Science<a class="headerlink" href="#statistics-and-health-data-science" title="Permalink to this headline">¶</a></h2>
<p>We end with some brief remarks about the application of statistics in health data science.</p>
<div class="section" id="focus-on-the-research-question">
<h3>Focus on the research question<a class="headerlink" href="#focus-on-the-research-question" title="Permalink to this headline">¶</a></h3>
<p>The more complicated the statistical analysis becomes, the easier it is to get lost in the technical details. As a data scientist, it is always important to be able to take a step back and re-focus on the underlying research question.</p>
<p>Ask yourself:</p>
<ul class="simple">
<li><p>What is the research question?</p></li>
<li><p>What assumptions can I reasonably make, taking into account where and when the data were collected and how they were collected?</p></li>
<li><p>Does the proposed statistical analysis answer the research question?</p></li>
<li><p>How can I assess the robustness of the conclusions of my analysis to the key assumptions I have made?</p></li>
</ul>
</div>
<div class="section" id="know-your-data">
<h3>Know your data<a class="headerlink" href="#know-your-data" title="Permalink to this headline">¶</a></h3>
<p>We cannot stress too much the importance of being familiar with your data. Where does it come from? How was it collected? How accurate are measurements? Do similar biases affect measurements from different units/places/times?</p>
<p>A hugely important step in any data science project is to look at your data. The most sophisticated analysis will produce invalid results if based on data that contains substantial errors or incorrectly assembled datasets.</p>
</div>
<div class="section" id="continue-to-learn">
<h3>Continue to learn<a class="headerlink" href="#continue-to-learn" title="Permalink to this headline">¶</a></h3>
<p>This module has introduced some key building blocks, concepts and statistical tools that will be very useful for data science projects. However, there are many more statistical techniques that we have not touched on. In your career as a health data scientist, you will continue to learn new methods and approaches.</p>
<p>We hope that this module has provided a solid foundation to build on!</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By MSc Health Data Science, LSHTM<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>