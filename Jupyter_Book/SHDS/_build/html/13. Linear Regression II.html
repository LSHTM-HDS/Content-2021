
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>13. Linear Regression II &#8212; Statistics for Health Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="14 Logistic Regression" href="14.%20Logistic%20Regression.html" />
    <link rel="prev" title="12. Linear Regression I" href="12.%20Linear%20Regression%20I.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistics for Health Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00.%20Welcome.html">
   Welcome to Statistics for Health Data Science
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00.%20Acknowledgements.html">
   Acknowledgements
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01.%20Introduction.html">
   1 Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basic probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="02.a.%20Probability.Discrete.html">
   2. Probability and Discrete Probability Distributions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="02.b.%20Probability.Discrete.html">
     2.1 Bayes’ Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02.c.%20Probability.Discrete.html">
     2.2 The binomial distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02.d.%20Probability.Discrete.html">
     2.3 The Poisson distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="03.a.%20Continuous%20Probability%20Distributions.html">
   3. Continuous probability distributions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="03.b.%20Continuous%20Probability%20Distributions.html">
     3.1 Continuous random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.c.%20Continuous%20Probability%20Distributions.html">
     3.2 Useful continuous distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.d.%20Continuous%20Probability%20Distributions.html">
     3.3 Uses of the standard Normal distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.e.%20Continuous%20Probability%20Distributions.html">
     3.5 Are the data normally distributed?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.f.%20Continuous%20Probability%20Distributions.html">
     3.5 Joint distributions and correlations
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical Inference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="04.a.%20Population.and.samples.html">
   4. Populations and Samples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="04.b.%20Population.and.samples.html">
     4.1 Sampling from a population
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.c.%20Population.and.samples.html">
     4.2 Statistical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.d.%20Population.and.samples.html">
     4.3 Sampling distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.e.%20Population.and.samples.html">
     4.4 Obtaining the sampling distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.f.%20Population.and.samples.html">
     4.5 Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.g.%20Population.and.samples.html">
     Appendix: additional reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="05.a.%20Likelihood.html">
   5. Likelihood
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="05.b.%20Likelihood.html">
     5.1 Introduction to maximum likelihood estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.c.%20Likelihood.html">
     5.2 The likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.d.%20Likelihood.html">
     5.3 Log likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.e.%20Likelihood.html">
     5.4 Finding the MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.f.%20Likelihood.html">
     5.5 Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="06.a.%20Maximum%20Likelihood.html">
   6. Maximum Likelihood
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="06.b.%20Maximum%20Likelihood.html">
     6.1 Likelihood and log-likelihood with
     <span class="math notranslate nohighlight">
      \(n\)
     </span>
     independent observations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.c.%20Maximum%20Likelihood.html">
     6.2 Properties of maximum likelihood estimators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.d.%20Maximum%20Likelihood.html">
     6.3 Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.e.%20Maximum%20Likelihood.html">
     Appendix: Additional Reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="07.a.%20Frequentist%20I.html">
   7. Frequentist I: Confidence Intervals (CIs)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="07.b.%20Frequentist%20I.html">
     7.1 Introduction to confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.c.%20Frequentist%20I.html">
     7.2 95% confidence intervals for the mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.d.%20Frequentist%20I.html">
     7.3 Interpretation of confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.e.%20Frequentist%20I.html">
     7.4 Approximate confidence intervals for parameters estimated using large samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.f.%20Frequentist%20I.html">
     7.5 Confidence Intervals using resampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.g.%20Frequentist%20I.html">
     7.6 Summary: Use of confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.h.%20Frequentist%20I.html">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="08.a.%20Frequentist%20II.html">
   8. Frequentist II: Hypothesis tests
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="08.b.%20Frequentist%20II.html">
     8.1 Proving and disproving hypotheses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.c.%20Frequentist%20II.html">
     8.2 The p-value
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.d.%20Frequentist%20II.html">
     8.3 Connection between p-values and confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.e.%20Frequentist%20II.html">
     8.4 Other (mis-)interpretations of p-values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.f.%20Frequentist%20II.html">
     8.5 Calculating p-values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.g.%20Frequentist%20II.html">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="09.a.%20Bayesian%20Statistics%20I.html">
   9. Bayesian Statistics I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="09.b.%20Bayesian%20Statistics%20I.html">
     Part 1: 9.1 Introduction to Bayesian Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.c.%20Bayesian%20Statistics%20I.html">
     9.2 Bayes Theorem (recap)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.d.%20Bayesian%20Statistics%20I.html">
     9.3 The Bayesian paradigm in Health data science problems.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.e.%20Bayesian%20Statistics%20I.html">
     Part 2:  9.4 Bayes thorem for discrete and continous data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.f.%20Bayesian%20Statistics%20I.html">
     9.5 Bayesian inference on proportions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.g.%20Bayesian%20Statistics%20I.html">
     9.6 Summarising Posteriors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.h.%20Bayesian%20Statistics%20I.html">
     9.7 Prior Predictions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09.i.%20Bayesian%20Statistics%20I.html">
     9.8 Conjugacy
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="10.a.%20Bayesian%20Statistics%20II.html">
   10. Bayesian Statistics II: Normal data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="10.b.%20Bayesian%20Statistics%20II.html">
     10.1 Example: CD4 cell counts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10.c.%20Bayesian%20Statistics%20II.html">
     10.2 Calculating the posterior for the mean of a Normal distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10.d.%20Bayesian%20Statistics%20II.html">
     10.3 Credible Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10.e.%20Bayesian%20Statistics%20II.html">
     10.4 Predictions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10.f.%20Bayesian%20Statistics%20II.html">
     10.5 Multiparameter models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10.g.%20Bayesian%20Statistics%20II.html">
     Further Resources
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical modelling
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="11.%20Types%20of%20Investigation.html">
   11. Types of Investigation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12.%20Linear%20Regression%20I.html">
   12. Linear Regression I
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   13. Linear Regression II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14.%20Logistic%20Regression.html">
   14 Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15.%20Poisson%20Regression%20Model.html">
   15. Generalised Linear Models: Poisson Regression for Count Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16.%20Extensions%20Confounding%2C%20standardization%2C%20and%20collapsibility.html">
   16. Extensions: Confounding, standardization, and collapsibility
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/13. Linear Regression II.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/LSHTM-HDS/Content-2021"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/LSHTM-HDS/Content-2021/issues/new?title=Issue%20on%20page%20%2F13. Linear Regression II.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/LSHTM-HDS/Content-2021/master?urlpath=tree/docs/13. Linear Regression II.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   13.1 Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagnostics-for-checking-the-assumptions-of-linear-regression">
   13.2 Diagnostics for checking the assumptions of linear regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assumptions-of-the-linear-regression-model">
     13.2.1 Assumptions of the linear regression model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-plots-to-investigate-assumptions">
     13.2.2 Using plots to investigate assumptions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#scatter-plots-of-the-outcome-against-independent-variables">
       13.2.2.1 Scatter plots of the outcome against independent variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plots-of-residuals-against-fitted-values-or-covariates">
       13.2.2.2 Plots of residuals against fitted values or covariates
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#normal-plots-of-residuals">
       13.2.2.3 Normal plots of residuals
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#plots-based-on-cook-s-distance">
       13.2.2.4 Plots based on Cook’s distance
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-tests-of-assumptions">
     13.2.3 Statistical tests of assumptions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-using-the-birthweight-data">
     13.2.4 Examples using the birthweight data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-make-the-following-observations">
   We make the following observations:
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dealing-with-violations-of-assumptions">
     13.3.5 Dealing with violations of assumptions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#checking-the-data">
       13.3.5.1 Checking the data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#transformations">
       13.3.5.2 Transformations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sensitivity-analyses">
       13.3.5.3 Sensitivity analyses
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#including-higher-order-terms-and-interaction-terms-in-a-linear-regression-model">
   13.4 Including higher-order terms and interaction terms in a linear regression model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-quadratic-regression-model">
     13.4.1 The quadratic regression model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#more-complex-modelling-of-non-linear-associations">
       13.4.1.1 More complex modelling of non-linear associations
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelling-interaction-terms">
     13.4.2. Modelling interaction terms
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-general-formulation-of-a-linear-regression-model-with-an-interaction-term-between-two-covariates">
       13.4.2.1 The general formulation of a linear regression model with an interaction term between two covariates
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interaction-between-a-continuous-variable-and-a-binary-variable">
       13.4.2.2 Interaction between a continuous variable and a binary variable
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interaction-between-two-binary-variables">
       13.4.2.3 Interaction between two binary variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#interaction-between-two-continuous-predictor-variables">
       13.4.2.4 Interaction between two continuous predictor variables
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collinearity">
     13.4.3 Collinearity
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#different-types-of-investigation">
   13.5 Different types of investigation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     13.5.1 Different types of investigation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analysis-of-risk-factors">
     13.5.2 Analysis of risk factors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction">
     13.5.3 Prediction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#causal-inference">
     13.5.4 Causal inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-using-the-birthweight-data">
     13.5.6 Example using the birthweight data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       13.5.6.1 Analysis of risk factors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prediction-analysis">
       13.5.6.2 Prediction analysis
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       13.5.6.3 Causal inference
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-of-chapter">
   13.6 Summary of chapter
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression-ii">
<h1>13. Linear Regression II<a class="headerlink" href="#linear-regression-ii" title="Permalink to this headline">¶</a></h1>
<p>In this session, we continue exploring linear regression modelling, focusing on the assumptions underlying this model.</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>apply a range of graphical techniques to investigate the assumptions of the linear regression model;</p></li>
<li><p>explain how to modify the linear regression model to allow for a non-linear association between an independent variable and an outcome;</p></li>
<li><p>fit and interpret linear regression models that include interaction terms;</p></li>
<li><p>interpret the results of a linear regression differently, depending on the type of research question.</p></li>
</ul>
</div><p><strong>Acknowledgements:</strong> Thank you to Jennifer Nicholas, Chris Frost and Ruth Keogh whose notes on linear regression and generalised linear models were particularly useful in the development of the current session.</p>
<div class="section" id="introduction">
<h2>13.1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In the last lesson, we introduced the linear regression model and learnt how to estimate the model parameters and conduct statistical inference on our estimates. We also briefly discussed the assumptions of a linear regression model, but did not consider the validity of the assumptions in our examples. In this session, we will learn about a variety of diagnostic tools that can be used to test the assumptions we make (Section 3). We then learn how to fit more complex linear models which are useful in situations where the covariate is not linearly associated with the outcome (Section 4). We end the lesson by discussing how the type of research question being asked influences the way we present and interpret the results from a linear regression model (Section 5).</p>
<p>The examples in the current lesson will use the same birthweight data as was used in the examples in the previous lesson. The data can be downloaded using the following code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Load data</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="diagnostics-for-checking-the-assumptions-of-linear-regression">
<h2>13.2 Diagnostics for checking the assumptions of linear regression<a class="headerlink" href="#diagnostics-for-checking-the-assumptions-of-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>As previously discussed, the linear regression model makes certain assumptions and all inferences made from a model are contingent on these assumptions being correct. It is therefore important that we have statistical techniques (or <strong>diagnostic tools</strong>) to investigate these assumptions.</p>
<p>In practice, it is rare for all the assumptions of a statistical procedure to hold exactly. We may have evidence in the data, or prior knowledge about the data, that lead us to believe that the assumptions made by the model do not hold. This does not necessarily mean that the results from the model should be disregarded, since statistical procedures are <strong>robust</strong> to departures from assumptions in many settings. When conducting statistical analyses, it is a good idea to first try to establish to what extent assumptions hold and then consider whether the methods used can be adapted to improve the extent to which assumptions hold. If adaptations cannot be made, it is necessary to consider to what extent the results of an analysis can be trusted.</p>
<p>In this section we largely focus on diagnostic tools that can be used to identify assumption violations. Some pointers are given to possible adaptations and alternative techniques that can be used when assumptions are violated, however issues of robustness are not considered in great detail. It is worth noting that, broadly speaking, the central limit theorem implies that departures from assumptions are less important for large datasets than for small ones, and so assumption violations are less of a concern when working with big data.</p>
<div class="section" id="assumptions-of-the-linear-regression-model">
<h3>13.2.1 Assumptions of the linear regression model<a class="headerlink" href="#assumptions-of-the-linear-regression-model" title="Permalink to this headline">¶</a></h3>
<p>The assumptions made by the linear regression model are as follows:</p>
<ol class="simple">
<li><p><strong>Linearity:</strong> There is a linear relationship between the dependent variable <span class="math notranslate nohighlight">\(Y\)</span> and each of the independent variables. Here we are contrasting a linear relationship with a non-linear relationship, not with no relationship. A model in which one of the regression coefficients is zero can satisfy the assumptions of linear regression.</p></li>
<li><p><strong>Normality:</strong> The true residuals follow a normal distribution.</p></li>
<li><p><strong>Homoscedasticity:</strong> The true residual variance is constant i.e. the scatter of points around the true regression line has the same variance, irrespective of the value of <span class="math notranslate nohighlight">\(x_i\)</span>. The converse of this feature is termed <strong>heteroscedasticity</strong>.</p></li>
<li><p><strong>Independence:</strong> The observations of <span class="math notranslate nohighlight">\(y_i\)</span> are independent.</p></li>
</ol>
<p>In this session we will focus on the first three assumptions. Violations of the independence assumption are often more apparant from the context of a study than from the data itself. For example, if we carry out a study in which the blood pressure of 100 people are each measured twice, and then treat the 200 measurements as independent in the statistical analysis it is clear that the assumption of independence is violated.</p>
<p>Notice that the normality and homoscedasticity assumptions concern the <em>true</em> residuals, defined in terms of deviations from the model defined by population parameters. Since true residuals can never be observed in practice, we have to use the observed residuals (obtained by replacing the population parameters with their estimates). In fact, observed residuals are neither independent nor do they have constant variance, but in most settings the departures from independence and homoscedasticity are very small. Consequently, we can proceed as if the observed residuals were the true residuals when investigating assumptions.</p>
</div>
<div class="section" id="using-plots-to-investigate-assumptions">
<h3>13.2.2 Using plots to investigate assumptions<a class="headerlink" href="#using-plots-to-investigate-assumptions" title="Permalink to this headline">¶</a></h3>
<p>It is a good idea to explore your data using a number of simple plots. Here we will introduce the most useful plots for both simple and multivariable linear regression models.</p>
<div class="section" id="scatter-plots-of-the-outcome-against-independent-variables">
<h4>13.2.2.1 Scatter plots of the outcome against independent variables<a class="headerlink" href="#scatter-plots-of-the-outcome-against-independent-variables" title="Permalink to this headline">¶</a></h4>
<p>For simple linear regression models, a scatter plot of the outcome against the independent variable can usually make serious violations of assumptions apparent. Such plots are particularly good for identfiying non-linearity, heteroscedasticity and <strong>outliers</strong> (points which lie atypically far from the regression line).</p>
<p>Let our outcome and independent variable be denoted by <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, respectively. The figure below depicts four different scenarios where various assumptions are violated. In Scenario A, there is a slight curvature in the scatter of points between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, suggesting a non-linear relationship which violates the linearity assumption. In Scenario B, the variance of <span class="math notranslate nohighlight">\(Y\)</span> is larger for larger values of <span class="math notranslate nohighlight">\(X\)</span>, violating the homoscedasticity assumption. In Scenario C, the linearity and homoscedasticity assumptions appear to hold, but there is a possible outlier (circled in red). Scenario D depicts an ideal situation for simple linear regression, where there appears to be no violations.</p>
<p><img alt="simple_plots.png" src="attachment:simple_plots.png" /></p>
<p>For multivariable linear regression, the linearity assumption requires that the relationship between the outcome and each independent variable is linear <em>conditional on the other covariates in the model</em>. So, there is no requirement that the relationship between the outcome and each individual covariate is linear when other covariates are ignored. This means that assessment of the fit of a multivariable linear regression cannot be inferred from a series of scatter plots relating the outcome to each covariate. Such plots can be useful for detecting points with extreme values, but the residual plots considered next are more useful for multivariable models.</p>
</div>
<div class="section" id="plots-of-residuals-against-fitted-values-or-covariates">
<h4>13.2.2.2 Plots of residuals against fitted values or covariates<a class="headerlink" href="#plots-of-residuals-against-fitted-values-or-covariates" title="Permalink to this headline">¶</a></h4>
<p>Plots of the observed residuals against the fitted values are useful for investigating the assumptions of linearity and homoscedasticity. For linearity: if a non-linear relationship is present, then the residuals will not be equally distributed above and below zero across the range of fitted values. For homoscedasticity: if there is heterogeneity in the residuals, the variance of residuals will not be constant across the range of fitted values.</p>
<p>The figure below uses the same data from Scenarios A-D above, but displays the observed residuals against fitted values. We can see that linearity is violated in Scenario A, since the scatter points are not equally distributed above and below the line at <span class="math notranslate nohighlight">\(\epsilon=0\)</span>. Furthermore, in Scenario B we can see that the variance of residuals increase with increasing <span class="math notranslate nohighlight">\(\hat{y}\)</span>, indicating a violation of homoscedasticity.</p>
<p><img alt="residual_plots.png" src="attachment:residual_plots.png" /></p>
<p>It can also be useful to plot residuals against each covariate, as a futher check for a linear relationship between <span class="math notranslate nohighlight">\(Y\)</span> and each of the independent variables (conditional on the other covariates in the model). If there are only a small number of covariates in the model, then these plots can be done for all variables. However, if the model is very complex, it may be judged sufficient to only plot residuals against fitted values and residuals against the most important covariates.</p>
</div>
<div class="section" id="normal-plots-of-residuals">
<h4>13.2.2.3 Normal plots of residuals<a class="headerlink" href="#normal-plots-of-residuals" title="Permalink to this headline">¶</a></h4>
<p>Normal plots (such as the <strong>Q-Q plot</strong>) provide the best means of visually detecting departures from normality. The normal Q-Q plot plots observed values against a standard normal distribution with the same number of points. If the data are perfectly normally distributed, the points on a Q-Q plot would lie on the line <span class="math notranslate nohighlight">\(Y=X\)</span>. Deviations from this line indicate deviations from normality. Q-Q plots of residuals can be used to investigate the normality assumption.</p>
<p>As previously mentioned, the observed residuals do not have constant variance even when true residuals do. Therefore, some authors suggest using <strong>standardised</strong> residuals in the normal plots (since standardised residuals do have constant variance). On the other hand, some prefer to work with the observed residuals since these have the same units as the outcome. In practice, the differences between the two approaches are minor.</p>
<p>The figure below depicts normal Q-Q plots for the standardised residuals in Scenarios A-D. In such plots we might expect to see some deviation from the straight line in the extreme values of the residuals and so the variation in the tails are not of great concern. In Scenario A however, there is deviation away from the line <span class="math notranslate nohighlight">\(Y=X\)</span> towards the middle, indicating a violation of the normality assumption. Furthermore, the outlier in Scenario C may need further investigation (we discuss outliers further in a subsequent section).</p>
<p><img alt="normal_residual_plots.png" src="attachment:normal_residual_plots.png" /></p>
</div>
<div class="section" id="plots-based-on-cook-s-distance">
<h4>13.2.2.4 Plots based on Cook’s distance<a class="headerlink" href="#plots-based-on-cook-s-distance" title="Permalink to this headline">¶</a></h4>
<p>Cook’s distance is a measure of the <strong>influence</strong> of an observation. An influential observation is one that has a large impact on the model parameter estimates. It is worth checking the influence of observations, particularly potential outliers, to see if they are having a much larger impact on model fit than we would expect.</p>
<p>For a model with <span class="math notranslate nohighlight">\(p\)</span> parameters (with estimated residual variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>), the Cook’s distance for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation (<span class="math notranslate nohighlight">\(D_i\)</span>) is obtained by refitting the model excluding this observation and obtaining new fitted values (<span class="math notranslate nohighlight">\(\hat{y_{j(i)}}\)</span>) for all <span class="math notranslate nohighlight">\(n\)</span> observations (including the omitted one). <span class="math notranslate nohighlight">\(D_i\)</span> is then defined as:</p>
<div class="math notranslate nohighlight">
\[D_i = \frac{\sum_{i=1}^n(\hat{y}_{j(i)}-\hat{y}_i)^2}{(p+1)\hat{\sigma}^2}\]</div>
<p>The higher the value of <span class="math notranslate nohighlight">\(D_i\)</span>, the more influential the observation.</p>
<p>It can be informative to display Cook’s distances graphically. The figure below plots the Cook’s distances for each observation in Scenarios A-D.</p>
<p><img alt="cooks_distance.png" src="attachment:cooks_distance.png" /></p>
<p>In Scenario C the outlier identified in the previous plots and potentially problematic has a much higher Cook’s distance than the other observations, indicating that it is highly influential and worth further investigation.</p>
</div>
</div>
<div class="section" id="statistical-tests-of-assumptions">
<h3>13.2.3 Statistical tests of assumptions<a class="headerlink" href="#statistical-tests-of-assumptions" title="Permalink to this headline">¶</a></h3>
<p>It might be anticipated that the assumptions of the linear regression model can be investigated using formal hypothesis tests. Indeed there exist a number of statistical tests for normalilty including the Kolmorogorov-Smirnov test and the Shapiro-Wilk test. Further, there exist statistical tests for heteroscedasticity of rediduals.</p>
<p>However, these tests suffer from the drawback that they tend to only have statistical power to detect model violations when datasets are large and when datasets are large the central limit theorem means that the consequences of these violations of are less important than in small datasets. With large datasets, tests of normality and heteroscedasticity can often be statistically significant, but the impact of these violations may be practically unimportant. For these reasons, the tests are considered by many statisticians to be of limited practical use and so details of these procedures will not be given here.</p>
</div>
<div class="section" id="examples-using-the-birthweight-data">
<h3>13.2.4 Examples using the birthweight data<a class="headerlink" href="#examples-using-the-birthweight-data" title="Permalink to this headline">¶</a></h3>
<p>We will use some of the graphical tools discussed above to assess the validity of assumptions in the multivariable model defined in the previous chapter (Model 3). Recall Model 3 was defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Model 3: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i + \epsilon_i \]</div>
<p>The outcome <span class="math notranslate nohighlight">\(y_i\)</span> denotes the birthweight (in oz) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby. The covariates <span class="math notranslate nohighlight">\(l_i\)</span> and <span class="math notranslate nohighlight">\(h_i\)</span> denote the length of pregnancy (i.e. number of gestational days), and the height of the mother (in inches) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby, respectively.</p>
<p>The code below fits Model 3 to the birthweight data, and then produces (1) a plot of residuals against fitted values (2) a Q-Q plot of the standardised residuals and (3) a plot of Cook’s distances by observation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Load the data</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>

<span class="c1">#Fit Model 3 to the data</span>
<span class="n">model3</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">Maternal.Height</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="c1">#Set the graphical space so that two plots are shown side-by-side in one row</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>

<span class="c1">#Plot the residuals against the fitted values</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">model3</span><span class="o">$</span><span class="n">fitted.values</span><span class="p">,</span> <span class="n">model3</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">main</span> <span class="o">=</span> <span class="s">&quot;Residual plot&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Fitted values&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Residuals&quot;</span><span class="p">,</span> <span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span><span class="o">=</span><span class="m">0</span><span class="p">)</span>

<span class="c1">#Obtain the standardised residuals</span>
<span class="n">Standardised.Residuals</span><span class="o">&lt;-</span><span class="nf">rstandard</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span>

<span class="c1">#Normal Q-Q plot of the standardised residuals </span>
<span class="nf">qqnorm</span><span class="p">(</span><span class="n">Standardised.Residuals</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Normal Q-Q plot&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Expected normal value&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Standardised Residuals&quot;</span><span class="p">)</span>
<span class="nf">qqline</span><span class="p">(</span><span class="n">Standardised.Residuals</span><span class="p">)</span>

<span class="c1">#Plot of Cook&#39;s distance</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="m">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/13. Linear Regression II_7_0.png" src="_images/13. Linear Regression II_7_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="we-make-the-following-observations">
<h2>We make the following observations:<a class="headerlink" href="#we-make-the-following-observations" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p><strong>Linearity:</strong> The residuals are equally distributed above and below zero in the “Residual plot”</p></li>
<li><p><strong>Normality:</strong> There do not appear to be any serious departures from normality, based on the “Normal Q-Q plot”</p></li>
<li><p><strong>Homoscedasticity:</strong> The variance of residuals are constant across the fitted values (based on the “Residual plot”)</p></li>
</ol>
<p>However, the Cook’s distance plot reveals that observation 239 is highly influential, compared to the remaining observations. Observations 405 and 820 also have a relatively high Cook’s distance. Sensitivity analyses may be required to assess model fit with and without these observations (this is discussed in Section 3.5.3).</p>
<p>Finally, we can assume that the independence assumption holds, since the birthweight of a baby from one mother is not expected to be associated with the birthweight of a baby from a different mother. Therefore, we can reasonably conclude that all the assumptions are met in this model (but there are some potentially problematic observations in terms of influence).</p>
<p>In the next section, we briefly introduce some of the statisical solutions available for when assumptions are not met.</p>
<div class="section" id="dealing-with-violations-of-assumptions">
<h3>13.3.5 Dealing with violations of assumptions<a class="headerlink" href="#dealing-with-violations-of-assumptions" title="Permalink to this headline">¶</a></h3>
<p>So far, we have discussed diagnostic tools that are useful for identifying possible violations of the assumptions of a linear model. Identification of potential violations of concern is only the first, and arguably the easiest, aspect of an exploration of the robustness of the results of fitting a model. Here, we briefly describe some approaches that can be used to deal with violations. When these approaches do not work, then more complex methods (beyond the scope of this lesson) may be needed to analyse the data.</p>
<div class="section" id="checking-the-data">
<h4>13.3.5.1 Checking the data<a class="headerlink" href="#checking-the-data" title="Permalink to this headline">¶</a></h4>
<p>Clearly, it is important that errors in data are eliminated as far as possible. In practice, ensuring that a large dataset is 100% error free may be impossible. Observations with large standardised residuals can potentially arise through data entry or coding errors and so a useful first step is to check such values with the data provider or original source of data, if available.</p>
</div>
<div class="section" id="transformations">
<h4>13.3.5.2 Transformations<a class="headerlink" href="#transformations" title="Permalink to this headline">¶</a></h4>
<p>Sometimes it can be useful to transform either the outcome variable and/or one or more of the covariates. The transformed variables are then used in the analysis in replacement of the original variables. There are a number of possible motivations for this:</p>
<ol class="simple">
<li><p>Transformations can be used to convert a non-linear relationship into a linear one. For example:</p></li>
</ol>
<p>$<span class="math notranslate nohighlight">\(y_i = \alpha(x_i)^{\beta} ⇒log(y_i) = log (\alpha)+\beta log (x_i)\)</span><span class="math notranslate nohighlight">\(.  
2. Transformations can be used to improve the normality of residuals. For example, the Box-Cox transformation is a power transformation for this purpose. 
3. Transformations can help stabilise the variance of residuals. For example, if \)</span>\hat{\sigma}^2<span class="math notranslate nohighlight">\( is proportional to \)</span>[E(Y)]^2<span class="math notranslate nohighlight">\( then \)</span>y^<em>=log(y)<span class="math notranslate nohighlight">\( is a useful variance-stabilising transformation. Alternatively, if \)</span>\hat{\sigma}^2<span class="math notranslate nohighlight">\( is proportional to \)</span>[E(Y)]^3<span class="math notranslate nohighlight">\( then \)</span>y^</em>=1/\sqrt{y}$ can be used.</p>
</div>
<div class="section" id="sensitivity-analyses">
<h4>13.3.5.3 Sensitivity analyses<a class="headerlink" href="#sensitivity-analyses" title="Permalink to this headline">¶</a></h4>
<p>If we observe potentially problematic outliers, sensitivity analyses can be used to assess how problematic they are. This involves repeating the analysis after omitting the outlier (or group of outliers) and considering the extent to which the results are altered.</p>
<p>However, even if the outlier affects the results (and/or assumptions) it is not a good idea to simply drop the data point. If it is not a data error, then it is a legitimate observation that should be included and understanding the reasons why it is an outlier could be important. In most cases, it is preferable to report the results including all data points, but discuss the impact removing the outlier had on the results.</p>
</div>
</div>
</div>
<div class="section" id="including-higher-order-terms-and-interaction-terms-in-a-linear-regression-model">
<h2>13.4 Including higher-order terms and interaction terms in a linear regression model<a class="headerlink" href="#including-higher-order-terms-and-interaction-terms-in-a-linear-regression-model" title="Permalink to this headline">¶</a></h2>
<p>As we have already discussed, linear regression assumes that the relationship between the outcome and the independent variables is linear. As we already know, this is not always the case in real data. For example, suppose we are interested in the association between weight and age. On average, the weight of young adults will increase with age. However, at a certain age, the average weight may start to decrease. In this case, the association between weight and age would follow a non-linear (upside-down) <span class="math notranslate nohighlight">\(u\)</span>-shape. It could still be possible to model this relationship within the linear regression framework, by adding a <strong>second-order term</strong> to the model. This procedure is known as <strong>quadratic regression</strong>.</p>
<p>Now suppose we are interested in relating weight to age, sex and height (weight is the outcome and age, sex and height are the covariates). In this case, the estimated regression coefficient for height represents the effect of a unit increase in height on weight in people of the same age and sex. The model assumes that the coefficient relating weight to height is the same for all people of all ages and sexes. For example, that it is the same for two year old boys as in thirty-three year old women. But this is not necessarily true! It could be that the slope of the association between weight and height differs by sex and by age. If this is the case, we say there is an <strong>interaction</strong> between height and sex and between height and age.</p>
<p>In this section we will first learn how non-linear relationships can be accounted for in quadratic regression, then learn how to include interaction terms in a linear regression model.</p>
<div class="section" id="the-quadratic-regression-model">
<h3>13.4.1 The quadratic regression model<a class="headerlink" href="#the-quadratic-regression-model" title="Permalink to this headline">¶</a></h3>
<p>The quadratic regression model is a multivariable regression model with two independent variables where the second variable is the square of the first variable. Algebraically:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon \text{ where } \epsilon \sim NID(0,\sigma^2)\]</div>
<p>Despite the fact that one of the variables is the square of the other, this is still a linear regression model because the expectation of the outcome is a linear function of both parameters.</p>
<p>The figure below shows two scatter plots of the data used in Scenario A above. The plot on the left-hand side includes the fitted values of a linear regression model (with no higher-order terms included) and the right-hand side plot includes the fitted values of a quadratic regression model. By comparing the plots, we can see that the quadratic regression model does have a better fit, particularly at the extreme values of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p><img alt="quadratic_example.png" src="attachment:quadratic_example.png" /></p>
<p>Unfortunately, interpreting <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_2\)</span> is not as straightforward as in most linear models. The reason for this is that it is not possible to change <span class="math notranslate nohighlight">\(X^2\)</span> by 1 unit whilst holding <span class="math notranslate nohighlight">\(X\)</span> constant (although it is possible the other way round). Moreover, quadratic regression models are limited in terms of describing relationships between variables in most medical applications. Quadratic functions either increase to a maximum and the decline, or fall to a minimum and then increase. Further, the behaviour of a quadratic is symmetric about the turning point. Such relationships in medical research are rarely plausible.</p>
<p>However, the quadratic regression model can be useful as a test for non-linearity. Quadratic terms can be added to a regression model with more than one covariate, and if the results indicate that the quadratic term is an important variable, this is indicative of curvature in the relationship between the outcome and the variable of interest. This often motivates fitting a more realistic model to better describe the non-linear relationship.</p>
<p><em>Example</em>. We will use quadratic regression to assess the linearity assumption for the covariates in Model 3 (recall Model 3 relates birthweight to length of pregnancy and mother’s height).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Define the squared terms</span>
<span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days.squared</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.squared</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span>

<span class="c1">#Quadratic regression to test the linearity between birthweight and gestational days</span>
<span class="n">model_test1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days.squared</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model_test1</span><span class="p">)</span>

<span class="c1">#Quadratic regression to test the linearity between birthweight and mother&#39;s height</span>
<span class="n">model_test2</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.squared</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model_test2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + data$Maternal.Height + 
    data$Gestational.Days.squared)

Residuals:
    Min      1Q  Median      3Q     Max 
-54.143 -10.666   0.306  10.095  67.692 

Coefficients:
                                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                   -1.592e+02  5.127e+01  -3.104  0.00195 ** 
data$Gestational.Days          9.689e-01  3.612e-01   2.682  0.00741 ** 
data$Maternal.Height           1.286e+00  1.905e-01   6.748 2.35e-11 ***
data$Gestational.Days.squared -9.487e-04  6.611e-04  -1.435  0.15155    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.43 on 1170 degrees of freedom
Multiple R-squared:  0.1983,	Adjusted R-squared:  0.1962 
F-statistic: 96.45 on 3 and 1170 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + data$Maternal.Height + 
    data$Maternal.Height.squared)

Residuals:
    Min      1Q  Median      3Q     Max 
-55.675 -10.432   0.325  10.002  54.188 

Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                  300.38521  208.00423   1.444   0.1490    
data$Gestational.Days          0.45338    0.03003  15.098   &lt;2e-16 ***
data$Maternal.Height         -10.91959    6.51008  -1.677   0.0937 .  
data$Maternal.Height.squared   0.09539    0.05090   1.874   0.0612 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.42 on 1170 degrees of freedom
Multiple R-squared:  0.1993,	Adjusted R-squared:  0.1972 
F-statistic: 97.05 on 3 and 1170 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>The results indicate that the squared terms for gestational days and mother’s height are not important (as indicated by lack of small <span class="math notranslate nohighlight">\(p\)</span>-values), therefore we can assume that linearity has not been violated. This is consistent with our conclusions based on the normal residual plot.</p>
<p>In the next section, we briefly discuss some statistical options for modelling non-linear associations more realistically, when the linearity assumption is not met and the quadratic regression model is not appropriate.</p>
<div class="section" id="more-complex-modelling-of-non-linear-associations">
<h4>13.4.1.1 More complex modelling of non-linear associations<a class="headerlink" href="#more-complex-modelling-of-non-linear-associations" title="Permalink to this headline">¶</a></h4>
<p>The quadratic regression model belongs to a family of <strong>polynomial regression models</strong> and is the simplest model in that family. Further power terms can be added to the regression model in order to increase complexity. For example, a cubic regression model is one which includes a cubic term as well as a squared term.</p>
<p>An even more flexible approach is to use a <strong>piecewise polynomial model</strong>, which allows for a different polynomial function in different ranges of the observed values of <span class="math notranslate nohighlight">\(X\)</span>, defined according to specified <strong>knots</strong>. For example, a piecewise cubic polynomial model with two knots at <span class="math notranslate nohighlight">\(k_1\)</span> and <span class="math notranslate nohighlight">\(k_2\)</span> (<span class="math notranslate nohighlight">\(k_1 &lt; k_2\)</span>) takes the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split} y_i = 
\begin{cases}
\beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i \text{ when } x_i &lt; k_1 \\
\beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i \text{ when } k_1 \leq x_i &lt; k_2 \\
\beta_{03} + \beta_{13}x_i + \beta_{23}x_i^2 + \beta_{33}x_i^3 + \epsilon_i \text{ when } k_2 \geq x_i\\
\end{cases}
\end{split}\]</div>
<p>The flexibility of the model (and therefore its ability to model more complex relationships) can be increased by increasing the degree of polynomial and/or the number of knots. However, highly flexible models may overfit the data and make the model difficult to interpret. In general, it is a good idea to consider an appropriate trade-off between flexibility and interpretability.</p>
</div>
</div>
<div class="section" id="modelling-interaction-terms">
<h3>13.4.2. Modelling interaction terms<a class="headerlink" href="#modelling-interaction-terms" title="Permalink to this headline">¶</a></h3>
<p>The term <strong>interaction</strong> is used to describe situations in which the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> differs according to the level of one or more other covariates. In linear regression models it is the slope of the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> that depends on other factors.</p>
<p>In this section we introduce the general formulation of a linear model with an interaction term and discuss how to interpret the results. We will consider interactions between pairs of continuous variables, pairs of categorical variables and pairs of variables in which one is categorical and the other is continuous.</p>
<div class="section" id="the-general-formulation-of-a-linear-regression-model-with-an-interaction-term-between-two-covariates">
<h4>13.4.2.1 The general formulation of a linear regression model with an interaction term between two covariates<a class="headerlink" href="#the-general-formulation-of-a-linear-regression-model-with-an-interaction-term-between-two-covariates" title="Permalink to this headline">¶</a></h4>
<p>Suppose we wish to relate an outcome (<span class="math notranslate nohighlight">\(Y\)</span>) to two covariates (<span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>), but we want to allow the slope of the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> to differ according to the value of <span class="math notranslate nohighlight">\(X_2\)</span>. To allow for this we fit an interaction model that contains an additional variable (<span class="math notranslate nohighlight">\(X_3\)</span>) that is the product of <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>:</p>
<p>$<span class="math notranslate nohighlight">\(y_i =  \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \epsilon_i \text{ where }\epsilon_i \sim NID(0, \sigma^2)\)</span>$.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> = value of the outcome for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{1i}\)</span> = value of the first covariate for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{2i}\)</span> =  value of the second covariate for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{3i}\)</span> = <span class="math notranslate nohighlight">\(x_{1i} \times x_{2i}\)</span></p></li>
</ul>
<p>To understand why this model allows the slope of the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> to vary according to <span class="math notranslate nohighlight">\(X_2\)</span>, we can consider the form of the equation when we fix <span class="math notranslate nohighlight">\(X_2\)</span> to have a particular value, say <span class="math notranslate nohighlight">\(X_2=k\)</span>. In this situation the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> is as follows:</p>
<p><span class="math notranslate nohighlight">\(y_i = (\beta_0 + \beta_2k) + (\beta_1 + \beta_3k)x_{1i} + \epsilon_i\)</span>.</p>
<p>In other words, when <span class="math notranslate nohighlight">\(x_2=k\)</span> the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> is a linear one with both slope and intercept dependent upon <span class="math notranslate nohighlight">\(k\)</span>. The intercept is <span class="math notranslate nohighlight">\(\beta_0+\beta_2k\)</span> and the slope is <span class="math notranslate nohighlight">\(\beta_1 +  \beta_3k\)</span>.</p>
<p>By allowing the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> to vary according to <span class="math notranslate nohighlight">\(X_2\)</span>, we have also allowed the slope for the association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> to vary according to <span class="math notranslate nohighlight">\(X_1\)</span>. If we look at the form of the model when <span class="math notranslate nohighlight">\(X_1\)</span> takes particular value, say <span class="math notranslate nohighlight">\(X_1=m\)</span>, we find:</p>
<p><span class="math notranslate nohighlight">\(y_i = (\beta_0+\beta_1m) + (\beta_2+\beta_3m)x_{2i} + \epsilon_i\)</span>.</p>
<p>Again, the relationship between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> is a linear one with both slope and intercept dependent upon <span class="math notranslate nohighlight">\(m\)</span>.</p>
</div>
<div class="section" id="interaction-between-a-continuous-variable-and-a-binary-variable">
<h4>13.4.2.2 Interaction between a continuous variable and a binary variable<a class="headerlink" href="#interaction-between-a-continuous-variable-and-a-binary-variable" title="Permalink to this headline">¶</a></h4>
<p>The interaction model is particularly easy to interpret when one of the covariates (say <span class="math notranslate nohighlight">\(X_2\)</span>) is a binary, taking the values 0 and 1 (i.e. a dummy variable). The linear regression model then becomes:</p>
<p>\begin{align}
&amp;y_i = \beta_0 + \beta_1x_{1i} + \epsilon_i \text{ when } x_2=0\
&amp;y_i = (\beta_0 + \beta_2) + (\beta_1+\beta_3)x_{1i} + \epsilon_i \text{ when } x_2=1
\end{align}</p>
<p>The interpretation of each of the parameters is as follows.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept when <span class="math notranslate nohighlight">\(X_2=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_2\)</span> is the intercept when <span class="math notranslate nohighlight">\(X_2=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span> is the difference in intercepts between the two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the slope when <span class="math notranslate nohighlight">\(X_2=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1+\beta_3\)</span> is the slope when <span class="math notranslate nohighlight">\(X_2=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_3\)</span> is the difference in slopes between the two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span>.</p></li>
</ul>
<p><em>Example.</em> To demonstrate the impact of adding an interaction term, we will consider two models: (1) relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>) to length of pregnancy (<span class="math notranslate nohighlight">\(X_1\)</span>) and mother’s smoking status (<span class="math notranslate nohighlight">\(X_2\)</span>) and (2) relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>) to length of pregnancy (<span class="math notranslate nohighlight">\(X_1\)</span>), mother’s smoking status (<span class="math notranslate nohighlight">\(X_2\)</span>) and their interaction (<span class="math notranslate nohighlight">\(X_3\)</span>). In these models, <span class="math notranslate nohighlight">\(X_2=1\)</span> indicates that the mother smokes and <span class="math notranslate nohighlight">\(X_2=0\)</span> indicates that the mother does not smoke.</p>
<p>We first consider the model with no interaction term. The code below defines the model in R, summarises the results and produces a scatter plot of birthweight against gestational days, with the fitted values superimposed. The blue points (and line) on the scatter plot are observations in the group of babies whose mothers do not smoke and the red points (and line) are observations in the group of babies whose mothers do smoke.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Define a dummy variable for maternal.smoker</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="o">&lt;-</span><span class="m">0</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker</span><span class="o">==</span><span class="s">&quot;True&quot;</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">1</span>

<span class="c1">#Model without the interaction term</span>
<span class="n">no_int_model</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)</span>

<span class="c1">#Scatterplot</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
     <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;Birthweight&quot;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="n">no_int_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">1</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="n">no_int_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">),</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">coef</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)[</span><span class="m">3</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">no_int_model</span><span class="p">)[</span><span class="m">2</span><span class="p">],</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + factor(data$Maternal.Smoker2))

Residuals:
    Min      1Q  Median      3Q     Max 
-50.789 -11.035  -0.211  10.053  52.412 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    -3.18492    8.32945  -0.382    0.702    
data$Gestational.Days           0.45117    0.02968  15.200   &lt;2e-16 ***
factor(data$Maternal.Smoker2)1 -8.37440    0.97346  -8.603   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.25 on 1171 degrees of freedom
Multiple R-squared:  0.2157,	Adjusted R-squared:  0.2143 
F-statistic:   161 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
<img alt="_images/13. Linear Regression II_12_1.png" src="_images/13. Linear Regression II_12_1.png" />
</div>
</div>
<p>As can be seen from the above figure, the fitted values from the model with no interaction term form two straight lines with a common slope 0.45 ounces and intercepts -3.18 ounces for the non-smoking group and -3.18-8.37=-11.55 ounces for the smoking group. This type of model (no interactions) is sometimes known as a <strong>parallel lines</strong> regression model, because it restricts the lines to be parallel. It permits adjustment of the effect of one covariate for the effects of others, but forces the effects of a unit change in each covariate to be constant, whatever the level of the other covariate. This restriction is not appropriate if the slope effect of one covariate depends on the value of another covariate. Adding an interaction term removes this restriction.</p>
<p>Below, we fit the second model which includes an interaction term, and produce a second scatter plot with the fitted values from our new model superimposed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create the interaction term</span>
<span class="n">data</span><span class="o">$</span><span class="n">int1</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span>

<span class="c1">#Include the interaction term in our model</span>
<span class="n">int_model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">)</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">int1</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)</span>

<span class="c1">#Scatter plot</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">0</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
     <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&quot;Birthweight&quot;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">))</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="n">int_model1</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">1</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="n">int_model1</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;blue&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
<span class="nf">points</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span> <span class="o">==</span> <span class="m">1</span><span class="p">,</span> <span class="p">]</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="nf">rgb</span><span class="p">(</span><span class="n">red</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">green</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">blue</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0.25</span><span class="p">),</span> <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">3</span><span class="p">],</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">2</span><span class="p">]</span> <span class="o">+</span> <span class="nf">coef</span><span class="p">(</span><span class="n">int_model1</span><span class="p">)[</span><span class="m">4</span><span class="p">],</span> 
       <span class="n">col</span> <span class="o">=</span> <span class="s">&quot;red&quot;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + factor(data$Maternal.Smoker2) + 
    data$int1)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.023 -11.078  -0.084   9.995  50.499 

Coefficients:
                                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                     19.63964   10.29098   1.908 0.056580 .  
data$Gestational.Days            0.36962    0.03671  10.069  &lt; 2e-16 ***
factor(data$Maternal.Smoker2)1 -72.68713   17.23243  -4.218 2.65e-05 ***
data$int1                        0.23085    0.06176   3.738 0.000194 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.16 on 1170 degrees of freedom
Multiple R-squared:  0.2249,	Adjusted R-squared:  0.2229 
F-statistic: 113.2 on 3 and 1170 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
<img alt="_images/13. Linear Regression II_14_1.png" src="_images/13. Linear Regression II_14_1.png" />
</div>
</div>
<p>In our new model, the intercept and slope among the non-smoking group are 19.64 ounces and 0.37 ounces respectively. The intercept and slope among the smoking group are 19.64-72.69=-53.05 ounces and  0.37+0.23=0.60 ounces respectively. The interaction term has <span class="math notranslate nohighlight">\(p\)</span>=0.0001, so there is evidence that the slopes are different.</p>
</div>
<div class="section" id="interaction-between-two-binary-variables">
<h4>13.4.2.3 Interaction between two binary variables<a class="headerlink" href="#interaction-between-two-binary-variables" title="Permalink to this headline">¶</a></h4>
<p>When both of the covariates are binary, the model can be written as:</p>
<p>\begin{align}
&amp; y_i =  \beta_0 + \epsilon_i \text{ when } X_1=0 \text{ and } X_2=0 \
&amp; y_i = \beta_0 + \beta_1 + \epsilon_i \text{ when } X_1=1 \text{ and } X_2=0 \
&amp; y_i = \beta_0 + \beta_2 + \epsilon_i \text{ when } X_1=0 \text{ and } X_2=1 \
&amp; y_i = \beta_0 + \beta_1 + \beta_2 + \beta_3 + \epsilon_i \text{ when } X_1=1 \text{ and } X_2=1
\end{align}</p>
<p>If we denote the population mean of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X_1=i\)</span> and <span class="math notranslate nohighlight">\(X_2=j\)</span> by <span class="math notranslate nohighlight">\(\mu_{ij}\)</span>, then the interpretation of each of the parameters is as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the mean value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X_1=0\)</span> and <span class="math notranslate nohighlight">\(X_2=0\)</span> (<span class="math notranslate nohighlight">\(\mu_{00}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0+\beta_1\)</span> is the mean value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_2=0\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_1\)</span> is the difference in the mean values of <span class="math notranslate nohighlight">\(Y\)</span> between the two groups defined by <span class="math notranslate nohighlight">\(X_1\)</span> when <span class="math notranslate nohighlight">\(X_2=0\)</span> (<span class="math notranslate nohighlight">\(\mu_{10}-\mu_{00}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_2\)</span> is the mean value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X_1=0\)</span> and <span class="math notranslate nohighlight">\(X_2=1\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_2\)</span> is the difference in the mean values of <span class="math notranslate nohighlight">\(Y\)</span> between the two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span> when <span class="math notranslate nohighlight">\(X_1=0\)</span> (<span class="math notranslate nohighlight">\(\mu_{01}-\mu_{00}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span>+<span class="math notranslate nohighlight">\(\beta_1\)</span>+<span class="math notranslate nohighlight">\(\beta_2\)</span>+<span class="math notranslate nohighlight">\(\beta_3\)</span> is the mean value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_2=1\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_3\)</span> is the difference in the mean values of <span class="math notranslate nohighlight">\(Y\)</span> between the two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span> when <span class="math notranslate nohighlight">\(X_1=1\)</span> minus the difference in the mean values of <span class="math notranslate nohighlight">\(Y\)</span> between two groups defined by <span class="math notranslate nohighlight">\(X_2\)</span> when <span class="math notranslate nohighlight">\(X_1=0\)</span> <span class="math notranslate nohighlight">\([(\mu_{11}-\mu_{10})-(\mu_{01}-\mu_{00})]\)</span>.</p></li>
</ul>
<p>Interpretation of <span class="math notranslate nohighlight">\(\beta_3\)</span> is symmetric in <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>: it is also interpretable as the difference in the mean values of <span class="math notranslate nohighlight">\(Y\)</span> between the two groups defined by <span class="math notranslate nohighlight">\(X_1\)</span> when <span class="math notranslate nohighlight">\(X_2=1\)</span> minus the same difference when <span class="math notranslate nohighlight">\(X_2=0\)</span> <span class="math notranslate nohighlight">\([(\mu_{11}-\mu_{01})-(\mu_{10}-\mu_{00})]\)</span>.</p>
<p><em>Example.</em> As there is only one binary variable in the birthweight data (mother’s smoking status), we will create an additional binary variable by categorising the mother’s height (now denoted as <span class="math notranslate nohighlight">\(X_2\)</span>) into two groups:</p>
<div class="math notranslate nohighlight">
\[\begin{split} X_2 = 
\begin{cases}
0 \text{ when the mother's height is 64 inches or less} \\
1 \text{ when the mother's height is greater than 64 inches} 
\end{cases}
\end{split}\]</div>
<p>This is not something that we recommend in practice! We are categorising the variable here purely to demonstrate the inclusion of binary-binary interactions in linear regression models.</p>
<p>The R code below fits a linear regression model relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>) to mother’s smoking status (<span class="math notranslate nohighlight">\(X_1\)</span>) and mother’s height group (<span class="math notranslate nohighlight">\(X_2\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create a binary variable indicating height group</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height2</span><span class="o">&lt;-</span><span class="m">0</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height2</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">&gt;</span><span class="m">64</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">1</span>

<span class="c1">#Create the interaction term </span>
<span class="n">data</span><span class="o">$</span><span class="n">int2</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height2</span>

<span class="c1">#Model relating birthweight to mothers smoking status, height and their interaction</span>
<span class="n">int_model2</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker2</span><span class="p">)</span><span class="o">+</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height2</span><span class="p">)</span><span class="o">+</span><span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">int2</span><span class="p">))</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">int_model2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ factor(data$Maternal.Smoker2) + 
    factor(data$Maternal.Height2) + factor(data$int2))

Residuals:
    Min      1Q  Median      3Q     Max 
-71.662 -10.424  -0.424  11.338  53.576 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    120.4244     0.8639 139.394  &lt; 2e-16 ***
factor(data$Maternal.Smoker2)1  -9.7187     1.4072  -6.906 8.14e-12 ***
factor(data$Maternal.Height2)1   6.2379     1.3227   4.716 2.70e-06 ***
factor(data$int2)1               0.5351     2.1056   0.254    0.799    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 17.49 on 1170 degrees of freedom
Multiple R-squared:  0.09145,	Adjusted R-squared:  0.08912 
F-statistic: 39.26 on 3 and 1170 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p><em>Exercise:</em> Interpret each of the estimated regression coefficients in this model. What are the mean birthweights of babies in the four groups defined by mother’s smoking status and height group?</p>
</div>
<div class="section" id="interaction-between-two-continuous-predictor-variables">
<h4>13.4.2.4 Interaction between two continuous predictor variables<a class="headerlink" href="#interaction-between-two-continuous-predictor-variables" title="Permalink to this headline">¶</a></h4>
<p>We have seen that when <span class="math notranslate nohighlight">\(X_1\)</span> is continuous and <span class="math notranslate nohighlight">\(X_2\)</span> is binary, the interaction term is interpretable as the difference in slopes (<span class="math notranslate nohighlight">\(Y\)</span> on <span class="math notranslate nohighlight">\(X_1\)</span>) between two groups defined by the binary variable <span class="math notranslate nohighlight">\(X_2\)</span>. When both <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are continuous, the interaction term is interpretable as the difference in slopes (<span class="math notranslate nohighlight">\(Y\)</span> on <span class="math notranslate nohighlight">\(X_1\)</span>) per unit increase in <span class="math notranslate nohighlight">\(X_2\)</span>.</p>
<p><em>Example:</em> To illustrate continuous-continuous interactions, we will fit a model relating birthweight (<span class="math notranslate nohighlight">\(Y\)</span>) to length of pregnancy (<span class="math notranslate nohighlight">\(X_1\)</span>), mother’s height (<span class="math notranslate nohighlight">\(X_2\)</span>; <span class="math notranslate nohighlight">\(X_2\)</span> is now treated as a continuous variable) and the interaction between length of pregnancy and mother’s height (<span class="math notranslate nohighlight">\(X_3\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create the interaction term </span>
<span class="n">data</span><span class="o">$</span><span class="n">int3</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">*</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span>

<span class="c1">#Include the interaction term in the regression model </span>
<span class="n">int_model3</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">+</span><span class="n">data</span><span class="o">$</span><span class="n">int3</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">int_model3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = data$Birth.Weight ~ data$Gestational.Days + data$Maternal.Height + 
    data$int3)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.794 -10.572   0.204  10.265  54.515 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)           -27.200984 222.960540  -0.122    0.903
data$Gestational.Days   0.232172   0.799566   0.290    0.772
data$Maternal.Height    0.320110   3.473652   0.092    0.927
data$int3               0.003432   0.012453   0.276    0.783

Residual standard error: 16.45 on 1170 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1949 
F-statistic: 95.63 on 3 and 1170 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>To interpret the above output, let’s first focus on the relationship between birthweight and height in babies born at 200 gestational days.</p>
<p>\begin{align}
E(Y|X_1=200)&amp;=-27.201+0.232 \times 200+0.320 \times X_2+0.003 \times 200 \times X_2\
&amp;= 19.200 + 0.920X_2
\end{align}</p>
<p>Amongst babies born at 201 days the analogous relationship is:</p>
<p>\begin{align}
E(Y|X_1=201)&amp;=-27.201+0.232 \times 201+0.320 \times X_2+0.003 \times 201 \times X_2\
&amp;=  19.431 + 0.923X_2
\end{align}</p>
<p>Amongst babies born at 202 days the analogous relationship is:</p>
<p>\begin{align}
E(Y|X_1=202)&amp;=-27.201+0.232 \times 202+0.320 \times X_2+0.003 \times 202 \times X_2\
&amp;=  19.662 + 0.926X_2
\end{align}</p>
<p>The interpretation is that at each number of gestational days, the relationship between height and birthweight is linear with the slope of association increasing by 0.003 ounces (<span class="math notranslate nohighlight">\(\hat{\beta_3}\)</span>) for each one day increase. However, in this example the <span class="math notranslate nohighlight">\(p\)</span>-value associated with the estimated regression coefficient for the interaction term is 0.783, indicating that the data do not provide evidence of an interaction between gestational days and mother’s height on birthweight.</p>
</div>
</div>
<div class="section" id="collinearity">
<h3>13.4.3 Collinearity<a class="headerlink" href="#collinearity" title="Permalink to this headline">¶</a></h3>
<p>A potential issue that can arise from including higher-order terms or interaction terms in a linear regression model is collinearity. Collinearity occurs when there is correlation between one or more of the independent covariates. If the degree of correlation between covariates is high enough, it can cause the following problems:</p>
<ol class="simple">
<li><p>Estimated coefficients can swing in either direction: known important variables may have surprisingly small coefficient estimates and known less important variables may have surprisingly large coefficient estimates.</p></li>
<li><p>Increased variance of estimated coefficients, therefore reducing the statistical power of the model.</p></li>
</ol>
<p>Including higher-order terms or interaction terms can result in collinearity due to the inevitable correlation between variables, their powers and the interaction terms involving them. Having said that, collinearity is only a concern in particular situations.</p>
<p>Collinearity only affects the specific independent variables that are correlated. Therefore, if the aim of the analysis is to estimate the way <span class="math notranslate nohighlight">\(X\)</span> influences <span class="math notranslate nohighlight">\(Y\)</span> after adjusting for <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, and there is only correlation between <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, then collinearity is not a concern. Moreover, collinearity rarely affects the predicted outcomes, so if the aim of the analysis is to predict <span class="math notranslate nohighlight">\(Y\)</span> using data from <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, then collinearity between any of the covariates is not a concern. Finally, the severity of the problems caused by collinearity increases with the degree of correlation. Therefore, if only moderate or weak correlation is present, then collinearity is not a concern.</p>
<p>If we are in a situation where collinearity is causing problems, then we could either remove some of the highly correlated variables, or transform one of them. Examples of transformations include:</p>
<ol class="simple">
<li><p>Instead of using systolic and diastolic blood pressure as collinear predictor variables, use diastolic blood pressure and (systolic-diastolic blood pressure).</p></li>
<li><p>Instead of using height and weight as predictor variables, use height and body mass index (weight/height<span class="math notranslate nohighlight">\(^2\)</span>).</p></li>
<li><p>When fitting a quadratic regression model, use <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\((X-\bar{X})^2\)</span>, rather than <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(X^2\)</span> as covariates.</p></li>
</ol>
</div>
</div>
<div class="section" id="different-types-of-investigation">
<h2>13.5 Different types of investigation<a class="headerlink" href="#different-types-of-investigation" title="Permalink to this headline">¶</a></h2>
<p>We have now learnt how to fit linear models, interpret the results, test the assumptions and model more complex associations. We now consider the different types of research question that linear regression models can be used for. We will see how the type of research question being asked can influence the role of the covariates and the interpretation and presentation of the results.</p>
<div class="section" id="id1">
<h3>13.5.1 Different types of investigation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The three different types of investigation we focus on here are:</p>
<ol class="simple">
<li><p>Analysis of risk factors</p></li>
<li><p>Prediction analysis</p></li>
<li><p>Causal inference</p></li>
</ol>
<p>These types of investigation were introduced in Lesson 11 and we provide a brief reminder here.</p>
<p>A <strong>risk factor</strong> for a particular outcome is a variable that is associated with that outcome. In a risk factor analysis the aim is to investigate how one or more potential risk factors are associated with an outcome. For example, we may want investigate how age, sex, comorbidities etc. are associated with the risk of contracting coronavirus disease.</p>
<p>Prediction analysis uses variables in our data to predict a future observation of a particular outcome. The model in a prediction analysis may be referred to as a <strong>prediction model</strong>, a <strong>risk prediction model</strong> or a <strong>prognosis model</strong>. A well known example of a predition model is the <a class="reference external" href="https://qrisk.org/three/">QRISK</a> score which predicts a person’s risk of developing a heart attack or stroke over the next 10 years using a range of characteristics such as age, sex, smoking status etc.</p>
<p>Causal inference focuses on estimating the effect of a particular <strong>exposure</strong> or <strong>treatment</strong> of interest on an outcome.  For example, we may be interested in the <strong>casual effect</strong> of insulin use (treatment) on lung function (outcome) in patients with cystic fibrosis.</p>
<p><em>Exercise</em>. Recall the three examples given for types questions that we can investigate using statistical models (at the beginning of the previous chapter):</p>
<ul class="simple">
<li><p>Does birthweight increase with length of pregnancy?</p></li>
<li><p>Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?</p></li>
<li><p>Can we predict the risk of heart disease for our patients?</p></li>
</ul>
<p>What type of research question do you classify the above examples as?</p>
<p>In the following subsections we will consider how the different types of investigation influence how we perform and interpret linear regression analyses in more detail. In each case we will assume that we are interested in one outcome <span class="math notranslate nohighlight">\(Y\)</span> and three covariates <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span>.</p>
</div>
<div class="section" id="analysis-of-risk-factors">
<h3>13.5.2 Analysis of risk factors<a class="headerlink" href="#analysis-of-risk-factors" title="Permalink to this headline">¶</a></h3>
<p>Analyses of risk factors are concerned with estimating conditional and/or unconditional associations between the covariates (<span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span>) and the outcome (<span class="math notranslate nohighlight">\(Y\)</span>). Typical analyses may start with univariable regression models (e.g. three simple linear regression models relating <span class="math notranslate nohighlight">\(Y\)</span> to each covariate individually) and then proceed to a multivariable analysis. In such analyses we would report the estimated regression coefficients and associated confidence intervals.</p>
<p>In such analyses it is important to remember that association does not imply causation. Results from risk factor analyses are often mistakenly interpreted as causal. However, this type of analysis does not consider temporal ordering, or appropriately account for confounding (a concept described in more detail in a subsequent section) and so interpreting the regression coefficients in a risk factor analysis as causal effects can be misleading.</p>
<p>The results from a risk factor analysis may reveal some associations that are potentially causal and it might be of interest to follow up with a formal causal inference.</p>
</div>
<div class="section" id="prediction">
<h3>13.5.3 Prediction<a class="headerlink" href="#prediction" title="Permalink to this headline">¶</a></h3>
<p>In a prediction analysis, we would use <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span> and <span class="math notranslate nohighlight">\(X_3\)</span> to predict <span class="math notranslate nohighlight">\(Y\)</span> and the <span class="math notranslate nohighlight">\(X\)</span>-variables are often termed <strong>predictors</strong>. In this type of analysis, we are less concerned with how the covariate relates to each other and their temporal ordering. We simply want to achieve a good prediction of <span class="math notranslate nohighlight">\(Y\)</span>. The statistics of interest are the fitted values (often termed <strong>predictions</strong> or <strong>predicted values</strong>), rather than the estimated regression coefficients. When reporting the results of a prediction analysis we would report which variables were included in the prediction model and how they were modelled, along with diagnostics that measure the predictive performance of the model (e.g. the coefficient of determination and the mean square error).</p>
</div>
<div class="section" id="causal-inference">
<h3>13.5.4 Causal inference<a class="headerlink" href="#causal-inference" title="Permalink to this headline">¶</a></h3>
<p>Suppose we are interested in estimating the effect of <span class="math notranslate nohighlight">\(X_1\)</span> on <span class="math notranslate nohighlight">\(Y\)</span>. The focus of the analysis is on estimating the regression coefficient of <span class="math notranslate nohighlight">\(X_1\)</span> (<span class="math notranslate nohighlight">\(\beta_1\)</span>) and the results should report this estimate along with a confidence interval. However, in order to obtain an estimate of <span class="math notranslate nohighlight">\(\beta_1\)</span> that we can interpret causally, we need to control (or adjust) for the <strong>confounding effects</strong> of the other covariates. This can be a difficult task and for this reasons causal inference is often thought to be the most challenging type of investigation.</p>
<p>Confounding occurs when there are covariates that obscure the true causal association between the exposure of interest and the outcome. This will happen when the exposure and the outcome share common causes.</p>
<p><em>Example 1:</em> Suppose we are interested in the effect of air pollution on lung function. Here, air pollution is the exposure (<span class="math notranslate nohighlight">\(X\)</span>) and lung function is the outcome (<span class="math notranslate nohighlight">\(Y\)</span>). In our analysis we compare the average lung functions of people living in an area with low air pollution (the <strong>unexposed</strong> or <strong>control</strong> group) to people in an area with high air pollution (the <strong>exposed</strong> group), using a simple linear regression relating <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(X\)</span> (<span class="math notranslate nohighlight">\(X=1\)</span> for people in the exposed group and <span class="math notranslate nohighlight">\(X=0\)</span> for people in the unexposed group). We find that the average lung function is much worse in the exposed group, compared to the unexposed and so conclude that air pollution has a strong negative effect on lung function. However, suppose that the people in the unexposed group exercise more, on average, than the exposed group (poor air quality discourages people from spending time outside and therefore the exposed group exercise less). Since exercise is a predictor of lung function, the average lung function in the unexposed group would be higher, on average, than the lung function in the exposed group. Consequently, a simple comparison of average lung functions between the two groups, not accounting for exercise, would make the effect of air pollution seem larger than it actually is. In this case, exercise <strong>confounds</strong> the association between air pollution and lung function. To obtain an unbiased estimate of the causal effect of air pollution on lung function, we would need to adjust for exercise.</p>
<p><em>Example 2:</em> Suppose we are interested in the effect of taking vitamin supplements on risk of cardiovascular disease. We compare the rates of cardiovascular disease in people who take vitamins compared to those who don’t, and find that the group who do take vitamins are less likely to develop cardiovascular disease. We may conclude that taking vitamins lowers risk of cardiovascular disease. However, the people who do take vitamins are also more likely to exercise more, eat more healthily, drink less alcohol and not smoke, compared to those who do not take vitamins. It turns out, that their lifestyle choices are the real causes of the lower risk of cardiovascular disease, not the use of vitamins.</p>
<p><em>Exercise: identify the exposure, outcome and confounders in Example 2.</em></p>
</div>
<div class="section" id="example-using-the-birthweight-data">
<h3>13.5.6 Example using the birthweight data<a class="headerlink" href="#example-using-the-birthweight-data" title="Permalink to this headline">¶</a></h3>
<p>As an example of the ideas discussed above, we will return to the multivariable linear regression model we have previous defined, relating birthweight to length of pregnancy and mother’s height. We will consider how this model can be presented and interpreted differently, depending on the aims of the analysis.</p>
<p>Recall the model is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Model 3: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i +  \epsilon_i \]</div>
<p>A DAG depicting the assumed relationships between the variables in the model, and R output summarising the results of the model are given below.</p>
<p><img alt="ExampleDAG.png" src="attachment:ExampleDAG.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span>
<span class="nf">confint</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">parm</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days + Maternal.Height, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -88.51993   14.31910  -6.182 8.73e-10 ***
Gestational.Days   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
<div class="output text_html"><table>
<thead><tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr></thead>
<tbody>
	<tr><th scope=row>Gestational.Days</th><td>0.3934004</td><td>0.5113389</td></tr>
	<tr><th scope=row>Maternal.Height</th><td>0.9022310</td><td>1.6497197</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="section" id="id2">
<h4>13.5.6.1 Analysis of risk factors<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>Suppose our aim was to explore length of pregnancy and mother’s height as potential “risk factors” of birthweight. Then we would interpret these results as follows:</p>
<p>After adjusting for mother’s height, a daily increase in length of pregnancy was associated with an increase of 0.45 (0.39-0.51) ounces in mean birthweight. After adjusting for length of pregnancy, an increase of one inch the mother’s height was associated with an increase of 1.28 (0.90-1.65) ounces in mean birthweight.</p>
<p>Since 0 is not included in the 95% confidence intervals for height or length of pregnancy, we can conclude that there is evidence of conditional associations between mother’s height and birthweight, and length of pregnancy and birthweight.</p>
</div>
<div class="section" id="prediction-analysis">
<h4>13.5.6.2 Prediction analysis<a class="headerlink" href="#prediction-analysis" title="Permalink to this headline">¶</a></h4>
<p>Now suppose our aim was to predict the birthweight of future babies, using information on their mother’s height and the length of pregnancy. We are now less interested in the estimated regression coefficients and more interested in the predicted values. For example, suppose we wanted to predict the birthweight of a baby whose mother was 66 inches and whose pregnancy lasted 200 days. We would obtain the relevant predicted value and its 95% confidence interval:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">new.data</span><span class="o">&lt;-</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">Gestational.Days</span><span class="o">=</span><span class="m">200</span><span class="p">,</span><span class="n">Maternal.Height</span><span class="o">=</span><span class="m">66</span><span class="p">)</span>
<span class="nf">predict</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new.data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;confidence&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>
<tbody>
	<tr><td>86.16838</td><td>81.30474</td><td>91.03201</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Or, we may wish to obtain the relevant prediction interval:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">predict</span><span class="p">(</span><span class="n">model3</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new.data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>
<tbody>
	<tr><td>86.16838</td><td>53.54902</td><td>118.7877</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Based on the above results, we would predict that a baby whose mother was 66 inches tall and whose pregancy lasted 200 days would weigh somewhere between 81.30 and 91.03 ounces. Additionally, we estimate that 95% of babies whose mother was 66 inches tall and whose pregancy lasted 200 days would weigh between 53.55 and 118.79 ounces.</p>
<p>It’s important to also present statistics indicating the predictative performance of the model. In this case, <span class="math notranslate nohighlight">\(R^2=0.1969\)</span>, indicating that the model can only account for 19.7% of the total variation in the outcome.</p>
</div>
<div class="section" id="id3">
<h4>13.5.6.3 Causal inference<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>Finally, suppose our aim was to estimate the causal effect of length of pregnancy on birthweight. As is shown in the DAG, we assumed that mother’s height was a common cause of length of pregnancy and birthweight and therefore needed to adjust for it in the analysis to remove confounding bias.</p>
<p>From the output above, we would report <span class="math notranslate nohighlight">\(\hat{\beta_1}=0.45\)</span> and the 95% confidence interval: (0.39, 0.51). Based on these results, we can conclude that length of pregnancy does have a causal effect on birthweight (since 0 does not lie within the confidence interval). However, the validity of these findings rely on the assumption that there are <strong>no unmeasured variables causing confounding bias</strong> (which is a very strong assumption and can be difficult to justify).</p>
<p>It is important to understand that we cannot interpret <span class="math notranslate nohighlight">\(\hat{\beta_2}\)</span> in the same way as <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> in this analysis. We interpreted <span class="math notranslate nohighlight">\(\beta_1\)</span> causally, because mother’s height was the only confounding variable in the association between length of pregnancy and birthweight, and we controlled for it in the analysis. However, length of pregnancy is not a confounding variable for the association between mother’s height and birthweight. In fact, length of pregnancy lies on the causal pathway between mother’s height and birthweight, which means that some of the effect of mother’s height on birthweight can be explained through the length of pregnancy. According to our DAG, the <strong>total effect</strong> of mother’s height on birthweight comprises of two parts: (1) the <strong>direct effect</strong> (denoted by the path between height and birthweight) and (2) the <strong>indirect effect</strong> (denoted by the causal pathway that runs through length of pregnancy). Since we have adjusted for length of pregnancy in our analysis, we have controlled for the indirect effect of mother’s height on birthweight and therefore <span class="math notranslate nohighlight">\(\hat{\beta}_2\)</span> represents the direct effect only.</p>
<p>It is a common mistake in medical research to interpret all the estimated regression coefficients from a multivariable model in the same way, but as our example has shown, this can be misleading. This problem is known in the literature as the <strong>Table 2 fallacy</strong>.</p>
</div>
</div>
</div>
<div class="section" id="summary-of-chapter">
<h2>13.6 Summary of chapter<a class="headerlink" href="#summary-of-chapter" title="Permalink to this headline">¶</a></h2>
<p>In the current chapter, we have learnt the following:</p>
<ul class="simple">
<li><p>How to use graphical tools to investigate the assumptions of a linear regression model</p></li>
<li><p>Possible solutions to violations of the assumptions exist, including:</p>
<ul>
<li><p>Checking the data</p></li>
<li><p>Transformations</p></li>
<li><p>Sensitivity analyses</p></li>
</ul>
</li>
<li><p>How to use quadratic regression to model non-linear associations and test for linearity</p></li>
<li><p>Methods for modelling more complex non-linear relationship exist, including:</p>
<ul>
<li><p>Polynomial regression</p></li>
<li><p>Piecewise polynomial regression</p></li>
</ul>
</li>
<li><p>How to include interaction terms in a linear regression model and how to interpret their regression coefficients</p></li>
<li><p>When collinearity might be an issue and potential solutions</p></li>
<li><p>Regression models can be used to investigate different types of research questions, and the interpretation and presentation of the results depends on the type of question being investigated.</p></li>
</ul>
<p>All of our discussion on regression models so far has focussed on a continuous outcome. In the next chapter, we consider regression models that can be used when the outcome is binary.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "LSHTM-HDS/Content-2021",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="12.%20Linear%20Regression%20I.html" title="previous page">12. Linear Regression I</a>
    <a class='right-next' id="next-link" href="14.%20Logistic%20Regression.html" title="next page">14 Logistic Regression</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By MSc Health Data Science, LSHTM<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>