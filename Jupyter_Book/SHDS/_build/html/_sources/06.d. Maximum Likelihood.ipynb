{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Summary\n",
    "\n",
    "We now know how to obtain the likelihood and log-likelihood functions when you have an i.i.d. sample of observations. We can then obtain the maximum likelihood estimators of the parameters of the distribution. The MLE is an important tool as it has a number of important asymptotic properties, as we demonstrated using a simulation in R. Finally, we introduced the idea of a log-likelihood ratio, which is a way of comparing estimates of a parameter with the maximum likelihood estimate.   \n",
    "\n",
    "You may be wondering how you might measure the precision of your estimator. We will cover this in Session 7, where we will meet the concept of confidence intervals. Note that the maximum likelihood estimator, and confidence intervals, are tools from the \"frequentist\" or \"classical\" approach to statistics. In Sessions 9 and 10, you will meet the Bayesian approach to statistics, where the Likelihood will also play an important role. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
