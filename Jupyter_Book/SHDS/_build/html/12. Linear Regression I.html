
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12. Linear Regression I &#8212; Statistics for Health Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="13. Linear Regression II" href="13.%20Linear%20Regression%20II.html" />
    <link rel="prev" title="11. Types of Investigation" href="11.%20Types%20of%20Investigation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistics for Health Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00.%20Welcome.html">
   Welcome to Statistics for Health Data Science
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01.%20Introduction.html">
   1 Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basic probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="02.a.%20Probability.Discrete.html">
   2. Probability and Discrete Probability Distributions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="02.b.%20Probability.Discrete.html">
     2.1 Bayes’ Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02.c.%20Probability.Discrete.html">
     2.2 The binomial distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02.d.%20Probability.Discrete.html">
     2.3 The Poisson distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="03.a.%20Continuous%20Probability%20Distributions.html">
   3. Continuous probability distributions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="03.b.%20Continuous%20Probability%20Distributions.html">
     3.1 Continuous random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.c.%20Continuous%20Probability%20Distributions.html">
     3.2 Useful continuous distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.d.%20Continuous%20Probability%20Distributions.html">
     3.3 Uses of the standard Normal distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.e.%20Continuous%20Probability%20Distributions.html">
     3.5 Are the data normally distributed?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.f.%20Continuous%20Probability%20Distributions.html">
     3.5 Joint distributions and correlations
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical Inference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="04.a.%20Population.and.samples.html">
   4. Populations and Samples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="04.b.%20Population.and.samples.html">
     4.1 Sampling from a population
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.c.%20Population.and.samples.html">
     4.2 Statistical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.d.%20Population.and.samples.html">
     4.3 Sampling distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.e.%20Population.and.samples.html">
     4.4 Obtaining the sampling distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.f.%20Population.and.samples.html">
     4.5 Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.g.%20Population.and.samples.html">
     Appendix: additional reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="05.a.%20Likelihood.html">
   5. Likelihood
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="05.b.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.c.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.d.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.f.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.g.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="06.a.%20Maximum%20Likelihood.html">
   6. Maximum Likelihood
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="06.b.%20Maximum%20Likelihood.html">
     6.1 Likelihood and log-likelihood with
     <span class="math notranslate nohighlight">
      \(n\)
     </span>
     independent observations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.c.%20Maximum%20Likelihood.html">
     6.2 Properties of maximum likelihood estimators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.d.%20Maximum%20Likelihood.html">
     6.3 Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.e.%20Maximum%20Likelihood.html">
     Appendix: Additional Reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="07.a.%20Frequentist%20I.html">
   7. Frequentist I: Confidence Intervals (CIs)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="07.b.%20Frequentist%20I.html">
     7.1 Introduction to confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.c.%20Frequentist%20I.html">
     7.2 95% confidence intervals for the mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.d.%20Frequentist%20I.html">
     7.3 Interpretation of confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.e.%20Frequentist%20I.html">
     7.4 Approximate confidence intervals for parameters estimated using large samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.f.%20Frequentist%20I.html">
     7.5 Confidence Intervals using resampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.g.%20Frequentist%20I.html">
     7.6 Summary: Use of confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.h.%20Frequentist%20I.html">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="08.a.%20Frequentist%20II.html">
   8. Frequentist II: Hypothesis tests
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="08.b.%20Frequentist%20II.html">
     8.1 Proving and disproving hypotheses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.c.%20Frequentist%20II.html">
     8.2 The p-value
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.d.%20Frequentist%20II.html">
     8.3 Connection between p-values and confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.e.%20Frequentist%20II.html">
     8.4 Other (mis-)interpretations of p-values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.f.%20Frequentist%20II.html">
     8.5 Calculating p-values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.g.%20Frequentist%20II.html">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09.%20Bayesian%20Statistics%20I.html">
   9. Bayesian Statistics I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10.%20Bayesian%20Statistics%20II.html">
   10. Bayesian Statistics II: Normal data
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical modelling
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="11.%20Types%20of%20Investigation.html">
   11. Types of Investigation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   12. Linear Regression I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13.%20Linear%20Regression%20II.html">
   13. Linear Regression II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14.%20Logistic%20Regression.html">
   14 Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15.%20Poisson%20Regression%20Model.html">
   15. Generalised Linear Models: Poisson Regression for Count Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16.%20Extensions%20Confounding%2C%20standardization%2C%20and%20collapsibility.html">
   16. Extensions: Confounding, standardization, and collapsibility
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/12. Linear Regression I.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/LSHTM-HDS/Content-2021"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/LSHTM-HDS/Content-2021/issues/new?title=Issue%20on%20page%20%2F12. Linear Regression I.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/LSHTM-HDS/Content-2021/master?urlpath=tree/docs/12. Linear Regression I.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   12.1 Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-used-in-our-examples">
     12.1.1 Data used in our examples
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-linear-regression-model">
   12.2 Simple linear regression model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-simple-linear-regression-model">
     12.2.1 The simple linear regression model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-linear-predictor">
       12.2.1.1 The linear predictor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-error-term">
       12.2.1.2 The error term
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#different-ways-of-expressing-the-simple-linear-regression-model">
       12.2.1.3 Different ways of expressing the simple linear regression model
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#assumptions">
       12.2.1.4 Assumptions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation-of-the-population-parameters">
     12.2.2 Estimation of the population parameters.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ordinary-least-squares-estimates">
       12.2.2.1 Ordinary least squares estimates
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimation-of-the-residual-variance">
       12.2.2.2 Estimation of the residual variance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
       12.2.2.3 Maximum likelihood estimation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#properties-of-hat-beta-1">
       12.2.2.4 Properties of
       <span class="math notranslate nohighlight">
        \(\hat{\beta}_1\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples">
     12.2.3 Examples
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-1">
       12.2.3.1 Example 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-2">
       12.2.3.2 Example 2
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference">
     12.3.4 Inference
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hypothesis-testing">
       12.3.4.1 Hypothesis testing
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#confidence-intervals-for-the-regression-coefficients">
       12.3.4.2 Confidence intervals for the regression coefficients
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#confidence-intervals-for-a-fitted-value">
       12.3.4.3 Confidence intervals for a fitted value
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prediction-intervals">
       12.3.4.4 Prediction intervals
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multivariable-linear-regression">
   12.4 Multivariable linear regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-multivariable-linear-regression-model">
     12.4.1 The multivariable linear regression model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimation-of-the-parameters">
     12.4.2 Estimation of the parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering">
     12.4.4 Centering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#including-categorical-predictor-variables-in-linear-regression-models">
     12.4.4 Including categorical predictor variables in linear regression models
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#birthweight-data-model-4">
       12.4.4.1 Birthweight data: Model 4
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#issues-related-to-categorising-continuous-variables">
       12.4.4.2 Issues related to categorising continuous variables
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analysis-of-variance">
   12.5 Analysis of Variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anova-for-simple-linear-regression">
     12.5.1 ANOVA for simple linear regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-coefficient-of-determination">
       12.5.1.1 The coefficient of determination
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-anova-table">
       12.5.1.2 The ANOVA table.
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hypothesis-testing-using-anova">
       12.5.1.3 Hypothesis testing using ANOVA
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#similarity-between-f-tests-and-t-tests-in-simple-linear-regression">
       12.5.1.4 Similarity between F tests and t-tests in simple linear regression
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-using-model-1">
       12.5.1.5 Example using Model 1
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anova-for-multivariable-linear-regression">
     12.5.2 ANOVA for multivariable linear regression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-global-f-test">
       12.5.2.2 The Global F test
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-partial-f-test">
     12.5.2 The partial F-test
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anova-for-models-with-categorical-independent-variables">
     12.5.3 ANOVA for models with categorical independent variables.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sum-of-squares-for-models-with-categorical-variables">
       12.5.3.1 Sum of squares for models with categorical variables
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       12.5.3.2 The ANOVA table
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-f-test">
       12.5.3.3 The F-test
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-of-chapter">
   12.6 Summary of chapter
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proofs">
   12.7 Proofs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proof-for-the-ordinary-least-squares-estimates-in-simple-linear-regression">
     12.7.1 Proof for the ordinary least squares estimates in simple linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proof-that-the-ols-estimates-are-also-the-maximum-likelihood-estimates">
     12.7.2 Proof that the OLS estimates are also the maximum likelihood estimates
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression-i">
<h1>12. Linear Regression I<a class="headerlink" href="#linear-regression-i" title="Permalink to this headline">¶</a></h1>
<p>This is the first of four sessions that explore regression modelling. In this, and the next, session we consider linear regression modelling. These are models where the outcome of interest is a continuous variable. We then extend those ideas to regression models for other variable types, such as binary (logistic regression) and count (Poisson regression).</p>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session, you will be able to:</p>
<ul class="simple">
<li><p>explain, in general, the rationale behind parametric statistical models;</p></li>
<li><p>fit and interpret a linear regression model;</p></li>
<li><p>describe the main properties of ordinary least squares estimators;</p></li>
<li><p>perform and interpret an analysis of variance.</p></li>
</ul>
</div><p><strong>Acknowledgements:</strong>  Thank you to Jennifer Nicholas and Chris Frost whose notes on linear regression were particularly useful in the development of the current lesson.</p>
<div class="section" id="introduction">
<h2>12.1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>A parametric statistical model is an algebraic description of how one or more <strong>outcome</strong> variables are influenced by <strong>covariates</strong>. Such models are widely used in medical research. Some examples of questions that we can investigate using statistical models include:</p>
<ul class="simple">
<li><p>Does birthweight increase with length of pregnancy?</p></li>
<li><p>Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?</p></li>
<li><p>Can we predict the risk of heart disease for our patients?</p></li>
</ul>
<p>In the above examples, the outcome variables are birthweight, inflammation and heart disease. In the first two examples, the length of pregnancy and drug use are covariates. In the third example, no covariates are explicitly mentioned. However, when answering the third question, researchers may want to consider a range of patient characteristics that are associated with the risk of heart disease as covariates in their model, for example: diet, exercise, comorbodities, medications etc.</p>
<p>Statistical models contain <strong>population parameters</strong> and representations of <strong>uncertainty</strong>. The population parameters are unknown quantities that we want to estimate from our sample and the uncertainty is a measure of the variability in the outcome variable that is not explained by the covariates.</p>
<p>This is the first of two lessons on linear regression. In this lesson, we will learn how to define linear regression models, how to estimate their population parameters and how to estimate measures of uncertainty. We begin by introducing the <strong>simple linear regression model</strong> (Section 3) which includes one outcome and one covariate. We then introduce the <strong>multivariable linear regression model</strong> (Section 4) which is an extension of the simple linear regression model to situations with multiple covariates. In the final part of this lesson, we learn how to conduct an <strong>analysis of variance</strong> of statistical models (Section 5). In the next lesson we will discuss how statistical models can be used in the different types of investigation that were discussed last week, and how the type of investigation influences the presentation and interpretation of results obtained using statistical models.</p>
<p>Before delving in, it is worth making a note of the different terminologies that you may come across in the medical literature. Here, I have already used the terms: outcome and covariates. Table 1 summarises alternatives terms that may be used to describe the same concepts.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Outcome</p></th>
<th class="head"><p>Covariates</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(Y\)</span>-variable</p></td>
<td><p><span class="math notranslate nohighlight">\(x\)</span>-variables</p></td>
</tr>
<tr class="row-odd"><td><p>Dependent variable</p></td>
<td><p>Independent variable</p></td>
</tr>
<tr class="row-even"><td><p>Response variable</p></td>
<td><p>Regressors</p></td>
</tr>
<tr class="row-odd"><td><p>Output variable</p></td>
<td><p>Input variables</p></td>
</tr>
</tbody>
</table>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>               | Explanatory variables
               | Predictor variables
</pre></div>
</div>
<p>Table 1: Different terminology used for outcome and covariates</p>
<p>Finally, it is important to understand that statistical models make <strong>assumptions</strong> about the form of relationships between outcomes and covariates. Although we can examine our data to investigate the validity of these assumptions (using methods covered in the next lesson), we can never be certain that the model is correct.</p>
<div class="section" id="data-used-in-our-examples">
<h3>12.1.1 Data used in our examples<a class="headerlink" href="#data-used-in-our-examples" title="Permalink to this headline">¶</a></h3>
<p>For our examples we will use data on babies and their mothers. The data contains a random sample of 1,174 mothers and their newborn babies. The column Birth Weight contains the birth weight of the baby, in ounces; Gestational Days is the number of gestational days, that is, the number of days the baby was in the womb. There is also data on maternal age, maternal height, maternal pregnancy weight, and whether or not the mother was a smoker.</p>
<p>The following code can be used to download and look at the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Load data</span>
<span class="n">data</span><span class="o">&lt;-</span> <span class="nf">read.csv</span><span class="p">(</span><span class="s">&#39;https://www.inferentialthinking.com/data/baby.csv&#39;</span><span class="p">)</span>

<span class="c1">#Look at the first 10 rows of the data</span>
<span class="nf">head</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th scope=col>Birth.Weight</th><th scope=col>Gestational.Days</th><th scope=col>Maternal.Age</th><th scope=col>Maternal.Height</th><th scope=col>Maternal.Pregnancy.Weight</th><th scope=col>Maternal.Smoker</th></tr></thead>
<tbody>
	<tr><td>120  </td><td>284  </td><td>27   </td><td>62   </td><td>100  </td><td>False</td></tr>
	<tr><td>113  </td><td>282  </td><td>33   </td><td>64   </td><td>135  </td><td>False</td></tr>
	<tr><td>128  </td><td>279  </td><td>28   </td><td>64   </td><td>115  </td><td>True </td></tr>
	<tr><td>108  </td><td>282  </td><td>23   </td><td>67   </td><td>125  </td><td>True </td></tr>
	<tr><td>136  </td><td>286  </td><td>25   </td><td>62   </td><td> 93  </td><td>False</td></tr>
	<tr><td>138  </td><td>244  </td><td>33   </td><td>62   </td><td>178  </td><td>False</td></tr>
</tbody>
</table>
</div></div>
</div>
</div>
</div>
<div class="section" id="simple-linear-regression-model">
<h2>12.2 Simple linear regression model<a class="headerlink" href="#simple-linear-regression-model" title="Permalink to this headline">¶</a></h2>
<p>The simple linear regression model is used to model the relationship between one single variable (<span class="math notranslate nohighlight">\(X\)</span>) and a single outcome (<span class="math notranslate nohighlight">\(Y\)</span>). For example, suppose we are interested in investigating the following relationships in our birthweight data:</p>
<ol class="simple">
<li><p>Association between the length of pregnancy (i.e. number of gestational days) and birthweight.</p></li>
<li><p>Association between mother’s smoking status and birthweight.</p></li>
</ol>
<p>An important first step in an analysis is to summarise and display the data. Below is a scatterplot and boxplot displaying the relevant data for Examples 1 and 2 respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the plot area into a 1x2 array</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1">#Example 1: Scatter Plot</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span>  <span class="n">main</span><span class="o">=</span><span class="s">&quot;Example 1&quot;</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational Days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span>

<span class="c1">#Example 2: Box plot</span>
<span class="nf">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Smoker</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Example 2&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Mother smokes&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12. Linear Regression I_6_0.png" src="_images/12. Linear Regression I_6_0.png" />
</div>
</div>
<p><em>Example 1:</em> Birthweight and gestational days appear to be highly correlated, where an increase in gestational days is associated with increased birthweight.</p>
<p><em>Example 2:</em> It appears that mother’s who do not smoke give birth to heavier babies, on average, than mothers who do smoke.</p>
<p>Before defining a regression model, we have to decide which is the independent variable and which is the outcome (i.e. the dependent variable). In this context, it is natural to consider birthweight as the outcome: conceptually, it makes little sense to investigate how birthweight influences length of pregnancy or the mother’s smoking status.  However, it is not necessarily always as straightforward. Suppose we were investigating the association between age and weight. It is possible that we might be interested in age as a predictor of weight, or in weight as a predictor of age. The aim of the analysis will guide the choice of outcome.</p>
<p>While the outcome is the same in our two examples, an important difference is the type of independent variable. In Example 1, the independent variable (length of pregnancy) is a continuous variable, whereas in Example 2, the independent variable (mother’s smoking status) is binary (yes or no). Using these examples, we will later see how the two different types of variables are modelled differently in linear regression.</p>
<div class="section" id="the-simple-linear-regression-model">
<h3>12.2.1 The simple linear regression model<a class="headerlink" href="#the-simple-linear-regression-model" title="Permalink to this headline">¶</a></h3>
<p>The equation for the simple linear regression model, relating <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight">
\[Y = \beta_0 + \beta_1 X + \epsilon \]</div>
<p>There are two components of this model: the <strong>linear predictor</strong> and the <strong>error term</strong>. The linear predictor represents the variation in <span class="math notranslate nohighlight">\(Y\)</span> that can be predicted using the model: <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 X\)</span>. The error term, denoted by <span class="math notranslate nohighlight">\(\epsilon\)</span>, represents the variation in <span class="math notranslate nohighlight">\(Y\)</span> that cannot be predicted. This variation is sometimes referred to as the <strong>random error</strong> or <strong>noise</strong>.</p>
<p>The subsequent two sections take a closer look at the linear predictor and error term, respectively.</p>
<div class="section" id="the-linear-predictor">
<h4>12.2.1.1 The linear predictor<a class="headerlink" href="#the-linear-predictor" title="Permalink to this headline">¶</a></h4>
<p>The linear predictor is the algebraic relationship between the mean of the outcome and the independent variable. When <span class="math notranslate nohighlight">\(X\)</span> takes a particular value, <span class="math notranslate nohighlight">\(X=x\)</span>, the value of the linear predictor, <span class="math notranslate nohighlight">\(\beta_0+\beta_1x\)</span>, is interpreted as the expected value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X\)</span> takes the value <span class="math notranslate nohighlight">\(x\)</span>:</p>
<p>$<span class="math notranslate nohighlight">\(E(Y|X=x) = \beta_0 + \beta_1 x\)</span>$.</p>
<p>This value (<span class="math notranslate nohighlight">\(E(Y|X=x)\)</span>) is known as the <strong>fitted</strong>  (or <strong>predicted</strong>) value of <span class="math notranslate nohighlight">\(Y\)</span> and is often denoted by <span class="math notranslate nohighlight">\(\hat{Y}\)</span> (pronounced “Y hat”).</p>
<p>The specification of the linear predictor has two parameters: <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. These are interpreted as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept. It is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X\)</span> takes the value 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the slope (or gradient). It is the expected change in <span class="math notranslate nohighlight">\(Y\)</span> per one unit increase in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ul>
<p>It is worth emphasising that this model assumes that <strong>the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is linear</strong> (hence the name <em>linear</em> regression!). It is important to note that it is possible to have more complex relationships between variables that do not meet this assumption (see examples in the plots below), in which case simple linear regression would not be an appropriate method to use.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">### Set random number generator</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">24082098</span><span class="p">)</span>

<span class="c1">#Set graphical display to show 2 plots in a row</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1">#Simulate a linear X-Y relationship and plot</span>
<span class="n">x</span><span class="o">&lt;-</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
<span class="n">ylin</span><span class="o">&lt;-</span><span class="n">x</span><span class="o">+</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ylin</span><span class="p">,</span><span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">yaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Linear Association&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Y&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">xlab</span><span class="o">=</span><span class="s">&quot;X&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>

<span class="c1">#Simulate a non-linear X-Y relationship and plot</span>
<span class="n">ynonlin</span><span class="o">&lt;-</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ynonlin</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-0.5</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="n">yaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">xaxt</span><span class="o">=</span><span class="s">&quot;n&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Non-linear Association&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Y&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="n">xlab</span><span class="o">=</span><span class="s">&quot;X&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">cex.lab</span><span class="o">=</span><span class="m">1.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12. Linear Regression I_8_0.png" src="_images/12. Linear Regression I_8_0.png" />
</div>
</div>
</div>
<div class="section" id="the-error-term">
<h4>12.2.1.2 The error term<a class="headerlink" href="#the-error-term" title="Permalink to this headline">¶</a></h4>
<p>As previously stated, the error term, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the variance in <span class="math notranslate nohighlight">\(Y\)</span> that cannot be predicted by the model. <span class="math notranslate nohighlight">\(\epsilon\)</span> is a vector of length <span class="math notranslate nohighlight">\(n\)</span>  (where <span class="math notranslate nohighlight">\(n\)</span> is the sample size), where the individual values are known as <strong>residuals</strong> and are equal to the differences between the observed <span class="math notranslate nohighlight">\(y\)</span> and their fitted values from the model (<span class="math notranslate nohighlight">\(\beta_0 + \beta_1x\)</span>).</p>
<p>We assume that <span class="math notranslate nohighlight">\(\epsilon\)</span> has a normal distribution with mean 0 and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, where <span class="math notranslate nohighlight">\(\sigma^2\)</span> is termed the <strong>residual variance</strong> (i.e. the variance of the residuals):</p>
<div class="math notranslate nohighlight">
\[\epsilon \sim N(0,\sigma^2)\]</div>
<p><strong>N.B.</strong> The true residual is equal to the difference between the observed value <span class="math notranslate nohighlight">\(y\)</span> and its value as predicted from the true model (<span class="math notranslate nohighlight">\(\beta_0+\beta_1x\)</span>). Note that these true residuals are defined in terms of deviations from the model defined by population parameters. The term residual is also used to define deviations from a fitted regression model (i.e. a model in which <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are replaced by the estimates). The terms <strong>true</strong> residual and <strong>observed</strong> (or <strong>fitted</strong> or <strong>estimated</strong>) residual can be used when we need to make this distinction clear. However, for the remainder of this lesson, this term “residuals” is used to refer to observed residuals.</p>
</div>
<div class="section" id="different-ways-of-expressing-the-simple-linear-regression-model">
<h4>12.2.1.3 Different ways of expressing the simple linear regression model<a class="headerlink" href="#different-ways-of-expressing-the-simple-linear-regression-model" title="Permalink to this headline">¶</a></h4>
<p>Suppose we have a sample size of <span class="math notranslate nohighlight">\(n\)</span> and we let <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(x_i\)</span> <span class="math notranslate nohighlight">\((i=1,...,n)\)</span> denote the observed outcome and value of <span class="math notranslate nohighlight">\(X\)</span> for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation, respectively. Then, we can write the simple linear regression model as:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \beta_1 x_{i}+ \epsilon_i \text{ where } \epsilon_i \sim NID(0, \sigma^2).\]</div>
<p>Here <span class="math notranslate nohighlight">\(NID\)</span> stands for “Normally and Independently Distributed”. A key assumption of linear regression model is that all of the observations are independent.</p>
<p>This relationship can also be expressed using matrix algebra:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon} \text{ where }\epsilon \sim N(0,\mathbf{I}\sigma^2)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{vmatrix}y_1\\y_2 \\. \\. \\. \\y_n \end{vmatrix}=\begin{vmatrix}1 &amp; x_1 \\ 1 &amp; x_2 \\1 &amp; . \\1 &amp; .  \\ 1&amp; . \\1 &amp; x_n \end{vmatrix}\begin{vmatrix} \beta_0 \\ \beta_1 \end{vmatrix}+\begin{vmatrix}\epsilon_1\\ \epsilon_2 \\ . \\ . \\. \\ \epsilon_n \end{vmatrix} \end{split}\]</div>
<p>In this formulation, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n \times 2\)</span> matrix, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> are vectors of length <span class="math notranslate nohighlight">\(n\)</span> whilst <span class="math notranslate nohighlight">\(\beta\)</span> is a vector of length 2.</p>
</div>
<div class="section" id="assumptions">
<h4>12.2.1.4 Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">¶</a></h4>
<p>It is worth emphasising the four key assumptions that we have made in the simple linear regression model:</p>
<ol class="simple">
<li><p><strong>Linearity</strong>: The relationship between <span class="math notranslate nohighlight">\(X\)</span> and the mean of <span class="math notranslate nohighlight">\(Y\)</span> is linear.</p></li>
<li><p><strong>Normality</strong>: The residuals are normally distributed.</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of residuals are constant across all values of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><strong>Independence</strong>: All observations are independent of each other.</p></li>
</ol>
<p>It is important to bear these assumptions in mind when conducting a linear regression analysis. In the next lesson we will discuss diagnostic tools that can be used to test the validity of these assumptions.</p>
</div>
</div>
<div class="section" id="estimation-of-the-population-parameters">
<h3>12.2.2 Estimation of the population parameters.<a class="headerlink" href="#estimation-of-the-population-parameters" title="Permalink to this headline">¶</a></h3>
<p>In the specification of the simple linear regression model there are three population parameters (<span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, and <span class="math notranslate nohighlight">\(\sigma\)</span>). Since we do not know these parameters, we need to estimate them based on a sample from our population. We will use the symbols <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, and <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> to represent the sample estimates of the true population parameters.</p>
<p>There are many different methods available for obtaining estimates of the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. In this section, we focus on an approach that works by minimising the amount of error in the model. These estimates are called the <strong>ordinary least squares estimates</strong> (the reason for this name will become clear in the next section).</p>
<div class="section" id="ordinary-least-squares-estimates">
<h4>12.2.2.1 Ordinary least squares estimates<a class="headerlink" href="#ordinary-least-squares-estimates" title="Permalink to this headline">¶</a></h4>
<p>The ordinary least square (OLS) estimates are those which minimise the sum of squared deviations from the fitted regression line. Since the residuals, <span class="math notranslate nohighlight">\(\epsilon\)</span>, measure deviations from the fitted regression line, the sum is denoted by <span class="math notranslate nohighlight">\(SS_{RES}\)</span> (“<span class="math notranslate nohighlight">\(SS\)</span>” stands for Sum of Squares and “<span class="math notranslate nohighlight">\(RES\)</span>” is shorthand for RESiduals). Formally, the OLS estimators are the values of <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> that minimise:</p>
<div class="math notranslate nohighlight">
\[SS_{RES} = \sum_{i=1}^n \hat{\epsilon_i}^2 = \sum_{i=1}^n (y_i - \hat{\beta_0} -\hat{\beta_1}x_i)^2.\]</div>
<p>The ordinary least squares estimates of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are given by the following:</p>
<p>\begin{align}
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1}\bar{x} \
\hat{\beta_1} &amp;= \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
\end{align}</p>
<p>where <span class="math notranslate nohighlight">\(\bar{y}=\frac{\sum_{i=1}^n y_i}{n}\)</span> and <span class="math notranslate nohighlight">\(\bar{x} = \frac{\sum_{i=1}^n x_i}{n}\)</span>. A proof of this result is given at the end of this lesson.</p>
</div>
<div class="section" id="estimation-of-the-residual-variance">
<h4>12.2.2.2 Estimation of the residual variance<a class="headerlink" href="#estimation-of-the-residual-variance" title="Permalink to this headline">¶</a></h4>
<p>The residual variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, is equal to the mean squared size of the residuals. So, an intuitively appealing estimator of <span class="math notranslate nohighlight">\(\sigma^2\)</span> is given by dividing the residual sum of squares by the number of observations:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma^2} = \sum_{i=1}^n \frac{\epsilon_i^2}{n} = \sum_{i=1}^n (y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2/n\]</div>
<p>However, this is a biased estimator. The bias arises because the observed values tend, on average, to lie closer to the fitted line (defined by <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>) than they do to the true regression line (defined by <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>). This is an exact parallel to the way the variablility of a sample around its mean underestimates the variability around the population mean.</p>
<p>It can be shown that an unbiased estimator of the residual variance in the simple linear regression model is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma^2}  = \sum_{i=1}^n \frac{\hat{\epsilon_i}^2}{n-1}=\sum_{i=1}^n (y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2/(n-2)\]</div>
<p>This quantity is referred to as the residual mean square. It is often denoted by <span class="math notranslate nohighlight">\(MS_{RES}\)</span>, where “<span class="math notranslate nohighlight">\(MS\)</span>” stands for Mean Square and “<span class="math notranslate nohighlight">\(RES\)</span>” is shorthand for residual. The denominator is <span class="math notranslate nohighlight">\((n-2)\)</span> because fitting the model first requires the estimation of two parameters (<span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>) and the estimation of these parameters is said to reduce the information about the variance by two degrees of freedom.</p>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h4>12.2.2.3 Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h4>
<p>An alternative approach to estimating the model parameters is maximum likelihood estimation. This approach selects the estimates which maximise the likelihood (or equivalently, the log-likelihood) of the parameter values. It can be shown that the ordinary least square estimates for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are also the maximum likelihood estimates (a proof of this result is at the end of the lesson).</p>
<p>The maximum likelihood estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span> is equal to the biased estimate given above, obtained by dividing the residual sum of squares by the number of observations.</p>
</div>
<div class="section" id="properties-of-hat-beta-1">
<h4>12.2.2.4 Properties of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span><a class="headerlink" href="#properties-of-hat-beta-1" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p>The parameter estimator for <span class="math notranslate nohighlight">\(\beta_1\)</span> can also be written as the ratio of the covariance between the independent variable and the outcome to the variance of the independent variable:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\hat{\beta_1} = \frac{cov(X,Y)}{SD(X)^2}\]</div>
<ol class="simple">
<li><p>If we use the notation <span class="math notranslate nohighlight">\(\hat{\beta}_{y|x}\)</span> to denote the estimate of the slope from a simple linear regression model with predictor variable <span class="math notranslate nohighlight">\(X\)</span> and outcome variable <span class="math notranslate nohighlight">\(Y\)</span>, it follows from the above result that:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\hat{\beta}_{y|x}=\frac{cov(X,Y)}{SD(X)^2}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\hat{\beta}_{x|y}=\frac{cov(X,Y)}{SD(Y)^2}$$. \\Hence, \\$$\hat{\beta}_{y|x}\hat{\beta}_{x|y} = r^2_{x,y}\end{aligned}\end{align} \]</div>
<p>Where <span class="math notranslate nohighlight">\(r_{x,y}\)</span> is the correlation coefficient between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</div>
</div>
<div class="section" id="examples">
<h3>12.2.3 Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>We now consider how estimates of the population parameters can be obtained in R using our two examples. Recall we are interested in investigating (1) the association between birthweight and length of pregnancy and (2) birthweight and mother’s smoking status.</p>
<p>In both examples, birthweight is the outcome. In Example 1, the independent variable is length of pregnancy, <span class="math notranslate nohighlight">\(L\)</span> (i.e. number of gestational days) and in Example 2, the independent variable is an indicator variable for whether or not the mother smokes, <span class="math notranslate nohighlight">\(S\)</span>.</p>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">lm()</span></code> to perform simple linear regressions in R. Click <a class="reference external" href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm">here</a> for details of how this command works.</p>
<div class="section" id="example-1">
<h4>12.2.3.1 Example 1<a class="headerlink" href="#example-1" title="Permalink to this headline">¶</a></h4>
<p>The following model defines our assumed relationship between the length of pregnancy (<span class="math notranslate nohighlight">\(L\)</span>) and a baby’s birthweight (<span class="math notranslate nohighlight">\(Y\)</span>):</p>
<div class="math notranslate nohighlight">
\[ \text{Model 1: }y_i = \beta_0 + \beta_1 l_i +  \epsilon_i \]</div>
<p>The following code can be used to perform this linear regression in R:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Example 1: Investigating the relationship between birthweight and length of pregancy</span>
<span class="n">model1</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The estimated intercept, <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> is equal to -10.75. This is interpreted as: the estimated mean birthweight of a child born after 0 gestational days is -10.75oz. Since there are no observations with 0 gestational days in the study, this is an extrapolation based on the observed data and an assumption of linearity. Estimates based on extrapolation should be interpreted with caution and in this case, the results make little sense because a negative weight is estimated. Moreover, no child is born after 0 gestational days and so this intercept is of little interest. Later on in the lesson, we will discuss a technique called <strong>centering</strong> which is often used to make more interpretable intercepts.</p></li>
<li><p>The estimated slope, <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is equal to 0.47. This is interpreted as: the mean birthweight of a baby is estimated to increase by 0.47oz for each daily increase in the gestational period.</p></li>
<li><p>The estimated residual standard error, <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> is equal to 16.74 (the estimated residual variance is equal to <span class="math notranslate nohighlight">\(16.74^2\)</span>). This means that the observed outcomes are scattered around the fitted regression line with a standard deviation of 16.74oz.</p></li>
</ul>
</div>
<div class="section" id="example-2">
<h4>12.2.3.2 Example 2<a class="headerlink" href="#example-2" title="Permalink to this headline">¶</a></h4>
<p>In our second example, the independent variable is binary. To include this in the model, we use a <strong>dummy</strong> variable that takes the value 1 if the mother smokes and 0 if the mother doesn’t smoke:</p>
<div class="math notranslate nohighlight">
\[\begin{split} s_{i}
\begin{cases}
    1 &amp; \text{ if the $i^{th}$ baby's mother smokes} \\
    0 &amp; \text{ if the $i^{th}$ baby's mother does not smoke}
\end{cases} \end{split}\]</div>
<p>We then define the following linear regression model:</p>
<div class="math notranslate nohighlight">
\[ \text{Model 2: }y_i = \alpha_0 + \alpha_1 s_i + \epsilon_i\]</div>
<p>When including binary (or categorical) variables in a linear regression in R, we can tell R to treat it as a factor variable using <code class="docutils literal notranslate"><span class="pre">factor()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Example 2: Investigating the relationship between birthweight and mother&#39;s smoking status.</span>
<span class="n">model2</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="nf">factor</span><span class="p">(</span><span class="n">Maternal.Smoker</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ factor(Maternal.Smoker), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-68.085 -11.085   0.915  11.181  52.915 

Coefficients:
                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                 123.0853     0.6645 185.221   &lt;2e-16 ***
factor(Maternal.Smoker)True  -9.2661     1.0628  -8.719   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 17.77 on 1172 degrees of freedom
Multiple R-squared:  0.06091,	Adjusted R-squared:  0.06011 
F-statistic: 76.02 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\alpha_0} = 123.09\)</span>. This is interpreted as the estimated mean birthweight (in oz) of a child with “dummy” variable equal to 0, i.e. it is the estimated mean birthweight of children whose mothers do not smoke.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\alpha_1}=-9.23\)</span>. The mean birthweight is estimated to decrease by 9.23oz per unit increase in the “dummy” variable. A unit increase in the dummy variable equates to moving from the non-smoking group to the smoking group, so we can interpret this as the difference in mean birthweights between the two groups.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma}=17.77\)</span>. The observed outcomes are scattered around the fitted regression line with a standard deviation of 17.77oz.</p></li>
</ul>
<p>Note that the outputs for Models 1 and 2 consist of a number of other values we have yet to discuss. We address these in the subsequent sections. In Section 3.4 we will learn how to conduct statistical inference on the estimated parameters, which will help us to interpret the standard errors, <span class="math notranslate nohighlight">\(t\)</span>-values and <span class="math notranslate nohighlight">\(p\)</span>-values in the above output. Later in Section 5 we will discuss analysis of variance which will help us to interpret the “R-squared” values and the <span class="math notranslate nohighlight">\(F\)</span>-test.</p>
</div>
</div>
<div class="section" id="inference">
<h3>12.3.4 Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h3>
<p>Most commonly, we wish to conduct statistical inference on the estimated slope. Consequently, we focus our attention here on <span class="math notranslate nohighlight">\(\beta_1\)</span>, but it is possible to apply the same methods to the intercept, <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<p>The estimated slope, <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> can be written as a linear combination of the observed values and is therefore a function of the random error (<span class="math notranslate nohighlight">\(\epsilon\)</span>):</p>
<p>\begin{align}
\hat{\beta_1} &amp;= \sum_{i=1}^n (\frac{(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}(y_i-\bar{y})) \
&amp;=\beta_1 + \sum_{i=1}^n (\frac{(x_i-\bar{x})}{\sum_{i=1}^n(x_i-\bar{x})^2}(\epsilon_i-\bar{\epsilon}))
\end{align}</p>
<p>The second equality is obtained by substituting in <span class="math notranslate nohighlight">\((y_i-\bar{y})=\beta_1(x_i-\bar{x})+(\epsilon_i-\bar{\epsilon})\)</span>.</p>
<p>Since the estimated parameter is a linear combination of the <span class="math notranslate nohighlight">\(\epsilon_i\)</span>, and <span class="math notranslate nohighlight">\(\epsilon_i \sim NID(0,\sigma^2)\)</span>, the estimated parameter itself is normally distributed. More specifically, it is normally distributed with mean <span class="math notranslate nohighlight">\(\beta_1\)</span> and standard deviation:</p>
<div class="math notranslate nohighlight">
\[SD(\hat{\beta_1})=\sqrt{\frac{\sigma^2}{ns_x^2}}\]</div>
<p>where <span class="math notranslate nohighlight">\(s_x^2\)</span> is the variance of <span class="math notranslate nohighlight">\(x\)</span>. It can be seen from the formula above that the variance of <span class="math notranslate nohighlight">\(\beta_1\)</span> increases with the size of the residual variance (as might be expected intuitively), decreases with increasing sample size (larger sample sizes give more precise estimates) and decreases as <span class="math notranslate nohighlight">\(s_x^2\)</span> increases (the wider range of <span class="math notranslate nohighlight">\(x\)</span> values the more precision of estimates).</p>
<p>Knowing the distribution of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, allows us to perform hypothesis tests and construct confidence intervals for <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<div class="section" id="hypothesis-testing">
<h4>12.3.4.1 Hypothesis testing<a class="headerlink" href="#hypothesis-testing" title="Permalink to this headline">¶</a></h4>
<p>To test the null hypothesis <span class="math notranslate nohighlight">\(H_0: \beta_1=B\)</span> against the alternative <span class="math notranslate nohighlight">\(H_1: \beta_1 \neq B\)</span>. We calculate the t-test statistic:</p>
<div class="math notranslate nohighlight">
\[t = \frac{(\hat{\beta_1}-B)}{SE(\hat{\beta_1})}\]</div>
<p>Where <span class="math notranslate nohighlight">\(SE()\)</span> denotes standard error. The standard error of an estimated regression coefficient is equal to the estimated standard deviation (i.e. we replace <span class="math notranslate nohighlight">\(\sigma\)</span> with <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> in the equation for <span class="math notranslate nohighlight">\(SD(\hat{\beta_1})\)</span> above):</p>
<p>$<span class="math notranslate nohighlight">\(SE(\hat{\beta_1})=\sqrt{\frac{\hat{\sigma}^2}{ns_x^2}}\)</span>$.</p>
<p>Replacing <span class="math notranslate nohighlight">\(\sigma^2\)</span> with <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> means that <span class="math notranslate nohighlight">\(t\)</span> follows a <span class="math notranslate nohighlight">\(t\)</span> distribution (rather than a <span class="math notranslate nohighlight">\(z\)</span> distribution) with <span class="math notranslate nohighlight">\((n-2)\)</span> degrees of freedom, if <span class="math notranslate nohighlight">\(H_0\)</span> is true. This allows us to calculate the <span class="math notranslate nohighlight">\(p\)</span>-value to test the null hypothesis that <span class="math notranslate nohighlight">\(\beta_1\)</span> equals <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>Most commonly, researchers test the hypothesis that <span class="math notranslate nohighlight">\(\beta_1=0\)</span>. If this is true, then there is no association between <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p><strong>Example:</strong> We can use the output from <code class="docutils literal notranslate"><span class="pre">summary(model1)</span></code> to conduct a hypothesis test to test the hypothesis that <span class="math notranslate nohighlight">\(\beta_1=0\)</span> in Model 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Example 1: Hypothesis tests</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>In the above output, the column <code class="docutils literal notranslate"><span class="pre">Std.Error</span></code> gives the standard errors associated with the estimated regression coefficients. The columns <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">value</span></code> and <code class="docutils literal notranslate"><span class="pre">Pr(&gt;|t|)</span></code> give the t-test statistic and associated <span class="math notranslate nohighlight">\(p\)</span>-value for a hypothesis test that the regression coefficient estimate is equal to 0.</p>
<p>To test the null hypothesis that <span class="math notranslate nohighlight">\(\beta_1=0\)</span> against the alternative <span class="math notranslate nohighlight">\(\beta_1 \neq  0\)</span>, the test statistic is 15.28 and the associated <span class="math notranslate nohighlight">\(p\)</span>-value is <span class="math notranslate nohighlight">\(&lt;2\times10^{-16}\)</span>. This is a very small <span class="math notranslate nohighlight">\(p\)</span>-value and therefore the data provide strong evidence against the null hypothesis. Based on these results, we can conclude that birthweight is associated with length of pregnancy. To convince yourself that these values are correct, you can calculate the standard error and test statistic by hand, using the above formulas.</p>
<p><em>Exercise:</em> Using the output for <code class="docutils literal notranslate"><span class="pre">summary(model2)</span></code> given in the previous section, conduct a hypothesis test to test the null hypothesis that <span class="math notranslate nohighlight">\(\alpha_1=0\)</span>.</p>
</div>
<div class="section" id="confidence-intervals-for-the-regression-coefficients">
<h4>12.3.4.2 Confidence intervals for the regression coefficients<a class="headerlink" href="#confidence-intervals-for-the-regression-coefficients" title="Permalink to this headline">¶</a></h4>
<p>Confidence intervals give us more information than the estimated regression coefficient, because they take into account the uncertainty in our estimates. Confidence intervals are constructed in such a way so that they will contain the true population parameter a specified proportion of the time, typically 95%.</p>
<p>A formal interpretation of the 95% confidence interval is as follows: if the analysis was repeated 100 times and 95% confidence intervals were obtained each time, then 95% of those confidence intervals would contain the true population parameter. More informally, confidence intervals are often interpreted as a range of plausible values for the true population parameter.</p>
<p>The definition of a <span class="math notranslate nohighlight">\((1-\alpha)\)</span>% confidence interval for the parameter <span class="math notranslate nohighlight">\(\beta_1\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta_1} \pm t_{\frac{\alpha}{2}, n-2}SE(\hat{\beta_1})\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{\frac{\alpha}{2}, n-2}\)</span> represents the <span class="math notranslate nohighlight">\((1-\alpha/2)\)</span> centile of a <span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\((n-2)\)</span> degrees of freedom.</p>
<p>If 0 lies in the confidence interval, we would conclude that the independent variable and outcome are not associated, because it is plausible that <span class="math notranslate nohighlight">\(\beta_1=0\)</span>. If 0 does not lie within the confidence interval, then there is evidence of an association.</p>
<p>Note that if <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large, the <span class="math notranslate nohighlight">\(t\)</span>-distribution is well approximated by a normal distribution. In this case, a 95% confidence interval can be found by:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta_1} \pm 1.96 \times SE(\hat{\beta_1})\]</div>
<p><em>Example:</em> Calculate a 95% for <span class="math notranslate nohighlight">\(\beta_1\)</span> (using the values given in the R output above):</p>
<p>\begin{align}
&amp;95% \text{ CI}: \hat{\beta_1} \pm 1.96 \times SE(\hat{\beta_1}) \
\rightarrow &amp;95% \text{ CI}: 0.46656 \pm 1.96 \times 0.03054 \
\rightarrow &amp;95% \text{ CI}: (0.407, 0.526)
\end{align}</p>
<p>Since 0 does not lie within the interval, we conclude that there is evidence of an association between birthweight and length of pregancy (as indicated by the results of the hypothesis test).</p>
<p>Alternatively, we can obtain confidence intervals using <code class="docutils literal notranslate"><span class="pre">confint</span></code> in R:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Confidence intervals for beta_1</span>
<span class="nf">confint</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">parm</span><span class="o">=</span><span class="m">2</span><span class="p">,</span> <span class="n">level</span> <span class="o">=</span> <span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr></thead>
<tbody>
	<tr><th scope=row>Gestational.Days</th><td>0.4066435</td><td>0.5264702</td></tr>
</tbody>
</table>
</div></div>
</div>
<p><em>Exercise:</em> Edit the above code to find 95% and 90% confidence intervals for <span class="math notranslate nohighlight">\(\hat{\alpha_1}\)</span> in Model 2. <strong>Hint</strong>: Use the <a class="reference external" href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/confint">R documentation</a> for the command <code class="docutils literal notranslate"><span class="pre">confint</span></code> as a guide.</p>
<p>So far we have only discussed conducting inference on the estimated regression coefficients. However, it may also be of interest to determine <strong>confidence intervals for the fitted outcomes</strong>, or <strong>prediction intervals</strong>. The subsequent two sections describe and illustrate these two concepts, respectively.</p>
</div>
<div class="section" id="confidence-intervals-for-a-fitted-value">
<h4>12.3.4.3 Confidence intervals for a fitted value<a class="headerlink" href="#confidence-intervals-for-a-fitted-value" title="Permalink to this headline">¶</a></h4>
<p>Confidence intervals can be obtained for the fitted value of <span class="math notranslate nohighlight">\(Y\)</span> given a particular value of <span class="math notranslate nohighlight">\(X\)</span>. We denote the fitted value when <span class="math notranslate nohighlight">\(X=x\)</span> as <span class="math notranslate nohighlight">\(y_x\)</span>. The expected value of <span class="math notranslate nohighlight">\(y_x\)</span> is equal to <span class="math notranslate nohighlight">\(\hat{Y}=\beta_0+\beta_1x\)</span> and its variance is given by:</p>
<div class="math notranslate nohighlight">
\[V(\hat{y}_x) = \sigma^2 (\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}})\]</div>
<p>where <span class="math notranslate nohighlight">\(SS_{xx}=\sum_{i=1}^n(x_i-\bar{x})^2\)</span>, i.e. the sum of squares of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Therefore, the 95% confidence interval is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{y_x} \pm t_{n-2, 0.975}\hat{\sigma} \sqrt{\frac{1}{n}+ \frac{(x-\bar{x})^2}{SS_{xx}}}\]</div>
<p>95% confidence intervals can be obtained for values of the independent variable that do not belong in the data. However, the width of the confidence interval increases with the distance from the mean (as can be seem from the formula and figure given below). Care must be taken when extrapolating outside the range of the observed data as this makes an un-testable assumption that linearity continues outside the observed data range.</p>
<p><em>Example</em>. The R code below calculates a 95% confidence interval for the fitted value of birthweight of a baby born after 280 gestational days.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Confidence interval for a fitted value </span>
<span class="n">new.data</span><span class="o">&lt;-</span><span class="nf">data.frame</span><span class="p">(</span><span class="n">Gestational.Days</span><span class="o">=</span><span class="m">280</span><span class="p">)</span>
<span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new.data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;confidence&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>
<tbody>
	<tr><td>119.8818</td><td>118.9215</td><td>120.8421</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The 95% confidence interval for <span class="math notranslate nohighlight">\(y_{280}\)</span> is (118.9, 120.8). Informally, we can interpret this as: it is plausible that the true value of <span class="math notranslate nohighlight">\(y_{280}\)</span> lies between 118.9 and 120.8.</p>
</div>
<div class="section" id="prediction-intervals">
<h4>12.3.4.4 Prediction intervals<a class="headerlink" href="#prediction-intervals" title="Permalink to this headline">¶</a></h4>
<p>A 95% prediction interval (otherwise known as <strong>reference range</strong>) is the interval in which 95% of future observations are expected to lie. To estimate this for <span class="math notranslate nohighlight">\(X=x\)</span>, we have to take into accout the uncertainty in our estimated regression coefficients and the additional random error in an observation (<span class="math notranslate nohighlight">\(\sigma^2\)</span>).</p>
<p>The variance of an individual prediction is given by:</p>
<p>\begin{align}
V(\hat{y_x}) + \sigma^2  &amp;= \sigma^2 (\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}})+ \sigma^2\
&amp;= \sigma^2(1+\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}})
\end{align}</p>
<p>A 95% prediction interval is then given by:</p>
<div class="math notranslate nohighlight">
\[\hat{y_x} \pm t_{n-2, 0.975} \hat{\sigma} \sqrt{1+ \frac{1}{n}+ \frac{(x-\bar{x})^2}{S_{xx}}}\]</div>
<p><em>Example</em>. The R code below calculates a 95% prediction interval for the birthweight of babies who are born after 280 days.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Prediction interval</span>
<span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new.data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>
<tbody>
	<tr><td>119.8818</td><td>87.01496</td><td>152.7486</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The 95% prediction interval for <span class="math notranslate nohighlight">\(y_{280}\)</span> is (87.0, 152.7). This means that we would expect 95% of babies born after 280 gestational days to weigh between 87 and 152.7 ounces.</p>
<p><em>Examples continued.</em> The code below produces two scatterplots of gestational days against birthweight with the linear regression line of best fit (obtained from Model 1) superimposed. The blue lines on the left-hand side plot represent the 95% confidence intervals for the fitted values across the entire range of gestational days. The blue lines on the right-hand side plot represent the 95% prediction intervals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Set the graphical space so that two plots are shown side-by-side in one row</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1">#Confidence intervals for predicted values</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;95% Confidence intervals&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>

<span class="n">conf_interval</span><span class="o">&lt;-</span><span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;confidence&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">3</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>

<span class="c1">#Reference ranges</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">Birth.Weight</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Gestational days&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Birthweight (oz)&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;95% Prediction intervals&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>

<span class="n">conf_interval</span><span class="o">&lt;-</span><span class="nf">predict</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">conf_interval</span><span class="p">[,</span><span class="m">3</span><span class="p">],</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/12. Linear Regression I_23_0.png" src="_images/12. Linear Regression I_23_0.png" />
</div>
</div>
</div>
</div>
</div>
<div class="section" id="multivariable-linear-regression">
<h2>12.4 Multivariable linear regression<a class="headerlink" href="#multivariable-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Multivariable linear regression extends the simple linear regression model to situations in which we wish to relate two or more independent variables to one outcome. Where there are multiple independent variables, we will refer to them as <strong>covariates</strong>.</p>
<p>There can be a number of different reasons why we would want to add more covariates in our linear regression model. Recall these two examples of questions we might want to answer using statistical models (given at the beginning of this lesson):</p>
<ul class="simple">
<li><p>Does taking drug A reduce inflammation more than taking drug B in patients with arthritis?</p></li>
<li><p>Can we predict the risk of heart disease for our patients?</p></li>
</ul>
<p>In the first example, we could use a statistical model with inflammation as the outcome and drug use as the independent variable of interest, but we may need to <strong>control</strong> (or <strong>adjust</strong>) for the <strong>confounding</strong> effects of other patient characteristics (age, gender, other medication use, etc.). In the second example, there are many different factors that could be associated with risk of heart disease (age, gender, lifestyle choices, etc.) and we may wish to include all such factors in a statistical model to predict heart disease.</p>
<p>Here, we introduce the multivariable linear regression model and describe how to estimate and interpret the parameters in the model using an example from the birthweight data. In particular, we will consider two statistical models which relate birthweight to length of pregnancy and mother’s height. In the first of these two models (Model 3) we will model the mother’s height as a continuous variable and in the second (Model 4), we will model the mother’s height as a categorical variable.</p>
<p><em>A note on notation.</em> There can be some confusion between the terms <strong>multivariable</strong> models and <strong>multivariate</strong> models. Multivariate models are those which have more than one outcome variable. Such models are beyond the scope of this module; we focus our attention on <strong>univariate</strong> models which have only one outcome. Both simple linear regression models and multivariable linear regression models are considered as univariate.</p>
<div class="section" id="the-multivariable-linear-regression-model">
<h3>12.4.1 The multivariable linear regression model<a class="headerlink" href="#the-multivariable-linear-regression-model" title="Permalink to this headline">¶</a></h3>
<p>Suppose we wish to relate an outcome (<span class="math notranslate nohighlight">\(Y\)</span>) to <span class="math notranslate nohighlight">\(p\)</span> predictor variables <span class="math notranslate nohighlight">\((X_1, X_2, ..., X_p)\)</span>. The appropriate multivariable linear regression model is a straightforward extension of the simple linear regression model:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned} y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ,..., \beta_p x_{ip}+\epsilon_i \text{ with } \epsilon_i \sim NID(0,\sigma^2)$$. \\Where, \\\begin{split}\begin{align}
&amp;y_i = \text{value of the dependent variable for the ith participant}\\
&amp;x_{ji} = \text{value of the jth predictor variable for the ith participant}. 
\end{align}\end{split}\\The parameters in the model are interpreted as follows:\\+ $\beta_0$ is the intercept. It is the expectation of $Y$ when all the $X_j's$ are zero.
+ $\beta_j$ is the expected change in $Y$ for a 1 unit increase in $X_j$ *with all the other covariates held constant*. \\The $\beta_j's$ are the **regression coefficients** (otherwise known as **partial regression coefficients**). Each one measures the effect of one covariate controlled (or adjusted) for all of the others. \\#### 12.4.1.1 The multivariable linear regression model in matrix notation\\Similarly to the simple linear regression model, the multivariable linear regression model can be expressed using matrix algebra. \\$$\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon} \text{ where }\epsilon \sim N(0,\mathbf{I}\sigma^2)\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{vmatrix}y_1\\y_2 \\. \\. \\. \\y_n \end{vmatrix}=\begin{vmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\ 1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2p}  \\1 &amp; . \\1 &amp; .  \\ 1&amp; . \\1 &amp; x_{p1} &amp; x_{p} &amp; ... &amp; x_{pn} \end{vmatrix}\begin{vmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ ... \\ \beta_p \end{vmatrix}+\begin{vmatrix}\epsilon_1\\ \epsilon_2 \\ . \\ . \\. \\ \epsilon_n \end{vmatrix} \end{split}\]</div>
<p>In this formulation, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n \times (p+1)\)</span> matrix, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> are vectors of length <span class="math notranslate nohighlight">\(n\)</span> whilst <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> is a vector of length <span class="math notranslate nohighlight">\((p+1)\)</span>.</p>
</div>
<div class="section" id="estimation-of-the-parameters">
<h3>12.4.2 Estimation of the parameters<a class="headerlink" href="#estimation-of-the-parameters" title="Permalink to this headline">¶</a></h3>
<p>The regression coefficients in multivariable linear regression can be estimated by minimising the residual sum of squares:</p>
<p>\begin{align} SS_{RES} &amp;= \sum_{i=1}^n \hat{\epsilon}<em>i^2 = \sum</em>{i=1}^n (y_i-\hat{y})^2 \
&amp;= \sum_{i=1}^n (y_i-\hat{\beta}_0-\hat{\beta}<em>1x</em>{1i}-…-\hat{\beta}<em>px</em>{pi})^2 \end{align}</p>
<p>The closed form solution, obtained by solving the <span class="math notranslate nohighlight">\((p+1)\)</span> simultaneous equations that result from setting the partial derivatives of the above equation with respect to each parameter estimate to zero, can be written succinctly using matrix notation:</p>
<div class="math notranslate nohighlight">
\[\mathbf{\hat{\beta}}= (\mathbf{X'X})^{-1}X'Y\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{\hat{\beta}}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span>. Its distribution is as follows:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\mathbf{\hat{\beta}} \sim \mathbf{N(\beta, (X'X)^{-1}\sigma^2)}$$.\\This expresses the fact that the elements of $\mathbf{\hat{\beta}}$ follow a multivariate normal distribution whose variances and covariances are given by $\mathbf{(X'X)^{-1}\sigma^2}$. \\It can also be shown that the following is an unbiased estimator for $\sigma^2$:\\\begin{split}\begin{align}
\hat{\sigma}^2 &amp;= \sum_{i=1}^n \frac{\hat{\epsilon_i}^2}{(n-(p+1))}\\
              &amp;=\sum_{i=1}^n (y_i - \hat{\beta_0} - \hat{\beta}_1x_{1i}- ... - \hat{\beta_p}x_{ip})^2/(n-(p+1))
\end{align}\end{split}\\While it is useful to know how these parameters are estimated, in practice they are often obtained using statistical software. Next, we demonstrate how to perform multivariable regression in R using the birthweight data and discuss the interpretation of the estimated regression coefficients. 
 
 ### 12.4.3 Birthweight data: Model 3
 
We are interested in investigating a model that relates birthweight to length of pregnancy and mother's height. We will use the following multivariable linear regression model: 
 
$$\text{Model 3: } y_i = \beta_0 + \beta_1 l_i + \beta_2h_i  + \epsilon_i \end{aligned}\end{align} \]</div>
<p>The outcome <span class="math notranslate nohighlight">\(y_i\)</span> denotes the birthweight (in oz) for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby. The predictors <span class="math notranslate nohighlight">\(l_i\)</span> and <span class="math notranslate nohighlight">\(h_i\)</span> denote the length of pregnancy (i.e. number of gestational days), and the height of the mother (in inches), for the <span class="math notranslate nohighlight">\(i^{th}\)</span> baby, respectively.</p>
<p>The linear regression can be conducted in R using the <code class="docutils literal notranslate"><span class="pre">lm()</span></code> command:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model 3: Relating birthweight to length of pregnancy and mother&#39;s height</span>
<span class="n">model3</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days</span><span class="o">+</span><span class="n">Maternal.Height</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days + Maternal.Height, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -88.51993   14.31910  -6.182 8.73e-10 ***
Gestational.Days   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p><strong>Interpretation of the regression coefficients</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_1=0.45\)</span>. This is the estimated regression coefficient for number of gestational days. It is interpreted as: the expected increase in a baby’s birthweight for each gestational day, amongst babies whose mothers were of the same height, is 0.45 ounces.</p></li>
</ul>
<p>It may be tempting to make causal inference from regression models such as Model 3, i.e. “longer pregnancies <strong>cause</strong> an increase in birthweight”. However, this is far from straightforward. Based on the results presented above, it would be reasonable to say that “birthweight increases with length of pregnancy”. However, it is much less reasonable to claim that higher birthweight is caused by longer pregnancies (based on these results alone), because there may be an unobserved third variable that is the “real” cause of both increased length of pregancy and birthweight. Causal statements require more than just the results of a statistical model to make them plausible; this is a topic that we return to in the next lesson.</p>
<p><em>Excerise</em>: What is the interpretation of <span class="math notranslate nohighlight">\(\hat{\beta}_2?\)</span></p>
<p><strong>Interpretation of the intercept</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_0=-88.52\)</span>. The interpretation is that the estimated mean birthweight for a child who was born after 0 gestastional days and whose mother’s height is 0 inches is -88.52 ouces. Clearly this is an absurd value to estimate because no babies are born that quickly and no mothers are that short. If we wish to obtain a more reasonable intercept, we can use a technique called <strong>centering</strong>.</p></li>
</ul>
</div>
<div class="section" id="centering">
<h3>12.4.4 Centering<a class="headerlink" href="#centering" title="Permalink to this headline">¶</a></h3>
<p>In many analyses, interpreting the intercept is not as important as interpreting the estimated regression coefficients and so it does not matter if our intercept is an absurd value (as in the example above). However, if we do wish to obtain a reasonable intercept, we can <strong>center</strong> the independent variables.</p>
<p>Centering means subtracting a constant from every value of the independent variable. This essentially shifts the scale of the predictor (the point 0 is shifted to the chosen constant), but does not affect the units of the variable. Consequently, the new interpretation of the intercept would be the mean of <span class="math notranslate nohighlight">\(Y\)</span> when the independent variable is equal to the constant. The estimated regression coefficient of the independent variable is not affected.</p>
<p>As as example, we will repeat the analysis above, but center the covariates on their mean value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#What are the mean gestational days and mothers height in our data?</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="p">)</span>

<span class="c1">#Create new (centered) variables in our data</span>
<span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days.Centered</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Gestational.Days</span><span class="p">)</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Centered</span><span class="o">&lt;-</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">-</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="p">)</span>

<span class="c1">#Redefine Model 3 using the centered variables</span>
<span class="n">model3</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="n">Gestational.Days.Centered</span><span class="o">+</span><span class="n">Maternal.Height.Centered</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  148.0   272.0   280.0   279.1   288.0   353.0 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  53.00   62.00   64.00   64.05   66.00   72.00 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days.Centered + Maternal.Height.Centered, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               119.46252    0.47980 248.983  &lt; 2e-16 ***
Gestational.Days.Centered   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>Now the intercept (<span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>) is equal to 119.46. This is interpreted as: the estimated mean birthweight for a child who was born after 279.1 gestastional days and whose mother’s height is 64.05 inches is 119.46 ouces. Additionally, notice that the estimated regression coefficients for gestational days and mother’s height, and their associated standard errors have not changed.</p>
</div>
<div class="section" id="including-categorical-predictor-variables-in-linear-regression-models">
<h3>12.4.4 Including categorical predictor variables in linear regression models<a class="headerlink" href="#including-categorical-predictor-variables-in-linear-regression-models" title="Permalink to this headline">¶</a></h3>
<p>We have already seen that binary variables can be included in linear regression models through the creation of a “dummy” variable taking the value 0 and 1. If we wish to include a categorical variable with <span class="math notranslate nohighlight">\(c\)</span> categories (where <span class="math notranslate nohighlight">\(c&gt;2\)</span>), the extension of this is to create (<span class="math notranslate nohighlight">\(c-1\)</span>) dummy variables. We illustrate this approach for a categorical variable (<span class="math notranslate nohighlight">\(X\)</span>) which has three possible values: 0,1,2. Since <span class="math notranslate nohighlight">\(X\)</span> has 3 categories, 2 dummy variables (<span class="math notranslate nohighlight">\(U_1\)</span> and <span class="math notranslate nohighlight">\(U_2\)</span>) are required. We define <span class="math notranslate nohighlight">\(U_1\)</span> and <span class="math notranslate nohighlight">\(U_2\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} u_{1i}
\begin{cases}
    1 &amp; \text{ if } x_{i}=1\\
    0 &amp; \text{ if } x_{i} \neq 1
\end{cases} \end{split}\]</div>
<p>and
$<span class="math notranslate nohighlight">\( u_{2i}
\begin{cases}
    1 &amp; \text{ if } x_{i}=2\\
    0 &amp; \text{ if } x_{i} \neq 2
\end{cases}
\)</span>$</p>
<p>Then a multivariable  linear regression model relating <span class="math notranslate nohighlight">\(Y\)</span> to our categorical variable <span class="math notranslate nohighlight">\(X\)</span> is:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \beta_1 u_{1i} + \beta_2 u_{2i} + \epsilon_i \text{ where } \epsilon_i \sim NID(0, \sigma^2)\]</div>
<p>The equation above can also be written as follows:</p>
<p>\begin{align}
y_i &amp; = \beta_0 + \epsilon_i \text{ if } x_i=0 \
y_i &amp; = \beta_0 + \beta_1 +  \epsilon_i \text{ if } x_i=1 \
y_i &amp; = \beta_0 + \beta_2 + \epsilon_i \text{ if } x_i=2 \
\end{align}</p>
<p>This makes explicit the interpretation of the parameters in the model.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_1\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X=1\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_1\)</span> is the difference in the expectation of <span class="math notranslate nohighlight">\(Y\)</span> between groups defined by <span class="math notranslate nohighlight">\(X=0\)</span> and <span class="math notranslate nohighlight">\(X=1\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 + \beta_2\)</span> is the expectation of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X=2\)</span>. Hence <span class="math notranslate nohighlight">\(\beta_2\)</span> is the difference in the expectation of <span class="math notranslate nohighlight">\(Y\)</span> between groups defined by <span class="math notranslate nohighlight">\(X=0\)</span> and <span class="math notranslate nohighlight">\(X=2\)</span>.</p></li>
</ul>
<p>In this parameterisation of the model, the group defined by <span class="math notranslate nohighlight">\(X=0\)</span> is often referred to as the baseline group. There is no statistical reason why one group rather than another should be chosen as the baseline group and it can sometimes be desirable to re-parameterise a model of this type to estimate parameters representating differences in mean levels from a particular baseline group.</p>
<div class="section" id="birthweight-data-model-4">
<h4>12.4.4.1 Birthweight data: Model 4<a class="headerlink" href="#birthweight-data-model-4" title="Permalink to this headline">¶</a></h4>
<p>As an example, we will create a categorical variable based on the mother’s height in our birthweight data, then define a model relating birthweight to length of pregnancy and height categorised into groups. Let <span class="math notranslate nohighlight">\(h_{i}\)</span> denote the height group for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation, defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split} h_{i}
\begin{cases}
    1 &amp; \text{ if the mother's height is less than or equal to 62 inches} \\
    2 &amp; \text{ if the mother's height is greater than 62 inches and less than or equal 64 inches } \\
    3 &amp; \text{ if the mother's height is greater than 64 inches and less than or equal to 66 inches} \\
    4 &amp; \text{ if the mother's height is greater than 66 inches } 
\end{cases} \end{split}\]</div>
<p>We now define three dummy variables:</p>
<div class="math notranslate nohighlight">
\[\begin{split} w_{1i}
\begin{cases}
    1 &amp; \text{ if } h_{i}=2\\
    0 &amp; \text{ if } h_{i} \neq 2
\end{cases} \end{split}\]</div>
<p>and
$<span class="math notranslate nohighlight">\( w_{2i}
\begin{cases}
    1 &amp; \text{ if } h_{i}=3\\
    0 &amp; \text{ if } h_{i} \neq 3
\end{cases}
\)</span>$</p>
<p>and
$<span class="math notranslate nohighlight">\( w_{3i}
\begin{cases}
    1 &amp; \text{ if } h_{i}=4\\
    0 &amp; \text{ if } h_{i} \neq 4
\end{cases}
\)</span>$</p>
<p>Then, our multivariable linear regression model is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Model 4: } y_i = \beta_0 + \beta_{1}w_{1,i} + \beta_{2}w_{2,i} + \beta_{3}w_{3,i} + \beta_4 l_i + \epsilon_i \]</div>
<p>The following code creates a variable for height group and then analyses the data using Model 4:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create variable for height group</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Category</span><span class="o">&lt;-</span><span class="m">1</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Category</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">&gt;</span><span class="m">62</span> <span class="o">&amp;</span> <span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">&lt;=</span><span class="m">64</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">2</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Category</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">&gt;</span><span class="m">64</span> <span class="o">&amp;</span> <span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">&lt;=</span><span class="m">66</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">3</span>
<span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Category</span><span class="p">[</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height</span><span class="o">&gt;</span><span class="m">66</span><span class="p">]</span><span class="o">&lt;-</span><span class="m">4</span>
<span class="nf">table</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">Maternal.Height.Category</span><span class="p">)</span>

<span class="c1">#Model 4: Relating birthweight to length of pregnancy and mother&#39;s height group. </span>
<span class="n">model4</span><span class="o">&lt;-</span><span class="nf">lm</span><span class="p">(</span><span class="n">Birth.Weight</span><span class="o">~</span><span class="nf">factor</span><span class="p">(</span><span class="n">Maternal.Height.Category</span><span class="p">)</span><span class="o">+</span><span class="n">Gestational.Days</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1   2   3   4 
318 340 324 192 
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ factor(Maternal.Height.Category) + 
    Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-52.956 -10.573   0.326  10.097  54.991 

Coefficients:
                                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                       -12.20299    8.40979  -1.451   0.1470    
factor(Maternal.Height.Category)2   3.92195    1.28351   3.056   0.0023 ** 
factor(Maternal.Height.Category)3   5.67130    1.30124   4.358 1.43e-05 ***
factor(Maternal.Height.Category)4   9.74727    1.50461   6.478 1.36e-10 ***
Gestational.Days                    0.45636    0.03008  15.173  &lt; 2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.45 on 1169 degrees of freedom
Multiple R-squared:  0.1971,	Adjusted R-squared:  0.1943 
F-statistic: 71.72 on 4 and 1169 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p><strong>Interpretation of the estimated regression coefficients</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_0=-12.20\)</span>. The estimated mean birthweight for a child who was born after 0 gestational days and whose mother is less than 62 inches tall is -12.20 ounces.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_1=3.92\)</span>. This is the expected difference in birthweight between babies whose mothers are in height group 1 and babies whose mothers are in height group 2, taking into account the length of pregnancy.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\beta}_2=5.67\)</span>. This is the expected difference in birthweight between babies whose mothers are in height group 1 and babies whose mothers are in height group 3, taking into account the length of pregnancy.</p></li>
</ul>
<p><em>Excerise</em>: What are the interpretations of <span class="math notranslate nohighlight">\(\hat{\beta}_3\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_4?\)</span></p>
</div>
<div class="section" id="issues-related-to-categorising-continuous-variables">
<h4>12.4.4.2 Issues related to categorising continuous variables<a class="headerlink" href="#issues-related-to-categorising-continuous-variables" title="Permalink to this headline">¶</a></h4>
<p>In the above example, we have categorised an continuous variable (height) in order to demonstrate how a categorical variable should be included in a linear regression model. This is important to know, since there are many variables that are categorical by definition and may be required for a statistical analysis. For example: cancer stage, ethnicity, education level, etc. While these examples should be included as a categorical variable in a linear regression model, it is not, in general, recommended to categorise a continuous variable in a linear model. We did so above, purely for pedagogical reasons.</p>
<p>One of the problems with categorising continuous variables is that it is difficult to decide what the cut-off for each category should be. In the example above we used quartiles to define the categories. This is particularly problematic because the quartiles are estimated using the sample data (therefore are subject to sampling error) and do not represent the quartiles of the same variable in the population. Even when sample data are not used to select cut-off criteria, it is rare for different researchers to choose the same cut-offs each time, which makes comparisons between studies difficult. Moreover, categorisation requires additional covariates to be included in the model. In our example, Model 4 contains four covariates, compared to only two in Model 3. Increasing the number of covariates leads to loss of power and greater uncertainty when estimating the regression coefficients.</p>
</div>
</div>
</div>
<div class="section" id="analysis-of-variance">
<h2>12.5 Analysis of Variance<a class="headerlink" href="#analysis-of-variance" title="Permalink to this headline">¶</a></h2>
<p>When fitting statistical models, we may wish to compare how well two models fit the data, to see which is most appropriate. Considering the models we have used in this chapter, we may want to compare (for example):</p>
<p>\begin{align}
\text{Comparison 1: }&amp; \text{Model 1 (birthweight~length of pregnancy) v Model 3 (birthweight~length of pregnancy+mothers height)} \
\text{Comparison 2: }&amp; \text{Model 2 (birthweight~mother’s smoking status) v Model 3 (birthweight~length of pregnancy+mothers height)} \
\end{align}</p>
<p>In these examples, Comparison 1 is much simpler than Comparison 2, because the models in Comparison 1 are <strong>nested</strong>.</p>
<p>Statistical models are said to be <strong>nested</strong> when one model (the simpler model) contains a subset of the covariates in the other one (the complex model) and no other additional variables. In Comparison 2, the models are not nested because the simpler model (Model 2) contains mother’s smoking status as a variable, which is not included in Model 3. Nested models can be compared using <strong>Analysis of Variance (ANOVA)</strong> (the comparison of non-nested models is much more complicated and is beyond the scope of this module).</p>
<p>The main idea of ANOVA is that: if the complex model better describes the data than the simpler model, then we would expect a reasonably large amount of the residual variation that is unexplained by the simpler model to be explained by the complex one. ANOVA provides a statistical framework that can formally test this.</p>
<p>We will first consider ANOVA in the context of simple linear regression, where the simpler model assumes no association between the outcome and the independent variable (the <strong>null</strong> model). We will then consider ANOVA in the context of multivariable linear regression and we end by learning how ANOVA can be used to test for differences between groups in a categorical variable.</p>
<div class="section" id="anova-for-simple-linear-regression">
<h3>12.5.1 ANOVA for simple linear regression<a class="headerlink" href="#anova-for-simple-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>The total variation in <span class="math notranslate nohighlight">\(Y\)</span> (otherwise known as the <strong>sum of squares of the <span class="math notranslate nohighlight">\(Y\)</span>’s</strong>), is denoted by <span class="math notranslate nohighlight">\(SS_{TOT}\)</span> and is equal to:</p>
<div class="math notranslate nohighlight">
\[ SS_{TOT} = \sum_{i=1}^n (y_i-\bar{y})^2\]</div>
<p>This can be partitioned into two components: the predictable variation in <span class="math notranslate nohighlight">\(Y\)</span> (<span class="math notranslate nohighlight">\(SS_{REG}\)</span>) and the unpredictable variation in <span class="math notranslate nohighlight">\(Y\)</span> (<span class="math notranslate nohighlight">\(SS_{RES}\)</span>). A key result for ANOVA is:</p>
<p>\begin{align}
SS_{TOT} &amp;= SS_{REG}+SS_{RES} \
\rightarrow \sum_{i=1}^n (Y_i-\bar{Y})^2 &amp;= \sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 + \sum_{i=1}^n(Y_i-\hat{Y_i})^2
\end{align}</p>
<p>In the above equations, <span class="math notranslate nohighlight">\(SS_{TOT}\)</span> (i.e the TOTal sum of squares), represents all of the variation in <span class="math notranslate nohighlight">\(Y\)</span> about its mean value. <span class="math notranslate nohighlight">\(SS_{REG}\)</span> (i.e. the REGression sum of squares) represents the variation of the predicted values <span class="math notranslate nohighlight">\(\hat{Y}\)</span> about the mean. <span class="math notranslate nohighlight">\(SS_{RES}\)</span> (i.e. the RESidual sum of squares) represents the variation of the observed values about their predicted values.</p>
<p>Using the sums of squares defined above, we can calculate the proportion of variance explained by statistical model, known as the <strong>coefficient of determination</strong>.</p>
<div class="section" id="the-coefficient-of-determination">
<h4>12.5.1.1 The coefficient of determination<a class="headerlink" href="#the-coefficient-of-determination" title="Permalink to this headline">¶</a></h4>
<p>The proportion of variation which is explained by a statistical model is denoted by <span class="math notranslate nohighlight">\(R^2\)</span> and is given by:</p>
<p>$<span class="math notranslate nohighlight">\(R^2 = \frac{SS_{REG}}{SS_{TOT}}\)</span>$.</p>
<p><em>Example.</em> The coefficient of determination is given in the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> output for a linear regression in R. In Model 1, <span class="math notranslate nohighlight">\(R^2=0.1661\)</span> (see output below). This means that Model 1 explains 16.6% of the total variation in <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#The coefficient of determination</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>While <span class="math notranslate nohighlight">\(R^2\)</span> is sometimes used an overall measure of goodness-of-fit (or predictive performance), it isn’t used to formally compare models. This is because <span class="math notranslate nohighlight">\(R^2\)</span> will never decrease when new covariates are added to a model (provided that the number and identity of observations remains the same). Therefore, using <span class="math notranslate nohighlight">\(R^2\)</span> for model comparisons, we would always conclude that the more complex model is at least as good a fit as the simpler model, even if this is not true. We can calculate an adjusted <span class="math notranslate nohighlight">\(R^2\)</span> (the adjusted <span class="math notranslate nohighlight">\(R^2\)</span> for Model 1 is given in the output above) that accounts for this issue. Alternatively, we can use analysis of variance (ANOVA). To describe ANOVA, we need to first define the remaining statistics that are commonly summarised in what is known as the ANOVA table.</p>
</div>
<div class="section" id="the-anova-table">
<h4>12.5.1.2 The ANOVA table.<a class="headerlink" href="#the-anova-table" title="Permalink to this headline">¶</a></h4>
<p>Each of the sum of squares defined above (<span class="math notranslate nohighlight">\(SS_{TOT}\)</span>, <span class="math notranslate nohighlight">\(SS_{REG}\)</span> and <span class="math notranslate nohighlight">\(SS_{RES}\)</span>) have an associated <strong>degrees of freedom (d.f.)</strong>. The d.f. for the total sum of squares is <span class="math notranslate nohighlight">\((n-1)\)</span>, since the variance of <span class="math notranslate nohighlight">\(Y\)</span> is <span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_i-\bar{Y})^2/(n-1)\)</span>. The d.f. for the regression sum of squares in the number of covariates in the regression model (when a simple linear regression model is used this is equal to 1). The residual d.f. is found by subtracting the regression d.f. from the total d.f. The sums of squares also have associated mean squares, which are obtained by dividing the sum of squares by its associated degrees of freedom (note that the residual mean square is then equal to <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>). These statistics are summarised in an ANOVA table for simple linear regression (Table 2).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Regression</p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG}=\frac{SS_{REG}}{1}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Residual</p></td>
<td><p>n-2</p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{RES}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{RES}=\frac{SS_{RES}}{n-2}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p>n-1</p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Table 2: The ANOVA Table for simple linear regression</p>
<p>The values in this table can be used to conduct formal hypothesis tests.</p>
</div>
<div class="section" id="hypothesis-testing-using-anova">
<h4>12.5.1.3 Hypothesis testing using ANOVA<a class="headerlink" href="#hypothesis-testing-using-anova" title="Permalink to this headline">¶</a></h4>
<p>ANOVA is used to test the null hypothesis that the simpler of the two nested models better fits the data. In simple linear regression, the simpler model is the null model, in which case:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> The null model is a better fit</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1:\)</span> The simple linear regression model is a better fit</p></li>
</ul>
<p>To test the null hypothesis defined above, we use an <span class="math notranslate nohighlight">\(F\)</span> statistic, defined as:</p>
<div class="math notranslate nohighlight">
\[F = \frac{MS_{REG}}{MS_{RES}}\]</div>
<p>This ratio measures how much more variation in <span class="math notranslate nohighlight">\(Y\)</span> is explained by the model than would be expected by chance. If the model does not fit the data well, then we would expect this ratio to be equal to 1. The larger the value of <span class="math notranslate nohighlight">\(F\)</span>, the stronger the evidence that the complex model is a better fit. To obtain a <span class="math notranslate nohighlight">\(p\)</span>-value for a formal hypothesis test, <span class="math notranslate nohighlight">\(F\)</span> can be compared to the <span class="math notranslate nohighlight">\(F_{1,(n-2)}\)</span> distribution (where 1 and (n-2) are the relevant degrees of freedom for the mean squares).</p>
</div>
<div class="section" id="similarity-between-f-tests-and-t-tests-in-simple-linear-regression">
<h4>12.5.1.4 Similarity between F tests and t-tests in simple linear regression<a class="headerlink" href="#similarity-between-f-tests-and-t-tests-in-simple-linear-regression" title="Permalink to this headline">¶</a></h4>
<p>Since the null model is defined as: <span class="math notranslate nohighlight">\(Y=\beta_0+\epsilon\)</span> and the simple linear regression model is defined as: <span class="math notranslate nohighlight">\(Y=\beta_0+\beta_1X+\epsilon\)</span>. The hypotheses above can be rewritten as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \beta_1 \neq 0\)</span></p></li>
</ul>
<p>In other words, the <span class="math notranslate nohighlight">\(F\)</span>-test for a simple linear regression model is the same as the <span class="math notranslate nohighlight">\(t\)</span>-test of the null hypothesis that the slope parameter is equal to 0. Indeed, the two tests are equivalent with <span class="math notranslate nohighlight">\(F=t^2\)</span>. Consequently, it is not particularly common to use <span class="math notranslate nohighlight">\(F\)</span>-tests in the simple linear regression model, they are more useful for assessing more complex models with multiple covariates.</p>
</div>
<div class="section" id="example-using-model-1">
<h4>12.5.1.5 Example using Model 1<a class="headerlink" href="#example-using-model-1" title="Permalink to this headline">¶</a></h4>
<p>The <span class="math notranslate nohighlight">\(F\)</span> test statistic and associated <span class="math notranslate nohighlight">\(p\)</span>-value are given in the summary output for linear models (shown below):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># F-test</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.348 -11.065   0.218  10.101  57.704 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -10.75414    8.53693   -1.26    0.208    
Gestational.Days   0.46656    0.03054   15.28   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.74 on 1172 degrees of freedom
Multiple R-squared:  0.1661,	Adjusted R-squared:  0.1654 
F-statistic: 233.4 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>Alternatively, we can use <code class="docutils literal notranslate"><span class="pre">anova()</span></code> which outputs the ANOVA table as well as the relevant <span class="math notranslate nohighlight">\(F\)</span> statistic and <span class="math notranslate nohighlight">\(p\)</span>-value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># F-test using anova()</span>
<span class="nf">anova</span><span class="p">(</span><span class="n">model1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>
<tbody>
	<tr><th scope=row>Gestational.Days</th><td>   1        </td><td> 65449.51   </td><td>65449.5131  </td><td>233.4293    </td><td>3.395226e-48</td></tr>
	<tr><th scope=row>Residuals</th><td>1172        </td><td>328608.34   </td><td>  280.3825  </td><td>      NA    </td><td>          NA</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>As can be seen from both outputs above, the <span class="math notranslate nohighlight">\(F\)</span>-statistic is equal to 233.4 with <span class="math notranslate nohighlight">\(p\)</span>-value <span class="math notranslate nohighlight">\(&lt;2.2\times10^{-16}\)</span>. With such a small <span class="math notranslate nohighlight">\(p\)</span>-value there is strong evidence against the null and therefore we conclude that the model which includes gesational days is a better fit (i.e. we conclude that <span class="math notranslate nohighlight">\(\beta_1 \neq 0\)</span>).</p>
<p>Notice that in the <code class="docutils literal notranslate"><span class="pre">summmary()</span></code> output, the <span class="math notranslate nohighlight">\(p\)</span>-values for the F-test and the t-test for gestational days are identical, and that <span class="math notranslate nohighlight">\(F=233.4=15.24^2=t^2\)</span>.</p>
</div>
</div>
<div class="section" id="anova-for-multivariable-linear-regression">
<h3>12.5.2 ANOVA for multivariable linear regression<a class="headerlink" href="#anova-for-multivariable-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>In the context of multivariable linear regression, ANOVA can be used to test whether a more complex model is a better fit than the null model (<strong>the Global F test</strong>), or whether a more complex model is a better fit than a simpler model that includes a subset of the covariates in the complex model (<strong>the partial F test</strong>). Each test requires slight modifications to the ANOVA table defined above and we will discuss these in turn.</p>
<div class="section" id="the-global-f-test">
<h4>12.5.2.2 The Global F test<a class="headerlink" href="#the-global-f-test" title="Permalink to this headline">¶</a></h4>
<p>The general formulation of the ANOVA table (suitable for simple and multivariable linear regression models) is given in Table 3, where <span class="math notranslate nohighlight">\(p\)</span> is the number of covariates in the model.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Regression</p></td>
<td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG}=\frac{SS_{REG}}{p}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Residual</p></td>
<td><p><span class="math notranslate nohighlight">\(n-(p+1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{RES}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{RES}=\frac{SS_{RES}}{n-p-1}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\(n-p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Table 3: The ANOVA Table</p>
<p>Note that this is equivalent to Table 2 when <span class="math notranslate nohighlight">\(p=1\)</span>.</p>
<p>The Global F test tests the null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) that the null model is a better fit than the more complex model against the alternative hypothesis (<span class="math notranslate nohighlight">\(H_1\)</span>) that the complex model is a better fit. Or, equivalently:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> All slope parameters in the complex model are equal to 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1:\)</span> At least one of the slope parameters in the complex model is not equal to 0.</p></li>
</ul>
<p>The appropriate <span class="math notranslate nohighlight">\(F\)</span> statistic is the ratio <span class="math notranslate nohighlight">\(MS_{REG}/MS_{RES}\)</span> (as defined in Table 3). Under the null hypothesis, <span class="math notranslate nohighlight">\(F\)</span> follows an <span class="math notranslate nohighlight">\(F_{p,(n-(p+1))}\)</span> distribution.</p>
<p><em>Example:</em> We can use <code class="docutils literal notranslate"><span class="pre">summary()</span></code> to conduct a global F test for Model 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#ANOVA for Model 3 </span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ Gestational.Days.Centered + Maternal.Height.Centered, 
    data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-53.829 -10.589   0.246  10.254  54.403 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               119.46252    0.47980 248.983  &lt; 2e-16 ***
Gestational.Days.Centered   0.45237    0.03006  15.051  &lt; 2e-16 ***
Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e-11 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 16.44 on 1171 degrees of freedom
Multiple R-squared:  0.1969,	Adjusted R-squared:  0.1955 
F-statistic: 143.5 on 2 and 1171 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>Our hypotheses are defined as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0\)</span>: the regression coefficients for both gestational days and mother’s height are equal to 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1\)</span>: the regression coefficient for either gestational days or mother’s height (or both) is not equal to 0.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(F\)</span> statistic is 143.5 with a <span class="math notranslate nohighlight">\(p\)</span>-value <span class="math notranslate nohighlight">\(&lt;2.2 \times 10^{-11}\)</span>. Therefore, there is strong evidence against the null and we can conclude that at least one of the estimated regression coefficients is non-zero (i.e Model 3 is a better fit than the null model).</p>
</div>
</div>
<div class="section" id="the-partial-f-test">
<h3>12.5.2 The partial F-test<a class="headerlink" href="#the-partial-f-test" title="Permalink to this headline">¶</a></h3>
<p>The global <span class="math notranslate nohighlight">\(F\)</span>-test is a joint test of the statistical signficance of all the slope parameters in a linear regression model. On the other hand, the partial <span class="math notranslate nohighlight">\(F\)</span>-test compares the fit of a complex model (say Model A with <span class="math notranslate nohighlight">\(p\)</span> predictors) with a simpler model (say Model B with <span class="math notranslate nohighlight">\(p-k\)</span> predictors).</p>
<p>The key to the partial <span class="math notranslate nohighlight">\(F\)</span>-test is the construction of an Analysis of Variance table that partitions the sum of squares explained by the complex model into that explained by the simple model and the extra sum of squares only explained by the complex model. Using the notation that <span class="math notranslate nohighlight">\(SS_{REG_A}\)</span> denotes the sum of squares explained by the complex model, whilst <span class="math notranslate nohighlight">\(SS_{REG_B}\)</span> denotes the sum of squares explained by the simpler model, the ANOVA table is as shown in Table 3.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Explained by Model B</p></td>
<td><p><span class="math notranslate nohighlight">\(p-k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG_B}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG_B}=\frac{SS_{REG_B}}{p-k}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Explained by Model A</p></td>
<td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG_A}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG_A}=\frac{SS_{REG_A}}{p}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Extra explained by Model A</p></td>
<td><p><span class="math notranslate nohighlight">\(k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{REG_A}-SS_{REG_B}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{REG_X}=\frac{(SS_{REG_A}-SS_{REG_B}}{k}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Residual from Model A</p></td>
<td><p><span class="math notranslate nohighlight">\(n-(p_1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{RES_A}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{RES}=\frac{SS_{RES_A}}{n-(p+1)}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\((n-1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Table 3: The ANOVA table comparing the fit of a model ( Model A) with <span class="math notranslate nohighlight">\(p\)</span> predictors with that of one (Model B) with <span class="math notranslate nohighlight">\((p-k)\)</span> predictors</p>
<p>The partial <span class="math notranslate nohighlight">\(F\)</span>-test tests the null hypothesis that all of the slope parameters included in Model A but omitted from Model B are equal to zero. The alternative hypothesis is that at least one of the additional parameters in Model A is not equal to 0.</p>
<p>The appropriate test statistic (<span class="math notranslate nohighlight">\(F\)</span>) is the ratio of extra mean sum of squares in Model A to the mean residual sum of squares from Model A. Under the null hypothesis, this test statistic follows an <span class="math notranslate nohighlight">\(F\)</span>-distribution:</p>
<div class="math notranslate nohighlight">
\[\text{Under } H_0: F = \frac{MS_{REG_X}}{MS_{RES}} \sim F_{k,(n-(p+1))}\]</div>
<p><em>Example</em>: We can use <code class="docutils literal notranslate"><span class="pre">anova()</span></code> to conduct a partial F-test to compare Models 1 and 3:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> Model 1 is the better fit</p></li>
<li><p><span class="math notranslate nohighlight">\(H_0:\)</span> Model 3 is the better fit</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">anova</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span> <span class="n">model3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>
<tbody>
	<tr><td>1172        </td><td>328608.3    </td><td>NA          </td><td>      NA    </td><td>      NA    </td><td>          NA</td></tr>
	<tr><td>1171        </td><td>316482.2    </td><td> 1          </td><td>12126.13    </td><td>44.86728    </td><td>3.266475e-11</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-statistic is 44.87 with a <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(3.23\times 10^{-11}\)</span>. This is strong evidence against the null, and hence the data indicates that Model 3 is the better fit.</p>
<p>In this case, the two models only differed by one variable (mother’s height) and so the hypotheses could be re-written as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_2=0\)</span>, where <span class="math notranslate nohighlight">\(\beta_2\)</span> is the regression coefficient for mother’s height.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_2 \neq 0\)</span>.</p></li>
</ul>
<p>In other words, when the models only differ by one variable, the partial F test is equivalent to the t test of the null hypothesis that the regression coefficient for that variable is equal to 0. Notice in our example that the results of the partial F test are the same as the t-test for <span class="math notranslate nohighlight">\(\beta_2=0\)</span>, with <span class="math notranslate nohighlight">\(F=t^2\)</span>.</p>
<p>For this reason, partial F tests are more useful in situations where we wish to compare models that differ by more than one variable.</p>
</div>
<div class="section" id="anova-for-models-with-categorical-independent-variables">
<h3>12.5.3 ANOVA for models with categorical independent variables.<a class="headerlink" href="#anova-for-models-with-categorical-independent-variables" title="Permalink to this headline">¶</a></h3>
<p>Another useful application of ANOVA is to test for differences in means between categories of a categorical variable.</p>
<p>Suppose we are interested in the association between an outcome <span class="math notranslate nohighlight">\(Y\)</span> and a categorical variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(K\)</span> groups. We have already seen how to define a multivariable linear regression model using dummy variables for this situation. An alternative model, often termed the <strong>ANOVA model</strong>, is as follows:</p>
<p>Let <span class="math notranslate nohighlight">\(y_{ki}\)</span> be the value of the outcome for the <span class="math notranslate nohighlight">\(i^{th}\)</span> observation in the <span class="math notranslate nohighlight">\(k^{th}\)</span> group (<span class="math notranslate nohighlight">\(i=1,...,n_k\)</span> and <span class="math notranslate nohighlight">\(k=1,...,K\)</span>). The ANOVA model is then defined as:</p>
<div class="math notranslate nohighlight">
\[y_{ki}=\mu_k + \epsilon_{ki} \text{, where } \epsilon_{ki} \sim NID(0,\sigma^2)\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mu_k\)</span> is the mean of the outcome in the <span class="math notranslate nohighlight">\(k^{th}\)</span> group. With this representation, the null and alternative hypothesis are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \mu_k= \mu\)</span> (i.e. the means in all groups defined by the categorical variables are equal to a common value).</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \mu_k \neq \mu\)</span> (i.e. the group means are not all equal).</p></li>
</ul>
<div class="section" id="sum-of-squares-for-models-with-categorical-variables">
<h4>12.5.3.1 Sum of squares for models with categorical variables<a class="headerlink" href="#sum-of-squares-for-models-with-categorical-variables" title="Permalink to this headline">¶</a></h4>
<p>For models with a single independent categorical variable the fitted values are simply the group means (<span class="math notranslate nohighlight">\(\bar{y_k}\)</span>). Under the null hypothesis that the group means are all equal, the fitted values are all equal to the overall mean (<span class="math notranslate nohighlight">\(\bar{y}\)</span>). This leads to new terminology for the residual sum of squares (<span class="math notranslate nohighlight">\(SS_{RES}\)</span>) and the sum of squares explained by the model (<span class="math notranslate nohighlight">\(SS_{REG}\)</span>):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(SS_{RES} = \sum_{k=1}^K \sum_{i=1}^{n_k} (y_{ki} - \bar{y}_k)^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(SS_{REG} = \sum_{k=1}^K \sum_{i=1}^{n_k} (\bar{y}_k - \bar{y})^2 = \sum_{k=1}^K n_k (\bar{y}_k - \bar{y})^2 \)</span></p></li>
</ul>
<p>In this case, the residual sum of squares is often termed the <strong>within group sum of squares <span class="math notranslate nohighlight">\((SS_{Within})\)</span></strong> and the regression sum of squares is often termed the <strong>between group sum of squares <span class="math notranslate nohighlight">\((SS_{Between})\)</span></strong>.</p>
</div>
<div class="section" id="id1">
<h4>12.5.3.2 The ANOVA table<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>When there are <span class="math notranslate nohighlight">\(K\)</span> groups, the degrees of freedom for the within groups sum of squares is <span class="math notranslate nohighlight">\(n-K\)</span> (because the model includes <span class="math notranslate nohighlight">\(K\)</span> parameters) and the degrees of freedom for the between groups sum of squares is <span class="math notranslate nohighlight">\(K-1\)</span> (because the null model contains a single parameter, the overall mean). Hence the ANOVA table is as follows:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>d.f.</p></th>
<th class="head"><p>SS</p></th>
<th class="head"><p>Mean Square</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Between groups</p></td>
<td><p><span class="math notranslate nohighlight">\(K-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{Between}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{Between}=\frac{SS_{Between}}{(K-1)}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>WIthin Groups</p></td>
<td><p><span class="math notranslate nohighlight">\(n-K\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{Within}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{Within}=\frac{SS_{RES}}{n-K}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\(n-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(SS_{TOT}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(MS_{TOT}=\frac{SS_{TOT}}{n-1}\)</span></p></td>
</tr>
</tbody>
</table>
<p>Table 4: The ANOVA Table for models with categorical variables</p>
</div>
<div class="section" id="the-f-test">
<h4>12.5.3.3 The F-test<a class="headerlink" href="#the-f-test" title="Permalink to this headline">¶</a></h4>
<p>To test the null hypothesis that the means in all groups are equal to a common value, the appropriate <span class="math notranslate nohighlight">\(F\)</span>-statistic is:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}F = \frac{MS_{Between}}{MS_{Within}} \sim F_{(K-1), n-K} \text{ under } H_0$$. \\If this test obtains a small $p$-value, then we have evidence that the means in the groups are not all the same. However, it does not tell us which of the group means differed from which other group means. For this reason, if we do find evidence of difference in means on an $F$-test, we may want to follow up with further analysis. Such further analysis may include pair-wise comparisons of means through analysis restricted to two groups. \\*Example* We conduct an F-test to compare the average birthweights between babies whose mothers smoke and whose mothers don't smoke using the birthweight data. \\Let $\mu_1$ and $\mu_0$ denote the mean birthweight for babies whose mothers do smoke and don't smoke, respectively. Then, the relevant hypotheses are: \\+ $H_0: \mu_1= \mu_0$ (i.e. the birthweight of a baby does not depend on whether the mother smoked)
+ $H_1: \mu_1\neq \mu_0$\\Recall that we previously defined Model 2 to related birthweight and mother's smoking status: \\$$ y_i = \alpha_0 + \alpha_1 s_i + \epsilon_i\end{aligned}\end{align} \]</div>
<p>Where <span class="math notranslate nohighlight">\(Y\)</span> denotes the birthweight and</p>
<div class="math notranslate nohighlight">
\[\begin{split} s_{i}
\begin{cases}
    1 &amp; \text{ if the mother smokes} \\
    0 &amp; \text{ if the mother does not smoke}
\end{cases} \end{split}\]</div>
<p>We can rewrite this equation using the ANOVA model as follows:
$<span class="math notranslate nohighlight">\(
\begin{align}
    y_{1i} &amp;=\mu_1 + \epsilon_{1i}  \text{ if the mother smokes} \\
    y_{0i} &amp;=\mu_0 + \epsilon_{0i}  \text{ if the mother does not smoke}
\end{align} \)</span><span class="math notranslate nohighlight">\(
Where \)</span>y_{ki}<span class="math notranslate nohighlight">\( is the mean birthweight in the \)</span>k^{th}<span class="math notranslate nohighlight">\( group (groups are defined by mother's smoking status), \)</span>\mu_1 = \beta_0 + \beta_1<span class="math notranslate nohighlight">\( and \)</span>\mu_0=\beta_0<span class="math notranslate nohighlight">\( (in other words, our null hypothesis can be rewritten as: \)</span>\beta_1=0$).</p>
<p>We can use either <code class="docutils literal notranslate"><span class="pre">anova()</span></code> or <code class="docutils literal notranslate"><span class="pre">summary()</span></code> to conduct the test in R:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">anova</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table>
<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>
<tbody>
	<tr><th scope=row>factor(Maternal.Smoker)</th><td>   1        </td><td> 24002.06   </td><td>24002.0638  </td><td>76.0167     </td><td>9.461068e-18</td></tr>
	<tr><th scope=row>Residuals</th><td>1172        </td><td>370055.79   </td><td>  315.7473  </td><td>     NA     </td><td>          NA</td></tr>
</tbody>
</table>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Birth.Weight ~ factor(Maternal.Smoker), data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-68.085 -11.085   0.915  11.181  52.915 

Coefficients:
                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                 123.0853     0.6645 185.221   &lt;2e-16 ***
factor(Maternal.Smoker)True  -9.2661     1.0628  -8.719   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 17.77 on 1172 degrees of freedom
Multiple R-squared:  0.06091,	Adjusted R-squared:  0.06011 
F-statistic: 76.02 on 1 and 1172 DF,  p-value: &lt; 2.2e-16
</pre></div>
</div>
</div>
</div>
<p>In the ANOVA table, the <code class="docutils literal notranslate"><span class="pre">factor(Maternal.Smoker)</span></code> row gives the between groups results and the <code class="docutils literal notranslate"><span class="pre">Residuals</span></code> row gives the within group results.</p>
<p>The <span class="math notranslate nohighlight">\(F\)</span>-statistic is equal to 76.02 with a <span class="math notranslate nohighlight">\(p\)</span>-value equal to <span class="math notranslate nohighlight">\(9.46\times10^{-18}\)</span>. This evidence suggests that there is a difference in the mean birthweight between the two groups defined by mother’s smoking status.</p>
</div>
</div>
</div>
<div class="section" id="summary-of-chapter">
<h2>12.6 Summary of chapter<a class="headerlink" href="#summary-of-chapter" title="Permalink to this headline">¶</a></h2>
<p>In the current chapter, we have learnt the following:</p>
<ul class="simple">
<li><p>The definitions of the simple and multivariable linear regression model</p></li>
<li><p>How to express the above models in a single equation and in matrix form</p></li>
<li><p>What assumptions are made in linear regression</p></li>
<li><p>How to estimate and interpret the parameters in a linear regression model</p></li>
<li><p>How to conduct hypothesis tests to test the null hypothesis that an estimated regression coefficient is equal to some constant</p></li>
<li><p>How to obtain and interpret confidence intervals for estimated regression coefficients and fitted values</p></li>
<li><p>How to obtain and interpret prediction intervals for fitted values</p></li>
<li><p>How to include categorical variables in a linear regression model</p></li>
<li><p>What it means to center a variable in linear regression</p></li>
<li><p>How to perform ANOVA to compare nested models</p></li>
<li><p>How to perform ANOVA to compare group means</p></li>
</ul>
<p>In the next chapter we will learn about some diagnostics tools that are available for assessing the assumptions of a linear model and how to model more complex relationships between covariates and the outcome. We will also discuss how the type of investigation influences the presentation and interpretation of the results of linear regression.</p>
</div>
<div class="section" id="proofs">
<h2>12.7 Proofs<a class="headerlink" href="#proofs" title="Permalink to this headline">¶</a></h2>
<div class="section" id="proof-for-the-ordinary-least-squares-estimates-in-simple-linear-regression">
<h3>12.7.1 Proof for the ordinary least squares estimates in simple linear regression<a class="headerlink" href="#proof-for-the-ordinary-least-squares-estimates-in-simple-linear-regression" title="Permalink to this headline">¶</a></h3>
<p>Recall the ordinary least square estimates of the intercept (<span class="math notranslate nohighlight">\(\beta_0\)</span>) and slope (<span class="math notranslate nohighlight">\(\beta_1\)</span>) in simple linear regression are:</p>
<p>\begin{align}
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_0}\bar{x} \
\hat{\beta_1} &amp;= \frac{\sum_{i=1}^2 (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x_i})^2}
\end{align}</p>
<p>Proof:</p>
<p>To solve for the value of <span class="math notranslate nohighlight">\(\beta_0\)</span> that minimises <span class="math notranslate nohighlight">\(SS_{RES}\)</span>, we differentiate <span class="math notranslate nohighlight">\(SS_{RES}\)</span> with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and set the differential to zero:</p>
<div class="math notranslate nohighlight">
\[ \frac{d(SS_{RES})}{d(\hat{\beta}_0)} = \sum_{i=1}^n -2(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)=0\]</div>
<p>Since <span class="math notranslate nohighlight">\(\sum_{i=1}^n (y_i)=n\bar{y}\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n(x_i)=n\bar{x}\)</span>, we can simplify to:</p>
<div class="math notranslate nohighlight">
\[-n\bar{y}+n\hat{\beta}_0+n\hat{\beta}_1\bar{x}=0\]</div>
<p>Rearranging the above and divide by <span class="math notranslate nohighlight">\(n\)</span> to give:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$. \\To solve for the value of $\beta_1$ that minimises $SS_{RES}$, we have to differentiate with respect to $\hat{\beta}_1$. First, we substitute in our solution for $\hat{\beta}_0$ as follows: \\$$SS_{RES}=\sum_{i=1}^n(y_i-(\bar{y}-\hat{\beta}_1\bar{x})-\hat{\beta}_1x_i)^2=\sum_{i=1}^n ((y_i-\bar{y})-\hat{\beta}_1(x_i-\bar{x}))^2\end{aligned}\end{align} \]</div>
<p>Now differentiating the above with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> and setting the differential to zero gives:</p>
<div class="math notranslate nohighlight">
\[\frac{d(SS_{RES})}{d(\hat{\beta}_1)} = \sum_{i=1}^n -2(x_i-\bar{x})(y_i-\bar{y})+2\hat{\beta}_1(x_i-\bar{x})^2=0\]</div>
<p>Rearranging gives:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_1\sum_{i=1}^n (x_i-\bar{x})^2 = \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})\]</div>
<div class="math notranslate nohighlight">
\[\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\]</div>
</div>
<div class="section" id="proof-that-the-ols-estimates-are-also-the-maximum-likelihood-estimates">
<h3>12.7.2 Proof that the OLS estimates are also the maximum likelihood estimates<a class="headerlink" href="#proof-that-the-ols-estimates-are-also-the-maximum-likelihood-estimates" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(Y_i \sim NID(\mu, \sigma^2)\)</span>, the log likelihood function is:</p>
<div class="math notranslate nohighlight">
\[l(\mu | y_1,...,y_n) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu)^2\]</div>
<p>So, for the simple linear regression model:</p>
<p>$<span class="math notranslate nohighlight">\(l(\beta_0, \beta_1 | y_1,...,y_n) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \beta_1x_i-\beta_0)^2\)</span>$.</p>
<p>Therefore, for any fixed positive value for <span class="math notranslate nohighlight">\(\sigma^2\)</span>, maximising the log likelihood function is equivalent to minimising <span class="math notranslate nohighlight">\(SS_{RES}\)</span> and so the OLS estimates are also maximum likelihood estimates of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="11.%20Types%20of%20Investigation.html" title="previous page">11. Types of Investigation</a>
    <a class='right-next' id="next-link" href="13.%20Linear%20Regression%20II.html" title="next page">13. Linear Regression II</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By MSc Health Data Science, LSHTM<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>