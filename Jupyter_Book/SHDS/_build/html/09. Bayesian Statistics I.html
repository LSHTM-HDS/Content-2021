
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9. Bayesian Statistics I &#8212; Statistics for Health Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="10. Bayesian Statistics II: Normal data" href="10.%20Bayesian%20Statistics%20II.html" />
    <link rel="prev" title="Further resources" href="08.g.%20Frequentist%20II.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Statistics for Health Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00.%20Welcome.html">
   Welcome to Statistics for Health Data Science
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01.%20Introduction.html">
   1 Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Basic probability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="02.a.%20Probability.Discrete.html">
   2. Probability and Discrete Probability Distributions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="02.b.%20Probability.Discrete.html">
     2.1 Bayes’ Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02.c.%20Probability.Discrete.html">
     2.2 The binomial distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02.d.%20Probability.Discrete.html">
     2.3 The Poisson distribution
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="03.a.%20Continuous%20Probability%20Distributions.html">
   3. Continuous probability distributions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="03.b.%20Continuous%20Probability%20Distributions.html">
     3.1 Continuous random variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.c.%20Continuous%20Probability%20Distributions.html">
     3.2 Useful continuous distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.d.%20Continuous%20Probability%20Distributions.html">
     3.3 Uses of the standard Normal distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.e.%20Continuous%20Probability%20Distributions.html">
     3.5 Are the data normally distributed?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03.f.%20Continuous%20Probability%20Distributions.html">
     3.5 Joint distributions and correlations
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical Inference
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="04.a.%20Population.and.samples.html">
   4. Populations and Samples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="04.b.%20Population.and.samples.html">
     4.1 Sampling from a population
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.c.%20Population.and.samples.html">
     4.2 Statistical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.d.%20Population.and.samples.html">
     4.3 Sampling distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.e.%20Population.and.samples.html">
     4.4 Obtaining the sampling distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.f.%20Population.and.samples.html">
     4.5 Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04.g.%20Population.and.samples.html">
     Appendix: additional reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="05.a.%20Likelihood.html">
   5. Likelihood
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="05.b.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.c.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.d.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.f.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05.g.%20Likelihood.html">
     5. Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="06.a.%20Maximum%20Likelihood.html">
   6. Maximum Likelihood
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="06.b.%20Maximum%20Likelihood.html">
     6.1 Likelihood and log-likelihood with
     <span class="math notranslate nohighlight">
      \(n\)
     </span>
     independent observations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.c.%20Maximum%20Likelihood.html">
     6.2 Properties of maximum likelihood estimators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.d.%20Maximum%20Likelihood.html">
     6.3 Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06.e.%20Maximum%20Likelihood.html">
     Appendix: Additional Reading
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="07.a.%20Frequentist%20I.html">
   7. Frequentist I: Confidence Intervals (CIs)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="07.b.%20Frequentist%20I.html">
     7.1 Introduction to confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.c.%20Frequentist%20I.html">
     7.2 95% confidence intervals for the mean
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.d.%20Frequentist%20I.html">
     7.3 Interpretation of confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.e.%20Frequentist%20I.html">
     7.4 Approximate confidence intervals for parameters estimated using large samples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.f.%20Frequentist%20I.html">
     7.5 Confidence Intervals using resampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.g.%20Frequentist%20I.html">
     7.6 Summary: Use of confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07.h.%20Frequentist%20I.html">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="08.a.%20Frequentist%20II.html">
   8. Frequentist II: Hypothesis tests
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="08.b.%20Frequentist%20II.html">
     8.1 Proving and disproving hypotheses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.c.%20Frequentist%20II.html">
     8.2 The p-value
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.d.%20Frequentist%20II.html">
     8.3 Connection between p-values and confidence intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.e.%20Frequentist%20II.html">
     8.4 Other (mis-)interpretations of p-values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.f.%20Frequentist%20II.html">
     8.5 Calculating p-values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08.g.%20Frequentist%20II.html">
     Further resources
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   9. Bayesian Statistics I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10.%20Bayesian%20Statistics%20II.html">
   10. Bayesian Statistics II: Normal data
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Statistical modelling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="11.%20Types%20of%20Investigation.html">
   11. Types of Investigation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12.%20Linear%20Regression%20I.html">
   12. Linear Regression I
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13.%20Linear%20Regression%20II.html">
   13. Linear Regression II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14.%20Logistic%20Regression.html">
   14 Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15.%20Poisson%20Regression%20Model.html">
   15. Generalised Linear Models: Poisson Regression for Count Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16.%20Extensions%20Confounding%2C%20standardization%2C%20and%20collapsibility.html">
   16. Extensions: Confounding, standardization, and collapsibility
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/09. Bayesian Statistics I.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/LSHTM-HDS/Content-2021"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/LSHTM-HDS/Content-2021/issues/new?title=Issue%20on%20page%20%2F09. Bayesian Statistics I.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/LSHTM-HDS/Content-2021/master?urlpath=tree/docs/09. Bayesian Statistics I.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intended-learning-objectives">
   9.1 Intended learning objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-1">
   Part 1
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-2">
   Part 2
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   9.2 Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#acknowledgements">
     Acknowledgements
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Part 1:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability">
   9.3 Probability
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference">
   9.4 Bayesian Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-theorem">
   9.5 Bayes Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   9.6 Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-bayesian-paradigm-in-health-data-science-problems">
   9.7 The Bayesian paradigm in Health data science problems.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Part 2:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayes-thorem-for-discrete-and-continous-data">
   9.8 Bayes thorem for discrete and continous data.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference-on-proportions">
   9.9 Bayesian inference on proportions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-beta-prior">
     9.9.1 The Beta prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior">
     9.9.2 Posterior
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summarising-posteriors">
   9.10 Summarising Posteriors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-predictions">
   9.11 Prior Predictions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-prediction">
     9.11.1 Posterior Prediction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugacy">
   9.12 Conjugacy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   9.13 Exercise
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayesian-statistics-i">
<h1>9. Bayesian Statistics I<a class="headerlink" href="#bayesian-statistics-i" title="Permalink to this headline">¶</a></h1>
<div class="section" id="intended-learning-objectives">
<h2>9.1 Intended learning objectives<a class="headerlink" href="#intended-learning-objectives" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="part-1">
<h2>Part 1<a class="headerlink" href="#part-1" title="Permalink to this headline">¶</a></h2>
<div class="alert alert-block alert-warning">
<b> Intended learning outcomes</b> 
<p>By the end of this session you will be able to:</p>
<ul class="simple">
<li><p>Compare the notions of probability and likelihood in Bayesian and Frequentist paradigms</p></li>
<li><p>Understand the notions of prior and posterior distributions</p></li>
<li><p>Apply Bayes Theorem in the discrete case</p></li>
</ul>
</div>
</div>
<div class="section" id="part-2">
<h2>Part 2<a class="headerlink" href="#part-2" title="Permalink to this headline">¶</a></h2>
<div class="alert alert-block alert-warning">
<p>By the end of this session you will be able to:</p>
<p>Understand and apply the basic principles of Bayesian analysis using proportions, specifically:</p>
<ul class="simple">
<li><p>Use the beta distribution as a prior and derive the posterior distribution</p></li>
<li><p>Obtain credible HPD intervals for the parameter</p></li>
<li><p>Obtain prior and posterior predictive distributions</p></li>
<li><p>Understand the concept of conjugate priors</p></li>
</ul>
</div>
</div>
<div class="section" id="introduction">
<h2>9.2 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>So far in this module, we have looked at frequentist or classical statistical ideas, such as maximum likelihood estimation, hypothesis testing and p-values. Underlying the frequentist approach is the belief that there is a true state of reality, and that parameters have a fixed and true value. Probabilities are long-run frequencies; for example, the probability of a driver in London having a car accident is a fixed value between 0 and 1. A typical way of estimating this value is to take a sample of observations, construct a likelihood function for these observations, and to obtain the parameter value that maximizes the likelihood. When we take a Bayesian approach, the parameter we wish to estimate is considered to be a random variable, and probabilities may represent a subjective belief about the state of uncertainty, or there may be a data generating distribution underlying the random parameter. For example, you may have a prior belief about the probability of a driver in London having a car accident, and after collecting a sample of data, you combine your prior beliefs with the likelihood for those observations to construct an updated belief - the posterior. Your belief may change in light of the data. Bayesian methods sometimes require numerical integration, and cheaper computing has made Bayesian approaches more feasible in the last 20 years. Bayesian approaches are likely to be an important part of working in Health Data Science; in the next two sessions, we introduce the fundamental principles.</p>
<div class="section" id="acknowledgements">
<h3>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this headline">¶</a></h3>
<p>These notes are heavily based on the Foundations course material created by Alex Lewin and Alexina Mason, which was previously developed by James Carpenter, Marcel Zwahlen and Beat Neuenschwander. Some sections are inspired also by notes from Michail Papathomas. We are grateful for their work and permission to re-use.</p>
</div>
</div>
<div class="section" id="id1">
<h2>Part 1:<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="probability">
<h2>9.3 Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h2>
<p>In Session 2, we learned about probability in the frequentist sense: the proportion of times an event occurs in the long-run. Let’s have a look at the following two scenarios:</p>
<ol class="simple">
<li><p>A research group wishes to know the probability that a baby who is born in a particular hospital ward has cystic fibrosis. They look at the records on screening tests done at birth to investigate.</p></li>
<li><p>A 34 year old woman attends her GP practice, worried that she has cancer because she has had feelings of “fullness” and “bloating” as well as mild nausea for the last 2 weeks. The patient mentions ovarian, bowel and pancreatic cancer as concerns having read about her symptoms on the internet. The rest of the history as well as physical examination are unremarkable. If the GP’s assessment of the risk were above a certain level, the GP might refer the patient for tests (collect more data). In this case, the GP concludes that the current information about the patient suggests there is a very low risk that the patient has cancer.</p></li>
</ol>
<blockquote>
<div><p>What is the quantity that we trying to estimate in each scenario?<br />
What is the frequentist definition of probability in each of these settings? Does it make sense?</p>
</div></blockquote>
<p>A key problem with the frequentist paradigm is that the “long-run” frequency definition is not always relevant, or even appropriate, as we see in the second example above. Further, notice that the GP uses information from different sources to draw his/her conclusion about the probability that the patient has cancer. This synthesis of information can be incorporated into a Bayesian framework. A frequentist, in contrast, would tackle this problem by thinking about:</p>
<blockquote>
<div><p>a) the probability of the patient having these symptoms, given that she has cancer;<br />
b) the probability of the patient having these symptoms, given that she does not have cancer;</p>
</div></blockquote>
<p>and comparing the two probabilities. Note that this does not take into account the extra information about the context.</p>
</div>
<div class="section" id="bayesian-inference">
<h2>9.4 Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">¶</a></h2>
<p>The underlying concept for Bayesian inference essentially works as follows. We have some population parameter <span class="math notranslate nohighlight">\(\theta\)</span> which we wish to make inference on, and the likelihood <span class="math notranslate nohighlight">\(p(y|\theta)\)</span> which tells us how likely different values of <span class="math notranslate nohighlight">\(y\)</span> are, conditional on different parameter values <span class="math notranslate nohighlight">\(\theta\)</span>. In the frequentist approach, <span class="math notranslate nohighlight">\(\theta\)</span> is considered to be a fixed, but unknown, constant. Inference is then based on the likelihood <span class="math notranslate nohighlight">\(p(\mathbf{y}|\theta)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{y} = \left\{y_1, . . . ,y_n\right\}\)</span> is a sample of observations from the population. The frequentist approach looks at the distribution of the data given <span class="math notranslate nohighlight">\(\theta\)</span> to estimate <span class="math notranslate nohighlight">\(\theta\)</span> by using, for example, the maximum likelihood approach which we covered in Session 6.</p>
<p>In the Bayesian paradigm, we no longer assume that the parameters have a fixed true value, but consider <span class="math notranslate nohighlight">\(\theta\)</span> to be a random quantity with an unknown distribution, which we wish to estimate. This distribution is denoted by <span class="math notranslate nohighlight">\(p(\theta|y)\)</span>, and so we look at the distribution of the parameter, having seen data <span class="math notranslate nohighlight">\(y\)</span>. To achieve this, we will have to specify a prior probability distribution, denoted <span class="math notranslate nohighlight">\(p(\theta)\)</span>, which represents our initial beliefs about the distribution of <span class="math notranslate nohighlight">\(\theta\)</span>
prior to observing any data. In some situations, when we are trying to estimate a parameter <span class="math notranslate nohighlight">\(\theta\)</span> we have some knowledge, about the possible value of <span class="math notranslate nohighlight">\(\theta\)</span> before we take into account the data that we observe.</p>
<p>For example, consider the way a physician makes diagnostic decisions. A patient presents with a set of symptoms, concerned that they might have a certain disease. The physician assesses the probability that this patient has this disease, based on symptoms, family history, alternative explanations of symptoms and prevalence of the disease (their prior view that the patient has the disease). The physician might send the patient for a diagnostic test (collects some data) if her prior assessment of risk is above some threshold. Then the physician re-assesses the chance that the patient has this disease, taking account of the results and reliability of the diagnostic test (updates their prior in light of the data to get a posterior view on whether the patient has the disease). Depending on their certainty, the physician may then send the patient for further diagnostic tests. This thought process can be represented by the figure below and is analogous to Bayesian thinking.</p>
<p><img alt="Physician.png" src="attachment:Physician.png" /></p>
<p>In this example, the physician is assessing the probability that the patient has the disease. It is the physician’s prior probability based on their own training, knowledge and experience; a colleague may have a different prior probability. Here, prior probability is being defined subjectively. The size of the probability represents the physician’s degree of belief about the occurrence of an event, i.e. their own personal assessment of how likely an event is, based on the evidence available to them before the test results are given. This definition corresponds more closely to the everyday, intuitive  usage of probability than a frequentist interpretation (where the probability of a particular event occurring can be interpreted as the proportion of times the event would/does occur in a large number of similar trials or situations). The prior probability of the event might come from direct data, known prevalance of disease in a population, or data from related populations. If such prior information does not exist, then it can be formally elicited from experts, but we would want to acknowledge the uncertainty in the experts’ knowledge.</p>
</div>
<div class="section" id="bayes-theorem">
<h2>9.5 Bayes Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h2>
<p>Let’s remind ourselves of Bayes theorem for discrete events, which we met in Session 2 (probability):</p>
<p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are events, then</p>
<p>\begin{equation}
P(A|B) = \frac{ P(B|A) P(A) } {P(B)} \propto P(B|A) P(A),
\end{equation}</p>
<p>or in words:
\begin{equation}
\mbox{posterior probability of A given B} \propto \mbox{the likelihood of B given A} \times \mbox{the prior probability of A}.
\end{equation}</p>
<p>Also, if <span class="math notranslate nohighlight">\(A_i\)</span> is a set of mutually exclusive and exhaustive events, i.e. <span class="math notranslate nohighlight">\( p( \bigcup\limits_i A_i ) = \sum\limits_i p(A_i) = 1\)</span> and <span class="math notranslate nohighlight">\(A_i \cap A_j = \emptyset\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span>, then</p>
<p>\begin{equation}
p(A_i|B) = \frac{ p(B|A_i) p(A_i) } {\sum\limits_j p(B|A_j) p(A_j) }.
\end{equation}</p>
<p>The calculation of the denominator is more difficult if we have continuous parameters as it requires integration over A; we will discuss this in the next section.</p>
<p>We will illustrate Bayes Theorem further with the diagnostic test example for Covid-19 below. We see Bayesian reasoning is purely probabilistic. Bayes theorem gives us a principled way to update prior probabilities on the basis of new data.</p>
</div>
<div class="section" id="example">
<h2>9.6 Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.bmj.com/content/bmj/369/bmj.m1808.full.pdf">Watson (2020)</a> discusses some interesting issues around the interpretation of Covid-19 diagnostic tests. Typically, a clinician estimates a pre-test probability (a prior probability) of having Covid-19 for a particular area, which is derived from knowledge about local rates of Covid-19. Then, given a patient’s test result, the post-test probability (the posterior probability) of having Covid-19 is obtained. The posterior probability depends on the pre-test probability, as well as the sensitivity and specificity of the test, which are difficult to estimate; often, sensitivity is over-estimated. The article discusses how one can be fairly confident about a positive test result, but more caution is needed for a negative test result, as there may still be quite a high chance that a person has Covid-19. We illustrate this with Bayes’ theorem.</p>
<p>Suppose that, in a student hall of residence, the prevalence of Covid-19 if you have a persistent cough is <span class="math notranslate nohighlight">\(75\%\)</span>. Suppose we assume that the test will be positive in Covid-19 patients <span class="math notranslate nohighlight">\(70\%\)</span> of the time (sensitivity is 0.7), and it will be negative in non-Covid-19 patients <span class="math notranslate nohighlight">\(95\%\)</span> of the time (specificity is 0.95). Given that a student in this hall with a persistent cough tests negative, what is the probability that they have Covid-19? In other words, what is the probability of a false negative?</p>
<p>Let us denote by <span class="math notranslate nohighlight">\(C+\)</span> the event that a person has Covid-19, and <span class="math notranslate nohighlight">\(C-\)</span> the event that a person does not have Covid-19.  Further we denote by <span class="math notranslate nohighlight">\(T+\)</span> and <span class="math notranslate nohighlight">\(T-\)</span> the events that a person has a positive and a negative test, respectively. The information we are given is that:</p>
<p>\begin{align}
p(C+)&amp;=0.75  \
p(T+|C+)&amp;=0.70 \
p(T-|C-)&amp;=0.95\
\end{align}</p>
<p>Now, what we want is:
\begin{align}
p(\mbox{false negative}) = p(C+|T-)&amp;= \frac{p(T-|C+)p(C+)}{p(T-)} \
&amp;= \frac{p(T-|C+)p(C+)}{p(T-|C+)p(C+) + p(T-|C-)p(C-)} \
&amp;= \frac{(1-0.7) \times 0.75}{(1-0.7) \times 0.75 + 0.95 \times 0.25} \
&amp;= \frac{0.225}{0.4625} \
&amp;= 0.4864
\end{align}</p>
<p>You can see that, despite the negative test result, due to the very high prevalence of Covid-19 in the hall of residence and the relatively low sensitivity rate, there is still a 48.64% chance that a person has Covid-19.</p>
<p>Suppose a different student has no symptoms. The prevalence of Covid-19 in asymptomatic people is 0.1. They use the same diagnostic test and the test result is positive. What is the probability that this student with a positive test result has Covid-19? In other words, what is <span class="math notranslate nohighlight">\(p(C+|T+)\)</span>?</p>
<p>Solution:
\begin{align}
p(C+|T+) &amp;= \frac{p(T+|C+)p(C+)}{p(T+)} \
&amp;= \frac{p(T+|C+)p(C+)}{p(T+|C+)p(H+) + p(T+|C-)p(C-)} \
&amp;= \frac{0.7 \times 0.1}{0.7 \times 0.1 + (1-0.95) \times 0.9} \
&amp;= \frac{0.07}{0.115} \
&amp;= 0.609
\end{align}</p>
<p>This means that, amongst all the people who test positive, <span class="math notranslate nohighlight">\(60.9\%\)</span> will actually have the disease. After a positive result from a test, the probability that you have Covid-19 increase from <span class="math notranslate nohighlight">\(10\%\)</span> to <span class="math notranslate nohighlight">\(61\%\)</span>.</p>
<p>Note that these results are specific to the the prevalence of Covid-19 in the area, as well as the sensitivity and specificity of the diagnostic test. The code below reproduces the leaf-plot from <a class="reference external" href="https://www.bmj.com/content/bmj/369/bmj.m1808.full.pdf">Watson (2020)</a>. The <span class="math notranslate nohighlight">\(x\)</span>-axis is the pre-test probability of having Covid-19. The corresponding <span class="math notranslate nohighlight">\(y\)</span>-values on the lower curve (lower leaf) are the post-test probabilities of having Covid-19, following a negative test result. The corresponding <span class="math notranslate nohighlight">\(y\)</span>-values on the upper curve (upper leaf) are the post-test probabilities of having Covid-19, following a positive test result. The correponding values on the diagonal (<span class="math notranslate nohighlight">\(y=x\)</span>) line represent probabilities if no test is carried out.</p>
<p>In our first example, the prevalence in symptomatic people is 0.75, so we follow the orange arrows to find that the post-test probability after a negative result 0.4864. In the second example, the prevalence in asymptomatic people is 0.1. We follow the purple arrows to find that the post-test probability after a positive result is 0.609. How do you think the shape of the lower and upper leaves would change, if sensitivity was higher? If specificity was lower? Re-run the code with different values to check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#function takes as arguments the sensitivitiy of the test (sensi) </span>
<span class="c1">#and the specificity (speci)</span>

<span class="n">leafplot</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">sensi</span><span class="p">,</span> <span class="n">speci</span><span class="p">){</span>
  
  <span class="n">pretest</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span> <span class="c1">#possible pre-test probabilities </span>
  
  <span class="c1">#probability of having Covid-19 after a positive test result </span>
  <span class="n">pos.test</span> <span class="o">&lt;-</span> <span class="n">sensi</span><span class="o">*</span><span class="n">pretest</span><span class="o">/</span><span class="p">(</span><span class="n">sensi</span><span class="o">*</span><span class="n">pretest</span><span class="o">+</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">speci</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">pretest</span><span class="p">))</span>
  
  <span class="c1">#probability of having Covid-19 after a negative test result </span>
  <span class="n">neg.test</span> <span class="o">&lt;-</span> <span class="p">((</span><span class="m">1</span><span class="o">-</span><span class="n">sensi</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">pretest</span><span class="p">))</span><span class="o">/</span><span class="p">((</span><span class="m">1</span><span class="o">-</span><span class="n">sensi</span><span class="p">)</span><span class="o">*</span><span class="n">pretest</span><span class="o">+</span><span class="n">speci</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">pretest</span><span class="p">))</span>
  
  <span class="c1">#plot leaves</span>
  <span class="nf">plot</span><span class="p">(</span><span class="n">pretest</span><span class="p">,</span> <span class="n">pos.test</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;darkgreen&quot;</span><span class="p">,</span> 
     <span class="n">xlab</span><span class="o">=</span><span class="s">&quot;Pre-test Probability&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;Post-test Probability&quot;</span><span class="p">)</span>
  <span class="nf">points</span><span class="p">(</span><span class="n">pretest</span><span class="p">,</span> <span class="n">neg.test</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;darkgreen&quot;</span><span class="p">)</span>
  <span class="nf">abline</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="m">0</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;darkgreen&quot;</span><span class="p">)</span>
  <span class="nf">legend</span><span class="p">(</span><span class="s">&quot;topleft&quot;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Positive Test&quot;</span><span class="p">,</span> <span class="s">&quot;Negative Test&quot;</span><span class="p">),</span>
        <span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Purple&quot;</span><span class="p">,</span> <span class="s">&quot;Orange&quot;</span><span class="p">),</span> <span class="n">lty</span><span class="o">=</span><span class="m">1</span><span class="p">,</span> <span class="n">bg</span><span class="o">=</span><span class="s">&quot;transparent&quot;</span><span class="p">)</span>
  
  <span class="c1">#plot arrows </span>
    <span class="c1">#we use pretest[11] to get the prevalence value of 0.1, and </span>
    <span class="c1">#pretest[76] to get the prevalence value of 0.75 in the vector &quot;pretest&quot;</span>
    
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">pretest</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">pos.test</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">)</span>
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">pos.test</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">pos.test</span><span class="p">[</span><span class="m">11</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;purple&quot;</span><span class="p">)</span>
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">pretest</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">neg.test</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
  <span class="nf">arrows</span><span class="p">(</span><span class="n">pretest</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">neg.test</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="m">0</span><span class="p">,</span> <span class="n">neg.test</span><span class="p">[</span><span class="m">76</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="m">15</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;orange&quot;</span><span class="p">)</span>
  
  <span class="p">}</span>

<span class="nf">leafplot</span><span class="p">(</span><span class="n">sensi</span><span class="o">=</span><span class="m">0.7</span><span class="p">,</span> <span class="n">speci</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>


<span class="c1">#See what happens to the plot when you change sensitivity and specificity! </span>
<span class="c1">#leafplot(0.95, 0.8)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09. Bayesian Statistics I_4_0.png" src="_images/09. Bayesian Statistics I_4_0.png" />
</div>
</div>
</div>
<div class="section" id="the-bayesian-paradigm-in-health-data-science-problems">
<h2>9.7 The Bayesian paradigm in Health data science problems.<a class="headerlink" href="#the-bayesian-paradigm-in-health-data-science-problems" title="Permalink to this headline">¶</a></h2>
<p>We end this section with a discussion on the Bayesian approach in Health data science problems. Some features of the Bayesian paradigm are particularly useful in this context:</p>
<blockquote>
<div><ol class="simple">
<li><p>Bayes theorem provides a statistically principled method for combining data. Thus, we can take into account the context within which the data are generated. For example, results of a diagnostic test may have a different interpretation/consequence if used in a symptomatic patient than in a general screening programme. The prior probability of disease would be higher in the former than the latter. Priors can then be updated by the test result to give an assessment of disease risk specific to the local prevalence.</p></li>
<li><p>For problems where there are multiple or diverse sources of data which must be combined, the Bayesian framework provides a natural environment for doing so. Examples where Bayesian synthesis of information is common are:<br />
•	models of biological systems, for example genetic and genomic pathways,<br />
•	models of the natural history of diseases over time and relationships with clinical events,<br />
•	economic models of disease trajectories and cost-effect trade-offs for interventions that interrupt the trajectories,<br />
•	ecological studies of pollutant emissions and effects on population health,<br />
•	demographic studies, for example to study migration,<br />
•	speech recognition software,<br />
•	other pattern recognition models such as medical imaging or search engines,<br />
•	epidemic modelling.<br />
In all these examples complex data is synthesised and/or used to update outputs.</p></li>
<li><p>Bayesian models fit well into decision theory methodology, providing we can also specify consequences of model outputs.</p></li>
<li><p>In many examples, especially those that aim to model complicated processes, some of the data inputs are very sparse, or even non-existent. In such cases, prior data may be formally elicited from an expert panel and incorporated in a Bayesian analysis. Examples include multiple evidence synthesis and identification of latent groups.</p></li>
<li><p>Bayesians are allowed to make direct probability statements about unknown quantities. Frequentists cannot make these direct probability statements because the unknown model parameters are assumed fixed.</p></li>
<li><p>In recent years the resources available to complete Bayesian analysis have increased, including bespoke software and packages within commercial statistical software.</p></li>
</ol>
</div></blockquote>
<p>But Bayesian methods are not that widely used in statistics compared with more classical approaches because they have some limitations.</p>
<blockquote>
<div><ol class="simple">
<li><p>Sometimes the need for a prior distribution is a barrier if little is known about a parameter and researchers fall back on priors that are weakly informative. In that case, it is not easy to see how much benefit comes from a Bayesian analysis.</p></li>
<li><p>Because of the need to use Bayesian updating via a prior distribution, the analysis almost always requires a parametric approach. This limits the structure of the analysis models. Although non-parametric Bayesian methods are available for some situations, they often have underlying parametric assumptions.</p></li>
<li><p>The numerical integration methods usually required for realistic problems are often computationally expensive. This is especially true if there are multiple sources of evidence to be combined.</p></li>
<li><p>Many statisticians are unfamiliar with the methods and associated software.</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="id2">
<h2>Part 2:<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="bayes-thorem-for-discrete-and-continous-data">
<h2>9.8 Bayes thorem for discrete and continous data.<a class="headerlink" href="#bayes-thorem-for-discrete-and-continous-data" title="Permalink to this headline">¶</a></h2>
<p>In part 1, we looked at Bayes theorem in the discrete case. We turn to the more general case of Bayes thorem to make inference about an unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span>, which could be discrete or continuous. The probability distribution for <span class="math notranslate nohighlight">\(\theta\)</span> reflects our uncertainty about it before seeing the data, <em>prior distribution</em>, <span class="math notranslate nohighlight">\(p(\theta)\)</span>. Once the data data <span class="math notranslate nohighlight">\(y\)</span> is known, we condition on it. Using Bayes theorem we obtain a conditional probability distribution for unobserved quantities of interest given the data. If <span class="math notranslate nohighlight">\(\theta\)</span> is continuous, we have:</p>
<p>\begin{equation}
p(\theta \mid y)= \frac{ p(\theta), p(y \mid \theta)}{\int  p(\theta),p(y \mid \theta),d\theta},
\end{equation}</p>
<p>and <span class="math notranslate nohighlight">\(\theta\)</span> is discrete and takes values in the set <span class="math notranslate nohighlight">\(\Theta\)</span>, we have:</p>
<p>\begin{equation}
p(\theta \mid y)= \frac{ p(\theta), p(y \mid \theta)}{\sum_{\theta \in \Theta}  p(\theta) p(y \mid \theta) }.
\end{equation}</p>
<p>We call <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> the <em>posterior distribution</em>.</p>
<p>Note that the Bayesian approach is naturally synthetic in that it allows data from different sources to be combined, according to Bayes principles. This approach is most useful when there is informative prior information. We note that the Bayesian approach can be recursive, so <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> may be used as a prior when calculating <span class="math notranslate nohighlight">\(p(\theta \mid y, z)\)</span> for a second data set <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>The denominator, <span class="math notranslate nohighlight">\({\int  p(\theta)\,p(y \mid \theta)\,d\theta}\)</span> or <span class="math notranslate nohighlight">\(\sum_{\theta \in \Theta}  p(\theta) p(y \mid \theta)\)</span>, is a constant with respect to <span class="math notranslate nohighlight">\(\theta\)</span>. One of the challenges of using Bayesian approaches is that the integration can be analytically intractable, so that numerical methods are needed (for example, numerical integration or Markov Chain Monte Carlo methods). You can learn more about this in the Bayesian module in the second semester, if you choose to take it. In this introductory course, we will only look at examples where this constant need not be calculated, since the form of the posterior can be inferred by inspection once observing that the posterior is proportional to the product of the prior and likelihood:</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid y) \propto p(\theta)\,p(y \mid \theta).\]</div>
<p>We will see how this works for the inference of proportions.</p>
</div>
<div class="section" id="bayesian-inference-on-proportions">
<h2>9.9 Bayesian inference on proportions<a class="headerlink" href="#bayesian-inference-on-proportions" title="Permalink to this headline">¶</a></h2>
<p>Consider a new drug being developed for the relief of chronic pain. To find out about its efficacy, we propose to run a single-arm early-phase clinical trial in which we give this drug to a number <span class="math notranslate nohighlight">\(n\)</span> of randomly selected patients. Because patients are independent of each other, so it seems reasonable to model the data using the Binomial distribution, <span class="math notranslate nohighlight">\(Y\sim Bin(n,\theta).\)</span> We have that <span class="math notranslate nohighlight">\(\theta\in [0,1]\)</span> is the probability of pain relief (success) in each patient, and this is unknown. We then make the observation that there are <span class="math notranslate nohighlight">\(y\)</span> successes out of <span class="math notranslate nohighlight">\(n\)</span> independent trials. As a reminder, the probability distribution function of the Binomial distribution is:</p>
<p>\begin{equation}
p \left(y \mid \theta \right) = {n \choose y} \theta^y (1-\theta)^{n-y}.
\end{equation}</p>
<p>To proceed, we need to have a prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>. Let us consider three possible prior distributions:</p>
<ol class="simple">
<li><p>An uninformative prior, where all values of <span class="math notranslate nohighlight">\(\theta\)</span> are equally probable.<br />
You essentially have no prior information about the effectiveness of the drug.</p></li>
<li><p>A symmetrical, concave prior that is centered at 0.5.<br />
You think that the drug is likely to be effective for patients around half of the time.</p></li>
<li><p>An asymmetrical prior with a spike at 0.1.<br />
You think that the drug is generally ineffective, and feel quite strongly about it.</p></li>
</ol>
<p>Now, the Beta distribution is a flexible distribution that can represent each of these prior beliefs by appropriate choice of its parameters. It is also convenient because it has a similar form to the Binomial distribution.</p>
<div class="section" id="the-beta-prior">
<h3>9.9.1 The Beta prior<a class="headerlink" href="#the-beta-prior" title="Permalink to this headline">¶</a></h3>
<p>The Beta distribution is a flexible two parameter distribution that is restricted to the interval between 0 and 1, and so it is a reasonable form for a probability distribution for a proportion. The two parameters, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, are often called  “shape” parameters. Given <span class="math notranslate nohighlight">\(\theta \sim \hbox{Beta}(a,b)\)</span>, the probability density function, expectation and variance of the distribution are as follows:</p>
<p>\begin{eqnarray}
p(\theta|a, b) &amp; = &amp; \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } ,,
\theta^{a-1} ; (1-\theta)^{b-1}  \mbox{  where  } \theta \in (0,1) \label{eqn.beta1}\
{E}( \theta |a, b) &amp;= &amp;   \frac{a}{a+ b }     \label{eqn.beta2}\
{Var}( \theta |a, b) &amp;= &amp;   \frac{a b}{ (a+ b)^2 (a+ b+1)} \label{eqn.beta3}
\end{eqnarray}</p>
<p>The <em>Gamma function</em> <span class="math notranslate nohighlight">\(\Gamma(x)\)</span> is defined for positive integers as <span class="math notranslate nohighlight">\(\Gamma (x)=(x-1)!\)</span>, and has a more complex form for real numbers.</p>
<p>This prior distribution is very flexible. For example:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(a=1, b=1\)</span> results in the uniform distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(a=2, b=2\)</span> results in a symmetrical distribution centered on <span class="math notranslate nohighlight">\(p=0.5\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a=2, b=9\)</span> results in an asymmetrical distribution with a spike at <span class="math notranslate nohighlight">\(p=0.1\)</span>.</p></li>
</ol>
<p>These are the priors we specified earlier; they are plotted below. Note that the higher the values of <span class="math notranslate nohighlight">\(a, b,\)</span> the
smaller the variance of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">7</span><span class="p">,</span> <span class="n">repr.plot.height</span><span class="o">=</span><span class="m">3</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(1,1) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">2</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(2,2) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">9</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(2,9) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09. Bayesian Statistics I_8_0.png" src="_images/09. Bayesian Statistics I_8_0.png" />
</div>
</div>
</div>
<div class="section" id="posterior">
<h3>9.9.2 Posterior<a class="headerlink" href="#posterior" title="Permalink to this headline">¶</a></h3>
<p>Now, we apply Bayes theorem to obtain the posterior distribution using a <span class="math notranslate nohighlight">\(Beta(a,b)\)</span> distribution for the prior:</p>
<p>\begin{align}
\begin{split}
p(\theta \mid y) &amp;= \frac{ p(\theta), p(y \mid \theta)} {\int  p(\theta),p(y \mid \theta),d\theta}\
&amp; \propto  p(\theta), p(y \mid \theta)\
&amp; =\frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1}  {n \choose y} \theta^y (1-\theta)^{n-y} \
&amp; \propto \theta^{a-1} (1-\theta)^{b-1} \theta^y (1-\theta)^{n-y} \
&amp; \propto \theta^{a+y-1} (1-\theta)^{b+n-y-1}
\end{split}
\end{align}</p>
<p>Now by inspection, we can see that this is in the form of a Beta distribution: we have that the posterior is proportional to <span class="math notranslate nohighlight">\(\theta^{a+y-1} (1-\theta)^{b+n-y-1}\)</span>. In other words, the posterior is <span class="math notranslate nohighlight">\(Beta(a+y, b+n-y).\)</span> This distribution has mean given by: <span class="math notranslate nohighlight">\(\frac{a+y}{a+b+n}\)</span> and variance <span class="math notranslate nohighlight">\(\frac{(a+y)(b+n-y)}{(a+b+n)^2(a+b+n+1)}.\)</span></p>
<p>Suppose the data we observe is <span class="math notranslate nohighlight">\(y=4\)</span> successes out of a total of <span class="math notranslate nohighlight">\(10\)</span> patients. Then:</p>
<ol class="simple">
<li><p>With the uniform <span class="math notranslate nohighlight">\(Beta(1,1)\)</span> prior, our posterior is <span class="math notranslate nohighlight">\(Beta(5, 7)\)</span>.</p></li>
<li><p>With the symmetrical <span class="math notranslate nohighlight">\(Beta(2, 2)\)</span> prior, our posterior is <span class="math notranslate nohighlight">\(Beta(6, 8)\)</span>.</p></li>
<li><p>With the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, our posterior is <span class="math notranslate nohighlight">\(Beta(6, 15)\)</span>.</p></li>
</ol>
<p>We plot the possible distibutions below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.01</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">7</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span>  <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(5, 7) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">8</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(6, 8) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(6, 15) Distribution&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">),</span> <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09. Bayesian Statistics I_10_0.png" src="_images/09. Bayesian Statistics I_10_0.png" />
</div>
</div>
<p>We can see that the uninformative prior leads to the posterior with the highest variance amongst the three. The narrow prior in the third example shifts the posterior distribution to the right. We can see that different choices of prior lead to different results. For this reason, it is often recommended to repeat analyses with different priors to see how much the results change: this is called <em>sensitivity analysis</em>.</p>
</div>
</div>
<div class="section" id="summarising-posteriors">
<h2>9.10 Summarising Posteriors<a class="headerlink" href="#summarising-posteriors" title="Permalink to this headline">¶</a></h2>
<p>We often display the posterior distribution graphically to get a sense of the information that we have about the parameter. However, other ways to summarize the distribution can be helpful.</p>
<p>We may also wish to summarise the posterior distribution by a credible interval. Remember that a classical <span class="math notranslate nohighlight">\(100(1−\alpha)\%\)</span> confidence interval is defined such that, if the data collection process is repeated again and again, then in the long run, <span class="math notranslate nohighlight">\(100(1 − \alpha)\%\)</span>of the confidence intervals formed would contain the true parameter value. Now, a Bayesian <span class="math notranslate nohighlight">\(100(1 − \alpha)\%\)</span> credible interval is an interval which contains <span class="math notranslate nohighlight">\(100(1 − \alpha)\% \)</span> of the posterior distribution of the parameter. There may be several different credible intervals such that the interval contains <span class="math notranslate nohighlight">\(100(1 − \alpha)\%\)</span> of the distribution. The <span class="math notranslate nohighlight">\(100(1 -\alpha) \%\)</span> Highest Posterior Density (HPD) interval (<span class="math notranslate nohighlight">\(\alpha \in (0,1)\)</span>) is the credible interval with the smallest range of values for <span class="math notranslate nohighlight">\(\theta\)</span> (providing the posterior is concave). Algebraically, this is the region <span class="math notranslate nohighlight">\([\theta_L,\theta_U]\)</span> that contains <span class="math notranslate nohighlight">\(100( 1 - \alpha)\%\)</span>
of the probability, such that:</p>
<p><span class="math notranslate nohighlight">\(P(\theta \in [
\theta_L,\theta_U])=1-\alpha\)</span> such that for all <span class="math notranslate nohighlight">\(\theta_O \notin  [
\theta_L,\theta_U]\)</span> and all  <span class="math notranslate nohighlight">\(\theta_I\in[
\theta_L,\theta_U],\)</span> <span class="math notranslate nohighlight">\(p(\theta_O|y) &lt;
p(\theta_I|y).\)</span></p>
<p>In our previous example, when we used the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, our posterior was <span class="math notranslate nohighlight">\(Beta(6, 15)\)</span>. The posterior mean is <span class="math notranslate nohighlight">\(\frac{6}{6+15}=0.286\)</span>. The <span class="math notranslate nohighlight">\(90\%\)</span> HPDI is <span class="math notranslate nohighlight">\((0.13, 0.44)\)</span>. We plot the distribution below and check that the area between these two values gives us 0.9. Now, note that the interval (0.144, 0.47) also gives us an area of 0.9, but this interval is wider. In a sense, the HPDI is the “tightest” interval so that the area under the posterior distribution is <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="nf">dbeta</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">),</span> <span class="n">type</span><span class="o">=</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="n">main</span><span class="o">=</span><span class="s">&quot;Beta(6, 15) Distribution with 90% credible interval&quot;</span><span class="p">,</span>  <span class="n">xlab</span><span class="o">=</span><span class="nf">expression</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">ylab</span><span class="o">=</span><span class="s">&quot;density&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.44</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="m">0.13</span><span class="p">,</span>  <span class="n">lty</span><span class="o">=</span><span class="s">&quot;dashed&quot;</span><span class="p">)</span>

<span class="c1">#Area under the 90% HDPI</span>
<span class="nf">pbeta</span><span class="p">(</span><span class="m">0.44</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span><span class="o">-</span><span class="nf">pbeta</span><span class="p">(</span><span class="m">0.13</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span>

<span class="c1">#The interval (0.47. 0.144) also a 90% credible interval </span>
<span class="nf">pbeta</span><span class="p">(</span><span class="m">0.47</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span><span class="o">-</span><span class="nf">pbeta</span><span class="p">(</span><span class="m">0.144</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.897009716676625</div><div class="output text_html">0.90491783788117</div><img alt="_images/09. Bayesian Statistics I_12_2.png" src="_images/09. Bayesian Statistics I_12_2.png" />
</div>
</div>
</div>
<div class="section" id="prior-predictions">
<h2>9.11 Prior Predictions<a class="headerlink" href="#prior-predictions" title="Permalink to this headline">¶</a></h2>
<p>Before observing a quantity <span class="math notranslate nohighlight">\(y\)</span>, we can provide its predictive distribution by integrating out the unknown parameter,</p>
<div class="math notranslate nohighlight">
\[p(y) = \int p(y|\theta) p(\theta) d\theta.\]</div>
<p>Predictions are useful in many settings, for example forecasting, cost-effectiveness models and design of
studies. In the trial described earlier in this section, we had 10 patients. Suppose we are interested in predicting the number of patients who will have a positive response. Recall that the Beta distribution is a suitable prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>, the proportion of positive responses. We have:</p>
<p>\begin{align}
\theta &amp;\sim \hbox{Beta}(a,b) \
y &amp;\sim \hbox{Binomial}(\theta,n)
\end{align}</p>
<p>The exact predictive distribution <span class="math notranslate nohighlight">\(p(y)\)</span> can be computed analytically and is known as the <em>Beta-Binomial</em> distribution. It has the complex form with three parameters,  number of trials <span class="math notranslate nohighlight">\(n\)</span> and shape parameters, <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<p>\begin{align}
p(y) &amp;=  \frac{ \Gamma (a+ b)}{ \Gamma (a) \Gamma (b) }  {n \choose y}  \frac{\Gamma (a+ y) \Gamma (b+n-y)}{\Gamma (a+b+n)} \
\mathbb{E}(y) &amp;=  n \frac{a}{a+b}
\end{align}</p>
<p>Given that we use the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, our predictive distribution would be:</p>
<p>\begin{equation}
p(y) =  \frac{ \Gamma (11)}{ \Gamma (2) \Gamma (9) }  {10 \choose y}  \frac{\Gamma (2+ y) \Gamma (19-y)}{\Gamma (21)},
\end{equation}</p>
<p>with <span class="math notranslate nohighlight">\(\mathbb{E}(y) =  10 \frac{2}{11} = 1.81\)</span>. So, before observing any data, we would predict around 2 patients to have a positive response out of 10.</p>
<div class="section" id="posterior-prediction">
<h3>9.11.1 Posterior Prediction<a class="headerlink" href="#posterior-prediction" title="Permalink to this headline">¶</a></h3>
<p>Suppose that have observed <span class="math notranslate nohighlight">\(y\)</span>, and we want to predict future observations <span class="math notranslate nohighlight">\(z\)</span>, assuming that <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are independent, conditional on <span class="math notranslate nohighlight">\(\theta\)</span>. The posterior predictive distribution for <span class="math notranslate nohighlight">\(z\)</span> is given by,</p>
<p>\begin{align}
p(z|y) &amp;= \int p(z, \theta | y) d \theta \
&amp;= \int p(z |y, \theta) p(\theta |y ) d \theta \
&amp; = \int p(z | \theta) p(\theta |y ) d \theta
\end{align}</p>
<p>We are now weighting the probability distribution function for <span class="math notranslate nohighlight">\(z\)</span> with our posterior belief after having observed <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>For our example, we found that the posterior distribution <span class="math notranslate nohighlight">\(p(\theta |y ) \)</span> is a Beta(<span class="math notranslate nohighlight">\(a+y, b+n-y\)</span>) distribution. Thus our posterior predictive distribution is a Beta-binomial distribution with the number of trials <span class="math notranslate nohighlight">\(n_p\)</span> and shape parameters <span class="math notranslate nohighlight">\(a+y, b+n-y\)</span>.</p>
<p>Now, given that we use the asymmetrical <span class="math notranslate nohighlight">\(Beta(2, 9)\)</span> prior, and then observe that <span class="math notranslate nohighlight">\(y=4\)</span> patients out of <span class="math notranslate nohighlight">\(n=10\)</span> had a successful result, and we wish to predict how many sucesses <span class="math notranslate nohighlight">\(z\)</span> out of <span class="math notranslate nohighlight">\(n_p=20\)</span> to expect, our posterior predictive distribution is a Beta-binomial with parameters <span class="math notranslate nohighlight">\(20\)</span> and shape parameters <span class="math notranslate nohighlight">\(6\)</span> and <span class="math notranslate nohighlight">\(15\)</span>. The expectation of this distribution is <span class="math notranslate nohighlight">\(\mathbb{E}(y) =  20 \frac{6}{21} \approx 6\)</span> patients.</p>
</div>
</div>
<div class="section" id="conjugacy">
<h2>9.12 Conjugacy<a class="headerlink" href="#conjugacy" title="Permalink to this headline">¶</a></h2>
<p>In the example with the Beta-Binomial model, we found that using the Beta distribution for the prior lead us to a posterior distribution that is also a Beta distribution. This is not a coincidence. Often, a particular distributional family is chosen for the prior, so that the resulting posterior distribution belongs to the same family. This is called a conjugate prior. Below are the conjugate priors for some common likelihood models.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Likelihood</p></th>
<th class="text-align:left head"><p>Conjugate Prior</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Bernoulli</p></td>
<td class="text-align:left"><p>Beta</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Binomial</p></td>
<td class="text-align:left"><p>Beta</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Poisson</p></td>
<td class="text-align:left"><p>Gamma</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Geometric</p></td>
<td class="text-align:left"><p>Beta</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Normal</p></td>
<td class="text-align:left"><p>Normal, Gamma and a few others</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Exponential</p></td>
<td class="text-align:left"><p>Gamma</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Gamma</p></td>
<td class="text-align:left"><p>Gamma</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="exercise">
<h2>9.13 Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h2>
<p>Suppose that there is an experiment where <span class="math notranslate nohighlight">\(n\)</span> patients are asked to try different treatments each time they get a headache. We are interested in the number of different treatments a patient takes before they find one that is successful. For patient <span class="math notranslate nohighlight">\(i\)</span>, for <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span>, we denote by <span class="math notranslate nohighlight">\(y_i\)</span> the number of treatments tried before the first success. Note that <span class="math notranslate nohighlight">\(\left\{ y_1, y_2, ..., y_n \right\}\)</span> are a sample from a Geometric distribution: <span class="math notranslate nohighlight">\(y_i \sim Geom(\theta)\)</span>. The probability density function of a geometric distribution is:</p>
<div class="math notranslate nohighlight">
\[p(y | \theta) = \theta (\theta -1)^{y-1}\]</div>
<p>Suppose we wish to make inference on <span class="math notranslate nohighlight">\(\theta\)</span>. By specifying a Beta prior for <span class="math notranslate nohighlight">\(\theta\)</span>: <span class="math notranslate nohighlight">\(\theta \sim Beta(a, b)\)</span>, derive the posterior distribution of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Solution:</p>
<p>\begin{align}
\begin{split}
p(\theta \mid y_1, …, y_n)
&amp; \propto  p(\theta) \prod_{i=1}^n p(y_i \mid \theta)\
&amp; \propto  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1} \prod_{i=1}^n \theta (\theta -1)^{y-1}\
&amp; \propto  \frac{\Gamma(a+ b)}{ \Gamma(a)\Gamma(b) } \theta^{a-1} (1-\theta)^{b-1}  \theta^n (\theta -1)^{\sum_{i=1}^n y_i-n}\
&amp; \propto  \theta^{a+n-1} (\theta -1)^{\sum_{i=1}^n y_i -n +b-1}
\end{split}
\end{align}</p>
<p>This is a Beta distribution with parameters <span class="math notranslate nohighlight">\(a+n\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^n y_i-n+b\)</span>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="08.g.%20Frequentist%20II.html" title="previous page">Further resources</a>
    <a class='right-next' id="next-link" href="10.%20Bayesian%20Statistics%20II.html" title="next page">10. Bayesian Statistics II: Normal data</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By MSc Health Data Science, LSHTM<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>