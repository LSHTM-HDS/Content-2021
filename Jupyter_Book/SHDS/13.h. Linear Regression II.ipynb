{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.7 Optional Reading: Analysis of Variance \n",
    "\n",
    "When fitting statistical models, we may wish to compare how well two models fit the data, to see which is most appropriate. Considering the models we have used in this chapter, we may want to compare (for example):\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Comparison 1: }& \\text{Model 1 (birthweight~length of pregnancy) v Model 3 (birthweight~length of pregnancy+mothers height)} \\\\\n",
    "\\text{Comparison 2: }& \\text{Model 2 (birthweight~mother's smoking status) v Model 3 (birthweight~length of pregnancy+mothers height)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "In these examples, Comparison 1 is much simpler than Comparison 2, because the models in Comparison 1 are **nested**. \n",
    "\n",
    "Statistical models are said to be **nested** when one model (the simpler model) contains a subset of the covariates in the other one (the complex model) and no other additional variables. In Comparison 2, the models are not nested because the simpler model (Model 2) contains mother's smoking status as a variable, which is not included in Model 3. Nested models can be compared using **Analysis of Variance (ANOVA)** (the comparison of non-nested models is much more complicated and is beyond the scope of this module). \n",
    "\n",
    "The main idea of ANOVA is that: if the complex model better describes the data than the simpler model, then we would expect a reasonably large amount of the residual variation that is unexplained by the simpler model to be explained by the complex one. ANOVA provides a statistical framework that can formally test this. \n",
    "\n",
    "We will first consider ANOVA in the context of simple linear regression, where the simpler model assumes no association between the outcome and the independent variable (the **null** model). We will then consider ANOVA in the context of multivariable linear regression and we end by learning how ANOVA can be used to test for differences between groups in a categorical variable. \n",
    "\n",
    "## 13.7 ANOVA for simple linear regression \n",
    "\n",
    "The total variation in $Y$ (otherwise known as the **sum of squares of the $Y$'s**), is denoted by $SS_{TOT}$ and is equal to:\n",
    "\n",
    "$$ SS_{TOT} = \\sum_{i=1}^n (y_i-\\bar{y})^2$$\n",
    "\n",
    "This can be partitioned into two components: the predictable variation in $Y$ ($SS_{REG}$) and the unpredictable variation in $Y$ ($SS_{RES}$). A key result for ANOVA is:\n",
    "\n",
    "\\begin{align}\n",
    "SS_{TOT} &= SS_{REG}+SS_{RES} \\\\\n",
    "\\rightarrow \\sum_{i=1}^n (Y_i-\\bar{Y})^2 &= \\sum_{i=1}^n (\\hat{Y_i}-\\bar{Y})^2 + \\sum_{i=1}^n(Y_i-\\hat{Y_i})^2\n",
    "\\end{align}\n",
    "\n",
    "In the above equations, $SS_{TOT}$ (i.e the TOTal sum of squares), represents all of the variation in $Y$ about its mean value. $SS_{REG}$ (i.e. the REGression sum of squares) represents the variation of the predicted values $\\hat{Y}$ about the mean. $SS_{RES}$ (i.e. the RESidual sum of squares) represents the variation of the observed values about their predicted values. \n",
    "\n",
    "Using the sums of squares defined above, we can calculate the proportion of variance explained by statistical model, known as the **coefficient of determination**. \n",
    "\n",
    "### 13.7.1 The coefficient of determination\n",
    "\n",
    "The proportion of variation which is explained by a statistical model is denoted by $R^2$ and is given by: \n",
    "\n",
    "$$R^2 = \\frac{SS_{REG}}{SS_{TOT}}$$. \n",
    "\n",
    "*Example.* The coefficient of determination is given in the ```summary()``` output for a linear regression in R. In Model 1, $R^2=0.1661$ (see output below). This means that Model 1 explains 16.6\\% of the total variation in $Y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Birth.Weight ~ Gestational.Days, data = data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-49.348 -11.065   0.218  10.101  57.704 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      -10.75414    8.53693   -1.26    0.208    \n",
       "Gestational.Days   0.46656    0.03054   15.28   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 16.74 on 1172 degrees of freedom\n",
       "Multiple R-squared:  0.1661,\tAdjusted R-squared:  0.1654 \n",
       "F-statistic: 233.4 on 1 and 1172 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The coefficient of determination\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While $R^2$ is sometimes used an overall measure of goodness-of-fit (or predictive performance), it isn't used to formally compare models. This is because $R^2$ will never decrease when new covariates are added to a model (provided that the number and identity of observations remains the same). Therefore, using $R^2$ for model comparisons, we would always conclude that the more complex model is at least as good a fit as the simpler model, even if this is not true. We can calculate an adjusted $R^2$ (the adjusted $R^2$ for Model 1 is given in the output above) that accounts for this issue. Alternatively, we can use analysis of variance (ANOVA). To describe ANOVA, we need to first define the remaining statistics that are commonly summarised in what is known as the ANOVA table. \n",
    "\n",
    "### 13.7.2 The ANOVA table. \n",
    "\n",
    "Each of the sum of squares defined above ($SS_{TOT}$, $SS_{REG}$ and $SS_{RES}$) have an associated **degrees of freedom (d.f.)**. The d.f. for the total sum of squares is $(n-1)$, since the variance of $Y$ is $\\sum_{i=1}^n (Y_i-\\bar{Y})^2/(n-1)$. The d.f. for the regression sum of squares in the number of covariates in the regression model (when a simple linear regression model is used this is equal to 1). The residual d.f. is found by subtracting the regression d.f. from the total d.f. The sums of squares also have associated mean squares, which are obtained by dividing the sum of squares by its associated degrees of freedom (note that the residual mean square is then equal to $\\hat{\\sigma}^2$). These statistics are summarised in an ANOVA table for simple linear regression (Table 2). \n",
    "\n",
    "\n",
    "Source      | d.f.  | SS         | Mean Square                        | \n",
    "------------|-------|------------|------------------------------------|\n",
    "Regression  | 1     | $SS_{REG}$ | $MS_{REG}=\\frac{SS_{REG}}{1}$      |\n",
    "Residual    | n-2   | $SS_{RES}$ | $MS_{RES}=\\frac{SS_{RES}}{n-2}$  | \n",
    "Total       | n-1   | $SS_{TOT}$ | $MS_{TOT}=\\frac{SS_{TOT}}{n-1}$    | \n",
    "\n",
    "Table 2: The ANOVA Table for simple linear regression \n",
    "\n",
    "The values in this table can be used to conduct formal hypothesis tests. \n",
    "\n",
    "### 13.7.3 Hypothesis testing using ANOVA\n",
    "\n",
    "ANOVA is used to test the null hypothesis that the simpler of the two nested models better fits the data. In simple linear regression, the simpler model is the null model, in which case:\n",
    "\n",
    "+ $H_0:$ The null model is a better fit\n",
    "+ $H_1:$ The simple linear regression model is a better fit\n",
    "\n",
    "To test the null hypothesis defined above, we use an $F$ statistic, defined as:\n",
    "\n",
    "$$F = \\frac{MS_{REG}}{MS_{RES}}$$\n",
    "\n",
    "This ratio measures how much more variation in $Y$ is explained by the model than would be expected by chance. If the model does not fit the data well, then we would expect this ratio to be equal to 1. The larger the value of $F$, the stronger the evidence that the complex model is a better fit. To obtain a $p$-value for a formal hypothesis test, $F$ can be compared to the $F_{1,(n-2)}$ distribution (where 1 and (n-2) are the relevant degrees of freedom for the mean squares).\n",
    "\n",
    "### 13.7.4 Similarity between F tests and t-tests in simple linear regression \n",
    "\n",
    "Since the null model is defined as: $Y=\\beta_0+\\epsilon$ and the simple linear regression model is defined as: $Y=\\beta_0+\\beta_1X+\\epsilon$. The hypotheses above can be rewritten as:\n",
    "\n",
    "+ $H_0: \\beta_1 = 0$ \n",
    "+ $H_1: \\beta_1 \\neq 0$ \n",
    "\n",
    "In other words, the $F$-test for a simple linear regression model is the same as the $t$-test of the null hypothesis that the slope parameter is equal to 0. Indeed, the two tests are equivalent with $F=t^2$. Consequently, it is not particularly common to use $F$-tests in the simple linear regression model, they are more useful for assessing more complex models with multiple covariates. \n",
    "\n",
    "### 13.7.5 Example using Model 1\n",
    "\n",
    "The $F$ test statistic and associated $p$-value are given in the summary output for linear models (shown below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Birth.Weight ~ Gestational.Days, data = data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-49.348 -11.065   0.218  10.101  57.704 \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)      -10.75414    8.53693   -1.26    0.208    \n",
       "Gestational.Days   0.46656    0.03054   15.28   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 16.74 on 1172 degrees of freedom\n",
       "Multiple R-squared:  0.1661,\tAdjusted R-squared:  0.1654 \n",
       "F-statistic: 233.4 on 1 and 1172 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F-test\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use ```anova()``` which outputs the ANOVA table as well as the relevant $F$ statistic and $p$-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A anova: 2 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Gestational.Days</th><td>   1</td><td> 65449.51</td><td>65449.5131</td><td>233.4293</td><td>3.395226e-48</td></tr>\n",
       "\t<tr><th scope=row>Residuals</th><td>1172</td><td>328608.34</td><td>  280.3825</td><td>      NA</td><td>          NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A anova: 2 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\\\\n",
       "  & <int> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tGestational.Days &    1 &  65449.51 & 65449.5131 & 233.4293 & 3.395226e-48\\\\\n",
       "\tResiduals & 1172 & 328608.34 &   280.3825 &       NA &           NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A anova: 2 × 5\n",
       "\n",
       "| <!--/--> | Df &lt;int&gt; | Sum Sq &lt;dbl&gt; | Mean Sq &lt;dbl&gt; | F value &lt;dbl&gt; | Pr(&gt;F) &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| Gestational.Days |    1 |  65449.51 | 65449.5131 | 233.4293 | 3.395226e-48 |\n",
       "| Residuals | 1172 | 328608.34 |   280.3825 |       NA |           NA |\n",
       "\n"
      ],
      "text/plain": [
       "                 Df   Sum Sq    Mean Sq    F value  Pr(>F)      \n",
       "Gestational.Days    1  65449.51 65449.5131 233.4293 3.395226e-48\n",
       "Residuals        1172 328608.34   280.3825       NA           NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F-test using anova()\n",
    "anova(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from both outputs above, the $F$-statistic is equal to 233.4 with $p$-value $<2.2\\times10^{-16}$. With such a small $p$-value there is strong evidence against the null and therefore we conclude that the model which includes gesational days is a better fit (i.e. we conclude that $\\beta_1 \\neq 0$). \n",
    "\n",
    "Notice that in the ```summmary()``` output, the $p$-values for the F-test and the t-test for gestational days are identical, and that $F=233.4=15.24^2=t^2$. \n",
    "\n",
    "\n",
    "## 13.8 ANOVA for multivariable linear regression\n",
    "\n",
    "In the context of multivariable linear regression, ANOVA can be used to test whether a more complex model is a better fit than the null model (**the Global F test**), or whether a more complex model is a better fit than a simpler model that includes a subset of the covariates in the complex model (**the partial F test**). Each test requires slight modifications to the ANOVA table defined above and we will discuss these in turn. \n",
    "\n",
    "### 13.8.1 The Global F test\n",
    "\n",
    "The general formulation of the ANOVA table (suitable for simple and multivariable linear regression models) is given in Table 3, where $p$ is the number of covariates in the model. \n",
    "\n",
    "Source      | d.f.      | SS         | Mean Square                        | \n",
    "------------|-----------|------------|------------------------------------|\n",
    "Regression  | $p$       | $SS_{REG}$ | $MS_{REG}=\\frac{SS_{REG}}{p}$      |\n",
    "Residual    | $n-(p+1)$ | $SS_{RES}$ | $MS_{RES}=\\frac{SS_{RES}}{n-p-1}$  | \n",
    "Total       | $n-p$     | $SS_{TOT}$ | $MS_{TOT}=\\frac{SS_{TOT}}{n-1}$    | \n",
    "\n",
    "Table 3: The ANOVA Table \n",
    "\n",
    "Note that this is equivalent to Table 2 when $p=1$. \n",
    "\n",
    "The Global F test tests the null hypothesis ($H_0$) that the null model is a better fit than the more complex model against the alternative hypothesis ($H_1$) that the complex model is a better fit. Or, equivalently:\n",
    "\n",
    "+ $H_0:$ All slope parameters in the complex model are equal to 0.  \n",
    "+ $H_1:$ At least one of the slope parameters in the complex model is not equal to 0. \n",
    "\n",
    "The appropriate $F$ statistic is the ratio $MS_{REG}/MS_{RES}$ (as defined in Table 3). Under the null hypothesis, $F$ follows an $F_{p,(n-(p+1))}$ distribution. \n",
    "\n",
    "*Example:* We can use ```summary()``` to conduct a global F test for Model 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Birth.Weight ~ Gestational.Days.Centered + Maternal.Height.Centered, \n",
       "    data = data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-53.829 -10.589   0.246  10.254  54.403 \n",
       "\n",
       "Coefficients:\n",
       "                           Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)               119.46252    0.47980 248.983  < 2e-16 ***\n",
       "Gestational.Days.Centered   0.45237    0.03006  15.051  < 2e-16 ***\n",
       "Maternal.Height.Centered    1.27598    0.19049   6.698 3.27e-11 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 16.44 on 1171 degrees of freedom\n",
       "Multiple R-squared:  0.1969,\tAdjusted R-squared:  0.1955 \n",
       "F-statistic: 143.5 on 2 and 1171 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ANOVA for Model 3 \n",
    "summary(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypotheses are defined as:\n",
    "\n",
    "+ $H_0$: the regression coefficients for both gestational days and mother's height are equal to 0.\n",
    "+ $H_1$: the regression coefficient for either gestational days or mother's height (or both) is not equal to 0. \n",
    "\n",
    "The $F$ statistic is 143.5 with a $p$-value $<2.2 \\times 10^{-11}$. Therefore, there is strong evidence against the null and we can conclude that at least one of the estimated regression coefficients is non-zero (i.e Model 3 is a better fit than the null model). \n",
    "\n",
    "## 13.9 The partial F-test \n",
    "\n",
    "The global $F$-test is a joint test of the statistical signficance of all the slope parameters in a linear regression model. On the other hand, the partial $F$-test compares the fit of a complex model (say Model A with $p$ predictors) with a simpler model (say Model B with $p-k$ predictors). \n",
    "\n",
    "The key to the partial $F$-test is the construction of an Analysis of Variance table that partitions the sum of squares explained by the complex model into that explained by the simple model and the extra sum of squares only explained by the complex model. Using the notation that $SS_{REG_A}$ denotes the sum of squares explained by the complex model, whilst $SS_{REG_B}$ denotes the sum of squares explained by the simpler model, the ANOVA table is as shown in Table 3. \n",
    "\n",
    "Source                     | d.f.      | SS                      | Mean Square                                      \n",
    "---------------------------|-----------|-------------------------|-------------------------------------------------\n",
    "Explained by Model B       | $p-k$     | $SS_{REG_B}$            | $MS_{REG_B}=\\frac{SS_{REG_B}}{p-k}$   \n",
    "Explained by Model A       | $p$       | $SS_{REG_A}$            | $MS_{REG_A}=\\frac{SS_{REG_A}}{p}$   \n",
    "Extra explained by Model A | $k$       | $SS_{REG_A}-SS_{REG_B}$ | $MS_{REG_X}=\\frac{(SS_{REG_A}-SS_{REG_B}}{k}$   \n",
    "Residual from Model A      | $n-(p_1)$ | $SS_{RES_A}$            | $MS_{RES}=\\frac{SS_{RES_A}}{n-(p+1)}$         \n",
    "Total                      | $(n-1)$   | $SS_{TOT}$              | $MS_{TOT}=\\frac{SS_{TOT}}{n-1}$                 \n",
    "\n",
    "Table 3: The ANOVA table comparing the fit of a model ( Model A) with $p$ predictors with that of one (Model B) with $(p-k)$ predictors\n",
    "\n",
    "The partial $F$-test tests the null hypothesis that all of the slope parameters included in Model A but omitted from Model B are equal to zero. The alternative hypothesis is that at least one of the additional parameters in Model A is not equal to 0. \n",
    "\n",
    "The appropriate test statistic ($F$) is the ratio of extra mean sum of squares in Model A to the mean residual sum of squares from Model A. Under the null hypothesis, this test statistic follows an $F$-distribution:\n",
    "\n",
    "$$\\text{Under } H_0: F = \\frac{MS_{REG_X}}{MS_{RES}} \\sim F_{k,(n-(p+1))}$$\n",
    "\n",
    "\n",
    "*Example*: We can use ```anova()``` to conduct a partial F-test to compare Models 1 and 3: \n",
    "\n",
    "+ $H_0:$ Model 1 is the better fit \n",
    "+ $H_0:$ Model 3 is the better fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Res.Df</th><th scope=col>RSS</th><th scope=col>Df</th><th scope=col>Sum of Sq</th><th scope=col>F</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1172        </td><td>328608.3    </td><td>NA          </td><td>      NA    </td><td>      NA    </td><td>          NA</td></tr>\n",
       "\t<tr><td>1171        </td><td>316482.2    </td><td> 1          </td><td>12126.13    </td><td>44.86728    </td><td>3.266475e-11</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " Res.Df & RSS & Df & Sum of Sq & F & Pr(>F)\\\\\n",
       "\\hline\n",
       "\t 1172         & 328608.3     & NA           &       NA     &       NA     &           NA\\\\\n",
       "\t 1171         & 316482.2     &  1           & 12126.13     & 44.86728     & 3.266475e-11\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Res.Df | RSS | Df | Sum of Sq | F | Pr(>F) |\n",
       "|---|---|---|---|---|---|\n",
       "| 1172         | 328608.3     | NA           |       NA     |       NA     |           NA |\n",
       "| 1171         | 316482.2     |  1           | 12126.13     | 44.86728     | 3.266475e-11 |\n",
       "\n"
      ],
      "text/plain": [
       "  Res.Df RSS      Df Sum of Sq F        Pr(>F)      \n",
       "1 1172   328608.3 NA       NA        NA           NA\n",
       "2 1171   316482.2  1 12126.13  44.86728 3.266475e-11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anova(model1, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $F$-statistic is 44.87 with a $p$-value of $3.23\\times 10^{-11}$. This is strong evidence against the null, and hence the data indicates that Model 3 is the better fit. \n",
    "\n",
    "In this case, the two models only differed by one variable (mother's height) and so the hypotheses could be re-written as: \n",
    "\n",
    "+ $H_0: \\beta_2=0$, where $\\beta_2$ is the regression coefficient for mother's height. \n",
    "+ $H_0: \\beta_2 \\neq 0$. \n",
    "\n",
    "In other words, when the models only differ by one variable, the partial F test is equivalent to the t test of the null hypothesis that the regression coefficient for that variable is equal to 0. Notice in our example that the results of the partial F test are the same as the t-test for $\\beta_2=0$, with $F=t^2$. \n",
    "\n",
    "For this reason, partial F tests are more useful in situations where we wish to compare models that differ by more than one variable. \n",
    "\n",
    "\n",
    "## 13.10 ANOVA for models with categorical independent variables.\n",
    "\n",
    "Another useful application of ANOVA is to test for differences in means between categories of a categorical variable. \n",
    "\n",
    "Suppose we are interested in the association between an outcome $Y$ and a categorical variable $X$ with $K$ groups. We have already seen how to define a multivariable linear regression model using dummy variables for this situation. An alternative model, often termed the **ANOVA model**, is as follows: \n",
    "\n",
    "Let $y_{ki}$ be the value of the outcome for the $i^{th}$ observation in the $k^{th}$ group ($i=1,...,n_k$ and $k=1,...,K$). The ANOVA model is then defined as:\n",
    "\n",
    "$$y_{ki}=\\mu_k + \\epsilon_{ki} \\text{, where } \\epsilon_{ki} \\sim NID(0,\\sigma^2)$$\n",
    "\n",
    "Here, $\\mu_k$ is the mean of the outcome in the $k^{th}$ group. With this representation, the null and alternative hypothesis are: \n",
    "\n",
    "+ $H_0: \\mu_k= \\mu$ (i.e. the means in all groups defined by the categorical variables are equal to a common value). \n",
    "+ $H_1: \\mu_k \\neq \\mu$ (i.e. the group means are not all equal). \n",
    "\n",
    "### 13.10.1 Sum of squares for models with categorical variables\n",
    "\n",
    "For models with a single independent categorical variable the fitted values are simply the group means ($\\bar{y_k}$). Under the null hypothesis that the group means are all equal, the fitted values are all equal to the overall mean ($\\bar{y}$). This leads to new terminology for the residual sum of squares ($SS_{RES}$) and the sum of squares explained by the model ($SS_{REG}$):\n",
    "\n",
    "+ $SS_{RES} = \\sum_{k=1}^K \\sum_{i=1}^{n_k} (y_{ki} - \\bar{y}_k)^2$\n",
    "+ $SS_{REG} = \\sum_{k=1}^K \\sum_{i=1}^{n_k} (\\bar{y}_k - \\bar{y})^2 = \\sum_{k=1}^K n_k (\\bar{y}_k - \\bar{y})^2 $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this case, the residual sum of squares is often termed the **within group sum of squares $(SS_{Within})$** and the regression sum of squares is often termed the **between group sum of squares $(SS_{Between})$**.\n",
    "\n",
    "### 13.10.2 The ANOVA table\n",
    "\n",
    "When there are $K$ groups, the degrees of freedom for the within groups sum of squares is $n-K$ (because the model includes $K$ parameters) and the degrees of freedom for the between groups sum of squares is $K-1$ (because the null model contains a single parameter, the overall mean). Hence the ANOVA table is as follows: \n",
    "\n",
    "Source          | d.f.      | SS             | Mean Square                               | \n",
    "----------------|-----------|----------------|-------------------------------------------|\n",
    "Between groups  | $K-1$     | $SS_{Between}$ | $MS_{Between}=\\frac{SS_{Between}}{(K-1)}$ |\n",
    "WIthin Groups   | $n-K$     | $SS_{Within}$  | $MS_{Within}=\\frac{SS_{RES}}{n-K}$        | \n",
    "Total           | $n-1$     | $SS_{TOT}$     | $MS_{TOT}=\\frac{SS_{TOT}}{n-1}$           | \n",
    "\n",
    "Table 4: The ANOVA Table for models with categorical variables\n",
    "\n",
    "\n",
    "### 13.10.3 The F-test\n",
    "\n",
    "To test the null hypothesis that the means in all groups are equal to a common value, the appropriate $F$-statistic is:\n",
    "\n",
    "$$F = \\frac{MS_{Between}}{MS_{Within}} \\sim F_{(K-1), n-K} \\text{ under } H_0$$. \n",
    "\n",
    "If this test obtains a small $p$-value, then we have evidence that the means in the groups are not all the same. However, it does not tell us which of the group means differed from which other group means. For this reason, if we do find evidence of difference in means on an $F$-test, we may want to follow up with further analysis. Such further analysis may include pair-wise comparisons of means through analysis restricted to two groups. \n",
    "\n",
    "*Example* We conduct an F-test to compare the average birthweights between babies whose mothers smoke and whose mothers don't smoke using the birthweight data. \n",
    "\n",
    "Let $\\mu_1$ and $\\mu_0$ denote the mean birthweight for babies whose mothers do smoke and don't smoke, respectively. Then, the relevant hypotheses are: \n",
    "\n",
    "+ $H_0: \\mu_1= \\mu_0$ (i.e. the birthweight of a baby does not depend on whether the mother smoked)\n",
    "+ $H_1: \\mu_1\\neq \\mu_0$\n",
    "\n",
    "Recall that we previously defined Model 2 to related birthweight and mother's smoking status: \n",
    "\n",
    "$$ y_i = \\alpha_0 + \\alpha_1 s_i + \\epsilon_i$$\n",
    "\n",
    "Where $Y$ denotes the birthweight and\n",
    "\n",
    "$$ s_{i}\n",
    "\\begin{cases}\n",
    "    1 & \\text{ if the mother smokes} \\\\\n",
    "    0 & \\text{ if the mother does not smoke}\n",
    "\\end{cases} $$\n",
    "\n",
    "We can rewrite this equation using the ANOVA model as follows: \n",
    "$$\n",
    "\\begin{align}\n",
    "    y_{1i} &=\\mu_1 + \\epsilon_{1i}  \\text{ if the mother smokes} \\\\\n",
    "    y_{0i} &=\\mu_0 + \\epsilon_{0i}  \\text{ if the mother does not smoke}\n",
    "\\end{align} $$\n",
    "Where $y_{ki}$ is the mean birthweight in the $k^{th}$ group (groups are defined by mother's smoking status), $\\mu_1 = \\beta_0 + \\beta_1$ and $\\mu_0=\\beta_0$ (in other words, our null hypothesis can be rewritten as: $\\beta_1=0$). \n",
    "\n",
    "We can use either ```anova()``` or ```summary()``` to conduct the test in R: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>factor(Maternal.Smoker)</th><td>   1        </td><td> 24002.06   </td><td>24002.0638  </td><td>76.0167     </td><td>9.461068e-18</td></tr>\n",
       "\t<tr><th scope=row>Residuals</th><td>1172        </td><td>370055.79   </td><td>  315.7473  </td><td>     NA     </td><td>          NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\\\\n",
       "\\hline\n",
       "\tfactor(Maternal.Smoker) &    1         &  24002.06    & 24002.0638   & 76.0167      & 9.461068e-18\\\\\n",
       "\tResiduals & 1172         & 370055.79    &   315.7473   &      NA      &           NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Df | Sum Sq | Mean Sq | F value | Pr(>F) |\n",
       "|---|---|---|---|---|---|\n",
       "| factor(Maternal.Smoker) |    1         |  24002.06    | 24002.0638   | 76.0167      | 9.461068e-18 |\n",
       "| Residuals | 1172         | 370055.79    |   315.7473   |      NA      |           NA |\n",
       "\n"
      ],
      "text/plain": [
       "                        Df   Sum Sq    Mean Sq    F value Pr(>F)      \n",
       "factor(Maternal.Smoker)    1  24002.06 24002.0638 76.0167 9.461068e-18\n",
       "Residuals               1172 370055.79   315.7473      NA           NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Birth.Weight ~ factor(Maternal.Smoker), data = data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-68.085 -11.085   0.915  11.181  52.915 \n",
       "\n",
       "Coefficients:\n",
       "                            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)                 123.0853     0.6645 185.221   <2e-16 ***\n",
       "factor(Maternal.Smoker)True  -9.2661     1.0628  -8.719   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 17.77 on 1172 degrees of freedom\n",
       "Multiple R-squared:  0.06091,\tAdjusted R-squared:  0.06011 \n",
       "F-statistic: 76.02 on 1 and 1172 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anova(model2)\n",
    "summary(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ANOVA table, the ```factor(Maternal.Smoker)``` row gives the between groups results and the ```Residuals``` row gives the within group results. \n",
    "\n",
    "The $F$-statistic is equal to 76.02 with a $p$-value equal to $9.46\\times10^{-18}$. This evidence suggests that there is a difference in the mean birthweight between the two groups defined by mother's smoking status. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.11 Proofs\n",
    "\n",
    "### 13.11.1 Proof for the ordinary least squares estimates in simple linear regression\n",
    "\n",
    "Recall the ordinary least square estimates of the intercept ($\\beta_0$) and slope ($\\beta_1$) in simple linear regression are:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_0}\\bar{x} \\\\\n",
    "\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^2 (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x_i})^2}\n",
    "\\end{align}\n",
    "\n",
    "Proof:\n",
    "\n",
    "To solve for the value of $\\beta_0$ that minimises $SS_{RES}$, we differentiate $SS_{RES}$ with respect to $\\hat{\\beta}_0$ and set the differential to zero:\n",
    "\n",
    "$$ \\frac{d(SS_{RES})}{d(\\hat{\\beta}_0)} = \\sum_{i=1}^n -2(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_i)=0$$\n",
    "\n",
    "Since $\\sum_{i=1}^n (y_i)=n\\bar{y}$ and $\\sum_{i=1}^n(x_i)=n\\bar{x}$, we can simplify to:\n",
    "\n",
    "$$-n\\bar{y}+n\\hat{\\beta}_0+n\\hat{\\beta}_1\\bar{x}=0$$\n",
    "\n",
    "Rearranging the above and divide by $n$ to give:\n",
    "\n",
    "$$\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}$$. \n",
    "\n",
    "To solve for the value of $\\beta_1$ that minimises $SS_{RES}$, we have to differentiate with respect to $\\hat{\\beta}_1$. First, we substitute in our solution for $\\hat{\\beta}_0$ as follows: \n",
    "\n",
    "$$SS_{RES}=\\sum_{i=1}^n(y_i-(\\bar{y}-\\hat{\\beta}_1\\bar{x})-\\hat{\\beta}_1x_i)^2=\\sum_{i=1}^n ((y_i-\\bar{y})-\\hat{\\beta}_1(x_i-\\bar{x}))^2$$\n",
    " \n",
    "Now differentiating the above with respect to $\\hat{\\beta}_1$ and setting the differential to zero gives: \n",
    "\n",
    "$$\\frac{d(SS_{RES})}{d(\\hat{\\beta}_1)} = \\sum_{i=1}^n -2(x_i-\\bar{x})(y_i-\\bar{y})+2\\hat{\\beta}_1(x_i-\\bar{x})^2=0$$\n",
    "\n",
    "Rearranging gives:\n",
    "\n",
    "$$\\hat{\\beta}_1\\sum_{i=1}^n (x_i-\\bar{x})^2 = \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})$$\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n",
    "\n",
    "### 13.11.2 Proof that the OLS estimates are also the maximum likelihood estimates\n",
    "\n",
    "If $Y_i \\sim NID(\\mu, \\sigma^2)$, the log likelihood function is:\n",
    "\n",
    "$$l(\\mu | y_1,...,y_n) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\mu)^2$$\n",
    "\n",
    "So, for the simple linear regression model:\n",
    "\n",
    "$$l(\\beta_0, \\beta_1 | y_1,...,y_n) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\beta_1x_i-\\beta_0)^2$$. \n",
    "\n",
    "Therefore, for any fixed positive value for $\\sigma^2$, maximising the log likelihood function is equivalent to minimising $SS_{RES}$ and so the OLS estimates are also maximum likelihood estimates of $\\beta_0$ and $\\beta_1$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
